{"meta":{"title":"Astilt's blog","subtitle":"","description":"","author":"Astilt","url":"http://yoursite.com","root":"/"},"pages":[{"title":"categories","date":"2020-08-30T08:56:16.000Z","updated":"2022-11-22T01:27:54.238Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-08-30T08:57:26.000Z","updated":"2022-11-22T01:27:54.240Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"hive分区表","slug":"hive分区表","date":"2024-03-05T08:41:18.000Z","updated":"2024-03-05T10:19:20.483Z","comments":true,"path":"2024/03/05/hive分区表/","link":"","permalink":"http://yoursite.com/2024/03/05/hive%E5%88%86%E5%8C%BA%E8%A1%A8/","excerpt":"","text":"123456789101112131415161718-- -- 创建表-- CREATE TABLE astilt_par (-- id INT,-- name STRING,-- age INT-- )-- PARTITIONED BY (country STRING, city STRING) row format delimited fields terminated by &#x27;\\t&#x27;;-- 插入数据（静态分区）-- INSERT INTO TABLE astilt_par PARTITION (country=&#x27;USA&#x27;, city=&#x27;New York&#x27;) VALUES (1, &#x27;John&#x27;, 25);-- INSERT INTO TABLE astilt_par PARTITION (country=&#x27;USA&#x27;, city=&#x27;San Francisco&#x27;) VALUES (2, &#x27;Jane&#x27;, 30);-- INSERT INTO TABLE astilt_par PARTITION (country=&#x27;Canada&#x27;, city=&#x27;Toronto&#x27;) VALUES (3, &#x27;Bob&#x27;, 28);-- 删除指定分区-- ALTER TABLE astilt_par DROP IF EXISTS PARTITION (country=&#x27;USA&#x27;, city=&#x27;New York&#x27;);-- 查询数据-- select * from astilt_par;","categories":[],"tags":[]},{"title":"python虚拟环境","slug":"python虚拟环境","date":"2024-02-02T03:04:39.000Z","updated":"2024-02-28T07:03:53.303Z","comments":true,"path":"2024/02/02/python虚拟环境/","link":"","permalink":"http://yoursite.com/2024/02/02/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/","excerpt":"","text":"参考：https://sspai.com/post/75978 最开始使用 Python 时，相信很多读者都会安装一个全局的（Global）Python 解释器，然后就用这唯一的解释器去学习、并运行 Python 代码。 由于 Python 有着丰富的社区生态，在使用 Python 的过程中我们不可避免地会去安装别人写好的模块、库或包，而它们都可以被统称为依赖（Dependency）。依赖一词通俗易懂，它就好比是深陷于恋爱泥潭之中无法自拔的小情侣，如果离开了另一方就活不下去的那种状态。在我们的 Python 代码中如果缺少了这些第三方模块、库或包也同样无法正常运行。 为了安装依赖，我们也因此会接触到一个名为 pip 的安装与管理工具，并且不论是在正文还是在综合案例实践中，我们都会经常用到该工具。只有通过 pip 工具安装之后我们才能在代码中导入并使用第三方内容。pip 工具通常会随着 Python 解释器一起被安装，并且与对应的解释器版本相绑定。 但对于 Python 工程师来说，在工作时通常会接触到不同的工程项目（Project），而每个项目可能会使用到不同的 Python 版本，比如老旧的项目 A 使用 Python 2.7，最新的项目使用 Python 3.9，而不旧不新的已经上线的项目一般都使用 Python 3.7 等等。不同的 Python 版本具有不同的功能特性，相关依赖对这些版本的兼容情况又有所不同，所以我们无法直接用一个最新的版本来充当「万金油」。 在这种场景之下，只有一个唯一的全局解释器仅仅只能满足其中一个项目的需要。所以就需要一种机制可以让我们随时创建或删除不同的 Python 解释器，于是虚拟环境（Virtual environment）也就应运而生。 使用虚拟环境的好处在于： 一方面，可以帮我们快速创建干净、完全隔离的且不同版本的 Python 解释器以便我们能在不同项目中开发； 另一方面，可以避免因在不同项目中只使用唯一的全局解释器而导致的依赖冲突问题（Dependency Conflicts）。 依赖冲突是一个情况复杂的问题，不仅存在于 Python 中，在其他编程语言中也它也「声名远扬」，这里笔者以一个简单的例子说明： 假设我们现在有两个工程项目 PA 与 PB，它们分别使用到了 Django 2.0 和 Django 3.1（假设为最新版本）依赖。 在不使用虚拟环境而仅使用一个全局的 Python 3.6 解释器的情况，仅能存在一个 Django 版本： 在 PA 项目中使用 pip install django==2.0 只会令当前解释器安装 Django 2.0 版本； 倘若此时在 PB 项目中使用 pip install django 不指定版本号时，默认安装最新的 Django 3.1 版本，于是此时全局解释器中已经存在的 Django 2.0，则会为 Django 3.1 版本所覆盖并升级。 此时问题来了：如果我们要让 PA 项目能正常运行，就必须使用 Django 2.0，如果要令 PB 项目正常运行，那么就必须使用 Django 3.1。 而这个问题就是典型的的依赖冲突。 所以，为了避免这种依赖冲突的情况发生，在开发或运行项目前，通常会使用虚拟环境来创建一个纯净的 Python 运行环境，当中仅仅包含了 Python 解释器和必要的 pip 工具而没有任何第三方依赖。这个环境就类似于我们常说的「沙盒」（SandBox），在盒子内的东西既不会影响外部的事物，也不会被外部事物所影响。 于是乎为了解决上面所提到的依赖冲突问题，我们只需要分别新建一个包含了 Django 2.0 和包含了 Django 3.1 的虚拟环境，然后运行项目代码时切换到对应的虚拟环境即可。 这也就是为什么我们需要一个虚拟环境，因为只有统一的开发环境，才能确保程序在每个能拿到这份代码的协作者手中都是能跑起来的。就好比在中学时一个班里的同学水平有高有低，但都是要在同一个考场考试、做的都是同一套试卷一样。 尽管人们常说 Python 是门规范很宽松的语言，因此易于学习与使用，但也正是因为这样的「放任」才会出现了四分五裂的 Python 环境解决方案： 破局：Python 中那些创建虚拟环境与解决依赖冲突的方案 经过前一节的介绍，相信读者已经对「为什么需要有虚拟环境」这一问题有了基本的了解。 由于 Python 是门规范较为宽松的编程语言，而 Python 官方在早期对于虚拟环境或依赖管理并没有一个统一的解决方案，因此也就在民间出现了各种「非官方实现的」解决手段。 在 Python 社区生态里，我们可以有多种方式去创建一个干净虚拟环境，本小节笔者主要会简单介绍四个： venv virtualenv conda poetry venv venv 是 Python 官方在 Python 3.3 版本内置的一个标准库模块，其主要功能移植自 Virtualenv（在下一小节中展开），让我们可以快速生成一个虚拟环境。现在我们只要下载的 Python 解释器版本在 3.3 及以上就能直接使用，而无需安装其他依赖。 使用它的方式十分简单，只需要通过使用对应版本的 Python 解释器并运行以下命令即可，比如这里我们以 Python 3.10 版本解释器为例： 1$ python3.10 -m venv &lt;your-virtual-environment-name&gt; 通常我们在创建虚拟环境时需，要简单指定一个名称用以存放虚拟环境的内容，所以上述代码中的名称通常就保持和 venv 模块名称一致： 1$ python3.10 -m venv venv 通过对应版本的 Python 解释器调用 venv 模块得到的虚拟环境，其版本也与解释器保持一致，也即在创建的 venv 目录中期 Python 解释器也为 3.10。并且在不指定目录的情况下，默认会在当前运行命令的目录中创建虚拟环境。 目前像 Pycharm 这样的 IDE 或 VS Code 类似的文本编辑器（需要安装插件）已经会正确识别到 venv 目录，并在运行代码时自动帮我们切换到虚拟环境中。 但如果我们是在终端上手动切换虚拟环境，那么就需要自己进行手动激活（Active）。由于 venv 模块创建的虚拟环境目录在 macOS 和 Windows 内容有所不同，所以就导致二者在激活虚拟环境时的命令也有所差异。 在 macOS 中，激活命令 active 会存放在同名的 venv 目录下的 bin/ 目录中。但在使用前需要如下图所示，通过 Unix 中的 chmod +x 命令赋予激活命令执行权限。如果读者有像笔者一样事先对终端命令行界面进行了美化相关的设置，那么激活之后就能看到如图所示的环境名前缀（或后缀）提示符，这就表示我们已经成功切换到虚拟环境中； 而在 Windows 系统 bin/ 目录则为 Scripts/ 目录，操作上同理。 后续我们陆续介绍到的虚拟环境管理工具，除了 IDE 帮我们自动切换之外，大部分时候如果是在终端运行，那么就需要自己通过对应的激活命令手动切换。 当然，如果我们想要退出虚拟环境，要么直接关闭 IDE，要么直接在终端命令行界面输入 deactivate 命令即可（或是同样关闭终端）。 不过使用 venv 模块存在一个令人头疼的问题：如果我们有多个项目，那么就会存在多个由 venv 创建的虚拟环境，因此我们无法做到统一集中管理。 所以，Python 社区的里的其他方案也意识到了这一点，也都具备了统一在一个命令中管理虚拟环境的功能。 Virtualenv Virtualenv 是 Python 社区一款老牌、成熟的虚拟环境管理工具，经过多个版本迭代也具备丰富的功能。并且自从 Python 3.3 版本开始，它的部分功能已也被集成到了 venv 标准库中，足见其对于 Python 虚拟环境管理工作贡献的份量如何。 从某些程度上来说，Virtualenv 和 venv 的功能十分类似，但 Virtualenv 在其官方文档中也指出了 venv 的不足之处： is slower（创建速度慢） is not as extendable（可扩展性差） cannot create virtual environments for arbitrarily installed python versions（无法创建任意 Python 版本的虚拟环境） is not upgrade-able via pip（无法通过 pip 进行升级） does not have as rich programmatic API（没有丰富的 API 编程方法扩展） 而这些不足之处在 Virtualenv 里都有了比较完善的解决方案。 不过由于 virtualenv 本质上属于一个第三方工具，因此我们要使用 virtualenv 首先就得通过 pip 命令安装它。这里的 pip 命令存在于在全局解释器中： 12# 可能因为网络问题报错，需要进行终端代理 eg: export http_proxy=http://127.0.0.1:7890$ pip install virtualenv --user 安装成功后，它的用法也和 venv 十分类似，例如： 12# 注意需要virtualenv&lt;.exe&gt;加入环境变量$ virtualenv myenv 上述命名会在当前路径中多出一个名为 myenv 的 Python 虚拟环境，使用它时我们同样可以像使用 venv 模块那样，通过同样的命令去激活环境。 不过这样本质上和我们用 venv 没有太大的区别。由于 Virtualenv 提供了可供扩展的 API 接口，因此我们通常还需要借助第三方的 Virtualenv 扩展，来帮助我们完成统一管理环境的操作，而 Virtualenvwrapper 就是这样一个能满足我们需求的第三方扩展库。 见名知意，Virtualenvwrapper 实际是对 Virtualenv 进行包装（Wrap）的扩展库，我们在使用时一样需要通过 pip 工具进行安装安装。完成之后需要简单配置一下环境变量才能继续使用，以下是来自于官方的示例： 123456$ pip install virtualenvwrapper...$ export WORKON_HOME=~/Envs$ mkdir -p $WORKON_HOME$ source /usr/local/bin/virtualenvwrapper.sh$ mkvirtualenv env1 上述的 WORKON_HOME 变量即指定的是统一存放虚拟环境的文件夹路径，创建之后再调用 Virtualenvwrapper 提供的初始化命令进行自动配置，之后就通过 mkvirtualenv 来创建相应的虚拟环境。 如果我们需要激活环境，则是需要通过 Virtualenvwrapper 帮我们设置好的 worken 命令来激活并切换： 1$ workon env1 虽然 Virtualenvwrapper 是对 Virtualenv 的包装，但它并没有直接使用 Virtualenv 的命令，而是额外构造并提供了一批与虚拟环境相关的命令，读者在使用时最好需要参考其文档。 需要注意的是，Virtualenvwrapper 主要面向的是 macOS 和 Linux 操作系统，而如果我们想要在 Windows 上使用这一工具，那就必须要再安装一个由其他开发者二次封装的 virtualenvwrapepr-win，但该库也仅支持在 Windows 上的 CMD 终端上使用，而不支持在 Powershell 上使用。 Conda 相信有相当一大部分比例的已经事先学习过 Python 的新手，又或是如果从事数据分析、机器学习等数据科学相关工作的人，在学习或使用过程会经常接触到一个名为 Anaconda 的 Python 发行版本。在 Anaconda 中不仅内置了一个 Python 解释器，同时还内置了许多常用的数据科学软件包或工具。 但 Anaconda 为人所诟病的地方也在于它内置了太多东西，其中的大多数又用不到，导致最终体积较大，无异于让配置本就不富裕的电脑雪上加霜。当然 Anaconda 还有另外一个精简版 Miniconda，它只包含了少量的一些依赖库或包，有效减轻了电脑磁盘的负担。 但不论是使用 Anaconda 还是 Miniconda，我们都会进一步用到它们当中的核心工具——Conda，一个跨平台的包（或库）和虚拟环境管理系统，它能轻易地帮我们构建程序运行所需的环境、依赖、更新等步骤。 Conda 最大的一个优势就是它除了能安装 Python 依赖库或包之外，还能安装其他语言的一些依赖（比如 R 语言）；同时像 Tensorflow、Pytorch 这种业内常见的深度学习框架，往往会存在由 C/C++ 语言编写的部分，这些部分在安装时需要预先编译，而使用 Conda 安装时会自动连同已经事先编译好的二进制部分一起安装到虚拟环境中，避免了因操作系统不同而导致的编译问题。 不像 pip 或 virtualenv 这样的工具我们能够单独安装，要使用 Conda 我们通常会捆绑使用 Anaconda 或者 Miniconda，读者可以根据自己电脑的配置情况选择其一，具体的安装步骤可以直接参考官方文档，上面有着详细的操作过程。 安装完成之后，当我们在终端命令行中输入如下命令，并能获取到帮助信息就说明安装成功： 1$ conda --help Conda 命令行工具集成了众多的功能，以至于我们可以用 Conda 命令行工具来完成大部分操作，读者可以参考 Conda 官方的 Command reference 来获取有关命令的详细说明。 比如使用 Conda 创建虚拟环境： 1$ conda create --name myenv python=3.9 然后激活虚拟环境时只需要通过同样的 active 命令来操作即可： 1$ conda active myenv 当然我们也可以将 Conda 作为 pip 工具来安装依赖： 1$ conda install pandas seaborn 不过需要注意的是，尽管我们是在 Anaconda 或 Miniconda 中用到 Conda，但 Python 解释器本身就会自带 pip 工具。因此我们在使用 Conda 的过程中，切勿既通过 conda install 来安装依赖，又通过 pip 工具来安装依赖。 因为这就好比我们在驾驶时只用右脚来控制刹车和油门，而左脚要么控制离合器要么什么也不做一样，如果混用则极易引发事故。 这个道理也同样适用于 Conda，倘若我们将 conda 命令与 pip 命令作为安装依赖混用，那么 有一定几率产生的依赖冲突的 BUG。除此之外，通过 Conda 安装的版本有时候会比 pip 安装的版本会落后一些，并且 Conda 在安装前会检查环境依赖或更新情况，因此速度会相对较慢（不知道现在有没有改善）。 所以如果读者有打算使用 Conda 的想法，那么笔者强烈建议读者只将其用来创建、管理虚拟环境，而不是用来安装第三方依赖库或包。具体来说，就是通过 Conda 创建一个用于编写代码或程序的虚拟环境，然后再通过虚拟环境中的 pip 工具安装需要使用到的第三方依赖库或包即可。 使用 Conda 创建特定 Python 版本的虚拟环境时，Conda 会从默认的服务器上下载相关资源并构建。但由于 Conda 的服务器位于国外，在国内访问时会受到网络限制，这也进一步而影响到 Conda 的更新功能，因此在使用 Conda 之前我们最好像配置 pip 工具一样为 Conda 添加镜像源。 但好在清华大学开源软件镜像站包含 Conda 的镜像源，我们只需要按照 《Anaconda 镜像使用帮助》 文档中的内容进行配置即可，本节就不过多赘述。 Poetry Poetry 是一个面向未来的 Python 依赖管理和打包工具（笔者按：这里的「包」是指能够被其他人作为依赖安装的安装包，而非单独的应用程序），它支持了 Python 社区的 PEP 518 提案里的 pyproject.toml 新标准文件，用以管理我们与 Python 项目有关的内容（比如我们在 Click 实践案例中见到的 setup.py 打包文件）。 有了 pyproject.toml 文件之后我们再也不需要在 Python 项目中包含额外的说明、许可证文件、依赖文件 requirements.txt 等，统统由这个文件统一管理。 Poetry 就像 Go 语言的 Go mod、Rust 语言的 Cargo 工具一样，都是顺应着未来依赖管理更容易、开发更简单的趋势。目前已经可以看到有许多知名的 Python 开源项目都逐渐开始通过这一工具来管理，并且也都开始采用 pyproject.toml 文件。 虽然 Poetry 主要用于管理依赖和打包 Python 项目，但我们在使用 Poetry 时它也会自动创建一个包含了特定版本的 Python 解释器的虚拟环境，所以也可以算作是一个虚拟环境管理工具。 当然我们在使用 Poetry 时依旧需要通过 pip 命令对其进行安装： 1234$ pip install poetry# macOS$ pip3 install poetry 使用 Poetry 步骤其实也和笔者前面所介绍的其他虚拟环境管理工具类似，即在使用时需要像 Conda 那样通过命令行来操作： 1$ poetry &lt;command&gt; ... 使用 Poetry 的方式很简单，对于我们个人来说通常都是几步： 通过 Poetry 初始化项目。这里的初始化包括新建项目并初始化和对已创建项目初始化两种情况。 根据实际情况选择对应版本的 Python 解释器。默认情况下跟 Poetry 所使用的解释器版本相一致。 添加必要依赖（Required Dependencies）或开发依赖。 通过 Poetry 运行或启动项目和我们的代码。 初始化项目 如果我们已经存在了项目文件夹，那么就可以再使用 poetry init 来为已存在的项目生成 pyproject.toml 文件。当然我们也可以使用 poetry new 来直接创建一个项目，不过得到的结构文件需要自己手动调整以符合需要： 123$ mkdir myproject$ cd myproject$ poetry init 之后我们在 myproject 这个项目文件夹下，就可以看到 pyproject.toml 文件，其内容如下： 1234567891011121314[tool.poetry]name &#x3D; &quot;myproject&quot;version &#x3D; &quot;0.1.0&quot;description &#x3D; &quot;&quot;authors &#x3D; [&quot;None&quot;][tool.poetry.dependencies]python &#x3D; &quot;^3.9&quot;[tool.poetry.dev-dependencies][build-system]requires &#x3D; [&quot;poetry-core&gt;&#x3D;1.0.0&quot;]build-backend &#x3D; &quot;poetry.core.masonry.api&quot; 通过 poetry 初始化生成的 pyproject.toml 文件内容并不多，它会在我们后续添加相关依赖的时候被不断丰富。 所以一般情况下我们也不需要手动修改它，因为我们通过 poetry 命令行工具进行某些操作时，Poetry 会帮我们自动增删当中的内容。 需要注意的是：因为 Poetry 的大多数操作都是围绕 pyproject.toml 进行展开的，因此一定要在包含该文件的目录环境中使用 Poetry 操作，否则会报错。 选择相应版本的 Python 解释器 初始化项目之后 Poetry 还并未为我们构建一个 Python 的虚拟环境。因为它会在我们添加依赖的时候默认创建，而这个虚拟环境的 Python 解释器版本和调用 Poetry 时所用到的 Python 释器版本一致，因此通常我们并不需要额外指定。 但如果你有特殊需要，需要 Poetry 为项目构建一个使用指定版本 Python 解释器的虚拟环境，那么你可以使用 poetry env use &lt;special-version-python-interpreter-path&gt; 来进行构建。其中 use 后面输入的是指定版本的 Python 解释器路径。 但因为 pyproject.toml 中 tool.poetry.dependencies 部分指明了 python=&quot;^3.9&quot; 参数，即该项目使用的 Python 解释器最低版本为 3.9，因此我们如果指定的特定版本不在兼容范围内，那就需要我们手动修改。这里直接使用一般的文本编辑器修改完成后保存即可。 比如我们目前需要使用系统里存在的 Python 3.7 解释器，那么所以需要将 python=&quot;^3.9&quot; 改成 python=&quot;^3.7&quot; 然后再指定解释器路径即可。 ^符号是一个特殊的版本限定符号，在 Poetry 官方文档的这一节内容中可以找到关于这个及其他同类符号的所有说明。 在 Windows 下： 1PS C:\\Users\\Linxiaoyue\\Desktop\\myproject&gt; poetry env use &#x27;D:\\Program Files\\Python\\python37\\python.exe&#x27; 在 macOS 或 Linux 下： 1$ poetry env use /usr/local/python/python37/bin/python 指定完成并运行命令之后，我们就可以在终端命令行界面上看到 Poetry 帮我们创建了一个虚拟环境及其存放的路径。 管理依赖 如果我们没有指定特定版本的 Python 解释器，那么直到我们添加依赖时 Poetry 才会帮我们创建虚拟环境并将依赖添加到 pyproject.toml 文件中。 使用 Poetry 管理依赖比较简单，而且有点类似于 JavaScripts 的包管理工具 Yarn；但通常情况下我们要做的也就只有两个操作：添加和移除。 配置镜像路径 和使用 pip 工具以及 Conda 类似，在安装依赖之前，我们还是需要简单地配置一下 Poetry 安装依赖的镜像地址，避免我们下载依赖时走的是国外镜像仓库的网络，以加快安装过程。 但默认情况下，如果我们已经有配置了 pip 的镜像，那么 Poetry 则会直接使用该镜像来加速依赖下载过程，这一步配置镜像的过程可以跳过；反之，我们需要在 Poetry 的全局配置中进行设置，配置文件的具体路径如下： macOS/Linux：~/Library/Application Support/pypoetry Windows：C:\\Users\\&lt;username&gt;\\AppData\\Roaming\\pypoetry 进入到对应的文件夹后，就会看到一个名为 config.toml 的文件，此时我们可以用电脑中的编辑器打开它（推荐 VS Code 之类比较现代的编辑器），之后在当中我们可以选择相应的镜像源，这里我推荐使用豆瓣和清华的镜像源，任选其一即可，编辑完成后保存即可。 12repositories.douban = &quot;https://pypi.doubanio.com/simple&quot; # 豆瓣镜像源repositories.tuna = &quot;https://pypi.tuna.tsinghua.edu.cn/simple&quot; # 清华镜像源 关于 Poetry 配置文件的内容读者可以查阅 Configuration 一节文档。 添加依赖：add 配置好镜像路径之后，我们就可以直接使用 poetry add &lt;package-name&gt; 就可以安装依赖： 1poetry add requests 但默认情况下，使用 poetry add 添加的是项目的必要依赖，也即运行项目必须要安装的依赖。 如果在开发的时候我们不想导出某些在开发环境中使用的依赖，比如（Pytest 之类的第三方测试库），那么在添加依赖时需要使用 Poetry 为我们提供的开发环境依赖（Development Environment Denpendencies）选项 -D： 1poetry add -D pytest 安装成功后我们打开 pyproject.toml 文件中的 tool.poetry.dev-dependencies 就会多出一行 pytest=&quot;^6.2.3&quot; 12345678910111213141516[tool.poetry]name &#x3D; &quot;myproject&quot;version &#x3D; &quot;0.1.0&quot;description &#x3D; &quot;&quot;authors &#x3D; [&quot;None&quot;][tool.poetry.dependencies]python &#x3D; &quot;^3.7&quot;requests &#x3D; &quot;^2.25.1&quot;[tool.poetry.dev-dependencies]pytest &#x3D; &quot;^6.2.3&quot;[build-system]requires &#x3D; [&quot;poetry-core&gt;&#x3D;1.0.0&quot;]build-backend &#x3D; &quot;poetry.core.masonry.api&quot; 当然我们也可以一次性添加多个依赖，和 pip 工具的使用方式类似，只要通过空格隔开即可： 1poetry add -D black flake8 mypy 移除依赖：remove 移除依赖的方法也很简单，我们只需要使用 poetry remove &lt;package-name&gt; 即可。 一旦运行上述命令，Poetry 会连同依赖的依赖一起移除，确保我们的虚拟环境里没有残留的第三方库或包，这一功能在 pip 工具中并未实现。 和 add 类似，默认是操作公共依赖，如果只是要移除开发环境依赖，一样需要加上 -D 选项： 1poetry remove -D pytest 激活虚拟环境 使用 Poetry 激活虚拟环境也比较简单，它和我们前面说的 Conda 类似，即使用： 1poetry shell 除此之外，如果我们没创建环境，那么这个命令同样会帮助我们创建一个新的虚拟环境，然后再切换到环境中。 当然我们也可以不用切换到虚拟环境中，直接使用相关命令执行对应的命令或代码，即使用 poetry run &lt;command&gt;，但这种情况往往适用于命令较少的情况下。比如现在我们的项目文件夹中已经存在了一个名为 main.py 文件，此时我们想要在 Poetry 为我们创建好的虚拟环境来运行它，那么就根据命令操作： 1poetry run python main.py 这里在 run 之后需要接具体运行的命令行工具名称，之后才是参数。 导出依赖列表 由于我们使用 pyproject.toml 文件并由 Poetry 来帮助我们管理依赖，如果要是其他人也使用了 Poetry，那么在安装过程中倒是没什么问题，直接运行 poetry install 命令即可；但如果有的人仍使用传统的 pip 方式来安装依赖，那么我们为了兼顾别人，就可能需要额外导出一下我们现有的依赖文件，将其放入传统的 requirements.txt 文件中。 此时 Poetry 也给我们提供了一种导出的方式，即 poetry export 命令。这里我们需要加上 -o 选项输出到 requirements.txt 文件中，这个文件会自动在这个过程中被创建。 1poetry export -o requirements.txt 默认情况下，Poetry 会连同依赖对应版本的校验哈希值一起导出，并且只导出全局依赖；因此我们可以分别通过加上 --without-hashes 和 --dev 选项来导出适合的依赖文件： 12345# 不导出 Hash 值$ poetry export --without-hashes# 导出开发依赖$ poetry export --without-hashes --dev 以上就是 Poetry 最常见的一些用法，更多关于 Poetry 的操作和用法可以进一步参考 Poetry 的官方文档。 作为一个 Python 工程师，我是如何选择？ 通过以上的介绍，可能读者已经对 Python 社区中虚拟环境管理的方案已经有了一定的认识。读者可能也想知道像笔者这样作为一个日常使用 Python 的开发工程师，会选择上述哪种方案的以及当中的优劣如何。 因此在本章这最后的一小节里，笔者主要谈谈不论是在工作还是个人项目中，使用上述虚拟环境管理工具时的体验感受以及使用建议。 venv：快速实验与使用的好帮手 如果读者不是专门的 Python 开发工程师，那通常也很少接触到大型的或结构固定的项目代码，更多时候可能也只是出于个人兴趣或是在工作中将 Python 作为脚本使用。 因此如果不想太过折腾虚拟环境，又想避免把当前操作系统中的 Python 环境搞乱，那么最好的方式就是每次实践的过程中，用 Python 解释器自带的 venv 模块来快速生成一个干净的 Python 虚拟环境。 默认情况下它会直接根据当前所使用的 Python 解释器进行克隆，同时也会将生成的虚拟环境文件夹放在当前的文件路径中。比如我们现在桌面上有一个用于练习的 lab 文件夹当中存放着我们的代码，那么通过解释器自带的 venv 模块创建一个同名的 venv 虚拟环境文件夹之后，就会可以基于当中命令来运行我们的代码： 1234567lab ├── main.py └── venv ├── bin ├── include ├── lib └── pyvenv.cfg 注：venv 在 Windows 下创建的目录可能会和上述内容有所不同，即用一个名为 Scripts 的文件夹来存放相关命令而非上述内容中的 bin 文件夹。 venv 模块的缺点在于创建后的虚拟环境是完全独立的克隆体，无法被很好地统一管理，年代久远的情况下，可能就散落在自己都不曾注意的犄角旮旯文件夹中占用磁盘空间。但换句话来，说如果我们也只要将存放了 venv 生成的虚拟环境的项目文件夹删除，那么虚拟环境也就会被清理。 不过好处在于目前大部分的 IDE 或编辑器——比如 Pycharm、VS Code 等——都支持探测 venv 所生成的虚拟环境的功能。如果发现当前项目文件夹中存在了由 venv 模块创建的虚拟环境，那么也会自动将执行代码需要的 Python 解释器命令切换为虚拟环境中的解释器命令。 Virtualenv：有口皆碑的传统厂牌 Virtualenv 作为历史悠久、到目前也一直维护更新的虚拟环境的创建工具，一直都是有口皆碑且在使用中也是「稳如泰山」。虽然 venv 模块脱胎于 Virtualenv，但却不是完全移植，因此缺乏某些 Virtualenv 才有的功能，比如不能手动基于其他 Python 版本来创建虚拟环境。 因此，不论是刚接触 Python 的新手，还是在日常的开发中，使用 Virtualenv 这样老牌的工具来创建虚拟环境一般也没有什么问题。因为 Virtualenv 本身就历史悠久，已经更新迭代多个版本，其 API 方法或使用方式也相对稳定，对于已经使用过该工具来管理的开发者来说无疑是降低学习成本的好事情。即便在使用 Virtualenv 的过程中出现了一时无法解决的问题，那么在网上也能找到相对多的参考资料提供相应的解决方案。 Virtualenv 通常需要搭配 virtualenvwrapper 这一扩展工具来使用，特别是当我们系统中存在多个虚拟环境时，并且我们需要经常在这些虚拟环境中进行切换的场景时尤为有用。通过 virtualenvwrapper 我们可以快速实现查看当前虚拟环境以及创建、删除虚拟环境等，不过 virtualenvwrapper 在前期配置上会稍微有些繁琐。 Conda：数据科学和离线 Linux 环境最好的选择 对于数据科学，尤其是会涉及到机器学习、深度学习领域的使用者而言，Conda 应该是最好的选择。 因为不论是 Anaconda 还是 Miniconda 它们都是在 Conda 基础之上发展而来，都已经事先内置了一系列开箱即用的依赖库或第三库，对于新手而言比较友好，一定程度上可以规避掉安装依赖时的某些问题。并且 Conda 最大的优势在于，如果所使用的依赖库核心会使用类似于 C 或 C++ 语言的额外代码比如 PyTorch 或 TensorFlow 这样深度学习框架，那么通常在 Conda 的官方仓库上都有事先已经编译并被打包好的版本，可以直接通过命令一键安装而无须使用者手动编译。 同时，如果要在无网络环境的 Linux 服务器上使用 Python，那么 Conda 毫无疑问是有着绝对优势。 这里的无网络环境也就是所谓的「Offline」离线环境，这通常在 信息安全保密性要求较高 的企业或机构部门中最为常见。如果我们需要在该环境中安装东西，就必须事先在自己的电脑上提前下载并传入类似于光盘、U 盘之类的传输介质之中，导入至指定的文件路径再进行操作，十分繁琐。 而 Python 官方目前仅提供了 Windows 和 macOS 两个版本的便捷安装包，而 Linux 版本则不存在便捷安装包一说，需要使用者自己手动编译。除此之外，由于 Python 的官方版本解释器是由 CPython 来实现（即由 C 语言实现），编译时需要涉及到众多的动态链接库，但凡缺少一个最后都会导致某个功能无法正常使用。 所以这时候 Conda 也再次派上用场。 我们只需要通过 Conda 来创建指定 Python 版本的虚拟环境，然后将各种依赖事先安装，直接将整个环境打包成压缩包，最后再走一遍传输的流程并在 Linux 服务器上直接解压并使用即可。 通过 Conda 所创建的虚拟环境其实也就对应了一个 Python 解释器，并且它会自动为我们链接运行解释器所需要的一些动态链接库，这就是为什么我们可以直接拷贝解压之后就可以直接使用。 Poetry：标新立异的个人首选 如果是现在个人项目或者用于开源项目，那么目前被广泛的使用的 Poetry 可以算是依赖与虚拟环境管理的首选。比如我们在实践案例中用到的 FastAPI 就主要使用 Poetry。 一方面 Poetry 会在首次初始化时自动为项目创建一个虚拟环境，之后我们只需要在 IDE 或编辑器中指定该环境的解释器即可；另一方面，Poetry 不同于单一的 pip 工具，它除了提供依赖解析与安装功能之外，还支持对项目代码进行打包，而与之相关的内容或参数都统一放到来自于 PEP 518 这一 Python 增强提案的 pyproject.toml 配置文件中。 同时，Poetry 在安装和使用上类似于 Yarn 这一 JavaScripts 包管理工具，不论是添加、更新还是删除依赖，都会生成一个名为 poetry.lock 的版本锁或依赖锁文件： 123456789101112131415161718192021222324# poetry.lock[[package]]name &#x3D; &quot;click&quot;version &#x3D; &quot;8.1.2&quot;description &#x3D; &quot;Composable command line interface toolkit&quot;category &#x3D; &quot;main&quot;optional &#x3D; falsepython-versions &#x3D; &quot;&gt;&#x3D;3.7&quot;[package.dependencies]colorama &#x3D; &#123;version &#x3D; &quot;*&quot;, markers &#x3D; &quot;platform_system &#x3D;&#x3D; \\&quot;Windows\\&quot;&quot;&#125;...[metadata.files]click &#x3D; [ &#123;file &#x3D; &quot;click-8.1.2-py3-none-any.whl&quot;, hash &#x3D; &quot;sha256:24e1a4a9ec5bf6299411369b208c1df2188d9eb8d916302fe6bf03faed227f1e&quot;&#125;, &#123;file &#x3D; &quot;click-8.1.2.tar.gz&quot;, hash &#x3D; &quot;sha256:479707fe14d9ec9a0757618b7a100a0ae4c4e236fac5b7f80ca68028141a1a72&quot;&#125;,]colorama &#x3D; [ &#123;file &#x3D; &quot;colorama-0.4.4-py2.py3-none-any.whl&quot;, hash &#x3D; &quot;sha256:9f47eda37229f68eee03b24b9748937c7dc3868f906e8ba69fbcbdd3bc5dc3e2&quot;&#125;, &#123;file &#x3D; &quot;colorama-0.4.4.tar.gz&quot;, hash &#x3D; &quot;sha256:5941b2b48a20143d2267e95b1c2a7603ce057ee39fd88e7329b0c292aa16869b&quot;&#125;,]... 目前大部分编程语言的依赖管理工具都会自带和 poetry.lock 文件这样的版本锁机制，其目的在于能够将当前的依赖版本记录下来（这个过程主要由工具自动生成对应的校验内容），因而不论是在与别人协作、使用 CI/CD 工具甚至部署到生产环境时都 能确保所有使用的依赖版本及内容一致，进一步减少因环境不一致导致问题排查困难的问题。 虽然 Poetry 是一个不同于以往 venv、Virtualenv 以及 Conda 的新模式，并且到目前为止（截止至 2022 年 7 月 28 日）也已经演进到了 1.1.x 版本，但从 Github 项目仓库的 Issue 模块（笔者按：指与项目有关的问题反馈、功能要求、讨论的页面板块）未解决的数量来看，Poetry 依旧有着这样或那样的问题或小毛病，例如一直为人诟病的依赖解析时间过长的问题。 因此如果是在公司内部项目中，如果部门或小组本就有了被长期使用的工具，那么就最好不要与其他人背道而驰地使用 Poetry 来「标新立异」。但除此之外的情况，笔者却十分鼓励正在学习的读者去踊跃尝试 Poetry，毕竟个人往往都是在折腾中学习、成长。同时，Poetry 的命令并不算多，不论是学习还是使用的成本相对而言没有那么高，学习它也同样是为了与时俱进、拥抱未来。 结语 Python 社区里各种虚拟环境与依赖管理的工具层出不穷也一直是为人所诟病的地方，并且和 Python 一样有着悠久历史的编程语言比如 JavaScript（Node.js）、Java 等也会面临同样的问题。因此像 Rust 这样后起之秀的编程语言，就积极吸取了以往的经验教训，在设计上会将环境、依赖管理等工具统一整合到一起。 除了本章所介绍的工具之外，在 Python 社区中还存在 pipx、pyenv、pipenv 等等功能类似的工具，这也侧面反映出 Python 社区在依赖管理方案上群雄割据的混乱情况。 在 2018 年出现的 Python 增强提案 PEP 582 中又出现了一种和本文内容虚拟环境管理有所不同的一种依赖管理方式，使得 Python 项目再也不需要虚拟环境，而是在项目目录中存在一个名为 __pypackages__ 的特殊文件夹即可直接使用相应的 Python 解释器执行代码；其模式类似于前端工具 npm 的 node_modules 文件夹，会在每次添加依赖时自动在本地目录中下载依赖。 所以在 PEP 582 提案基础上又出现了类似于 PDM 的工具。 但不论这些工具如何的丰富，我们在对它们进行选择时，除了考虑场景之外，更多时候还主要是以稳定性为主。","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"python虚拟环境","slug":"python虚拟环境","permalink":"http://yoursite.com/tags/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"}]},{"title":"weaviate","slug":"weaviate","date":"2024-01-03T07:48:26.000Z","updated":"2024-01-03T08:28:51.813Z","comments":true,"path":"2024/01/03/weaviate/","link":"","permalink":"http://yoursite.com/2024/01/03/weaviate/","excerpt":"","text":"1、数据结构 Weaviate 中的每个数据对象始终属于一个类，并且具有一个或多个属性。Weaviate 将数据对象（表示为 JSON 文档）存储在基于类的集合中，还可以将vector表示附加到数据对象上。 12345678910111213&#123; &quot;id&quot;: &quot;779c8970-0594-301c-bff5-d12907414002&quot;, &quot;class&quot;: &quot;Author&quot;, &quot;properties&quot;: &#123; &quot;name&quot;: &quot;Alice Munro&quot;, (...) &#125;, &quot;vector&quot;: [ -0.16147631, -0.065765485, -0.06546908 ]&#125; 1.1 交叉引用 当数据对象彼此存在关系时，它们可以在 Weaviate 中通过交叉引用来表示。创建交叉引用不会影响任一方向的对象向量。 例如：Paul Krugman 为 The New York Times 编辑。可以在Author 对象的writeFor 通过uuid 体现出交叉引用 Publication 。 1234567891011121314151617181920212223242526# Publication 对象&#123; &quot;id&quot;: &quot;32d5a368-ace8-3bb7-ade7-9f7ff03eddb6&quot;, &quot;class&quot;: &quot;Publication&quot;, &quot;properties&quot;: &#123; &quot;name&quot;: &quot;The New York Times&quot; &#125;, &quot;vector&quot;: [...]&#125;# Author 对象，引用了uuid=&quot;32d5a368-ace8-3bb7-ade7-9f7ff03eddb6&quot;的Publication对象&#123; &quot;id&quot;: &quot;779c8970-0594-301c-bff5-d12907414002&quot;, &quot;class&quot;: &quot;Author&quot;, &quot;properties&quot;: &#123; &quot;name&quot;: &quot;Paul Krugman&quot;, ... &quot;writesFor&quot;: [ &#123; &quot;beacon&quot;: &quot;weaviate://localhost/32d5a368-ace8-3bb7-ade7-9f7ff03eddb6&quot;, &quot;href&quot;: &quot;/v1/objects/32d5a368-ace8-3bb7-ade7-9f7ff03eddb6&quot; &#125; ], &#125;, &quot;vector&quot;: [...]&#125; 1.2 Schema schema 又叫集合/类。即，类和属性的定义就是schema。每个类都有自己的向量空间，这意味着您可以将不同模型的向量附加到不同的类。可以通过设置交叉引用来链接类 2、模块 Weaviate 具有模块化结构。矢量化或备份等功能由可选模块处理。Weaviate 的core是一个纯矢量原生数据库，没有附加任何模块。如果选择不包含任何模块，则需要为每个数据条目输入一个向量。然后，您也可以通过向量搜索对象。 矢量化和排名：矢量化模块（如text2vec-*、multi2vec-*或img2vec-*模块）将数据转换为向量。排名模块与rerank-*模块一样，对结果进行排名。 读取器或生成器模块：在矢量化器模块之上使用。这些模块获取检索到的一组相关文档，并执行其他操作，例如回答问题或生成任务。阅读器模块的一个示例是qna-transformersmodule，它直接从文档中提取答案。另一方面，生成器模块将使用语言生成从给定文档生成答案。 其他：gcs-backup或 之类的内容text-spellcheck 3、存储 Weaviate 不依赖任何第三方数据库。分片的三个组件都位于 Weaviate 内。主要包含3个组件 对象存储，一个KV数据库 倒排索引 向量索引（默认为HNSW）","categories":[{"name":"向量数据库","slug":"向量数据库","permalink":"http://yoursite.com/categories/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"weaviate","slug":"weaviate","permalink":"http://yoursite.com/tags/weaviate/"}]},{"title":"k8s的nebula","slug":"k8s的nebula","date":"2023-12-14T12:52:48.000Z","updated":"2023-12-20T03:16:44.181Z","comments":true,"path":"2023/12/14/k8s的nebula/","link":"","permalink":"http://yoursite.com/2023/12/14/k8s%E7%9A%84nebula/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177# 是通过operator安装的nebula# 自定义资源kubectl api-resources | grep nebulanebulaclusters nc apps.nebula-graph.io true NebulaClusternebularestores rt apps.nebula-graph.io true NebulaRestore# 查看资源类型配置文件kubectl get nc -n kbstudio-prod -o yaml# 查看资源类型实例名称kubectl get nc -n kbstudio-prodNAME AGEnebula-nebula 19d# 查看资源类型*实例*配置文件kubectl get nc nebula-nebula -n kbstudio-prod -o yamlapiVersion: apps.nebula-graph.io/v1alpha1kind: NebulaClustermetadata: creationTimestamp: &quot;2023-11-04T03:24:07Z&quot; generation: 1 name: nebula-nebula namespace: kbstudio-prod resourceVersion: &quot;496942&quot; selfLink: /apis/apps.nebula-graph.io/v1alpha1/namespaces/kbstudio-prod/nebulaclusters/nebula-nebula uid: 9d245fbb-7ac1-11ee-ba43-005056a7b4easpec: enablePVReclaim: true graphd: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: dop.dtdream.com/middleware.share operator: Exists config: enable_authorize: &quot;true&quot; minloglevel: &quot;1&quot; redirect_stdout: &quot;false&quot; stderrthreshold: &quot;0&quot; timezone_name: UTC+08:00 env: - name: GLOG_logtostderr value: &quot;1&quot; image: hub.dtwarebase.tech/dop/nebula-graphd labels: app: nebula-nebula app.mw.dopware.tech/name: nebula env.mw.dopware.tech/name: prod instType: middleware name: nebula-nebula-graphd project.mw.dopware.tech/name: kbstudio type.mw.dopware.tech/name: nebula replicas: 3 resources: limits: memory: 6144Mi requests: memory: 3072Mi version: v3.4.1 imagePullPolicy: Always metad: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: dop.dtdream.com/middleware.share operator: Exists config: minloglevel: &quot;1&quot; redirect_stdout: &quot;false&quot; stderrthreshold: &quot;0&quot; timezone_name: UTC+08:00 dataVolumeClaim: resources: requests: storage: 2Gi storageClassName: nebula-local-storage env: - name: GLOG_logtostderr value: &quot;1&quot; image: hub.dtwarebase.tech/dop/nebula-metad labels: app: nebula-nebula app.mw.dopware.tech/name: nebula env.mw.dopware.tech/name: prod instType: middleware name: nebula-nebula-metad project.mw.dopware.tech/name: kbstudio type.mw.dopware.tech/name: nebula replicas: 3 resources: limits: memory: 3072Mi requests: memory: 1536Mi version: v3.4.1 nodeSelector: dop.dtdream.com/hostgroup: default reference: name: statefulsets.apps version: v1 schedulerName: default-scheduler storaged: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: dop.dtdream.com/middleware.share operator: Exists config: minloglevel: &quot;1&quot; redirect_stdout: &quot;false&quot; stderrthreshold: &quot;0&quot; timezone_name: UTC+08:00 dataVolumeClaims: - resources: requests: storage: 2Gi storageClassName: nebula-local-storage enableAutoBalance: true env: - name: GLOG_logtostderr value: &quot;1&quot; image: hub.dtwarebase.tech/dop/nebula-storaged labels: app: nebula-nebula app.mw.dopware.tech/name: nebula env.mw.dopware.tech/name: prod instType: middleware name: nebula-nebula-storaged project.mw.dopware.tech/name: kbstudio type.mw.dopware.tech/name: nebula replicas: 3 resources: limits: memory: 3072Mi requests: memory: 1536Mi version: v3.4.1 unsatisfiableAction: ScheduleAnyway # 查看存储卷kubectl get pv | grep nebula pvc-kbstudio-prod-nebula-nebula-metad-0 50Gi RWO Delete Bound kbstudio-prod/metad-data-nebula-nebula-metad-0 nebula-local-storage 40d pvc-kbstudio-prod-nebula-nebula-metad-1 50Gi RWO Delete Bound kbstudio-prod/metad-data-nebula-nebula-metad-1 nebula-local-storage 40d pvc-kbstudio-prod-nebula-nebula-metad-2 50Gi RWO Delete Bound kbstudio-prod/metad-data-nebula-nebula-metad-2 nebula-local-storage 40d pvc-kbstudio-prod-nebula-nebula-storaged-0 50Gi RWO Delete Bound kbstudio-prod/storaged-data-nebula-nebula-storaged-0 nebula-local-storage 40d pvc-kbstudio-prod-nebula-nebula-storaged-1 50Gi RWO Delete Bound kbstudio-prod/storaged-data-nebula-nebula-storaged-1 nebula-local-storage 40d pvc-kbstudio-prod-nebula-nebula-storaged-2 50Gi RWO Delete Bound kbstudio-prod/storaged-data-nebula-nebula-storaged-2 nebula-local-storage 40d# 查看某一个存储[root@controller1 ~]# kubectl describe pv pvc-kbstudio-prod-nebula-nebula-storaged-0Name: pvc-kbstudio-prod-nebula-nebula-storaged-0Labels: app.kubernetes.io/cluster=nebula-nebula app.kubernetes.io/component=storaged app.kubernetes.io/managed-by=nebula-operator app.kubernetes.io/name=nebula-graphAnnotations: nebula-graph.io/pod-name: nebula-nebula-storaged-0Finalizers: [kubernetes.io/pv-protection]StorageClass: nebula-local-storageStatus: BoundClaim: kbstudio-prod/storaged-data-nebula-nebula-storaged-0Reclaim Policy: DeleteAccess Modes: RWOVolumeMode: FilesystemCapacity: 50GiNode Affinity: Required Terms: Term 0: kubernetes.io/hostname in [10.10.8.96]Message:Source: Type: HostPath (bare host directory volume) Path: /data/middleware/kbstudio/prod/nebula-nebula-storaged-0 HostPathType:Events: &lt;none&gt;","categories":[{"name":"图数据库","slug":"图数据库","permalink":"http://yoursite.com/categories/%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"nebule","slug":"nebule","permalink":"http://yoursite.com/tags/nebule/"}]},{"title":"spark图计算以及优化","slug":"spark图计算以及优化","date":"2023-12-14T10:39:52.000Z","updated":"2023-12-14T10:52:47.320Z","comments":true,"path":"2023/12/14/spark图计算以及优化/","link":"","permalink":"http://yoursite.com/2023/12/14/spark%E5%9B%BE%E8%AE%A1%E7%AE%97%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/","excerpt":"","text":"1、中心性 1.1 度中心性 ​ 度中心性（Degree Centrality）：度中心性是指节点在网络中的度数，即与该节点直接相连的边的数量。度中心性越高，说明该节点在网络中的连接数越多，具有较高的影响力。应用场景包括社交网络分析、关键词提取等。节点的入边数量称为节点的入度（In-degree），出边数量则称为出度（Out-degree），如果忽略边的方向计算所有边的数量，就得到节点的度。 1.2 接近中心性 接近中心性（Closeness Centrality）：接近中心性是指节点到其他节点的平均距离的倒数。接近中心性越高，说明该节点与其他节点的联系越紧密，具有较高的传播能力。应用场景包括信息传播、疾病传播等。 接近中心性，即计算每个点到它可连通的其它各点的最短距离的平均值的倒数。计算公式： 其中，x 为待计算的目标节点，y 为通过边与 x 相连的任意一个节点（不包含x），k-1 为 y 的个数，d(x,y) 为 x 到 y 的最短距离。 特殊说明： 孤立实体的接近中心性分值为 0。 1.3 中介中心性 中介中心性（Betweenness Centrality）：中介中心性是指节点在网络中作为中介的频率。中介节点在网络中的作用是连接不同的社区，促进信息传递和交流。中介中心性越高，说明该节点在信息传播中的作用越大。应用场景包括社交网络分析、交通网络分析等。 TIPS:中介中心性的取值范围是 0 到 1，节点的分值越大，对于网络流通性或连通性的影响力越大。","categories":[{"name":"图计算","slug":"图计算","permalink":"http://yoursite.com/categories/%E5%9B%BE%E8%AE%A1%E7%AE%97/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"minikube","slug":"minikube","date":"2023-12-07T07:42:44.000Z","updated":"2023-12-07T07:44:38.668Z","comments":true,"path":"2023/12/07/minikube/","link":"","permalink":"http://yoursite.com/2023/12/07/minikube/","excerpt":"","text":"https://www.zhaowenyu.com/minikube-doc/","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"minikube","slug":"minikube","permalink":"http://yoursite.com/tags/minikube/"}]},{"title":"spark优化手段","slug":"spark优化手段","date":"2023-12-05T08:43:39.000Z","updated":"2023-12-05T08:52:57.008Z","comments":true,"path":"2023/12/05/spark优化手段/","link":"","permalink":"http://yoursite.com/2023/12/05/spark%E4%BC%98%E5%8C%96%E6%89%8B%E6%AE%B5/","excerpt":"","text":"一、常用参数 12345678910111213141516--driver-memory 4g : driver内存大小，一般没有广播变量(broadcast)时，设置4g足够，如果有广播变量，视情况而定，可设置6G，8G，12G等均可--executor-memory 4g : 每个executor的内存，正常情况下是4g足够，但有时处理大批量数据时容易内存不足，再多申请一点，如6G--num-executors 15 : 总共申请的executor数目，普通任务十几个或者几十个足够了，若是处理海量数据如百G上T的数据时可以申请多一些，100，200等--executor-cores 2 : 每个executor内的核数，即每个executor中的任务task数目，此处设置为2，即2个task共享上面设置的6g内存，每个map或reduce任务的并行度是executor数目*executor中的任务数yarn集群中一般有资源申请上限，如，executor-memory*num-executors &lt; 400G 等，所以调试参数时要注意这一点—-spark.default.parallelism 200 ： Spark作业的默认为500~1000个比较合适,如果不设置，spark会根据底层HDFS的block数量设置task的数量，这样会导致并行度偏少，资源利用不充分。该参数设为num-executors * executor-cores的2~3倍比较合适。-- spark.storage.memoryFraction 0.6 : 设置RDD持久化数据在Executor内存中能占的最大比例。默认值是0.6—-spark.shuffle.memoryFraction 0.2 ： 设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2，如果shuffle聚合时使用的内存超出了这个20%的限制，多余数据会被溢写到磁盘文件中去，降低shuffle性能—-spark.yarn.executor.memoryOverhead 1G ： executor执行的时候，用的内存可能会超过executor-memory，所以会为executor额外预留一部分内存，spark.yarn.executor.memoryOverhead即代表这部分内存 二、编程建议 避免创建重复的RDD，尽量复用同一份数据。 尽量避免使用shuffle类算子，因为shuffle操作是spark中最消耗性能的地方，reduceByKey、join、distinct、repartition等算子都会触发shuffle操作，尽量使用map类的非shuffle算子 用aggregateByKey和reduceByKey替代groupByKey,因为前两个是预聚合操作，会在每个节点本地对相同的key做聚合，等其他节点拉取所有节点上相同的key时，会大大减少磁盘IO以及网络开销。 repartition适用于RDD[V], partitionBy适用于RDD[K, V] mapPartitions操作替代普通map，foreachPartitions替代foreach filter操作之后进行coalesce操作，可以减少RDD的partition数量 如果有RDD复用，尤其是该RDD需要花费比较长的时间，建议对该RDD做cache，若该RDD每个partition需要消耗很多内存，建议开启Kryo序列化机制(据说可节省2到5倍空间),若还是有比较大的内存开销，可将storage_level设置为MEMORY_AND_DISK_SER 尽量避免在一个Transformation中处理所有的逻辑，尽量分解成map、filter之类的操作 多个RDD进行union操作时，避免使用rdd.union(rdd).union(rdd).union(rdd)这种多重union，rdd.union只适合2个RDD合并，合并多个时采用SparkContext.union(Array(RDD))，避免union嵌套层数太多，导致的调用链路太长，耗时太久，且容易引发StackOverFlow spark中的Group/join/XXXByKey等操作，都可以指定partition的个数，不需要额外使用repartition和partitionBy函数 尽量保证每轮Stage里每个task处理的数据量&gt;128M 如果2个RDD做join，其中一个数据量很小，可以采用Broadcast Join，将小的RDD数据collect到driver内存中，将其BroadCast到另外以RDD中，其他场景想优化后面会讲 2个RDD做笛卡尔积时，把小的RDD作为参数传入，如BigRDD.certesian(smallRDD) 若需要Broadcast一个大的对象到远端作为字典查询，可使用多executor-cores，大executor-memory。若将该占用内存较大的对象存储到外部系统，executor-cores=1， executor-memory=m(默认值2g),可以正常运行，那么当大字典占用空间为size(g)时，executor-memory为2*size，executor-cores=size/m(向上取整) 15.如果对象太大无法BroadCast到远端，且需求是根据大的RDD中的key去索引小RDD中的key，可使用zipPartitions以hash join的方式实现，具体原理参考下一节的shuffle过程 如果需要在repartition重分区之后还要进行排序，可直接使用repartitionAndSortWithinPartitions，比分解操作效率高，因为它可以一边shuffle一边排序 三、shuffle 优化 3.1 什么是shuffle操作 spark中的shuffle操作功能：将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join操作，类似洗牌的操作。这些分布在各个存储节点上的数据重新打乱然后汇聚到不同节点的过程就是shuffle过程。 3.2 哪些操作中包含shuffle操作 RDD的特性是不可变的带分区的记录集合，Spark提供了Transformation和Action两种操作RDD的方式。Transformation是生成新的RDD，包括map, flatMap, filter, union, sample, join, groupByKey, cogroup, ReduceByKey, cros, sortByKey, mapValues等；Action只是返回一个结果，包括collect，reduce，count，save，lookupKey等 Spark所有的算子操作中是否使用shuffle过程要看计算后对应多少分区： 若一个操作执行过程中，结果RDD的每个分区只依赖上一个RDD的同一个分区，即属于窄依赖，如map、filter、union等操作，这种情况是不需要进行shuffle的，同时还可以按照pipeline的方式，把一个分区上的多个操作放在同一个Task中进行 若结果RDD的每个分区需要依赖上一个RDD的全部分区，即属于宽依赖，如repartition相关操作（repartition，coalesce）、*ByKey操作（groupByKey，ReduceByKey，combineByKey、aggregateByKey等）、join相关操作（cogroup，join）、distinct操作，这种依赖是需要进行shuffle操作的 3.3 shuffle操作过程 shuffle过程分为shuffle write和shuffle read两部分 shuffle write： 分区数由上一阶段的RDD分区数控制，shuffle write过程主要是将计算的中间结果按某种规则临时放到各个executor所在的本地磁盘上（当前stage结束之后，每个task处理的数据按key进行分类，数据先写入内存缓冲区，缓冲区满，溢写spill到磁盘文件，最终相同key被写入同一个磁盘文件）创建的磁盘文件数量=当前stage中task数量*下一个stage的task数量 shuffle read：从上游stage的所有task节点上拉取属于自己的磁盘文件，每个read task会有自己的buffer缓冲，每次只能拉取与buffer缓冲相同大小的数据，然后聚合，聚合完一批后拉取下一批，边拉取边聚合。分区数由Spark提供的一些参数控制，如果这个参数值设置的很小，同时shuffle read的数据量很大，会导致一个task需要处理的数据非常大，容易发生JVM crash，从而导致shuffle数据失败，同时executor也丢失了，就会看到Failed to connect to host 的错误(即executor lost)。 shuffle过程中，各个节点会通过shuffle write过程将相同key都会先写入本地磁盘文件中，然后其他节点的shuffle read过程通过网络传输拉取各个节点上的磁盘文件中的相同key。这其中大量数据交换涉及到的网络传输和文件读写操作是shuffle操作十分耗时的根本原因 3.4 spark的shuffle类型 参数spark.shuffle.manager用于设置ShuffleManager的类型。Spark1.5以后，该参数有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark1.2以前的默认值，Spark1.2之后的默认值都是SortShuffleManager。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。 由于SortShuffleManager默认会对数据进行排序，因此如果业务需求中需要排序的话，使用默认的SortShuffleManager就可以；但如果不需要排序，可以通过bypass机制或设置HashShuffleManager避免排序，同时也能提供较好的磁盘读写性能。 HashShuffleManager流程： SortShuffleManager流程： 3.5 如何开启bypass机制 bypass机制通过参数spark.shuffle.sort.bypassMergeThreshold设置，默认值是200，表示当ShuffleManager是SortShuffleManager时，若shuffle read task的数量小于这个阈值（默认200）时，则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式写数据，但最后会将每个task产生的所有临时磁盘文件合并成一个文件，并创建索引文件。 这里给出的调优建议是，当使用SortShuffleManager时，如果的确不需要排序，可以将这个参数值调大一些，大于shuffle read task的数量。那么此时就会自动开启bypass机制，map-side就不会进行排序了，减少排序的性能开销，提升shuffle操作效率。但这种方式并没有减少shuffle write过程产生的磁盘文件数量，所以写的性能没有改变。 3.6 HashShuffleManager优化建议 如果使用HashShuffleManager，可以设置spark.shuffle.consolidateFiles参数。该参数默认为false，只有当使用HashShuffleManager且该参数设置为True时，才会开启consolidate机制，大幅度合并shuffle write过程产生的输出文件，对于shuffle read task 数量特别多的情况下，可以极大地减少磁盘IO开销，提升shuffle性能。参考社区同学给出的数据，consolidate性能比开启bypass机制的SortShuffleManager高出10% ~ 30%。 3.7 shuffle调优建议 除了上述的几个参数调优，shuffle过程还有一些参数可以提高性能： 123- spark.shuffle.file.buffer : 默认32M，shuffle Write阶段写文件时的buffer大小，若内存资源比较充足，可适当将其值调大一些（如64M），减少executor的IO读写次数，提高shuffle性能- spark.shuffle.io.maxRetries ： 默认3次，Shuffle Read阶段取数据的重试次数，若shuffle处理的数据量很大，可适当将该参数调大。 3.8 shuffle操作过程中的常见错误 SparkSQL中的shuffle错误： 123org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0org.apache.spark.shuffle.FetchFailedException:Failed to connect to hostname/192.168.xx.xxx:50268 RDD中的shuffle错误： 12345WARN TaskSetManager: Lost task 17.1 in stage 4.1 (TID 1386, spark050013): java.io.FileNotFoundException: /data04/spark/tmp/blockmgr-817d372f-c359-4a00-96dd-8f6554aa19cd/2f/temp_shuffle_e22e013a-5392-4edb-9874-a196a1dad97c (没有那个文件或目录)FetchFailed(BlockManagerId(6083b277-119a-49e8-8a49-3539690a2a3f-S155, spark050013, 8533), shuffleId=1, mapId=143, reduceId=3, message=org.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer&#123;file=/data04/spark/tmp/blockmgr-817d372f-c359-4a00-96dd-8f6554aa19cd/0e/shuffle_1_143_0.data, offset=997061, length=112503&#125; 处理shuffle类操作的注意事项： 减少shuffle数据量：在shuffle前过滤掉不必要的数据，只选取需要的字段处理 针对SparkSQL和DataFrame的join、group by等操作：可以通过 spark.sql.shuffle.partitions控制分区数，默认设置为200，可根据shuffle的量以及计算的复杂度提高这个值，如2000等 RDD的join、group by、reduceByKey等操作：通过spark.default.parallelism控制shuffle read与reduce处理的分区数，默认为运行任务的core总数，官方建议为设置成运行任务的core的2~3倍 提高executor的内存：即spark.executor.memory的值 分析数据验证是否存在数据倾斜的问题：如空值如何处理，异常数据（某个key对应的数据量特别大）时是否可以单独处理，可以考虑自定义数据分区规则，如何自定义可以参考下面的join优化环节 四、join性能优化 Spark所有的操作中，join操作是最复杂、代价最大的操作，也是大部分业务场景的性能瓶颈所在。所以针对join操作的优化是使用spark必须要学会的技能。 spark的join操作也分为Spark SQL的join和Spark RDD的join。 4.1 Spark SQL 的join操作 4.1.1 Hash Join Hash Join的执行方式是先将小表映射成Hash Table的方式，再将大表使用相同方式映射到Hash Table，在同一个hash分区内做join匹配。 hash join又分为broadcast hash join和shuffle hash join两种。其中Broadcast hash join，顾名思义，就是把小表广播到每一个节点上的内存中，大表按Key保存到各个分区中，小表和每个分区的大表做join匹配。这种情况适合一个小表和一个大表做join且小表能够在内存中保存的情况。如下图所示： 当Hash Join不能适用的场景就需要Shuffle Hash Join了，Shuffle Hash Join的原理是按照join Key分区，key相同的数据必然分配到同一分区中，将大表join分而治之，变成小表的join，可以提高并行度。执行过程也分为两个阶段： shuffle阶段：分别将两个表按照join key进行分区，将相同的join key数据重分区到同一节点 hash join阶段：每个分区节点上的数据单独执行单机hash join算法 Shuffle Hash Join的过程如下图所示： 4.1.2 Sort-Merge Join SparkSQL针对两张大表join的情况提供了全新的算法——Sort-merge join，整个过程分为三个步骤： Shuffle阶段：将两张大表根据join key进行重新分区，两张表数据会分布到整个集群，以便分布式进行处理 sort阶段：对单个分区节点的两表数据，分别进行排序 merge阶段：对排好序的两张分区表数据执行join操作。分别遍历两个有序序列，遇到相同的join key就merge输出，否则继续取更小一边的key，即合并两个有序列表的方式。 sort-merge join流程如下图所示。 4.2 Spark RDD的join操作 Spark的RDD join没有上面这么多的分类，但是面临的业务需求是一样的。如果是大表join小表的情况，则可以将小表声明为broadcast变量，使用map操作快速实现join功能，但又不必执行Spark core中的join操作。 如果是两个大表join，则必须依赖Spark Core中的join操作了。Spark RDD Join的过程可以自行阅读源码了解，这里只做一个大概的讲解。 spark的join过程中最核心的函数是cogroup方法，这个方法中会判断join的两个RDD所使用的partitioner是否一样，如果分区相同，即存在OneToOneDependency依赖，不用进行hash分区，可直接join；如果要关联的RDD和当前RDD的分区不一致时，就要对RDD进行重新hash分区，分到正确的分区中，即存在ShuffleDependency，需要先进行shuffle操作再join。因此提升join效率的一个思路就是使得两个RDD具有相同的partitioners。 所以针对Spark RDD的join操作的优化建议是： 如果需要join的其中一个RDD比较小，可以直接将其存入内存，使用broadcast hash join 在对两个RDD进行join操作之前，使其使用同一个partitioners，避免join操作的shuffle过程 如果两个RDD其一存在重复的key也会导致join操作性能变低，因此最好先进行key值的去重处理 4.3 数据倾斜优化 均匀数据分布的情况下，前面所说的优化建议就足够了。但存在数据倾斜时，仍然会有性能问题。主要体现在绝大多数task执行得都非常快，个别task执行很慢，拖慢整个任务的执行进程，甚至可能因为某个task处理的数据量过大而爆出OOM错误。 shuffle操作中需要将各个节点上相同的key拉取到某一个节点上的一个task处理，如果某个key对应的数据量特别大，就会发生数据倾斜。 4.3.1 分析数据分布 如果是Spark SQL中的group by、join语句导致的数据倾斜，可以使用SQL分析执行SQL中的表的key分布情况；如果是Spark RDD执行shuffle算子导致的数据倾斜，可以在Spark作业中加入分析Key分布的代码，使用countByKey()统计各个key对应的记录数。 4.3.2 数据倾斜的解决方案 这里参考美团技术博客中给出的几个方案。 1）针对hive表中的数据倾斜，可以尝试通过hive进行数据预处理，如按照key进行聚合，或是和其他表join，Spark作业中直接使用预处理后的数据。 2）如果发现导致倾斜的key就几个，而且对计算本身的影响不大，可以考虑过滤掉少数导致倾斜的key 3）设置参数spark.sql.shuffle.partitions，提高shuffle操作的并行度，增加shuffle read task的数量，降低每个task处理的数据量 4）针对RDD执行reduceByKey等聚合类算子或是在Spark SQL中使用group by语句时，可以考虑两阶段聚合方案，即局部聚合+全局聚合。第一阶段局部聚合，先给每个key打上一个随机数，接着对打上随机数的数据执行reduceByKey等聚合操作，然后将各个key的前缀去掉。第二阶段全局聚合即正常的聚合操作。 5）针对两个数据量都比较大的RDD/hive表进行join的情况，如果其中一个RDD/hive表的少数key对应的数据量过大，另一个比较均匀时，可以先分析数据，将数据量过大的几个key统计并拆分出来形成一个单独的RDD，得到的两个RDD/hive表分别和另一个RDD/hive表做join，其中key对应数据量较大的那个要进行key值随机数打散处理，另一个无数据倾斜的RDD/hive表要1对n膨胀扩容n倍，确保随机化后key值仍然有效。 6）针对join操作的RDD中有大量的key导致数据倾斜，对有数据倾斜的整个RDD的key值做随机打散处理，对另一个正常的RDD进行1对n膨胀扩容，每条数据都依次打上0~n的前缀。处理完后再执行join操作 五、其他错误总结 (1) 报错信息 1234java.lang.OutOfMemory, unable to create new native thread Caused by: java.lang.OutOfMemoryError: unable to create new native thread at java.lang.Thread.start0(Native Method) at java.lang.Thread.start(Thread.java:640) 解决方案： 上面这段错误提示的本质是Linux操作系统无法创建更多进程，导致出错，并不是系统的内存不足。因此要解决这个问题需要修改Linux允许创建更多的进程，就需要修改Linux最大进程数 （2）报错信息 由于Spark在计算的时候会将中间结果存储到/tmp目录，而目前linux又都支持tmpfs，其实就是将/tmp目录挂载到内存当中, 那么这里就存在一个问题，中间结果过多导致/tmp目录写满而出现如下错误 No Space Left on the device（Shuffle临时文件过多） 解决方案： 修改配置文件spark-env.sh,把临时文件引入到一个自定义的目录中去, 即: export SPARK_LOCAL_DIRS=/home/utoken/datadir/spark/tmp （3）报错信息 Worker节点中的work目录占用许多磁盘空间, 这些是Driver上传到worker的文件, 会占用许多磁盘空间 解决方案： 需要定时做手工清理work目录 （4）spark-shell提交Spark Application如何解决依赖库 解决方案： 利用–driver-class-path选项来指定所依赖的jar文件，注意的是–driver-class-path后如果需要跟着多个jar文件的话，jar文件之间使用冒号:来分割。 （5）内存不足或数据倾斜导致Executor Lost，shuffle fetch失败，Task重试失败等（spark-submit提交） 1234TaskSetManager: Lost task 1.0 in stage 6.0 (TID 100, 192.168.10.37): java.lang.OutOfMemoryError: Java heap spaceINFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.10.37:57139 (size: 42.0 KB, free: 24.2 MB)INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.10.38:53816 (size: 42.0 KB, free: 24.2 MB)INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 102, 192.168.10.37, ANY, 2152 bytes) 解决方案： 增加worker内存，或者相同资源下增加partition数目，这样每个task要处理的数据变少，占用内存变少 如果存在shuffle过程，设置shuffle read阶段的并行数","categories":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/categories/spark/"}],"tags":[{"name":"spark性能优化","slug":"spark性能优化","permalink":"http://yoursite.com/tags/spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"}]},{"title":"","slug":"M11","date":"2023-10-31T06:46:55.434Z","updated":"2023-10-31T06:46:55.434Z","comments":true,"path":"2023/10/31/M11/","link":"","permalink":"http://yoursite.com/2023/10/31/M11/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"kettle插件开发","slug":"kettle插件开发","date":"2023-10-30T08:15:20.000Z","updated":"2023-10-31T10:33:07.895Z","comments":true,"path":"2023/10/30/kettle插件开发/","link":"","permalink":"http://yoursite.com/2023/10/30/kettle%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/","excerpt":"","text":"https://help.hitachivantara.com/Documentation/Pentaho/Data_Integration_and_Analytics/9.5/Developer_center/Create_step_plugins 1、Kettle step插件开发接口 接口 基类 主要功能 StepMetaInterface BaseStepMeta 存储step设置信息验证step设置信息序列化step设置信息提供获取step类的方法 StepDialogInterface BaseStepDialog step属性信息配置窗口 StepInterface BaseStep 处理rows StepDataInterface BaseStepData 为数据处理提高数据存储 2、kettle中相关类的命名规则 StepInterface的实现类以插件的功能相关命名：*.java StepDataInterface的实现类：xxData.java *StepMetaInterface的实现类：xxMeta.java StepDialogInterface的实现类：xxDialog.java 3、转换执行过程 根据ktr文件执行 123456KettleEnvironment.init();TransMeta transMeta = new TransMeta(&quot;etl/capturing_rows.ktr&quot;);Trans trans = new Trans(transMeta);trans.prepareExecution(null);trans.startThreads();//执行该方法的时候，将会调用proccessRow()方法对每一行进行操作 trans.waitUntilFinished();","categories":[{"name":"kettle","slug":"kettle","permalink":"http://yoursite.com/categories/kettle/"}],"tags":[{"name":"插件","slug":"插件","permalink":"http://yoursite.com/tags/%E6%8F%92%E4%BB%B6/"}]},{"title":"kettle","slug":"kettle","date":"2023-10-17T01:22:06.000Z","updated":"2023-10-17T01:27:45.863Z","comments":true,"path":"2023/10/17/kettle/","link":"","permalink":"http://yoursite.com/2023/10/17/kettle/","excerpt":"","text":"一、Kettle简介 Kettle最早是一个开源的ETL（Extract-Transform-Load的缩写）工具，全称为KDE Extraction, Transportation, Transformation and Loading Environment。后来Kettle重命名为Pentaho Data Integration 。 它由Java开发，支持跨平台运行，其特性包括：支持100%无编码、拖拽方式开发ETL数据管道；可对接包括传统数据库、文件、大数据平台、接口、流数据等数据源；支持ETL数据管道加入机器学习算法。 Kettle是一个实现ETL开发的一款开发工具，Spoon是Kettle工具提供的图形化界面。","categories":[{"name":"ETL","slug":"ETL","permalink":"http://yoursite.com/categories/ETL/"}],"tags":[{"name":"ETL","slug":"ETL","permalink":"http://yoursite.com/tags/ETL/"}]},{"title":"向量余弦相似度","slug":"向量余弦相似度","date":"2023-10-11T12:42:12.000Z","updated":"2023-10-11T12:49:52.455Z","comments":true,"path":"2023/10/11/向量余弦相似度/","link":"","permalink":"http://yoursite.com/2023/10/11/%E5%90%91%E9%87%8F%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6/","excerpt":"","text":"假定三角形的三条边为 a, b 和 c，对应的三个角为 A, B 和 C，那么角 A 的余弦 2个向量之间的余弦公式： 12vec1.toArray.zip(vec2.toArray).map(d =&gt; d._1 * d._2).sum / (math.sqrt(vec1.toArray.map(d =&gt; d * d).sum) * math.sqrt(vec2.toArray.map(d =&gt; d * d).sum)) 使用Scala语言实现的向量操作，计算两个向量的余弦相似度。 vec1.toArray 和 vec2.toArray 将向量 vec1 和 vec2 转换为数组，方便对数组进行操作。 zip 函数将两个数组中的对应元素配对为元组。例如，如果 vec1 是 [a, b, c]，而 vec2 是 [x, y, z]，zip 会生成 [(a, x), (b, y), (c, z)]。 map 函数对每个元组中的元素执行操作。在这里，对每对元素执行乘法操作，即 d._1 * d._2，这会生成一个新数组，包含了对应位置元素的乘积。 sum 函数对新生成的数组中的所有元素求和。 接下来，计算向量 vec1 和 vec2 的模（即长度）的乘积。模的计算通过将向量中每个元素的平方求和，然后再取平方根实现。 最后，计算余弦相似度的分子（两向量对应元素的乘积之和）除以余弦相似度的分母（向量模的乘积）。 余弦相似度（Cosine Similarity）是衡量两个非零向量方向上的相似度的一种常用方法。在这个实现中，通过计算两个向量对应位置元素的乘积之和，除以两个向量模的乘积，来得到余弦相似度。","categories":[],"tags":[]},{"title":"知识图谱推理","slug":"知识图谱推理","date":"2023-09-20T01:02:06.000Z","updated":"2023-09-22T08:08:16.267Z","comments":true,"path":"2023/09/20/知识图谱推理/","link":"","permalink":"http://yoursite.com/2023/09/20/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%8E%A8%E7%90%86/","excerpt":"","text":"https://www.cnblogs.com/xiaoqi/tag/知识图谱/ https://yubincloud.github.io/notebook/pages/4f4fc0/ https://qianshuang.github.io/2018/10/06/KB_02/ https://bbs.huaweicloud.com/blogs/286074 https://developer.aliyun.com/article/1152527 1、推理基础任务 知识推理的几个基础任务主要包括知识补全、知识纠错、推理问答等。 知识补全，即通过算法，补全知识图谱中缺失的属性或者关系。实际构建的知识图谱，通常存在不完备的问题，即部分关系或属性会缺失。 知识图谱的纠错，即发现图谱中的错误知识进行修正。例如，实际构建的知识图谱还可能存在错误知识。实体的类型、实体间的关系、实体属性值均可能存在错误。 基于知识图谱的推理问答，即KBQA，通常应用于涉及多个实体，多个关系，多跳，比较等相对复杂的问答任务。 1）知识推理任务示例 如上图所示，以“姚沁蕾的妈妈是谁“为例。有一条常识是 “父亲的妻子是妈妈”，则可依据该常识，推理出姚沁蕾的妈妈是叶莉，进而补全”姚沁蕾”和“叶莉”之间的关系，提升知识图谱的完备性。 2）知识纠错任务示例 如上图所示，在某个影视知识图谱中，虚线框中的实体《春光灿烂猪八戒》，其类型为“电影“。它的属性有集数，主题曲、片尾曲等。而其他同为”电影“类别的实体，其属性多包含上映时期、票房，且大多没有集数这个属性。则推理可知，《春光灿烂猪八戒》这个实体的类型大概率存在错误，其正确类型应该是电视剧。 3）推理问答任务示例 如上图所示，面对“刘德华主演的电影中豆瓣评分大于8分的有哪些？“这样的问题，需要机器对该问题进行解析、理解，在知识图谱中完成查询、推理、比较动作，找到《天下无贼》和《无间道》作为答案返回。 2、属性图和RDF图 （1），市面上已知支持RDF推理功能的实现框架，比如，Jena，打开推理引擎后，查询速度极慢，同时由于推理引擎需要将数据全量载入内存，因此，只支持小数据集上图谱推理，性能有瓶颈，并且还需要做一整套的工具链，成本较高。 （2），出于各方面成本的考虑，虽然RDF模型语义完备，支持推理，解放了思想，但随之带来的高度复杂性却增加了成本，这也是大家常说学术界使用RDF模型，工业界都在讲属性图模型的原因。 3、演绎推理 演绎推理的过程需要明确定义的先验信息，所以基于演绎的知识图谱推理多围绕本体展开。一般都是学术界使用居多。 本体推理：OWL 逻辑编程推理：Datalog、OWL、ABox、TBox 查询重写：基于查询重写的推理机有多个，例如 Ontop ① 、 Mastro ② 、Stardog③ 、Ultrawrap ④ 、 Morph 产生式规则：一个产生式系统由事实集合、产生式集合和推理引擎三部分组成。 4、归纳推理 基于归纳的知识图谱推理主要是通过对知识图谱已有信息的分析和挖掘进行推理的， 最常用的信息为已有的三元组。按照推理要素的不同，基于归纳的知识图谱推理可以分为 以下几类： 基于图结构的推理 https://github.com/noon99jaki/pra PRA（Path Ranking Algorithm）利用了实体节点之 间的路径当作特征从而进行链接预测推理。它有两个任务 给定关系𝑟和头实体h预测可能的尾实体𝑡是什么，即在给定h, 𝑟的情 况下，预测哪个三元组(h, 𝑟,𝑡)成立的可能性比较大，叫作尾实体链接预测。 另一个是在给 定𝑟,𝑡的情况下，预测可能的头实体h是什么，叫作头实体链接预测 PRA 针对的知识图谱主要是自底向上自动化构建的含有较多噪声的图谱，例如 NELL，并将关系推理的问题形式化为一个排序问题，对每个关系的头实体预测和尾实体 预测都单独训练一条排序模型。PRA 将存在于知识图谱中的路径当作特征，并通过图上 的计算对每个路径赋予相应的特征值，然后利用这些特征学习一个逻辑斯蒂回归分类器完 成关系推理。在 PRA 中，每一个路径可以当作对当前关系判断的一个专家，不同的路径 从不同的角度说明了当前关系的存在与否。 基于规则学习的推理 基于规则的推理具有精确且可解释的特性，规则在学术界和工业界的推理场景都有重 要的应用。规则是基于规则推理的核心，所以规则获取是一个重要的任务。在小型的领域 知识图谱上，规则可以由领域专家提供，但在大型、综合的知识图谱方面，人工提供规则 的效率比较低，且很难做到全面和准确。所以，自动化的规则学习方法应运而生，旨在快 速有效地从大规模知识图谱上学习置信度较高的规则，并服务于关系推理任务。 首先介绍典型的规则学习方法 AMIE。http://www.mpi-inf.mpg.de/departments/ontologies/ projects/amie 基于关联规则挖掘⽅法(AMIE) 在不同数据集上的运行效果，从中可以看出 AMIE 在大规模知 识图谱上的效率较高。例如，在拥有 100 多万个实体以及近 700 万个三元组的 DBpedia 上，AMIE 仅需不到 3min 就能完成规则挖掘，产生 7000 条规则,并帮助推理出了 12 万 多个新的三元组。 AMIE 能挖掘的规则形如： father Of(𝑓, 𝑐) ← motherOf(𝑚, 𝑐) ∧ marriedTo(𝑚, 𝑓). AMIE 是一种霍恩规则，也是一种闭环规则，即整条规则可以在图中构成一个闭环结 构。在规则学习的任务中，最重要的是如何有效搜索空间，因为在大型的知识图谱上简单 地遍历所有可能的规则并评估规则的质量效率很低，几乎不可行。AMIE 定义了 3 个挖掘 算子（Mining Operators），通过不断在规则中增加挖掘算子来探索图上的搜索空间，并且 融入了对应的剪枝策略。3 个挖掘算子如下：  增加悬挂原子（Adding Dangling Atom）。即在规则中增加一个原子，这个原子包 含一个新的变量和一个已经在规则中出现的元素，可以是出现过的变量，也可以 是出现过的实体。  增加实例化的原子（Adding Instantiated Atom）。即在规则中增加一个原子，这个 原子包含一个实例化的实体以及一个已经在规则中出现的元素。 增加闭合原子（Adding Closing Atom）。即在规则中增加一个原子，这个原子包 含的两个元素都是已经出现在规则中的变量或实体。增加闭合原子之后，规则就 算构建完成了。 基于表示学习的推理。 基于图结构的推理和基于规则学习的推理，都显式地定义了推理学习所需的特征，而 基于表示学习的推理通过将知识图谱中包括实体和关系的元素映射到一个连续的向量空间 中，为每个元素学习在向量空间中表示，向量空间中的表示可以是一个或多个向量或矩 阵。表示学习让算法在学习向量表示的过程中自动捕捉、推理所需的特征，通过训练学 习，将知识图谱中离散符号表示的信息编码在不同的向量空间表示中，使得知识图谱的推 理能够通过预设的向量空间表示之间的计算自动实现，不需要显式的推理步骤。 知识图谱的表示学习受自然语言处理关于词向量研究的启发，因为在 word2vec 的结 果中发现了一些词向量具有空间平移性，例如： vec(king) − vec(queen) ≈ vec(man) − vec(woman) 即“king”的词向量减去“queen”的词向量的结果约等于“man”的词向量减去 “woman”的词向量的结果，这说明“king”和“queen”在语义上的关系与“man”和 “woman”之间的关系比较近似。而拓展到知识图谱上，就可以理解为拥有同一种关系的 头实体和尾实体对，在向量空间的表示可能具有平移不变性","categories":[{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"}],"tags":[{"name":"推理","slug":"推理","permalink":"http://yoursite.com/tags/%E6%8E%A8%E7%90%86/"}]},{"title":"Jaccard算法","slug":"Jaccard算法","date":"2023-09-08T03:09:40.000Z","updated":"2023-09-12T06:17:19.711Z","comments":true,"path":"2023/09/08/Jaccard算法/","link":"","permalink":"http://yoursite.com/2023/09/08/Jaccard%E7%AE%97%E6%B3%95/","excerpt":"","text":"杰卡德相似度 选择待比较和比较实体 待比较实体 比较实体 nGql A、B、C D、E、F GET SUBGRAPH 0 to 1 STEPS FROM A、B、C、D、E、F BOTH allRelationTypes YIELD EDGES AS relationships A、B、C ALL GET SUBGRAPH 0 to 3 STEPS FROM A、B、C BOTH allRelationTypes YIELD EDGES AS relationships 筛选出待比较和比较实体的子图，根据边计算节点间相似度，即会过滤掉子图里的独立节点。 使用MinHashLSH计算 假设筛选出来子图，有n个节点，他们邻居如下： row A的邻居 B的邻居 C的邻居 D的邻居 其他点的邻居 0 B A A A 1 C C C 2 D D B C … … … … … … n 在Jaccard计算上，量化算法公式： 它计算的结果会很精确，但是效率很低，因为检查了每一对集合里所有元素，此外，可能点之间相似度很低或者不相似，些检测是”浪费了计算时间”。 min-hash本质是一种降维手段。算法需要在对比集合相似度之前，先将需要对比点i和j的集合Ci、Cj进行一次最小哈希操作来降低集合的维度，得到其哈希签名sig(Ci)、sig（Cj） ，这便是上述定义里所提到的降维功能。 这时候我们再来对比sig(Ci)、sig（Cj） 之间的Jaccard距离即可，由于随机性，min-hash值对应的元素属于两个集合的交集的概率即是，我们可以将min-hash算法的公式总结如下： 1minhash(Ci,Cj)=Jaccard(sig(Ci),sig(Cj)) lsh局部敏感哈希本质是一种最大程度发现相似向量手段。使用min-hash可以降低比较点邻居的维度，但是比较点数量本身可能也包含数百万甚至数十亿个样本的数据集，这个时候找节点对时间复杂度还是。 所以，需要使用lsh方法来减少比较次数。理想情况下，我们只想比较我们认为是潜在匹配项或候选对的向量。lsh函数允许我们对同一样本进行多次分段和哈希处理。当我们发现一对向量至少一次被哈希为相同的值时，我们将它们标记为候选对- 即潜在匹配。 lsh的具体做法是在min-hash所得的哈希签名signature向量的基础上，将每一个向量分为几段，称之为band（即b），每个band包含r行，如果两个向量的其中一个或多个band相同，那么这两个向量就可能相似度较高；相同的band数越多，其相似度高的可能性越大。在任意一个band上被分到同一个桶内的点就互为candidate相似点，这样只需要计算所有candidate点的相似度就可以找到每个点的相似度。 示例：长度为 9 的两signature被分为 b = 3 个带，每个带包含 r = 3 行。每个子向量被散列到 k 个可能的桶之一中。由于第二个带中存在匹配（两个子向量具有相同的哈希值），因此我们将这些签名中的一对视为最近邻居的候选者。如下图： 对b和r的不同值可以构建不同的线图，如下：","categories":[{"name":"图算法","slug":"图算法","permalink":"http://yoursite.com/categories/%E5%9B%BE%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"Jaccard","slug":"Jaccard","permalink":"http://yoursite.com/tags/Jaccard/"}]},{"title":"","slug":"nebulaGraph错误总结","date":"2023-08-03T11:58:25.784Z","updated":"2023-08-04T03:12:53.031Z","comments":true,"path":"2023/08/03/nebulaGraph错误总结/","link":"","permalink":"http://yoursite.com/2023/08/03/nebulaGraph%E9%94%99%E8%AF%AF%E6%80%BB%E7%BB%93/","excerpt":"","text":"Used memory hits the high watermark(0.800000) of total system memory. num_queries_hit_memory_watermark 可以修改一下你的 graphd 里的 这个配置，重启一下 graphd 哈 默认是 0.8 表示 80% ，可以改成 0.9999","categories":[],"tags":[]},{"title":"nebulaGraph使用","slug":"nebulaGraph使用","date":"2023-03-21T07:44:32.000Z","updated":"2023-03-21T09:13:18.125Z","comments":true,"path":"2023/03/21/nebulaGraph使用/","link":"","permalink":"http://yoursite.com/2023/03/21/nebulaGraph%E4%BD%BF%E7%94%A8/","excerpt":"","text":"1、Space语句 https://docs.nebula-graph.com.cn/3.3.0/3.ngql-guide/9.space-statements/1.create-space/ 1.1 新建空间 1234#仅指定 VID 类型，其他选项使用默认值。create space IF NOT EXISTS my_space1(vid_type&#x3D;FIXED_STRING(30));#指定分片数量、副本数量、VID 类型CREATE SPACE IF NOT EXISTS my_space_2 (partition_num&#x3D;15, replica_factor&#x3D;1, vid_type&#x3D;FIXED_STRING(30)); #指定分片数量、副本数量、VID 类型，并添加描述。 CREATE SPACE IF NOT EXISTS my_space_3 (partition_num&#x3D;15, replica_factor&#x3D;1, vid_type&#x3D;FIXED_STRING(30)) comment&#x3D;&quot;测试图空间&quot;; 1.2 克隆空间 12#新建空间4，克隆之空间3CREATE SPACE IF NOT EXISTS my_space_4 as my_space_3; 1.3 查看空间 12345# 全部空间SHOW SPACES;# 查看指定SHOW CREATE SPACE my_space_4;DESC SPACE my_space_4; 1.4 使用空间 1USE SPACE my_space_4; 1.5 清空空间 CLEAR SPACE语句用于清空图空间中的点和边，但不会删除图空间本身以及其中的 Schema 信息。 数据清除后，如无备份，无法恢复。使用该功能务必谨慎。 CLEAR SPACE不是原子性操作。如果执行出错，请重新执行，避免残留数据。 图空间中的数据量越大，CLEAR SPACE消耗的时间越长。如果CLEAR SPACE的执行因客户端连接超时而失败，可以增大 Graph 服务配置中storage_client_timeout_ms参数的值。 CLEAR SPACE不会删除的数据包括： Tag 信息。 Edge type 信息。 原生索引和全文索引的元数据。 1CLEAR SPACE my_space_4; 1.6 删除空间 会删除空间和其中所有信息 1DROP SPACE my_space_4; 执行DROP SPACE语句删除图空间后，为什么磁盘的大小没变化？ 答：如果使用 3.1.0 之前版本的 NebulaGraph, DROP SPACE语句仅删除指定的逻辑图空间，不会删除硬盘上对应图空间的目录和文件。如需删除硬盘上的数据，需手动删除相应文件的路径，文件路径为&lt;nebula_graph_install_path&gt;/data/storage/nebula/&lt;space_id&gt;。其中&lt;space_id&gt;可以通过DESCRIBE SPACE &#123;space_name&#125;查看。 2、Tag语句 nGQL 中的 Tag 和 openCypher 中的 Label 相似，但又有所不同，例如它们的创建方式。 openCypher 中的 Label 需要在CREATE语句中与点一起创建。 nGQL 中的 Tag 需要使用CREATE TAG语句独立创建。Tag 更像是 MySQL 中的表。 2.1 新建标签 尝试使用新创建的 Tag 可能会失败，因为创建是异步实现的。为确保数据同步，后续操作能顺利进行，请等待 2 个心跳周期（20 秒）。 如果需要修改心跳间隔，请为所有配置文件修改参数heartbeat_interval_secs。 123456789CREATE TAG IF NOT EXISTS player(name string, age int);# 创建没有属性的 Tag。CREATE TAG IF NOT EXISTS no_property(); # 创建包含默认值的 Tag。CREATE TAG IF NOT EXISTS player_with_default(name string, age int DEFAULT 20);# 对字段 create_time 设置 TTL 为 100 秒。CREATE TAG IF NOT EXISTS woman(name string, age int, \\ married bool, salary double, create_time timestamp) \\ TTL_DURATION &#x3D; 100, TTL_COL &#x3D; &quot;create_time&quot;; 2.2删除标签 DROP删除所有点的tag 1DROP TAG test; 点可以有一个或多个 Tag。 如果某个点只有一个 Tag，删除这个 Tag 后，用户就无法访问这个点，下次 Compaction 操作时会删除该点，但与该点相邻的边仍然存在——这会造成悬挂边。 如果某个点有多个 Tag，删除其中一个 Tag，仍然可以访问这个点，但是无法访问已删除 Tag 所定义的所有属性。 删除 Tag 操作仅删除 Schema 数据，硬盘上的文件或目录不会立刻删除，而是在下一次 Compaction 操作时删除。 DELETE删除指定点上的指定 Tag。 1234#VID：指定要删除tag的点 ID。DELETE TAG &lt;tag_name_list&gt; FROM &lt;VID&gt;;INSERT VERTEX test1(p1, p2),test2(p3, p4) VALUES &quot;test&quot;:(&quot;123&quot;, 1, &quot;456&quot;, 2);DELETE TAG test1 FROM &quot;test&quot;; 2.3修改标签 尝试使用刚修改的 Tag 可能会失败，因为修改是异步实现的。为确保数据同步，后续操作能顺利进行，请等待 2 个心跳周期（20 秒）。 12345CREATE TAG IF NOT EXISTS t1 (p1 string, p2 int);ALTER TAG t1 ADD (p3 int, p4 string);ALTER TAG t1 TTL_DURATION &#x3D; 2, TTL_COL &#x3D; &quot;p2&quot;;ALTER TAG t1 COMMENT &#x3D; &#39;test1&#39;;ALTER TAG t1 ADD (p5 double NOT NULL DEFAULT 0.4 COMMENT &#39;p5&#39;) COMMENT&#x3D;&#39;test2&#39;; 2.4查看标签 1234#全部show tags;# 指定tagdesc tag tag_name; 3、Edge语句 nGQL 中的 Edge type 和 openCypher 中的关系类型相似，但又有所不同，例如它们的创建方式。 openCypher 中的关系类型需要在CREATE语句中与点一起创建。 nGQL 中的 Edge type 需要使用CREATE EDGE语句独立创建。Edge type 更像是 MySQL 中的表。 3.1 新建边 1234567# 创建没有属性的 Edge type。CREATE EDGE IF NOT EXISTS no_property();# 创建包含默认值的 Edge type。CREATE EDGE IF NOT EXISTS follow_with_default(degree int DEFAULT 20);# 对字段 p2 设置 TTL 为 100 秒。CREATE EDGE IF NOT EXISTS e1(p1 string, p2 int, p3 timestamp) \\ TTL_DURATION &#x3D; 100, TTL_COL &#x3D; &quot;p2&quot;; 3.2 删除边 12CREATE EDGE IF NOT EXISTS e1(p1 string, p2 int);DROP EDGE e1; 确保 Edge type 不包含任何索引，否则DROP EDGE时会报冲突错误[ERROR (-1005)]: Conflict!。删除索引请参见 drop index。 一个边只能有一个 Edge type，删除这个 Edge type 后，用户就无法访问这个边，下次 Compaction 操作时会删除该边。 删除 Edge type 操作仅删除 Schema 数据，硬盘上的文件或目录不会立刻删除，而是在下一次 Compaction 操作时删除。 3.2 修改边 确保要修改的属性不包含索引，否则ALTER EDGE时会报冲突错误[ERROR (-1005)]: Conflict!。删除索引请参见 drop index。 1234CREATE EDGE IF NOT EXISTS e1(p1 string, p2 int);ALTER EDGE e1 ADD (p3 int, p4 string);ALTER EDGE e1 TTL_DURATION &#x3D; 2, TTL_COL &#x3D; &quot;p2&quot;;ALTER EDGE e1 COMMENT &#x3D; &#39;edge1&#39;; 3.2 查看边 1234# 全部SHOW EDGES;# 指定typeDESC[RIBE] EDGE &lt;edge_type_name&gt; 4、Vertex语句 4.1 新增点 123456789# IF NOT EXISTS 仅检测 VID + Tag 的值是否相同，不会检测属性值。# IF NOT EXISTS 会先读取一次数据是否存在，因此对性能会有明显影响。INSERT VERTEX [IF NOT EXISTS] [tag_props, [tag_props] ...]VALUES VID: ([prop_value_list])# 一次插入 2 个点。 INSERT VERTEX t2 (name, age) VALUES &quot;13&quot;:(&quot;n3&quot;, 12), &quot;14&quot;:(&quot;n4&quot;, 8); # 一次插入两个 Tag 的属性到同一个点。 INSERT VERTEX t3 (p1), t4(p2) VALUES &quot;21&quot;: (321, &quot;hello&quot;); 4.2 删除点 1 4.3 修改点 4.4","categories":[],"tags":[]},{"title":"spring框架使用技巧","slug":"spring框架使用技巧-0","date":"2023-03-20T09:09:43.000Z","updated":"2023-03-22T12:31:45.977Z","comments":true,"path":"2023/03/20/spring框架使用技巧-0/","link":"","permalink":"http://yoursite.com/2023/03/20/spring%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7-0/","excerpt":"","text":"1、IOC启动流程 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Overridepublic void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // 准备工作，记录下容器的启动时间、标记“已启动”状态、检验配置文件格式 prepareRefresh(); // 获取 Spring 容器，默认是DefaultListableBeanFactory ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 设置 BeanFactory 的类加载器，添加几个 BeanPostProcessor，手动注册几个特殊的 bean 等 prepareBeanFactory(beanFactory); try &#123; // BeanFactory 准备工作完成后进行的后置处理工作，hook方法 postProcessBeanFactory(beanFactory); //=======以上是 BeanFactory 的预准备工作======= // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory方法 // SpringBoot 会在这里扫描 @Component 注解和进行自动配置 invokeBeanFactoryPostProcessors(beanFactory); // 注册和创建 BeanPostProcessor 的实现类（注意和之前的 BeanFactoryPostProcessor 的区别） registerBeanPostProcessors(beanFactory); // 初始化 MessageSource 组件（做国际化功能；消息绑定，消息解析） initMessageSource(); // 初始化当前 ApplicationContext 的事件广播器 initApplicationEventMulticaster(); // 具体的子类可以在这里初始化一些特殊的 Bean（在初始化 singleton beans 之前），Spring Boot 中新建webServer onRefresh(); // 注册事件监听器，监听器需要实现 ApplicationListener 接口 registerListeners(); // 初始化所有的 singleton beans（lazy-init 的除外） finishBeanFactoryInitialization(beanFactory); // 容器刷新完成操作，会完成webServer启动 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(&quot;Exception encountered during context initialization - &quot; + &quot;cancelling refresh attempt: &quot; + ex); &#125; destroyBeans(); cancelRefresh(ex); throw ex; &#125; finally &#123; resetCommonCaches(); &#125; &#125;&#125; 1.1 obtainFreshBeanFactory DefaultListableBeanFactory 12345678910111213141516171819202122232425// 刷新容器，解析注册xml方式beanDenfinition到BeanFactory中protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; refreshBeanFactory(); return getBeanFactory(); &#125;protected final void refreshBeanFactory() throws BeansException &#123; if (hasBeanFactory()) &#123; destroyBeans(); closeBeanFactory(); &#125; try &#123; DefaultListableBeanFactory beanFactory = createBeanFactory(); beanFactory.setSerializationId(getId()); // 设置是否允许同名覆盖，循环依赖 customizeBeanFactory(beanFactory); // xml定义bean，加载BeanDefinitions loadBeanDefinitions(beanFactory); this.beanFactory = beanFactory; &#125; catch (IOException ex) &#123; throw new ApplicationContextException(&quot;I/O error parsing bean definition source for &quot; + getDisplayName(), ex); &#125; &#125; 1.2 invokeBeanFactoryPostProcessors 整个 invokeBeanFactoryPostProcessors 方法围绕两个接口，BeanDefinitionRegistryPostProcessor 和 BeanFactoryPostProcessor，其中 BeanDefinitionRegistryPostProcessor 继承了 BeanFactoryPostProcessor 。 BeanDefinitionRegistryPostProcessor 主要用来在常规 BeanFactoryPostProcessor 检测开始之前注册其他 Bean 定义，说的简单点，就是 BeanDefinitionRegistryPostProcessor 具有更高的优先级，执行顺序在 BeanFactoryPostProcessor 之前。 123456789101112131415AbstractApplicationContext#refresh() --&gt; AbstractApplicationContext#invokeBeanFactoryPostProcessors() --&gt; PostProcessorRegistrationDelegate#invokeBeanFactoryPostProcessors() --&gt; PostProcessorRegistrationDelegate#invokeBeanDefinitionRegistryPostProcessors() --&gt; //开始，BeanDefinitionRegistryPostProcessor(BeanFactoryPostProcessor)ConfigurationClassPostProcessor#postProcessBeanDefinitionRegistry（）--&gt; ConfigurationClassPostProcessor#processConfigBeanDefinitions() --&gt; ConfigurationClassPostProcessor#parse() --&gt; ConfigurationClassPostProcessor#processConfigurationClass() --&gt;ConfigurationClassParser#parse() --&gt;// 配置注解 @Configuration @Bean @Import @ImportSelector ConfigurationClassParser#doProcessConfigurationClass() --&gt;ComponentScanAnnotationParser#parse() --&gt; // 扫描注解 ClassPathBeanDefinitionScanner#doScan() --&gt; ClassPathScanningCandidateComponentProvider#findCandidateComponents() --&gt; ClassPathScanningCandidateComponentProvider#scanCandidateComponents 加载 xml 配置文件 ​ 发生在基于 xml 配置文件的 ApplicationContext 中 refresh 方法的 BeanFactory 初始化阶段，此时 BeanFactory 刚刚构建完成，它会借助 XmlBeanDefinitionReader 来加载 xml 配置文件，并使用 DefaultBeanDefinitionDocumentReader 解析 xml 配置文件，封装声明的 标签内容并转换为 BeanDefinition 。 解析注解配置类 ​ 发生在 ApplicationContext 中 refresh 方法的 BeanDefinitionRegistryPostProcessor 执行阶段，该阶段首先会执行 ConfigurationClassPostProcessor 的 postProcessBeanDefinitionRegistry 方法。ConfigurationClassPostProcessor 中会找出所有的配置类，排序后依次解析，并借助 ClassPathBeanDefinitionScanner 实现包扫描的 BeanDefinition 封装，借助 ConfigurationClassBeanDefinitionReader 实现 @Bean 注解方法的 BeanDefinition 解析和封装。 编程式构造 ​ BeanDefinition 也是发生在 ApplicationContext 中 refresh 方法的 BeanDefinitionRegistryPostProcessor 执行阶段，由于 BeanDefinitionRegistryPostProcessor 中包含 ConfigurationClassPostProcessor ，而 ConfigurationClassPostProcessor 会执行 ImportBeanDefinitionRegistrar 的逻辑，从而达到编程式构造 BeanDefinition 并注入到 BeanDefinitionRegistry 的目的；另外，实现了 BeanDefinitionRegistryPostProcessor 的类也可以编程式构造 BeanDefinition ，注入 BeanDefinitionRegistry 。 1.3 finishBeanFactoryInitialization 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293protected &lt;T&gt; T doGetBean(String name, @Nullable Class&lt;T&gt; requiredType, @Nullable Object[] args, boolean typeCheckOnly) throws BeansException &#123; // 处理bean的alias String beanName = transformedBeanName(name); Object bean; // Eagerly check singleton cache for manually registered singletons. // 循环依赖的探测 Object sharedInstance = getSingleton(beanName); if (sharedInstance != null &amp;&amp; args == null) &#123; // logger ...... bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); &#125; else &#123; // Fail if we&#x27;re already creating this bean instance: // We&#x27;re assumably within a circular reference. // 如果原型bean之间互相依赖，则一定会引发无限循环，此处会抛出循环依赖的异常 if (isPrototypeCurrentlyInCreation(beanName)) &#123; throw new BeanCurrentlyInCreationException(beanName); &#125; // Check if bean definition exists in this factory. // 如果本地不存在当前bean的定义信息，则尝试让父容器实例化bean // 此举可以保证每个BeanFactory持有它应该有的bean，而不是所有的bean都集中在某一个BeanFactory中 BeanFactory parentBeanFactory = getParentBeanFactory(); if (parentBeanFactory != null &amp;&amp; !containsBeanDefinition(beanName)) &#123; // Not found -&gt; check parent. String nameToLookup = originalBeanName(name); if (parentBeanFactory instanceof AbstractBeanFactory) &#123; return ((AbstractBeanFactory) parentBeanFactory).doGetBean( nameToLookup, requiredType, args, typeCheckOnly); &#125; // else if parentBeanFactory.getBean ...... &#125; // 标记当前bean已经开始被创建了 if (!typeCheckOnly) &#123; markBeanAsCreated(beanName); &#125; try &#123; // 此处会合并BeanDefinition，并检查是否为抽象类(abstract则会抛出无法实例化的异常) RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); checkMergedBeanDefinition(mbd, beanName, args); // Guarantee initialization of beans that the current bean depends on. // 此处会处理 String[] dependsOn = mbd.getDependsOn(); if (dependsOn != null) &#123; for (String dep : dependsOn) &#123; // 循环依赖的检查：如果两个bean互相显式依赖，也会引发循环依赖的问题 if (isDependent(beanName, dep)) &#123; throw new BeanCreationException(mbd.getResourceDescription(), beanName, &quot;Circular depends-on relationship between &#x27;&quot; + beanName + &quot;&#x27; and &#x27;&quot; + dep + &quot;&#x27;&quot;); &#125; registerDependentBean(dep, beanName); try &#123; // 迫切初始化显式依赖的bean getBean(dep); &#125; // catch ...... &#125; &#125; // Create bean instance. // 单实例bean的初始化，最终调用createBean if (mbd.isSingleton()) &#123; sharedInstance = getSingleton(beanName, () -&gt; &#123; try &#123; return createBean(beanName, mbd, args); &#125; // catch ...... &#125;); bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); &#125; // 原型bean的初始化，直接调用createBean else if (mbd.isPrototype()) &#123; // It&#x27;s a prototype -&gt; create a new instance. Object prototypeInstance = null; try &#123; beforePrototypeCreation(beanName); prototypeInstance = createBean(beanName, mbd, args); &#125; finally &#123; afterPrototypeCreation(beanName); &#125; bean = getObjectForBeanInstance(prototypeInstance, name, beanName, mbd); &#125; // 处理自定义的scope ...... &#125; // catch ...... &#125; // 类型强转前的检查 ...... return (T) bean;&#125; 12345678910111213141516171819202122232425262728293031protected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; // Spring首先从singletonObjects（一级缓存）中尝试获取 Object singletonObject = this.singletonObjects.get(beanName); // 若是获取不到而且对象在建立中，则尝试从earlySingletonObjects(二级缓存)中获取 if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; synchronized (this.singletonObjects) &#123; singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; //若是仍是获取不到而且允许从singletonFactories经过getObject获取，则经过singletonFactory.getObject()(三级缓存)获取 singletonObject = singletonFactory.getObject(); //若是获取到了则将singletonObject放入到earlySingletonObjects,也就是将三级缓存提高到二级缓存中 this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; return (singletonObject != NULL_OBJECT ? singletonObject : null);&#125;//一级缓存，存放创建完成，初始完成的bean/** Cache of singleton objects: bean name to bean instance. */private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;&gt;(256);//三级缓存，存放bean工厂/** Cache of singleton factories: bean name to ObjectFactory. */private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap&lt;&gt;(16);//二级缓存（？），循环依赖中用到，三级缓存创建bean，此时刚创建完，值没有设置/** Cache of early singleton objects: bean name to bean instance. */private final Map&lt;String, Object&gt; earlySingletonObjects = new ConcurrentHashMap&lt;&gt;(16); Spring 一开始提前暴露的并不是实例化的 Bean，而是将 Bean 包装起来的 ObjectFactory。为什么要这么做呢？ 这实际上涉及到 AOP，如果创建的 Bean 是有代理的，那么注入的就应该是代理 Bean，而不是原始的 Bean。但是 Spring 一开始并不知道 Bean 是否会有循环依赖，通常情况下（没有循环依赖的情况下），Spring 都会在完成填充属性，并且执行完初始化方法之后再为其创建代理。但是，如果出现了循环依赖的话，Spring 就不得不为其提前创建代理对象，否则注入的就是一个原始对象，而不是代理对象。所以Spring 需要三级缓存的目的是为了在没有循环依赖的情况下，延迟代理对象的创建，使 Bean 的创建符合 Spring 的设计原则。 2、IOC的扩展点 扩展接口 作用 BeanFactoryPostProcessor 修改/新增容器中的 BeanDefinition BeanDefinitionRegistryPostProcessor 可以添加自定义的Bean BeanPostProcessor 支持在 Bean 初始化前后，对 Bean 进行处理 initializingBean 在 Bean实例化完成，所有属性注入完成之后执行 DisposableBean 在 Bean 销毁前执行 Aware 接口族 获得 Spring 容器资源 FactoryBean 复杂 Bean 注入 ApplicationListener 监听响应容器事件 2.1 BeanFactoryPostProcessor 它是在refresh方法的invokeBeanFactoryPostProcessors方法里调用的。 修改BeanDefinition 123456789101112131415161718192021@Componentpublic class MyBeanFactoryPostProcessor implements BeanFactoryPostProcessor &#123; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; System.out.println(&quot;IOC 容器调用了MyBeanFactoryPostProcessor中重写的postProcessBeanFactory方法&quot;); // 获取Bean定义名称，进行遍历 for(String name:beanFactory.getBeanDefinitionNames()) &#123; // 如果是person对象 if(&quot;person&quot;.equals(name)) &#123; System.out.println(&quot;将Person对象设置为懒加载&quot;); // 获取person的BeanDefinition(Bean定义对象) BeanDefinition beanDefinition = beanFactory.getBeanDefinition(name); // 将Person对象设置为懒加载 beanDefinition.setLazyInit(true); //修改属性的值 beanDefinition.getPropertyValues().add(&quot;userName&quot;,&quot;Tom&quot;); &#125; &#125; &#125;&#125; 加载BeanDefinition springBoot 中通过ConfigurationClassPostProcessor后置处理器加载注册了Java配置类、注解扫描方式的bean的BeanDefinitions到BeanFactory中。 2.2 BeanDefinitionRegistryPostProcessor 12345678910111213141516171819202122232425262728@Slf4j@Componentpublic class PersonBeanDefinitionRegistryPostProcessor implements BeanDefinitionRegistryPostProcessor &#123; @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException &#123; // 注册Bean定义，容器根据定义返回bean log.info(&quot;开始在注册自定义beanDefinition到容器&quot;); //构造bean定义 BeanDefinitionBuilder beanDefinitionBuilder = BeanDefinitionBuilder .genericBeanDefinition(UserInfoVO.class); //设置依赖 beanDefinitionBuilder.addPropertyReference(&quot;MyUser&quot;, &quot;myUser&quot;); BeanDefinition personManagerBeanDefinition = beanDefinitionBuilder .getRawBeanDefinition(); //注册bean定义 registry.registerBeanDefinition(&quot;myUser&quot;, personManagerBeanDefinition); log.info(&quot;结束在注册自定义beanDefinition到容器&quot;); &#125; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; BeanDefinition myUser = beanFactory.getBeanDefinition(&quot;myUser&quot;); myUser.getPropertyValues().add(&quot;name&quot;, &quot;hf&quot;); &#125;&#125; FactoryBean InitializingBean、ApplicationContextAware、DisposableBean、@PostConstruct、@PreDestroy注解 PropertySourceLoader ApplicationContextInitializer EnvironmentPostProcessor ApplicationRunner和CommandLineRunner BeanFactoryAware ApplicationContextAwareProcessor BeanNameAware SmartInitializingSingleton DisposableBean ApplicationListener 3、WebMvc扩展点 HandlerMapping HandlerInterceptor HandlerAdapter HandlerMethodArgumentResolver Converter ViewResolver HandlerExceptionResolver WebMvcConfigurer 4、自动装配 5、编写自己starter","categories":[],"tags":[]},{"title":"Spring框架使用技巧","slug":"Spring框架使用技巧","date":"2023-03-13T05:37:16.000Z","updated":"2023-03-17T03:25:30.709Z","comments":true,"path":"2023/03/13/Spring框架使用技巧/","link":"","permalink":"http://yoursite.com/2023/03/13/Spring%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/","excerpt":"","text":"1、IOC启动流程 refresh方法一共分成12个步骤，每个步骤解释如下： prepareRefresh() 容器启动前的准备工作 obtainFreshBeanFactory() 告诉子类刷新内部Bean工厂,解析Bean并注册到容器中（此时还没有初始化） prepareBeanFactory(beanFactory) 准备BeanFactory，设置beanFactory类加载器，添加多个beanPostProcesser postProcessBeanFactory(beanFactory) 允许子类上下文中对beanFactory做后期处理 invokeBeanFactoryPostProcessors(beanFactory) 调用BeanFactoryPostProcessor各个实现类的方法 registerBeanPostProcessors(beanFactory) 注册 BeanPostProcessor 的实现类 initMessageSource() 初始化ApplicationContext的MessageSource initApplicationEventMulticaster() 初始化事件广播 onRefresh() 初始化子类特殊bean registerListeners() 注册事件监听器 finishBeanFactoryInitialization(beanFactory) 初始化所有singleton bean finishRefresh() 广播事件，ApplicationContext初始化完成 总结：1 为准备环境；2-6 为准备 BeanFactory；7-12 为准备 ApplicationContext（其中11 为初始化 BeanFactory 中非延迟单例 bean） 1.1 prepareRefresh 1234//留给子类覆盖，初始化属性资源protected void initPropertySources() &#123; // For subclasses: do nothing by default. &#125; 1.2 obtainFreshBeanFactory 1 2、WebMvc可扩展点 3、自动装配 4、编写starter","categories":[],"tags":[]},{"title":"","slug":"Untitled","date":"2023-02-08T11:48:40.954Z","updated":"2023-02-08T12:07:40.925Z","comments":true,"path":"2023/02/08/Untitled/","link":"","permalink":"http://yoursite.com/2023/02/08/Untitled/","excerpt":"","text":"图谱信息列表 不需要鉴权，直接传参数调用就行。 接口地址 /kbstudio/thirdApi/graphInfoList 请求方式 GET 接口描述 `` 请求参数 参数名称 参数说明 是否必须 数据类型 tenantId 租户id true integer 响应状态 状态码 说明 20000 请求成功 40001 参数缺失，例如tenantId为空 50001 服务器内部错误 响应格式示例 1234567891011&#123; &quot;data&quot;:&#123; &quot;graphCount&quot;:9, //所有图谱总个数 &quot;entityLabelCount&quot;:34,//所有图谱实体标签个数 &quot;relationTypeCount&quot;:20,//所有图谱关系类型个数 &quot;entityInstanceCount&quot;:22222,// 所有图谱实体实例个数 &quot;relationInstanceCount&quot;:43423432 //所有图谱关系实例个数 &#125;, &quot;return_code&quot;:20000, &quot;return_msg&quot;:null&#125;","categories":[],"tags":[]},{"title":"扫描pdf添加类目","slug":"扫描pdf添加类目","date":"2023-01-17T02:39:51.000Z","updated":"2023-01-17T03:20:00.807Z","comments":true,"path":"2023/01/17/扫描pdf添加类目/","link":"","permalink":"http://yoursite.com/2023/01/17/%E6%89%AB%E6%8F%8Fpdf%E6%B7%BB%E5%8A%A0%E7%B1%BB%E7%9B%AE/","excerpt":"","text":"往往下载的扫描版本的pdf是没有可以跳转的目录的，因此需要给扫描pdf添加可以跳转的目录，方便查阅。 下载：FreePic2PDF 链接：https://pan.baidu.com/s/1nCfdE5Sv3pu2wiFugX_GUQ 提取码：fhds 提取目录 打开软件FreePic2PDF，点击右下角“更改pdf” 选择“从PDF取书签”→选择你的pdf文件→点击开始 找到生成的文件 修改txt文件 从豆瓣拷贝扫描pdf的目录到txt文件中，使用正则格式化目录。 1234&#x2F;&#x2F; 一级目录^(\\d&#123;1,2&#125;\\.\\d) 替换为 \\t\\1&#x2F;&#x2F; 二级目录^(\\s\\d&#123;1,2&#125;\\.\\d\\.\\d) 替换为 \\t\\t\\1 挂载已经修改的txt 可以定义basePage页码再FreePic2Pdf.itf文件中","categories":[{"name":"pdf","slug":"pdf","permalink":"http://yoursite.com/categories/pdf/"}],"tags":[{"name":"添加类目","slug":"添加类目","permalink":"http://yoursite.com/tags/%E6%B7%BB%E5%8A%A0%E7%B1%BB%E7%9B%AE/"}]},{"title":"ALGO系列-相似度算法","slug":"ALGO系列-相似度算法","date":"2023-01-16T05:37:16.000Z","updated":"2023-01-16T05:37:16.131Z","comments":true,"path":"2023/01/16/ALGO系列-相似度算法/","link":"","permalink":"http://yoursite.com/2023/01/16/ALGO%E7%B3%BB%E5%88%97-%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%AE%97%E6%B3%95/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"ALGO系列-中心性算法","slug":"ALGO系列-中心性算法","date":"2023-01-16T05:37:04.000Z","updated":"2023-01-16T05:37:04.296Z","comments":true,"path":"2023/01/16/ALGO系列-中心性算法/","link":"","permalink":"http://yoursite.com/2023/01/16/ALGO%E7%B3%BB%E5%88%97-%E4%B8%AD%E5%BF%83%E6%80%A7%E7%AE%97%E6%B3%95/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"ALGO系列-社区检测算法","slug":"ALGO系列-社区检测算法","date":"2023-01-16T05:36:46.000Z","updated":"2023-01-16T05:36:46.789Z","comments":true,"path":"2023/01/16/ALGO系列-社区检测算法/","link":"","permalink":"http://yoursite.com/2023/01/16/ALGO%E7%B3%BB%E5%88%97-%E7%A4%BE%E5%8C%BA%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"ALGO系列-路径寻找算法（续）","slug":"ALGO系列-路径寻找算法（续）","date":"2023-01-16T05:36:26.000Z","updated":"2023-01-16T05:36:26.426Z","comments":true,"path":"2023/01/16/ALGO系列-路径寻找算法（续）/","link":"","permalink":"http://yoursite.com/2023/01/16/ALGO%E7%B3%BB%E5%88%97-%E8%B7%AF%E5%BE%84%E5%AF%BB%E6%89%BE%E7%AE%97%E6%B3%95%EF%BC%88%E7%BB%AD%EF%BC%89/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"ALGO系列-路径寻找算法","slug":"ALGO系列-路径寻找算法","date":"2023-01-16T05:36:11.000Z","updated":"2023-01-16T10:38:54.110Z","comments":true,"path":"2023/01/16/ALGO系列-路径寻找算法/","link":"","permalink":"http://yoursite.com/2023/01/16/ALGO%E7%B3%BB%E5%88%97-%E8%B7%AF%E5%BE%84%E5%AF%BB%E6%89%BE%E7%AE%97%E6%B3%95/","excerpt":"","text":"1、定义 将要对图数据库中的什么进行遍历；一般从指定的一个或一组起始节点开始，根据过滤规则沿着特定关系依次访问其他相连节点的过程。该过程迭代执行，直到没有更多相连节点或者预设的结束条件满足时终止。路径扩展可以看作是图的遍历(Graph Traversal)的一种实现方式。 1.1 几个主要参数说明 labelFilter：标签过滤器 语法：[+-/&gt;] Label1,Label2...。 多个用逗号分隔。 输入 结果 NULL 默认，所有标签节点都会过滤 -Label 黑名单节点标签，遍历路径中排除这些节点 +Label 白名单，遍历路径中包含那些节点标签再次白名单的路径（不包括终止或结束标签）默认情况，如果白名单没有定义，则所有标签都被视为列入白名单。 /Label 终止过滤器，仅返回拥有给定标签的节点路径，并停止进一步遍历。终止过滤器规则优先于白名单过滤器。 &gt;Label * relationshipFilter：关系过滤器 sequence：标签和关系序列 uniqueness：唯一性规则 2、扩展过程 123456789101112131415161718192021CREATE (n1:&#96;人物&#96;:&#96;皇帝&#96;:&#96;文臣&#96;:&#96;武将&#96; &#123;name: &#39;刘备&#39;&#125;) -[:兄长]-&gt; (n2:&#96;人物&#96;:&#96;武将&#96; &#123;name: &#39;关羽&#39;&#125;), (n2) -[:兄长]-&gt; (n3:&#96;人物&#96;:&#96;武将&#96; &#123;name: &#39;张飞&#39;&#125;), (n1) -[:兄长]-&gt; (n3), (n1) -[:主公]-&gt; (n4:&#96;人物&#96;:&#96;武将&#96; &#123;name: &#39;赵云&#39;&#125;), (n1) -[:父子]-&gt; (n5:&#96;人物&#96;:&#96;皇帝&#96;:&#96;文臣&#96; &#123;name: &#39;刘禅&#39;&#125;), (n1) -[:主公]-&gt; (n6:&#96;人物&#96;:&#96;文臣&#96; &#123;name: &#39;诸葛亮&#39;&#125;), (n5) -[:主公]-&gt; (n4), (n5) -[:主公]-&gt; (n6), (a1:&#96;朝代&#96;&#123;name:&#39;蜀汉&#39;&#125;) -[:对手]-&gt; (a2:&#96;朝代&#96;&#123;name:&#39;曹魏&#39;&#125;), (a3:&#96;朝代&#96;&#123;name:&#39;西晋&#39;&#125;) -[:取代]-&gt; (a2), (m1:&#96;人物&#96;:&#96;文臣&#96; &#123;name: &#39;曹操&#39;&#125;) -[:父子]-&gt; (m2:&#96;人物&#96;:&#96;文臣&#96;:&#96;皇帝&#96; &#123;name: &#39;曹丕&#39;&#125;), (m1) -[:父子]-&gt; (m3:&#96;人物&#96;:&#96;文臣&#96; &#123;name: &#39;曹植&#39;&#125;), (m2) -[:兄长]-&gt; (m3), (m1) -[:主公]-&gt; (m4:&#96;人物&#96;:&#96;文臣&#96; &#123;name: &#39;司马懿&#39;&#125;), (m4) -[:父子]-&gt; (m5:&#96;人物&#96;:&#96;文臣&#96; &#123;name: &#39;司马昭&#39;&#125;), (m5) -[:父子]-&gt; (m6:&#96;人物&#96;:&#96;文臣&#96;:&#96;皇帝&#96; &#123;name: &#39;司马炎&#39;&#125;), (n1) -[:建立&#123;year:221&#125;]-&gt; (a1), (m2) -[:建立&#123;year:220&#125;]-&gt; (a2), (m6) -[:建立&#123;year:266&#125;]-&gt; (a3) 2.1 从给你节点出发寻找扩展路径，支持基本配置 123456&#x2F;&#x2F; startNode:Long 节点id 或者 节点对象；relationShipFilter：关系过滤器；labelFilter：标签过滤器；minLevel：最小遍历层次数；maxLevel：最大遍历层次数（-1 不限制）apoc.path.expand(startNode &lt;id&gt;|Node|list, &#39;relationShipFilter&#39;, &#39;labelFilter&#39;, minLevel, maxLevel ) yield path&#x2F;&#x2F; 从&#39;蜀汉&#39;节点开始， match(n:&#96;朝代&#96;&#123;name:&#39;蜀汉&#39;&#125;) call apoc.path.expand(n,null,&#39;-朝代&#39;,0,-1) yield path return path; 2.2从给定节点出发寻找扩展路径，支持复杂配置 1apoc.path.expandConfig(startNode &lt;id&gt;|Node|list, &#123;minLevel,maxLevel,uniqueness,relationshipFilter,labelFilter,uniqueness:&#39;RELATIONSHIP_PATH&#39;,bfs:true, filterStartNode:false, limit:-1, optional:false, endNodes:[], terminatorNodes:[], sequence, beginSequenceAtStart:true&#125;) yield path 2.3从给定节点出发寻找能到达的所有节点，并返回这些节点 1apoc.path.subgraphNodes(startNode &lt;id&gt;|Node|list, &#123;maxLevel,relationshipFilter,labelFilter,bfs:true, filterStartNode:false, limit:-1, optional:false, endNodes:[], terminatorNodes:[], sequence, beginSequenceAtStart:true&#125;) yield node 2.4从给定节点出发寻找所能到达的多有节点，并返回这些节点以及对应的关系 1apoc.path.subgraphAll(startNode &lt;id&gt;|Node|list, &#123;maxLevel,relationshipFilter,labelFilter,bfs:true, filterStartNode:false, limit:-1, endNodes:[], terminatorNodes:[], sequence, beginSequenceAtStart:true&#125;) yield nodes, relationships 2.5从给定节点出发寻找最小生成树，返回熊其实节点到树中每个节点的路径 1apoc.path.spanningTree(startNode &lt;id&gt;|Node|list, &#123;maxLevel,relationshipFilter,labelFilter,bfs:true, filterStartNode:false, limit:-1, optional:false, endNodes:[], terminatorNodes:[], sequence, beginSequenceAtStart:true&#125;) yield path","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"neo4j","slug":"neo4j","permalink":"http://yoursite.com/tags/neo4j/"}]},{"title":"APOC系列-查询执行管理","slug":"APOC系列-查询执行管理","date":"2023-01-16T05:34:59.000Z","updated":"2023-01-16T05:34:59.791Z","comments":true,"path":"2023/01/16/APOC系列-查询执行管理/","link":"","permalink":"http://yoursite.com/2023/01/16/APOC%E7%B3%BB%E5%88%97-%E6%9F%A5%E8%AF%A2%E6%89%A7%E8%A1%8C%E7%AE%A1%E7%90%86/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"APOC系列-并行节点查询","slug":"APOC系列-并行节点查询","date":"2023-01-16T05:34:45.000Z","updated":"2023-01-16T05:34:46.005Z","comments":true,"path":"2023/01/16/APOC系列-并行节点查询/","link":"","permalink":"http://yoursite.com/2023/01/16/APOC%E7%B3%BB%E5%88%97-%E5%B9%B6%E8%A1%8C%E8%8A%82%E7%82%B9%E6%9F%A5%E8%AF%A2/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"APOC系列-重构/优化图","slug":"APOC系列-重构-优化图","date":"2023-01-16T05:34:30.000Z","updated":"2023-01-16T05:34:30.261Z","comments":true,"path":"2023/01/16/APOC系列-重构-优化图/","link":"","permalink":"http://yoursite.com/2023/01/16/APOC%E7%B3%BB%E5%88%97-%E9%87%8D%E6%9E%84-%E4%BC%98%E5%8C%96%E5%9B%BE/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"APOC系列-数据导出","slug":"APOC系列-数据导出","date":"2023-01-16T05:34:19.000Z","updated":"2023-02-01T11:10:47.603Z","comments":true,"path":"2023/01/16/APOC系列-数据导出/","link":"","permalink":"http://yoursite.com/2023/01/16/APOC%E7%B3%BB%E5%88%97-%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA/","excerpt":"","text":"1、导入导出csv 1234CALL apoc.export.csv.query(&quot;MATCH (n)-[r]-&gt;(m) WHERE n.property &#x3D; &#39;value&#39; RETURN n,r,m&quot;, &quot;subgraph.csv&quot;, &#123;&#125;)LOAD CSV WITH HEADERS FROM &quot;file:&#x2F;&#x2F;&#x2F;subgraph.csv&quot; AS rowCREATE (:Label &#123; property: row.n_property &#125;)-[:RELTYPE &#123; property: row.r_property &#125;]-&gt;(:Label &#123; property: row.m_property &#125;) APOC (Awesome Procedures on Cypher)是一个插件，为neo4j图数据库提供额外的便捷的Cypher语法和功能，其中包括图操作、数据导入/导出、数据清理和管理等。 你可以使用APOC库中的 apoc.export.cypher.query 和 apoc.load.cypher 函数来完成将虚图持久化为neo4j的过程。 例如，以下Cypher语句可以将查询到的虚图重新持久化到neo4j中，并将实体标签重新命名为在原标签名称结尾加上“_new”： 12345678apoc.export.cypher.query(&quot;MATCH (n) RETURN n&quot;, &quot;&#x2F;tmp&#x2F;virtual_graph.cyp&quot;)YIELD file, nodes, relationships, propertiesCALL apoc.load.cypher(&quot;&#x2F;tmp&#x2F;virtual_graph.cyp&quot;, &#123; nodeLabels: &quot;n:Label1_new&quot;, relationshipType: &quot;RELATIONSHIP_new&quot;&#125;)YIELD nodes, relationships, properties, batchSize, timeTaken 请注意，这是一个示例，您可能需要根据您的特定需求进行调整。","categories":[{"name":"图数据库","slug":"图数据库","permalink":"http://yoursite.com/categories/%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"neo4j","slug":"neo4j","permalink":"http://yoursite.com/tags/neo4j/"}]},{"title":"APOC系列-虚拟图","slug":"APOC系列-虚拟图","date":"2023-01-16T05:34:11.000Z","updated":"2023-01-16T05:34:11.489Z","comments":true,"path":"2023/01/16/APOC系列-虚拟图/","link":"","permalink":"http://yoursite.com/2023/01/16/APOC%E7%B3%BB%E5%88%97-%E8%99%9A%E6%8B%9F%E5%9B%BE/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"APOC系列-数据集成","slug":"APOC系列-数据集成","date":"2023-01-16T05:34:02.000Z","updated":"2023-01-16T05:34:02.567Z","comments":true,"path":"2023/01/16/APOC系列-数据集成/","link":"","permalink":"http://yoursite.com/2023/01/16/APOC%E7%B3%BB%E5%88%97-%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"APOC系列-Cypher查询","slug":"APOC系列-Cypher查询","date":"2023-01-16T05:33:42.000Z","updated":"2023-01-16T05:33:42.455Z","comments":true,"path":"2023/01/16/APOC系列-Cypher查询/","link":"","permalink":"http://yoursite.com/2023/01/16/APOC%E7%B3%BB%E5%88%97-Cypher%E6%9F%A5%E8%AF%A2/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"APOC系列-数据加载","slug":"APOC系列-数据加载","date":"2023-01-16T05:33:26.000Z","updated":"2023-01-16T05:33:26.897Z","comments":true,"path":"2023/01/16/APOC系列-数据加载/","link":"","permalink":"http://yoursite.com/2023/01/16/APOC%E7%B3%BB%E5%88%97-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"APOC系列-图的遍历","slug":"APOC系列-图的遍历","date":"2023-01-16T05:33:16.000Z","updated":"2023-01-16T05:33:16.255Z","comments":true,"path":"2023/01/16/APOC系列-图的遍历/","link":"","permalink":"http://yoursite.com/2023/01/16/APOC%E7%B3%BB%E5%88%97-%E5%9B%BE%E7%9A%84%E9%81%8D%E5%8E%86/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"APOC系列-地理空间函数","slug":"APOC系列-地理空间函数","date":"2023-01-16T05:32:48.000Z","updated":"2023-01-16T05:32:48.211Z","comments":true,"path":"2023/01/16/APOC系列-地理空间函数/","link":"","permalink":"http://yoursite.com/2023/01/16/APOC%E7%B3%BB%E5%88%97-%E5%9C%B0%E7%90%86%E7%A9%BA%E9%97%B4%E5%87%BD%E6%95%B0/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"apoc使用","slug":"apoc使用","date":"2023-01-04T10:19:54.000Z","updated":"2023-01-11T07:09:51.321Z","comments":true,"path":"2023/01/04/apoc使用/","link":"","permalink":"http://yoursite.com/2023/01/04/apoc%E4%BD%BF%E7%94%A8/","excerpt":"","text":"http://we-yun.com/apoc/index34.html#_rename neo4 3.0 开始引入用户定义的从存储过程（非常类型关系型数据库的存储过程）： Java实现 可以在neo4j数据库启动时加载 可以方便的在cypher中调用 实现cypher很难实现的功能 APOC 包含300+强大函数和过程 最早表示&quot;A package of components&quot; 后来表示“Awesome procedures on cypher” 文本和查找索引 1CALL apoc.index.* Neo4j使用Lucene库来进行文本处理 文本索引过程用来对属性的文本内容进行NLP处理和创建索引 支持快速对节点和关系属性值进行全文查询 手工索引方式，需要随数据更新而定期更新 如果加载中文分词库，也能实现中文文本的索引 功能函数 1CALL apoc.text.* &#x2F;date.* &#x2F;number.* 字符串处理 时间戳 数字类型 日期 科学计数 图论算法 社区检测和社团划分（Community Detection） 1CALL apoc.algo.community() 标签传播 可自定义迭代层数和权重 对网络实施分区（partition） 路径扩展和图的遍历 1CALL apoc.path.* 广度优先和无重复的关系路径 可自定义遍历规则：起始节点、层级、包含关系及方向等等 按照节点类型进行过滤：排除(blacklist)、终止(ter)、结束(end)、包含(whitelist) 最大节点和关系数限制 子图遍历 生成树（spanning tree）遍历 中心性（Centrality algorithm） 1CALL apoc.algo.closeness()&#x2F;betweeness 紧密中心性（Closeness Centrality） 间接中心性（Betweenness Centrality） 计算节点在网络中的核心程度 发现社交网络中重要人物 发现诈骗团伙中的核心/老大 页面排序（Page rank） 1CALL apoc.algo.pagerank) 用来计算节点在整个网络的重要性 可以指定参与计算的节点 目前计算是基于全网的所有指定节点 地理空间函数 1CALL apoc.spatial.* 根据地址返回全球坐标 计算直线距离 按照距离远近排序节点 数据集成 1CALL apoc.load.* 加载JSON数据，调用RESTful API 加载关系型数据库数据，通过JDBC 流式化数据到Gephi 集成Elastic Search 加载XML文档 Cypher查询 1CALL apoc.cypher.^ 可以在apoc中调用cypher 好处 可以动态构建查询语句 控制查询的执行时间 条件化查询分支：when，case 灵活查询执行任务：批次查询、并行查询 虚拟图 1CALL apoc.create.* apoc 支持构建虚拟的节点和关系，从而构建虚拟的路径和子图 虚拟图类似关系型数据库的视图的概念，它可以被查询并放回数据，但是并不在物理存储z数据库中 虚拟图使得默写查询的更加灵活和高效 创建数据库中并不存在的节点和关系 缩小查询的相关子图的规模 控制遍历路径 虚拟节点的和关系ID都是负数 内存管理 重构和优化图 1CALL apoc.refactor.* 对已有的图进行转换操作，实现重构（Refactoring） 复制节点及其属性，包括不包括关系 合并节点 重建关系到性的节点 改变关系类型 将关系转换成节点 将节点转换为关系 将属性转换成分类的节点和相关节点建立 的关系 并行节点查询 1CALL apoc.search.* 在可能的情况下并行查询节点 结果可以是全部匹配的节点或者去除重复的节点 可以使用JSON格式定义要查询的节点的属性集 支持多种匹配类型，&gt;,&lt;, = , &lt;&gt;, &lt;= ,&gt;=, =~ 其他数据库特性 触发器(Trigger) 写入锁 显示数据库元数据（metedata） 数据轮廓（data profiling） 管理索引和限制 对节点和关系并发操作的支持：原子性 https://neo4j.com/blog/import-10m-stack-overflow-questions/ 1、apoc 安装配置 jar包下载：https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases/ 下载apoc jar包，放到$NEO4J_HOME/plugins目录下 修改$NEO4J_HOME/conf/neo4j.conf文件，添加如下配置（algo是算法包的配置，这里一起加进来了，不需要的去掉即可） 12345dbms.security.procedures.unrestricted=apoc.*,algo.*apoc.import.file.enabled=trueapoc.export.file.enabled=true##下面这条配置是可选的，表示使用neo4j的配置，比如导入数据的路径##apoc.import.file.use_neo4j_config=true 查看函数和过程 123call apoc.help(&#39;apoc&#39;);call dbms.procedures();call dbms.functions(); 2、改名 函数 描述 call apoc.refactor.rename.label(oldLabel, newLabel, [nodes]) rename a label from ‘oldLabel’ to ‘newLabel’ for all nodes. If ‘nodes’ is provided renaming is applied to this set only call apoc.refactor.rename.type(oldType, newType, [rels]) rename all relationships with type ‘oldType’ to ‘newType’. If ‘rels’ is provided renaming is applied to this set only call apoc.refactor.rename.nodeProperty(oldName, newName, [nodes]) rename all node’s property from ‘oldName’ to ‘newName’. If ‘nodes’ is provided renaming is applied to this set only call apoc.refactor.rename.typeProperty(oldName, newName, [rels]) rename all relationship’s property from ‘oldName’ to ‘newName’. If ‘rels’ is provided renaming is applied to this set only 123456&#x2F;&#x2F; 节点call apoc.refactor.rename.label(&quot;xiyou&quot;, &quot;xiyou2&quot;);&#x2F;&#x2F; 关系call apoc.refactor.rename.type(&quot;rel&quot;, &quot;rel2&quot;) 3、mysql导入 1call apoc.periodic.iterate(&#39;call apoc.load.jdbc(&quot;jdbc:mysql:&#x2F;&#x2F;10.80.73.104:3306&#x2F;db_kbstudio?user&#x3D;root&amp;password&#x3D;root&quot;,&quot;t_entity_online&quot; ) yield row&#39;,&#39;create (s:t_entity_online) set s&#x3D;row&#39;,&#123;batchSize:10000,parallel:true&#125;)","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"neo4j","slug":"neo4j","permalink":"http://yoursite.com/tags/neo4j/"}]},{"title":"Cypher语法","slug":"Cypher语法","date":"2023-01-04T03:05:15.000Z","updated":"2023-01-18T08:54:15.627Z","comments":true,"path":"2023/01/04/Cypher语法/","link":"","permalink":"http://yoursite.com/2023/01/04/Cypher%E8%AF%AD%E6%B3%95/","excerpt":"","text":"https://crazyyanchao.github.io/blog/2020/06/16/常用CYPHER查询-二.html 1、基本用法 1.1 字符画语法 Cypher 使用字符画来表示模式，使得我们在第一次学习这门语言时很容易记住它。如果你忘记了如何编写，只需要想一想图的样子就会对你有所帮助。 1(a)-[:KNOWS]-&gt;(b) 主要记住如下几点： 节点由圆括号表示，看起来像是圆圈。就像这样：(node) 关系用箭头来表示。像这样： -&gt; 关系相关的信息可以插入到方括号中。像这样：[:KNOWS] 1.2 定义数据 在使用 Cypher 时请记住以下几点： 节点通常有标签（一个或多个）。比如：”Person”,”User”,”Actor”,”Customer”,”Employee”等 节点通常有属性，属性提供有关节点的额外信息。比如：”name”,”age”,”born”等 关系也可以有属性 关系通常有一个类型（类似于节点的标签）。比如：”KNOWS”,”LIKES”,”WORKS_FOR”,”PURCHASED”等 让我们再来看一下上边的例子： 12MATCH (p:Person &#123; name:&quot;Homer Flinstone&quot; &#125;)RETURN p 我们可以看到： 节点被圆括号 () 包围 Person 是节点的标签 name 是节点的属性 1.3 新建节点 1CREATE (a:Artist &#123; name : &quot;筷子兄弟&quot; &#125;) 1.4 展示节点 CREATE 语句创建节点但是不展示节点。为了展示节点我们需要在它后边跟上 RETURN 语句。 我们来创建另一个节点，这次我们创建一个专辑，与之前不同的是这次我们在后边跟上 RETURN 语句 12CREATE (b:Album &#123; name : &quot;猛龙过江&quot;, released : &quot;2014&quot; &#125;)RETURN b 1.5 创建多个节点 我们可以通过用逗号分隔来一次性创建多个节点： 12CREATE (a:Album &#123; name: &quot;我们是太阳&quot;&#125;), (b:Album &#123; name: &quot;小水果&quot;&#125;) RETURN a,b 或者可以使用多个 CREATE 语句： 123CREATE (a:Album &#123; name: &quot;我们是太阳&quot;&#125;) CREATE (b:Album &#123; name: &quot;小水果&quot;&#125;) RETURN a,b 接下来，我们将在节点间建立关系。 1.6 创建关系 1234MATCH (a:Artist),(b:Album)WHERE a.name &#x3D; &quot;筷子兄弟&quot; AND b.name &#x3D; &quot;猛龙过江&quot;CREATE (a)-[r:RELEASED]-&gt;(b)RETURN r 首先我们使用 MATCH 语句查找我们要创建关系的两个点。 可能有很多节点带有 Artist 或 Album 标签，所以我们需要找到我们感兴趣的节点。在这个例子中，我们使用属性值来过滤它：使用之前赋值给每个节点的 name 属性。 接下来是用来创建关系的 CREATE 语句，在这个例子中，它通过我们在第一行中给出的变量名称（a 和 b）来引用两个节点，关系是通过字符画模式，用箭头指示关系方向来建立的：(a)-[r:RELEASED]-&gt;(b)。 我们给这个关系一个变量名 r 并且给了一个 RELEASE 类型（乐队发行专辑）。关系类型和节点的标签概念类似。 1.7 添加更多关系 1CREATE (p:Person &#123; name: &quot;王太利&quot; &#125;) 现在来创建关系并返回图： 1234MATCH (a:Artist),(b:Album),(p:Person)WHERE a.name &#x3D; &quot;筷子兄弟&quot; AND b.name &#x3D; &quot;猛龙过江&quot; AND p.name &#x3D; &quot;王太利&quot; CREATE (p)-[:PRODUCED]-&gt;(b), (p)-[:PERFORMED_ON]-&gt;(b), (p)-[:PLAYS_IN]-&gt;(a)RETURN a,b,p 1.8 创建索引 在 Neo4j 中，你可以给有标签的点的任何属性创建索引。一旦你创建了一个索引，Neo4j 将会管理它，在数据更新时保持最新的索引。 123&#x2F;&#x2F; 1.不需要给索引起名称，只需要设置字段即可；2.通过该字段查询都走索引（where、in、substring）CREATE INDEX ON :Album(name)drop index on :Album(name); 在上边的例子中，我们为所有标签为 Album 的点的 name 属性创建了一个索引。 1.9 查看索引 索引（约束）成为了数据库模式的一部分。 在 Neo4j 浏览器中，你可以使用 :schema 命令查看所有索引和约束。 1:schema 1.10 索引提示 索引创建完成后，当你在执行查询时会自动使用。 然而 Neo4j 也允许你强制提示一个或多个索引，你可以在你的查询语句中使用 USING INDEX ... 创建一个索引提示。 所以上边的示例可以像下边这样强制索引： 123MATCH (a:Album &#123;name: &quot;猛龙过江&quot;&#125;) USING INDEX a:Album(name) RETURN a 1.11 创建约束 约束有助于数据的完整性，因为它们阻止用户输入错误的数据类型。如果某个用户在应用了约束时输入了错误的类型会收到错误消息。 唯一约束 当你创建一个唯一约束时，Neo4j 将同时创建一个索引。Cypher 将使用该索引进行查询，就像使用其他索引一样。 因此不需要单独创建索引了，如果你尝试在已经有索引的情况下创建约束，你将会收到一个错误。 在 Neo4j 中创建唯一约束需要使用 CREATE CONSTRAINT ON 语句，像下边这样： 123&#x2F;&#x2F; 指定该属性必须包含唯一值（比如两个 Artist 节点不允许有相同值的 name 属性）CREATE CONSTRAINT ON (a:Artist) ASSERT a.name IS UNIQUE;DROP CONSTRAINT ON (a:Artist) ASSERT a.name IS UNIQUE; 在上边的例子中，我们为 Artist 标签的所有节点的 name 属性创建了唯一约束。 属性存在约束 确保具有特定标签的节点或具有特定类型的关系都存在某个属性（属性存在约束只在 Neo4j 企业版中可用） 12&#x2F;&#x2F; 指定 Artist 标签的所有节点都必须包含 name 属性。CREATE CONSTRAINT ON (a:Artist) ASSERT exists(a.name) 查看约束 1:schema 测试约束 执行下边的语句两次： 12CREATE (a:Artist &#123;name: &quot;周杰伦&quot;&#125;) RETURN a 2、常用查询 EXPlAIN cypher 查看 2.1 所有节点 12&#x2F;&#x2F; 所有节点match(n) return n; 2.2 节点属性 12Match (n:人) where n.name&#x3D;&#39;李泽钜&#39; return n;match (n) where n.name&#x3D;~&#39;.*测试.*&#39; return n; 2.3 节点id 1234&#x2F;&#x2F; 单个idmatch (user)-[:主要成就]-&gt;(p) where id(user)&#x3D;71 return p;&#x2F;&#x2F;多个idmatch (user) where id(user) in [2,23] match (user)-[:主要成就]-&gt;(p) return p; 2.4 更新属性 1match (user) where id(user) in [2,23] set user.group&#x3D;&#39;ADMINISTRATOR&#39;; 2.5 merge （match+create） 1MERGE (michael:Person &#123; name: &#39;Michael Douglas&#39; &#125;) RETURN michael; 2.5 统计个数 1234&#x2F;&#x2F; 所有节点个数MATCH (n) RETURN count(n);&#x2F;&#x2F; 所有关系MATCH ()--&gt;() RETURN count(*); 2.6 apoc 相关查询 1234CALL db.propertyKeys() YIELD propertyKey RETURN propertyKey as resultCALL db.schema()CALL dbms.functions()CALL dbms.procedures() 2.7 所有节点标签 1CALL db.labels() YIELD label RETURN label as result; 2.8 所有关系类型 1CALL db.relationshipTypes() YIELD relationshipType RETURN relationshipType as result 2.9 直接关系 1match m&#x3D;(:明星 &#123;名称:&#39;周迅&#39;&#125;)-[*..1]-() return m; 2.10 最短路径 123MATCH p &#x3D; shortestPath((周迅:明星 &#123;名称:&quot;周迅&quot;&#125;)-[*..6]-(王菲:明星 &#123;姓名:&quot;王菲&quot;&#125;))RETURN p;MATCH (ms:Person &#123; name:&#39;Andres&#39; &#125;),(cs:Person &#123; name:&#39;Taylor&#39; &#125;), p &#x3D; shortestPath((ms)-[r:Follow]-(cs)) RETURN p;MATCH p &#x3D; allShortestPaths((周迅:明星 &#123;名称:&#39;周迅&#39;&#125;)-[*..6]-(王菲:明星 &#123;姓名:&#39;王菲&#39;&#125;)) RETURN p; 2.11 给存在节点添加关系 123Match (n:Person &#123;id:&#39;erzi&#39;&#125;),(f:Person &#123;id:&#39;bozi&#39;&#125;) Merge (n)-[:fuqi]-&gt;(f)Match (n:人 &#123;name:&#39;哈良述&#39;&#125;),(f:人 &#123;name:&#39;哈革&#39;&#125;) Merge (n)-[:参与相同事件]-&gt;(f);Match (n:人 &#123;name:&#39;哈良述&#39;&#125;),(f:人 &#123;name:&#39;哈革&#39;&#125;) Merge (n)-[:从属相同组织]-&gt;(f); 2.12 更新节点属性 1Match (n:Person &#123;id:&#39;baba&#39;&#125;) set n.name&#x3D;&#39;张三&#39; return n; 2.23 删除标签 (remove) 1MATCH (n:Test) remove n:Test 2.24 删除属性 (remove) 1Match (n:Person &#123;id:&#39;baba&#39;&#125;) remove n.age return n 2.25 删除节点和关系（delete） 123MATCH (n:测试事件:人)-[r:成员]-(m:组织:测试事件) DELETE r; &#x2F;&#x2F; 删除关系MATCH (n:事 &#123;name:&#39;2&#39;&#125;)-[r:关键词]-(m:物 &#123;name:&#39;专题1468&#39;&#125;) DELETE r; &#x2F;&#x2F; 只是删除了关系MATCH (s:Teacher)-[r:teach]-&gt;(d:Student) delete r,s,d &#x2F;&#x2F; 删除节点和关系 2.26 排序 1234&#x2F;&#x2F;降序MATCH (n:Person) RETURN n order by n.id,n.name desc LIMIT 25&#x2F;&#x2F;正序 MATCH (n:Person) RETURN n order by n.id LIMIT 25 2.27 limit 和skip 12&#x2F;&#x2F; Limit : 显示多少行，最前边开始, Skip：跳过前多少行MATCH (n:Person) RETURN n order by n.id desc skip 2 LIMIT 25 2.28 union 和union all 12&#x2F;&#x2F; Union：把多段Match的return结果 上线组合成一个结果集，会自动去掉重复行；Union all：作用同union，但不去重；MATCH (n:Person) where n.age&gt;20 RETURN n.id,n.age union all MATCH (n:Person) where n.id&#x3D;&#39;erzi&#39; RETURN n.id,n.age 2.29 给已存在的节点添加标签 1match(n &#123;name:&#39;zhangsan&#39;&#125;) set n:lable1:lable2 return n; 2.30 结果收集到集合 1MATCH (n:&#96;事&#96;) RETURN collect(n.eid); 2.31 打平集合 1234WITH [[1, 2],[3, 4], 5] AS nestedUNWIND nested AS xUNWIND x AS yRETURN y 2.32 更新/新增节点（merge） 123MERGE (n:user &#123; id: &#39;1&#39;&#125;)ON CREATE SET n.name&#x3D; &#39;quan&#39;ON MATCH SET n.name&#x3D; &#39;quan&#39; 2.33 路径（不）经过 123456789&#x2F;&#x2F;按结点的标签：只经过标签为“A”的结点match p&#x3D;shortestPath((m:Person&#123;name:&#39;1&#39;&#125;)-[r:A*..4]-(n:Person&#123;name:&#39;4&#39;&#125;)) return p;&#x2F;&#x2F;按结点的属性：不经过name为’5’的结点match p&#x3D;shortestPath((m:Person&#123;name:&#39;1&#39;&#125;)-[r*1..4]-(n:Person&#123;name:&#39;4&#39;&#125;))where all(x in nodes(p) where x.name&lt;&gt;&#39;5&#39;)return p; &#x2F;&#x2F; 按关系的属性：只经过那些属性f小于4的关系match p&#x3D;shortestPath((m:Person&#123;name:&#39;1&#39;&#125;)-[r*1..4]-(n:Person&#123;name:&#39;4&#39;&#125;))where all(x in r where x.f&lt;4) return p; 2.34 模糊查询 12&#x2F;&#x2F;多值模糊查询match (n:Label) where n.attr &#x3D;~ &#39;.*a1.*|.*a2.*&#39; return n; 2.35 avg/max/min 12MATCH (n:现实人员)RETURN avg(n.influenceScore) 2.36 关系或查询 1(a)-[r:TYPE1|TYPE2]-&gt;(b) 2.37 排除标签查询 1234&#x2F;&#x2F; 排除单个match (a:IP &#123;name:&#39;223.104.33.80&#39;&#125;)--(b) where not b:QQ账号 return *;&#x2F;&#x2F; 排除多个CALL db.labels() YIELD label WITH label WHERE label &lt;&gt; &#39;Label1&#39; AND label &lt;&gt; &#39;Label2&#39; AND label &lt;&gt; &#39;Label3&#39; RETURN label 2.38 多个标签查询 1match (b) where b:QQ账号 OR b:微信账号 return b; 2.39 多个节点最短路径 1234567891011121314使用with定义一个列表变量，里面是所有的点；使用match匹配出所有的结点，将结点放到列表中，注意这里列表中的元素是结点类型，上面是业务id；使用两个unwind再次将结点列表打散到行，之前提到过两个unwind的结点也是以笛卡尔积的方式返回的（看这篇博客），所以这里是两两的任意组合，甚至两个结点相同的组合，实际上我们这里求最短路径1到2和2到1肯定是一样的，所以用id(source)&lt;id(target)来去除一半；最后是shortestPath函数，里面的source,target就是前面的组合；10，11，14，13，18，19with [3105, 200025928, 200025929, 151286502, 135660351] as id_listmatch (v:vertices) where v.id in id_list with collect(v) as nodes unwind nodes as source unwind nodes as target with source,target where id(source)&lt;id(target)match paths &#x3D; shortestPath((source)-[:HOLDER|MANAGER*0..2]-(target)) where all(x in nodes(paths) where x.id&lt;&gt;3105) with paths limit 20000 return paths 2.40 最短路径分析 1234567891011121314MATCH p&#x3D;allShortestPaths((m:Link &#123;name:&#39;Alan&#39;&#125;)-[*..6]-(n:Link &#123;name:&#39;Antonio&#39;&#125;))WHERE all(x in nodes(p) where id(x)&#x3D;1231) return p LIMIT 10这种类似的查询有没有更好的写法？（1）可以使用neo4j图算法库的方法（需要先安装graph algorithm 的plugin）：用The Dijkstra Shortest Path algorithm :MATCH (start:Loc&#123;name:&#39;A&#39;&#125;), (end:Loc&#123;name:&#39;F&#39;&#125;)CALL algo.shortestPath.stream(start, end, &#39;cost&#39;)YIELD nodeId, costMATCH (other:Loc) WHERE id(other) &#x3D; nodeIdRETURN other.name AS name, cost（2）也可以把结果写回：MATCH (start:Loc&#123;name:&#39;A&#39;&#125;), (end:Loc&#123;name:&#39;F&#39;&#125;)CALL algo.shortestPath(start, end, &#39;cost&#39;,&#123;write:true,writeProperty:&#39;sssp&#39;&#125;)YIELD writeMillis,loadMillis,nodeCount, totalCostRETURN writeMillis,loadMillis,nodeCount,totalCost 2.41 输出一条路径所有关系 123456789apoc.path.subgraphAll(startNode &lt;id&gt;Node&#x2F;list, &#123;maxLevel, relationshipFilter, labelFilter, bfs:true, filterStartNode:true, limit:-1&#125;) yield nodes, relationships; MATCH (n:朝代&#123;name:&#39;蜀汉&#39;&#125;)CALL apoc.path.subgraphNodes(n, &#123; relationshipFilter: NULL, labelFilter: NULL, maxLevel: 2&#125;)YIELD nodeRETURN node 2.42 合并属性 123MERGE (n:Node &#123;id: &#39;argan&#39;&#125;)SET n +&#x3D; &#123;id: &#39;argan&#39;, age: 30, sex: &#39;male&#39;, email: &#39;arganzheng@gmail.com&#39;&#125;RETURN n 2.43 节点的关系数 12MATCH (n:Person &#123;name:‘Keanu Reeves’&#125;)RETURN size((n)-[]-()) 2.44 寻找路径中节点 1234MATCH (startNode:行业 &#123;name:&#39;轻工&#39;&#125;) WITH startNodeCALL apoc.path.subgraphAll(startNode, &#123;maxLevel:2, relationshipFilter:&#39;NEXT&gt;&#39;, labelFilter:&#39;行业&#39;, bfs:true, filterStartNode:false, limit:-1&#125;) yield nodes,relationships RETURN *MATCH (startNode:行业 &#123;name:&#39;轻工&#39;&#125;) WITH startNodeCALL apoc.path.subgraphNodes(startNode, &#123;maxLevel:2, relationshipFilter:&#39;NEXT&gt;&#39;, labelFilter:&#39;行业&#39;, bfs:true, filterStartNode:false, limit:-1&#125;) yield node RETURN * 2.45 在其他图库中执行cypher 1CALL apoc.bolt.load(&quot;bolt:&#x2F;&#x2F;user:password@localhost:7687&quot;,&quot;match(p:Person &#123;name:&#123;name&#125;&#125;) return p&quot;, &#123;name:&#39;Michael&#39;&#125;) YIELD row RETURN row; 2.46 路径中节点 12match (n:&#96;组织机构&#96;:&#96;中文名称&#96;),(m:&#96;组织机构&#96;:&#96;中文名称&#96;)match p&#x3D;(n)-[*..2]-(m) return extract(node IN nodes(p) | node.name); 2.47 路径中关系 12match (n:&#96;组织机构&#96;:&#96;中文名称&#96;),(m:&#96;组织机构&#96;:&#96;中文名称&#96;)match p&#x3D;(n)-[*..2]-(m) return extract(r IN relationships(p) | TYPE(r)) 2.48 正则提取标签 1RETURN REDUCE(label&#x3D;&#39;&#39;,char IN REDUCE(label&#x3D;[],char IN apoc.text.regexGroups(&#39;证监会行业分类(2012版)&#39;, &#39;[\\u4e00-\\u9fa5a-zA-Z0-9]&#39;) | label+char) | label+char) 2.49 修改节点标签 1match(n:Person) remove n:Person set n:User； 2.50 修改关系类型 1match(n)-[r:拥有]-&gt;(m) create(n)-[r2:包括]-&gt;(m) set r2&#x3D;r with r delete r 3 、语句/函数 3.1 with WITH语句将分段的查询部分连接在一起，查询结果从一部分以管道形式传递给另外一部分作为开始点。 过滤聚合函数结果 1234MATCH (david &#123; name: &#39;Tom Hanks&#39; &#125;)--()--(otherPerson)WITH otherPerson, count(*) AS foafWHERE foaf &gt; 1RETURN otherPerson 在collect前对结果排序 1match(n:Person) with n order by n.name desc limit 3 return collect(n.name) 3.2 foreach 列表(lists)和路径(paths)是Cypher中的关键概念。可以使用foreach来更新其中的数据。它可以在路径或者聚合的列表的每个元素上执行更新命令。foreach括号中的变量是与外部分开的，这意味着foreach中创建的变量不能用于该语句之外。 在foreach括号内，可以执行任何的更新命令，包括CREATE，CREATE UNIQUE，DELETE和foreach。如果希望对列表中的每个元素执行额外的MATCH命令，使用UNWIND命令更合适。 1234567&#x2F;&#x2F; 这个查询将设置路径上所有节点的marked属性为true值。MATCH p &#x3D; (root &#123; name: &#39;root&#39; &#125;)-[r]-(A)FOREACH (n IN nodes(p)| SET n.marked &#x3D; TRUE )&#x2F;&#x2F;下面的查询将列表中的人全部加为&#39;A&#39;的朋友。MATCH (a &#123;name: &#39;root&#39; &#125;)FOREACH (name IN [&quot;Mike&quot;, &quot;Carl&quot;, &quot;Bruce&quot;] |CREATE (a)-[:FRIEND]-&gt;(:Person &#123;name: name&#125;)) 3.3 count 3.4 sum 3.5 avg 3.6 percentileDisc","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"neo4j","slug":"neo4j","permalink":"http://yoursite.com/tags/neo4j/"}]},{"title":"docker搭建haddop+hive+spark","slug":"docker搭建haddop-hive-spark","date":"2022-12-27T06:01:17.000Z","updated":"2022-12-27T06:04:35.066Z","comments":true,"path":"2022/12/27/docker搭建haddop-hive-spark/","link":"","permalink":"http://yoursite.com/2022/12/27/docker%E6%90%AD%E5%BB%BAhaddop-hive-spark/","excerpt":"","text":"转载：https://bambrow.com/20210625-docker-hadoop-1/ 准备工作 本项目基于 Docker 和 Docker Compose，搭建的集群包含以下部分： Hadoop Hive Spark 本项目参考了 Big Data Europe 的一些工作。项目中所使用的 Docker 镜像可能会被更新，可以参看他们的 Docker Hub 以获取最新镜像。 本项目所依赖的版本号如下： 123456Client: Version: 20.10.2Server: Docker Engine - Community Engine: Version: 20.10.6docker-compose version 1.29.1, build c34c88b2 快速开始 直接克隆我的项目并运行集群： 123git clone https:&#x2F;&#x2F;github.com&#x2F;bambrow&#x2F;docker-hadoop-workbench.gitcd docker-hadoop-workbench.&#x2F;start_demo.sh 也可以使用 docker-compose-v2.yml，该集群包含我简单修改的 spark-master 镜像，以及额外添加的 spark-history-server 镜像。 1.&#x2F;start_demo_v2.sh 使用 ./stop_demo.sh 或 ./stop_demo_v2.sh 关闭集群。你可以修改 start_demo.sh 与 stop_demo.sh 文件里的 DOCKER_COMPOSE_FILE 变量以使用其他版本的 YAML 文件。 集群内容 本集群包含以下 Container： namenode datanode resourcemanager nodemanager historyserver hive-server hive-metastore hive-metastore-postgresql presto-coordinator spark-master spark-worker spark-history-server (使用 v2 版本) 同时本集群需要用到名为 hadoop 的 network，以及以下 volume： hadoop_namenode hadoop_datanode hadoop_historyserver hive_metastore 这些都需要特别注意以避免冲突。 可交互的端口列表 综述 namenode: 9000, 9870 datanode: 9864 resourcemanager: 8088 nodemanager: 8042 historyserver: 8188 hive-server: 10000, 10002 hive-metastore: 9083 presto-coordinator: 8090 spark-master: 4040, 7077, 8080, 18080 (v2 版本 18080 接口由 spark-history-server 提供) spark-worker: 8081 如有冲突，可以在 docker-compose.yml 里更改暴露的端口。 UI 列表 Namenode: http://localhost:9870/dfshealth.html#tab-overview Datanode: http://localhost:9864/ ResourceManager: http://localhost:8088/cluster NodeManager: http://localhost:8042/node HistoryServer: http://localhost:8188/applicationhistory HiveServer2: http://localhost:10002/ Spark Master: http://localhost:8080/ Spark Worker: http://localhost:8081/ Spark Job WebUI: http://localhost:4040/ (当 Spark 任务在 spark-master 运行时才可访问) Presto WebUI: http://localhost:8090/ Spark History Server：http://localhost:18080/ HDFS 可以使用 hdfs dfs 连接到 hdfs://localhost:9000/ (请先在本机安装 Hadoop): 1hdfs dfs -ls hdfs:&#x2F;&#x2F;localhost:9000&#x2F; Hive 可以使用 Beeline 连接到 HiveServer2 (请先在本机安装 Hive): 1beeline -u jdbc:hive2:&#x2F;&#x2F;localhost:10000&#x2F;default -n hive -p hive Spark 可以使用 spark-shell 通过 thrift 协议连接到 Hive Metastore (请先在本机安装 Spark): 123456789101112131415161718192021222324252627282930313233$ spark-shellWelcome to ____ __ &#x2F; __&#x2F;__ ___ _____&#x2F; &#x2F;__ _\\ \\&#x2F; _ \\&#x2F; _ &#96;&#x2F; __&#x2F; &#39;_&#x2F; &#x2F;___&#x2F; .__&#x2F;\\_,_&#x2F;_&#x2F; &#x2F;_&#x2F;\\_\\ version 3.1.2 &#x2F;_&#x2F;Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 11.0.11)scala&gt; :paste&#x2F;&#x2F; Entering paste mode (ctrl-D to finish)import org.apache.spark.sql.SparkSessionval spark &#x3D; SparkSession.builder.master(&quot;local&quot;) .config(&quot;hive.metastore.uris&quot;, &quot;thrift:&#x2F;&#x2F;localhost:9083&quot;) .enableHiveSupport.appName(&quot;thrift-test&quot;).getOrCreatespark.sql(&quot;show databases&quot;).show&#x2F;&#x2F; Exiting paste mode, now interpreting.+---------+|namespace|+---------+| default|+---------+import org.apache.spark.sql.SparkSessionspark: org.apache.spark.sql.SparkSession &#x3D; org.apache.spark.sql.SparkSession@1223467f Presto 可以使用 Presto CLI 连接 Presto 并且读取 Hive 的数据： 1234wget https:&#x2F;&#x2F;repo1.maven.org&#x2F;maven2&#x2F;com&#x2F;facebook&#x2F;presto&#x2F;presto-cli&#x2F;0.255&#x2F;presto-cli-0.255-executable.jarmv presto-cli-0.255-executable.jar prestochmod +x presto.&#x2F;presto --server localhost:8090 --catalog hive --schema default 设置列表 以下列举了容器内部的一些设置所在的位置。后面的以 CONF 结尾的是它们在 hadoop.env 中的代号。你可以参考 hadoop.env 文件做额外的设置。 namenode 123456789101112131415161718192021222324 : - &#96;&#x2F;etc&#x2F;hadoop&#x2F;core-site.xml&#96; CORE_CONF - &#96;&#x2F;etc&#x2F;hadoop&#x2F;hdfs-site.xml&#96; HDFS_CONF - &#96;&#x2F;etc&#x2F;hadoop&#x2F;yarn-site.xml&#96; YARN_CONF - &#96;&#x2F;etc&#x2F;hadoop&#x2F;httpfs-site.xml&#96; HTTPFS_CONF - &#96;&#x2F;etc&#x2F;hadoop&#x2F;kms-site.xml&#96; KMS_CONF - &#96;&#x2F;etc&#x2F;hadoop&#x2F;mapred-site.xml&#96; MAPRED_CONF - hive-server - &#96;&#x2F;opt&#x2F;hive&#x2F;hive-site.xml&#96; HIVE_CONF很可惜 Spark 的设置不在这个列表里。在 &#96;spark-master&#96; 的 &#96;&#x2F;spark&#x2F;conf&#96; 文件夹下可以存放 Spark 的设置，我准备了 &#96;scripts&#x2F;spark-defaults.conf&#96; 与 &#96;scripts&#x2F;spark-hive-site.xml&#96; 两个文件，它们已经在启动脚本里自动上传。## 运行示例任务### 运行 MapReduce &#96;WordCount&#96;这部分基于 [Big Data Europe’s Hadoop Docker](https:&#x2F;&#x2F;github.com&#x2F;big-data-europe&#x2F;docker-hadoop) 的项目里的运行示例。首先我们运行一个辅助容器 &#96;hadoop-base&#96;： docker run -d --network hadoop --env-file hadoop.env --name hadoop-base bde2020/hadoop-base:2.0.0-hadoop3.2.1-java8 tail -f /dev/null 123接下来运行以下命令以准备数据并启动 MapReduce 任务： docker exec -it hadoop-base hdfs dfs -mkdir -p /input/ docker exec -it hadoop-base hdfs dfs -copyFromLocal -f /opt/hadoop-3.2.1/README.txt /input/ docker exec -it hadoop-base mkdir jars docker cp jars/WordCount.jar hadoop-base:jars/WordCount.jar docker exec -it hadoop-base /bin/bash hadoop jar jars/WordCount.jar WordCount /input /output 12345678接下来，你可以通过以下链接看到任务状态：- http:&#x2F;&#x2F;localhost:8088&#x2F;cluster&#x2F;apps- http:&#x2F;&#x2F;localhost:8188&#x2F;applicationhistory (运行结束后)当任务运行完成，运行以下命令查看结果： hdfs dfs -cat /output/* 1234567最后你可以使用 &#96;exit&#96; 退出该容器。### 运行 Hive 任务请首先确定 &#96;hadoop-base&#96; 正在运行中。关于如何启动此辅助容器，请参看上一节。接下来准备数据： docker exec -it hadoop-base hdfs dfs -mkdir -p /test/ docker exec -it hadoop-base mkdir test docker cp data hadoop-base:test/data docker exec -it hadoop-base /bin/bash hdfs dfs -put test/data/* /test/ hdfs dfs -ls /test exit 123然后新建 Hive 表： docker cp scripts/hive-beers.q hive-server:hive-beers.q docker exec -it hive-server /bin/bash cd / hive -f hive-beers.q exit 123接下来你就可以使用 Beeline 访问到这些数据了： beeline -u jdbc:hive2://localhost:10000/test -n hive -p hive 0: jdbc:hive2://localhost:10000/test&gt; select count(*) from beers; 12345678910同样，你可以通过以下链接看到任务状态：- http:&#x2F;&#x2F;localhost:8088&#x2F;cluster&#x2F;apps- http:&#x2F;&#x2F;localhost:8188&#x2F;applicationhistory (运行结束后)### 运行 Spark Shell在进行这一步前，请先参看前面两个章节以准备 Hive 数据并创建表格。然后运行以下命令： docker exec -it spark-master spark/bin/spark-shell 123进入 Spark Shell 后，你可以直接通过先前创建的 Hive 表进行操作： scala&gt; spark.sql(“show databases”).show ±--------+ |namespace| ±--------+ | default| | test| ±--------+ scala&gt; val df = spark.sql(“select * from test.beers”) df: org.apache.spark.sql.DataFrame = [id: int, brewery_id: int … 11 more fields] scala&gt; df.count res0: Long = 7822 12345678你可以在以下两个地址看到你的 Spark Shell 会话：- http:&#x2F;&#x2F;localhost:8080&#x2F;- http:&#x2F;&#x2F;localhost:4040&#x2F;jobs&#x2F; (运行时)如果你在运行 &#96;spark-shell&#96; 的时候遇到了以下警告： WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources 123该警告显示没有资源可以去运行你的任务，并提醒你去检查 worker 是否都已经被注册，而且拥有足够多的资源。此时你需要使用 &#96;docker logs -f spark-master&#96; 检查一下 &#96;spark-master&#96; 的日志。不出意外的话，你会看到下面的内容： WARN Master: Got heartbeat from unregistered worker worker-20210622022950-xxx.xx.xx.xx-xxxxx. This worker was never registered, so ignoring the heartbeat. 123456789这是在提示你有一个 worker 没有被注册，所以忽略了它的心跳。该 worker 没有被注册的原因很多，很可能是之前电脑被休眠过，导致 worker 掉线。这时你可以使用 &#96;docker-compose restart spark-worker&#96; 重启 &#96;spark-worker&#96;，重启完成后，该 worker 就会被自动注册。同样，如果要运行 &#96;spark-sql&#96;，可以使用这个命令：&#96;docker exec -it spark-master spark&#x2F;bin&#x2F;spark-sql&#96;。### 运行 Spark Submit 任务我们直接运行 Spark 内置的示例任务 Spark Pi： docker exec -it spark-master /spark/bin/spark-submit --class org.apache.spark.examples.SparkPi /spark/examples/jars/spark-examples_2.12-3.1.1.jar 100 你可以在以下两个地址看到你的 Spark Pi 任务： - http://localhost:8080/ - http://localhost:4040/jobs/ (运行时) 本教程的集群搭建与使用方法就介绍到这里。以后可能会增加一些别的组件，会另外写文章来说明。","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"访问者模式","slug":"访问者模式","date":"2022-11-23T03:29:20.000Z","updated":"2022-11-23T05:36:55.197Z","comments":true,"path":"2022/11/23/访问者模式/","link":"","permalink":"http://yoursite.com/2022/11/23/%E8%AE%BF%E9%97%AE%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"转载：https://www.jianshu.com/p/1f1049d0a0f4 访问者模式介绍 最复杂的设计模式，并且使用频率不高，《设计模式》的作者评价为：大多情况下，你不需要使用访问者模式，但是一旦需要使用它时，那就真的需要使用了。 访问者模式是一种将数据操作和数据结构分离的设计模式。（觉得太抽象，可以看下面的例子）。 访问者模式的使用场景 对象结构比较稳定，但经常需要在此对象结构上定义新的操作。 需要对一个对象结构中的对象进行很多不同的并且不相关的操作，而需要避免这些操作“污染”这些对象的类，也不希望在增加新操作时修改这些类。 访问者模式的UML类图 访问者模式 角色介绍 **Visitor：**接口或者抽象类，定义了对每个 Element 访问的行为，它的参数就是被访问的元素，它的方法个数理论上与元素的个数是一样的，因此，访问者模式要求元素的类型要稳定，如果经常添加、移除元素类，必然会导致频繁地修改 Visitor 接口，如果出现这种情况，则说明不适合使用访问者模式。 **ConcreteVisitor：**具体的访问者，它需要给出对每一个元素类访问时所产生的具体行为。 **Element：**元素接口或者抽象类，它定义了一个接受访问者（accept）的方法，其意义是指每一个元素都要可以被访问者访问。 **ElementA、ElementB：**具体的元素类，它提供接受访问的具体实现，而这个具体的实现，通常情况下是使用访问者提供的访问该元素类的方法。 **ObjectStructure：**定义当中所提到的对象结构，对象结构是一个抽象表述，它内部管理了元素集合，并且可以迭代这些元素提供访问者访问。 依然很抽象，看下面的栗子吧。 访问者模式的简单示例 年底，CEO和CTO开始评定员工一年的工作绩效，员工分为工程师和经理，CTO关注工程师的代码量、经理的新产品数量；CEO关注的是工程师的KPI和经理的KPI以及新产品数量。 由于CEO和CTO对于不同员工的关注点是不一样的，这就需要对不同员工类型进行不同的处理。访问者模式此时可以派上用场了。 12345678910111213// 员工基类public abstract class Staff &#123; public String name; public int kpi;// 员工KPI public Staff(String name) &#123; this.name = name; kpi = new Random().nextInt(10); &#125; // 核心方法，接受Visitor的访问 public abstract void accept(Visitor visitor);&#125; Staff 类定义了员工基本信息及一个 accept 方法，accept 方法表示接受访问者的访问，由子类具体实现。Visitor 是个接口，传入不同的实现类，可访问不同的数据。下面看看工程师和经理的代码： 12345678910111213141516// 工程师public class Engineer extends Staff &#123; public Engineer(String name) &#123; super(name); &#125; @Override public void accept(Visitor visitor) &#123; visitor.visit(this); &#125; // 工程师一年的代码数量 public int getCodeLines() &#123; return new Random().nextInt(10 * 10000); &#125;&#125; 12345678910111213141516// 经理public class Manager extends Staff &#123; public Manager(String name) &#123; super(name); &#125; @Override public void accept(Visitor visitor) &#123; visitor.visit(this); &#125; // 一年做的产品数量 public int getProducts() &#123; return new Random().nextInt(10); &#125;&#125; 工程师是代码数量，经理是产品数量，他们的职责不一样，也就是因为差异性，才使得访问模式能够发挥它的作用。Staff、Engineer、Manager 3个类型就是对象结构，这些类型相对稳定，不会发生变化。 然后将这些员工添加到一个业务报表类中，公司高层可以通过该报表类的 showReport 方法查看所有员工的业绩，具体代码如下： 123456789101112131415161718192021222324// 员工业务报表类public class BusinessReport &#123; private List&lt;Staff&gt; mStaffs = new LinkedList&lt;&gt;(); public BusinessReport() &#123; mStaffs.add(new Manager(&quot;经理-A&quot;)); mStaffs.add(new Engineer(&quot;工程师-A&quot;)); mStaffs.add(new Engineer(&quot;工程师-B&quot;)); mStaffs.add(new Engineer(&quot;工程师-C&quot;)); mStaffs.add(new Manager(&quot;经理-B&quot;)); mStaffs.add(new Engineer(&quot;工程师-D&quot;)); &#125; /** * 为访问者展示报表 * @param visitor 公司高层，如CEO、CTO */ public void showReport(Visitor visitor) &#123; for (Staff staff : mStaffs) &#123; staff.accept(visitor); &#125; &#125;&#125; 下面看看 Visitor 类型的定义， Visitor 声明了两个 visit 方法，分别是对工程师和经理对访问函数，具体代码如下： 12345678public interface Visitor &#123; // 访问工程师类型 void visit(Engineer engineer); // 访问经理类型 void visit(Manager manager);&#125; 首先定义了一个 Visitor 接口，该接口有两个 visit 函数，参数分别是 Engineer、Manager，也就是说对于 Engineer、Manager 的访问会调用两个不同的方法，以此达成区别对待、差异化处理。具体实现类为 CEOVisitor、CTOVisitor类，具体代码如下： 12345678910111213// CEO访问者public class CEOVisitor implements Visitor &#123; @Override public void visit(Engineer engineer) &#123; System.out.println(&quot;工程师: &quot; + engineer.name + &quot;, KPI: &quot; + engineer.kpi); &#125; @Override public void visit(Manager manager) &#123; System.out.println(&quot;经理: &quot; + manager.name + &quot;, KPI: &quot; + manager.kpi + &quot;, 新产品数量: &quot; + manager.getProducts()); &#125;&#125; 在CEO的访问者中，CEO关注工程师的 KPI，经理的 KPI 和新产品数量，通过两个 visitor 方法分别进行处理。如果不使用 Visitor 模式，只通过一个 visit 方法进行处理，那么就需要在这个 visit 方法中进行判断，然后分别处理，代码大致如下： 123456789101112public class ReportUtil &#123; public void visit(Staff staff) &#123; if (staff instanceof Manager) &#123; Manager manager = (Manager) staff; System.out.println(&quot;经理: &quot; + manager.name + &quot;, KPI: &quot; + manager.kpi + &quot;, 新产品数量: &quot; + manager.getProducts()); &#125; else if (staff instanceof Engineer) &#123; Engineer engineer = (Engineer) staff; System.out.println(&quot;工程师: &quot; + engineer.name + &quot;, KPI: &quot; + engineer.kpi); &#125; &#125;&#125; 这就导致了 if-else 逻辑的嵌套以及类型的强制转换，难以扩展和维护，当类型较多时，这个 ReportUtil 就会很复杂。而使用 Visitor 模式，通过同一个函数对不同对元素类型进行相应对处理，使结构更加清晰、灵活性更高。 再添加一个CTO的 Visitor 类： 1234567891011public class CTOVisitor implements Visitor &#123; @Override public void visit(Engineer engineer) &#123; System.out.println(&quot;工程师: &quot; + engineer.name + &quot;, 代码行数: &quot; + engineer.getCodeLines()); &#125; @Override public void visit(Manager manager) &#123; System.out.println(&quot;经理: &quot; + manager.name + &quot;, 产品数量: &quot; + manager.getProducts()); &#125;&#125; 重载的 visit 方法会对元素进行不同的操作，而通过注入不同的 Visitor 又可以替换掉访问者的具体实现，使得对元素的操作变得更灵活，可扩展性更高，同时也消除了类型转换、if-else 等“丑陋”的代码。 下面是客户端代码： 1234567891011public class Client &#123; public static void main(String[] args) &#123; // 构建报表 BusinessReport report = new BusinessReport(); System.out.println(&quot;=========== CEO看报表 ===========&quot;); report.showReport(new CEOVisitor()); System.out.println(&quot;=========== CTO看报表 ===========&quot;); report.showReport(new CTOVisitor()); &#125;&#125; 具体输出如下： 1234567891011121314=========== CEO看报表 ===========经理: 经理-A, KPI: 9, 新产品数量: 0工程师: 工程师-A, KPI: 6工程师: 工程师-B, KPI: 6工程师: 工程师-C, KPI: 8经理: 经理-B, KPI: 2, 新产品数量: 6工程师: 工程师-D, KPI: 6=========== CTO看报表 ===========经理: 经理-A, 产品数量: 3工程师: 工程师-A, 代码行数: 62558工程师: 工程师-B, 代码行数: 92965工程师: 工程师-C, 代码行数: 58839经理: 经理-B, 产品数量: 6工程师: 工程师-D, 代码行数: 53125 在上述示例中，Staff 扮演了 Element 角色，而 Engineer 和 Manager 都是 ConcreteElement；CEOVisitor 和 CTOVisitor 都是具体的 Visitor 对象；而 BusinessReport 就是 ObjectStructure；Client就是客户端代码。 访问者模式最大的优点就是增加访问者非常容易，我们从代码中可以看到，如果要增加一个访问者，只要新实现一个 Visitor 接口的类，从而达到数据对象与数据操作相分离的效果。如果不实用访问者模式，而又不想对不同的元素进行不同的操作，那么必定需要使用 if-else 和类型转换，这使得代码难以升级维护。 总结 我们要根据具体情况来评估是否适合使用访问者模式，例如，我们的对象结构是否足够稳定，是否需要经常定义新的操作，使用访问者模式是否能优化我们的代码，而不是使我们的代码变得更复杂。 访问者模式的优点。 各角色职责分离，符合单一职责原则 通过UML类图和上面的示例可以看出来，Visitor、ConcreteVisitor、Element 、ObjectStructure，职责单一，各司其责。 具有优秀的扩展性 如果需要增加新的访问者，增加实现类 ConcreteVisitor 就可以快速扩展。 使得数据结构和作用于结构上的操作解耦，使得操作集合可以独立变化 员工属性（数据结构）和CEO、CTO访问者（数据操作）的解耦。 灵活性 访问者模式的缺点。 具体元素对访问者公布细节，违反了迪米特原则 CEO、CTO需要调用具体员工的方法。 具体元素变更时导致修改成本大 变更员工属性时，多个访问者都要修改。 违反了依赖倒置原则，为了达到“区别对待”而依赖了具体类，没有以来抽象 访问者 visit 方法中，依赖了具体员工的具体方法。 作者：JamFF 链接：https://www.jianshu.com/p/1f1049d0a0f4 来源：简书 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"访问者","slug":"访问者","permalink":"http://yoursite.com/tags/%E8%AE%BF%E9%97%AE%E8%80%85/"}]},{"title":"neo4j","slug":"neo4j","date":"2022-11-23T00:56:25.000Z","updated":"2023-02-16T07:25:56.537Z","comments":true,"path":"2022/11/23/neo4j/","link":"","permalink":"http://yoursite.com/2022/11/23/neo4j/","excerpt":"","text":"https://www.cnblogs.com/andre-ma/p/13903418.html 1、安装 123456789101112131415161718version: &#x27;3&#x27;services: neo4j: image: neo4j:3.5.5 volumes: - ./conf:/var/lib/neo4j/conf - ./import:/var/lib/neo4j/import - ./plugins:/plugins - ./data:/data - ./logs:/var/lib/neo4j/logs restart: always ports: - 7474:7474 - 7687:7687 environment: - NEO4J_dbms_memory_heap_maxSize=4G - NEO4J_AUTH=neo4j/123456 2、导入csv 因为我们是 Docker 启动的 Neo4j，Load csv 导入取的默认目录为 /var/lib/neo4j/import,目前已经将csv放到挂载的本机import了。 2.1 Cypher load 导入csv 1234567891011&#x2F;&#x2F; 导入实体LOAD CSV WITH HEADERS FROM &quot;file:&#x2F;&#x2F;&#x2F;西游记&#x2F;triples.csv&quot; AS lineMERGE (p:person&#123;name:line.head&#125;)&#x2F;&#x2F; 导入实体LOAD CSV WITH HEADERS FROM &quot;file:&#x2F;&#x2F;&#x2F;西游记&#x2F;triples.csv&quot; AS lineMERGE (p:person&#123;name:line.tail&#125; &#x2F;&#x2F; 导入关系LOAD CSV WITH HEADERS FROM &quot;file:&#x2F;&#x2F;&#x2F;西游记&#x2F;triples.csv&quot; AS linematch (from:person&#123;name:line.tail&#125;),(to:person&#123;name:line.head&#125;)merge (from)-[r:rel&#123;label:line.label,relation:line.relation&#125;]-&gt;(to) 2.2 neo4j-admin 导入csv 3、Cypher是什么 Cypher是一种声明式图数据库查询语言，它具有丰富的表现力，能高效地查询和更新图数据。 Cypher查询语言很依赖于模式。只包含一个关系的简单模式连接了一对节点。例如，一个人LIVES_IN在某个城市或者某个城市PART_OF一个国家。使用了多个关系的复杂模式能够表达任意复杂的概念，可以支持各种有趣的使用场景。例如，下面的Cypher代码将两个简单的模式连接在一起： 1(:Person) -[:LIVES_IN]-&gt; (:City) -[:PART_OF]-&gt; (:Country) 3.1 节点语法 123456()(matrix)(:Movie)(matrix:Movie)(matrix:Movie &#123;title: &quot;The Matrix&quot;&#125;)(matrix:Movie &#123;title: &quot;The Matrix&quot;, released: 1997&#125;) 3.2 关系语法 123456----&gt;-[role]-&gt;-[:ACTED_IN]-&gt;-[role:ACTED_IN]-&gt;-[role:ACTED_IN &#123;roles: [&quot;Neo&quot;]&#125;]-&gt; 3.3 模式语法 将节点和关系的语法组合在一起可以表达模式。 1(keanu:Person:Actor &#123;name: &quot;Keanu Reeves&quot;&#125;)-[role:ACTED_IN &#123;roles: [&quot;Neo&quot;]&#125;]-&gt;(matrix:Movie &#123;title: &quot;The Matrix&quot;&#125; ) 4 Create 3.1 创建节点 CREATE (:)。节点标签名称，其实相当于Mysql数据库中的表名，而是节点名称，其实代指创建的此行数据。 123456&#x2F;&#x2F; 单个节点带属性CREATE (dept:Dept &#123; deptno:10,dname:&quot;Accounting&quot;,location:&quot;Hyderabad&quot; &#125;)&#x2F;&#x2F; 多个标签节点CREATE (n:Person:Swedish)&#x2F;&#x2F; 单个节点带属性，并返回 CREATE (a &#123; name: &#39;Andres&#39; &#125;) RETURN a 3.2 创建关系 1234567891011&#x2F;&#x2F;先创建2个节点CREATE (n:Person &#123;name: &#39;Node A&#39;&#125;),(m:Person &#123;name: &#39;Node B&#39;&#125;)&#x2F;&#x2F;查询节点并为其创建关系MATCH (a:Person),(b:Person)WHERE a.name &#x3D; &#39;Node A&#39; AND b.name &#x3D; &#39;Node B&#39;CREATE (a)-[r:RELTYPE]-&gt;(b)RETURN r&#x2F;&#x2F; 创建关系并为关系设置属性Match (a:Person &#123;name:&#39;Node A&#39;&#125;),(b:Person &#123;name:&#39;Node B&#39;&#125;)CREATE (a)-[r:RELTYPE &#123; name: &#39;abc&#39; &#125;]-&gt;(b)RETURN r 3.3 创建路径 1create p &#x3D; (m:Andres &#123;name:&#39;南京&#39;&#125;)-[r:SHUYU]-&gt;(jiangsu:Andres &#123;name:&#39;江苏省&#39;&#125;)&lt;-[s:SHUYU]-(andres:Andres &#123;name:&#39;徐州&#39;&#125;) return p","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"neo4j","slug":"neo4j","permalink":"http://yoursite.com/tags/neo4j/"}]},{"title":"JVM类加载器","slug":"JVM类加载器","date":"2022-09-27T08:50:43.000Z","updated":"2022-11-22T01:27:53.373Z","comments":true,"path":"2022/09/27/JVM类加载器/","link":"","permalink":"http://yoursite.com/2022/09/27/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8/","excerpt":"","text":"类与类加载器 #判断类是否“相等” 任意一个类，都由加载它的类加载器和这个类本身一同确立其在 Java 虚拟机中的唯一性，每一个类加载器，都有一个独立的类名称空间。 因此，比较两个类是否“相等”，只有在这两个类是由同一个类加载器加载的前提下才有意义，否则，即使这两个类来源于同一个 Class 文件，被同一个虚拟机加载，只要加载它们的类加载器不同，那么这两个类就必定不相等。 这里的“相等”，包括代表类的 Class 对象的 equals() 方法、isInstance() 方法的返回结果，也包括使用 instanceof 关键字做对象所属关系判定等情况。 #加载器种类 系统提供了 3 种类加载器： 启动类加载器（Bootstrap ClassLoader）： 负责将存放在 &lt;JAVA_HOME&gt;\\lib 目录中的，并且能被虚拟机识别的（仅按照文件名识别，如 rt.jar，名字不符合的类库即使放在 lib 目录中也不会被加载）类库加载到虚拟机内存中。 扩展类加载器（Extension ClassLoader）： 负责加载 &lt;JAVA_HOME&gt;\\lib\\ext 目录中的所有类库，开发者可以直接使用扩展类加载器。 应用程序类加载器（Application ClassLoader）： 由于这个类加载器是 ClassLoader 中的 getSystemClassLoader() 方法的返回值，所以一般也称它为“系统类加载器”。它负责加载用户类路径（classpath）上所指定的类库，开发者可以直接使用这个类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 当然，如果有必要，还可以加入自己定义的类加载器。 #双亲委派模型 #什么是双亲委派模型 双亲委派模型是描述类加载器之间的层次关系。它要求除了顶层的启动类加载器外，其余的类加载器都应当有自己的父类加载器。（父子关系一般不会以继承的关系实现，而是以组合关系来复用父加载器的代码） #工作过程 如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个加载请求（找不到所需的类）时，子加载器才会尝试自己去加载。 在 java.lang.ClassLoader 中的 loadClass 方法中实现该过程。 #为什么使用双亲委派模型 像 java.lang.Object 这些存放在 rt.jar 中的类，无论使用哪个类加载器加载，最终都会委派给最顶端的启动类加载器加载，从而使得不同加载器加载的 Object 类都是同一个。 相反，如果没有使用双亲委派模型，由各个类加载器自行去加载的话，如果用户自己编写了一个称为 java.lang.Object 的类，并放在 classpath 下，那么系统将会出现多个不同的 Object 类，Java 类型体系中最基础的行为也就无法保证。","categories":[],"tags":[]},{"title":"JVM类加载过程","slug":"JVM类加载过程","date":"2022-09-27T08:50:06.000Z","updated":"2022-11-22T01:27:53.374Z","comments":true,"path":"2022/09/27/JVM类加载过程/","link":"","permalink":"http://yoursite.com/2022/09/27/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B/","excerpt":"","text":"类加载过程包括 5 个阶段：加载、验证、准备、解析和初始化。 #加载 #加载的过程 “加载”是“类加载”过程的一个阶段，不能混淆这两个名词。在加载阶段，虚拟机需要完成 3 件事： 通过类的全限定名获取该类的二进制字节流。 将二进制字节流所代表的静态结构转化为方法区的运行时数据结构。 在内存中创建一个代表该类的 java.lang.Class 对象，作为方法区这个类的各种数据的访问入口。 #获取二进制字节流 对于 Class 文件，虚拟机没有指明要从哪里获取、怎样获取。除了直接从编译好的 .class 文件中读取，还有以下几种方式： 从 zip 包中读取，如 jar、war 等； 从网络中获取，如 Applet； 通过动态代理技术生成代理类的二进制字节流； 由 JSP 文件生成对应的 Class 类； 从数据库中读取，如 有些中间件服务器可以选择把程序安装到数据库中来完成程序代码在集群间的分发。 #“非数组类”与“数组类”加载比较 非数组类加载阶段可以使用系统提供的引导类加载器，也可以由用户自定义的类加载器完成，开发人员可以通过定义自己的类加载器控制字节流的获取方式（如重写一个类加载器的 loadClass() 方法）。 数组类本身不通过类加载器创建，它是由 Java 虚拟机直接创建的，再由类加载器创建数组中的元素类。 #注意事项 虚拟机规范未规定 Class 对象的存储位置，对于 HotSpot 虚拟机而言，Class 对象比较特殊，它虽然是对象，但存放在方法区中。 加载阶段与连接阶段的部分内容交叉进行，加载阶段尚未完成，连接阶段可能已经开始了。但这两个阶段的开始时间仍然保持着固定的先后顺序。 #验证 #验证的重要性 验证阶段确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 #验证的过程 文件格式验证 验证字节流是否符合 Class 文件格式的规范，并且能被当前版本的虚拟机处理，验证点如下： 是否以魔数 0XCAFEBABE 开头。 主次版本号是否在当前虚拟机处理范围内。 常量池是否有不被支持的常量类型。 指向常量的索引值是否指向了不存在的常量。 CONSTANT_Utf8_info 型的常量是否有不符合 UTF8 编码的数据。 … 元数据验证 对字节码描述信息进行语义分析，确保其符合 Java 语法规范。 字节码验证 本阶段是验证过程中最复杂的一个阶段，是对方法体进行语义分析，保证方法在运行时不会出现危害虚拟机的事件。 符号引用验证 本阶段发生在解析阶段，确保解析正常执行。 #准备 准备阶段是正式为类变量（或称“静态成员变量”）分配内存并设置初始值的阶段。这些变量（不包括实例变量）所使用的内存都在方法区中进行分配。 初始值“通常情况下”是数据类型的零值（0, null…），假设一个类变量的定义为： 1public static int value = 123; 那么变量 value 在准备阶段过后的初始值为 0 而不是 123，因为这时候尚未开始执行任何 Java 方法。 存在“特殊情况”：如果类字段的字段属性表中存在 ConstantValue 属性，那么在准备阶段 value 就会被初始化为 ConstantValue 属性所指定的值，假设上面类变量 value 的定义变为： 1public static final int value = 123; 那么在准备阶段虚拟机会根据 ConstantValue 的设置将 value 赋值为 123。 #解析 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。 #初始化 类初始化阶段是类加载过程的最后一步，是执行类构造器 &lt;clinit&gt;() 方法的过程。 &lt;clinit&gt;() 方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块（static {} 块）中的语句合并产生的，编译器收集的顺序是由语句在源文件中出现的顺序所决定的。 静态语句块中只能访问定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块中可以赋值，但不能访问。如下方代码所示： 1234567public class Test &#123; static &#123; i = 0; // 给变量赋值可以正常编译通过 System.out.println(i); // 这句编译器会提示“非法向前引用” &#125; static int i = 1;&#125; &lt;clinit&gt;() 方法不需要显式调用父类构造器，虚拟机会保证在子类的 &lt;clinit&gt;() 方法执行之前，父类的 &lt;clinit&gt;() 方法已经执行完毕。 由于父类的 &lt;clinit&gt;() 方法先执行，意味着父类中定义的静态语句块要优先于子类的变量赋值操作。如下方代码所示： 1234567891011121314static class Parent &#123; public static int A = 1; static &#123; A = 2; &#125;&#125;static class Sub extends Parent &#123; public static int B = A;&#125;public static void main(String[] args) &#123; System.out.println(Sub.B); // 输出 2&#125; &lt;clinit&gt;() 方法不是必需的，如果一个类没有静态语句块，也没有对类变量的赋值操作，那么编译器可以不为这个类生成 &lt;clinit&gt;() 方法。 接口中不能使用静态代码块，但接口也需要通过 &lt;clinit&gt;() 方法为接口中定义的静态成员变量显式初始化。但接口与类不同，接口的 &lt;clinit&gt;() 方法不需要先执行父类的 &lt;clinit&gt;() 方法，只有当父接口中定义的变量使用时，父接口才会初始化。 虚拟机会保证一个类的 &lt;clinit&gt;() 方法在多线程环境中被正确加锁、同步。如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的 &lt;clinit&gt;() 方法。","categories":[],"tags":[]},{"title":"JVM类加载","slug":"JVM类加载","date":"2022-09-27T08:49:31.000Z","updated":"2022-11-22T01:27:53.372Z","comments":true,"path":"2022/09/27/JVM类加载/","link":"","permalink":"http://yoursite.com/2022/09/27/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD/","excerpt":"","text":"类的生命周期 类从被加载到虚拟机内存开始，到卸载出内存为止，它的整个生命周期包括以下 7 个阶段： 加载 验证 准备 解析 初始化 使用 卸载 验证、准备、解析 3 个阶段统称为连接。 加载、验证、准备、初始化和卸载这 5 个阶段的顺序是确定的，类的加载过程必须按照这种顺序按部就班地开始（注意是“开始”，而不是“进行”或“完成”），而解析阶段则不一定：它在某些情况下可以在初始化后再开始，这是为了支持 Java 语言的运行时绑定。 #类加载过程中“初始化”开始的时机 Java 虚拟机规范没有强制约束类加载过程的第一阶段（即：加载）什么时候开始，但对于“初始化”阶段，有着严格的规定。有且仅有 5 种情况必须立即对类进行“初始化”： 在遇到 new、putstatic、getstatic、invokestatic 字节码指令时，如果类尚未初始化，则需要先触发其初始化。 对类进行反射调用时，如果类还没有初始化，则需要先触发其初始化。 初始化一个类时，如果其父类还没有初始化，则需要先初始化父类。 虚拟机启动时，用于需要指定一个包含 main() 方法的主类，虚拟机会先初始化这个主类。 当使用 JDK 1.7 的动态语言支持时，如果一个 java.lang.invoke.MethodHandle 实例最后的解析结果为 REF_getStatic、REF_putStatic、REF_invokeStatic 的方法句柄，并且这个方法句柄所对应的类还没初始化，则需要先触发其初始化。 这 5 种场景中的行为称为对一个类进行主动引用，除此之外，其它所有引用类的方式都不会触发初始化，称为被动引用。 #被动引用演示 Demo #Demo1 1234567891011121314151617181920212223242526272829/** * 被动引用 Demo1: * 通过子类引用父类的静态字段，不会导致子类初始化。 * * @author ylb * */class SuperClass &#123; static &#123; System.out.println(&quot;SuperClass init!&quot;); &#125; public static int value = 123;&#125;class SubClass extends SuperClass &#123; static &#123; System.out.println(&quot;SubClass init!&quot;); &#125;&#125;public class NotInitialization &#123; public static void main(String[] args) &#123; System.out.println(SubClass.value); // SuperClass init! &#125;&#125; 对于静态字段，只有直接定义这个字段的类才会被初始化，因此通过其子类来引用父类中定义的静态字段，只会触发父类的初始化而不会触发子类的初始化。 #Demo2 123456789101112131415/** * 被动引用 Demo2: * 通过数组定义来引用类，不会触发此类的初始化。 * * @author ylb * */public class NotInitialization &#123; public static void main(String[] args) &#123; SuperClass[] superClasses = new SuperClass[10]; &#125;&#125; 这段代码不会触发父类的初始化，但会触发“[L 全类名”这个类的初始化，它由虚拟机自动生成，直接继承自 java.lang.Object，创建动作由字节码指令 newarray 触发。 #Demo3 1234567891011121314151617181920212223/** * 被动引用 Demo3: * 常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化。 * * @author ylb * */class ConstClass &#123; static &#123; System.out.println(&quot;ConstClass init!&quot;); &#125; public static final String HELLO_BINGO = &quot;Hello Bingo&quot;;&#125;public class NotInitialization &#123; public static void main(String[] args) &#123; System.out.println(ConstClass.HELLO_BINGO); &#125;&#125; 编译通过之后，常量存储到 NotInitialization 类的常量池中，NotInitialization 的 Class 文件中并没有 ConstClass 类的符号引用入口，这两个类在编译成 Class 之后就没有任何联系了。 #接口的加载过程 接口加载过程与类加载过程稍有不同。 当一个类在初始化时，要求其父类全部都已经初始化过了，但是一个接口在初始化时，并不要求其父接口全部都完成了初始化，当真正用到父接口的时候才会初始化。","categories":[],"tags":[]},{"title":"JVM类文件结构","slug":"JVM类文件结构","date":"2022-09-27T08:48:43.000Z","updated":"2022-11-22T01:27:53.375Z","comments":true,"path":"2022/09/27/JVM类文件结构/","link":"","permalink":"http://yoursite.com/2022/09/27/JVM%E7%B1%BB%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84/","excerpt":"","text":"JVM 的“无关性” 谈论 JVM 的无关性，主要有以下两个： 平台无关性：任何操作系统都能运行 Java 代码 语言无关性： JVM 能运行除 Java 以外的其他代码 Java 源代码首先需要使用 Javac 编译器编译成 .class 文件，然后由 JVM 执行 .class 文件，从而程序开始运行。 JVM 只认识 .class 文件，它不关心是何种语言生成了 .class 文件，只要 .class 文件符合 JVM 的规范就能运行。 目前已经有 JRuby、Jython、Scala 等语言能够在 JVM 上运行。它们有各自的语法规则，不过它们的编译器 都能将各自的源码编译成符合 JVM 规范的 .class 文件，从而能够借助 JVM 运行它们。 Java 语言中的各种变量、关键字和运算符号的语义最终都是由多条字节码命令组合而成的， 因此字节码命令所能提供的语义描述能力肯定会比 Java 语言本身更加强大。 因此，有一些 Java 语言本身无法有效支持的语言特性，不代表字节码本身无法有效支持。 #Class 文件结构 Class 文件是二进制文件，它的内容具有严格的规范，文件中没有任何空格，全都是连续的 0/1。Class 文件 中的所有内容被分为两种类型：无符号数、表。 无符号数 无符号数表示 Class 文件中的值，这些值没有任何类型，但有不同的长度。u1、u2、u4、u8 分别代表 1/2/4/8 字节的无符号数。 表 由多个无符号数或者其他表作为数据项构成的复合数据类型。 Class 文件具体由以下几个构成: 魔数 版本信息 常量池 访问标志 类索引、父类索引、接口索引集合 字段表集合 方法表集合 属性表集合 #魔数 Class 文件的头 4 个字节称为魔数，用来表示这个 Class 文件的类型。 Class 文件的魔数是用 16 进制表示的“CAFE BABE”，是不是很具有浪漫色彩？ 魔数相当于文件后缀名，只不过后缀名容易被修改，不安全，因此在 Class 文件中标识文件类型比较合适。 #版本信息 紧接着魔数的 4 个字节是版本信息，5-6 字节表示次版本号，7-8 字节表示主版本号，它们表示当前 Class 文件中使用的是哪个版本的 JDK。 高版本的 JDK 能向下兼容以前版本的 Class 文件，但不能运行以后版本的 Class 文件，即使文件格式并未发生任何变化，虚拟机也必须拒绝执行超过其版本号的 Class 文件。 #常量池 版本信息之后就是常量池，常量池中存放两种类型的常量： 字面值常量 字面值常量就是我们在程序中定义的字符串、被 final 修饰的值。 符号引用 符号引用就是我们定义的各种名字：类和接口的全限定名、字段的名字和描述符、方法的名字和描述符。 #常量池的特点 常量池中常量数量不固定，因此常量池开头放置一个 u2 类型的无符号数，用来存储当前常量池的容量。 常量池的每一项常量都是一个表，表开始的第一位是一个 u1 类型的标志位（tag），代表当前这个常量属于哪种常量类型。 #常量池中常量类型 类型 tag 描述 CONSTANT_utf8_info 1 UTF-8 编码的字符串 CONSTANT_Integer_info 3 整型字面量 CONSTANT_Float_info 4 浮点型字面量 CONSTANT_Long_info 5 长整型字面量 CONSTANT_Double_info 6 双精度浮点型字面量 CONSTANT_Class_info 7 类或接口的符号引用 CONSTANT_String_info 8 字符串类型字面量 CONSTANT_Fieldref_info 9 字段的符号引用 CONSTANT_Methodref_info 10 类中方法的符号引用 CONSTANT_InterfaceMethodref_info 11 接口中方法的符号引用 CONSTANT_NameAndType_info 12 字段或方法的符号引用 CONSTANT_MethodHandle_info 15 表示方法句柄 CONSTANT_MethodType_info 16 标识方法类型 CONSTANT_InvokeDynamic_info 18 表示一个动态方法调用点 对于 CONSTANT_Class_info（此类型的常量代表一个类或者接口的符号引用），它的二维表结构如下： 类型 名称 数量 u1 tag 1 u2 name_index 1 tag 是标志位，用于区分常量类型；name_index 是一个索引值，它指向常量池中一个 CONSTANT_Utf8_info 类型常量，此常量代表这个类（或接口）的全限定名，这里 name_index 值若为 0x0002，也即是指向了常量池中的第二项常量。 CONSTANT_Utf8_info 型常量的结构如下： 类型 名称 数量 u1 tag 1 u2 length 1 u1 bytes length tag 是当前常量的类型；length 表示这个字符串的长度；bytes 是这个字符串的内容（采用缩略的 UTF8 编码） #访问标志 在常量池结束之后，紧接着的两个字节代表访问标志，这个标志用于识别一些类或者接口层次的访问信息，包括：这个 Class 是类还是接口；是否定义为 public 类型；是否被 abstract/final 修饰。 #类索引、父类索引、接口索引集合 类索引和父类索引都是一个 u2 类型的数据，而接口索引集合是一组 u2 类型的数据的集合，Class 文件中由这三项数据来确定类的继承关系。类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名。 由于 Java 不允许多重继承，所以父类索引只有一个，除了 java.lang.Object 之外，所有的 Java 类都有父类，因此除了 java.lang.Object 外，所有 Java 类的父类索引都不为 0。一个类可能实现了多个接口，因此用接口索引集合来描述。这个集合第一项为 u2 类型的数据，表示索引表的容量，接下来就是接口的名字索引。 类索引和父类索引用两个 u2 类型的索引值表示，它们各自指向一个类型为 CONSTANT_Class_info 的类描述符常量，通过该常量总的索引值可以找到定义在 CONSTANT_Utf8_info 类型的常量中的全限定名字符串。 #字段表集合 字段表集合存储本类涉及到的成员变量，包括实例变量和类变量，但不包括方法中的局部变量。 每一个字段表只表示一个成员变量，本类中的所有成员变量构成了字段表集合。字段表结构如下： 类型 名称 数量 说明 u2 access_flags 1 字段的访问标志，与类稍有不同 u2 name_index 1 字段名字的索引 u2 descriptor_index 1 描述符，用于描述字段的数据类型。 基本数据类型用大写字母表示； 对象类型用“L 对象类型的全限定名”表示。 u2 attributes_count 1 属性表集合的长度 u2 attributes attributes_count 属性表集合，用于存放属性的额外信息，如属性的值。 字段表集合中不会出现从父类（或接口）中继承而来的字段，但有可能出现原本 Java 代码中不存在的字段，譬如在内部类中为了保持对外部类的访问性，会自动添加指向外部类实例的字段。 #方法表集合 方法表结构与属性表类似。 volatile 关键字 和 transient 关键字不能修饰方法，所以方法表的访问标志中没有 ACC_VOLATILE 和 ACC_TRANSIENT 标志。 方法表的属性表集合中有一张 Code 属性表，用于存储当前方法经编译器编译后的字节码指令。 #属性表集合 每个属性对应一张属性表，属性表的结构如下： 类型 名称 数量 u2 attribute_name_index 1 u4 attribute_length 1 u1 info attribute_length","categories":[],"tags":[]},{"title":"JVM性能调优","slug":"JVM性能调优","date":"2022-09-27T08:47:55.000Z","updated":"2022-11-22T01:27:53.371Z","comments":true,"path":"2022/09/27/JVM性能调优/","link":"","permalink":"http://yoursite.com/2022/09/27/JVM%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/","excerpt":"","text":"VM 性能调优 在高性能硬件上部署程序，目前主要有两种方式： 通过 64 位 JDK 来使用大内存； 使用若干个 32 位虚拟机建立逻辑集群来利用硬件资源。 #使用 64 位 JDK 管理大内存 堆内存变大后，虽然垃圾收集的频率减少了，但每次垃圾回收的时间变长。 如果堆内存为 14 G，那么每次 Full GC 将长达数十秒。如果 Full GC 频繁发生，那么对于一个网站来说是无法忍受的。 对于用户交互性强、对停顿时间敏感的系统，可以给 Java 虚拟机分配超大堆的前提是有把握把应用程序的 Full GC 频率控制得足够低，至少要低到不会影响用户使用。 可能面临的问题： 内存回收导致的长时间停顿； 现阶段，64 位 JDK 的性能普遍比 32 位 JDK 低； 需要保证程序足够稳定，因为这种应用要是产生堆溢出几乎就无法产生堆转储快照（因为要产生超过 10GB 的 Dump 文件），哪怕产生了快照也几乎无法进行分析； 相同程序在 64 位 JDK 消耗的内存一般比 32 位 JDK 大，这是由于指针膨胀，以及数据类型对齐补白等因素导致的。 #使用 32 位 JVM 建立逻辑集群 在一台物理机器上启动多个应用服务器进程，每个服务器进程分配不同端口， 然后在前端搭建一个负载均衡器，以反向代理的方式来分配访问请求。 考虑到在一台物理机器上建立逻辑集群的目的仅仅是为了尽可能利用硬件资源，并不需要关心状态保留、热转移之类的高可用性能需求， 也不需要保证每个虚拟机进程有绝对的均衡负载，因此使用无 Session 复制的亲合式集群是一个不错的选择。 我们仅仅需要保障集群具备亲合性，也就是均衡器按一定的规则算法（一般根据 SessionID 分配） 将一个固定的用户请求永远分配到固定的一个集群节点进行处理即可。 可能遇到的问题： 尽量避免节点竞争全局资源，如磁盘竞争，各个节点如果同时访问某个磁盘文件的话，很可能导致 IO 异常； 很难高效利用资源池，如连接池，一般都是在节点建立自己独立的连接池，这样有可能导致一些节点池满了而另外一些节点仍有较多空余； 各个节点受到 32 位的内存限制； 大量使用本地缓存的应用，在逻辑集群中会造成较大的内存浪费，因为每个逻辑节点都有一份缓存，这时候可以考虑把本地缓存改成集中式缓存。 #调优案例分析与实战 #场景描述 一个小型系统，使用 32 位 JDK，4G 内存，测试期间发现服务端不定时抛出内存溢出异常。 加入 -XX:+HeapDumpOnOutOfMemoryError（添加这个参数后，堆内存溢出时就会输出异常日志）， 但再次发生内存溢出时，没有生成相关异常日志。 #分析 在 32 位 JDK 上，1.6G 分配给堆，还有一部分分配给 JVM 的其他内存，直接内存最大也只能在剩余的 0.4G 空间中分出一部分， 如果使用了 NIO，JVM 会在 JVM 内存之外分配内存空间，那么就要小心“直接内存”不足时发生内存溢出异常了。 #直接内存的回收过程 直接内存虽然不是 JVM 内存空间，但它的垃圾回收也由 JVM 负责。 垃圾收集进行时，虚拟机虽然会对直接内存进行回收， 但是直接内存却不能像新生代、老年代那样，发现空间不足了就通知收集器进行垃圾回收， 它只能等老年代满了后 Full GC，然后“顺便”帮它清理掉内存的废弃对象。 否则只能一直等到抛出内存溢出异常时，先 catch 掉，再在 catch 块里大喊 “System.gc()”。 要是虚拟机还是不听，那就只能眼睁睁看着堆中还有许多空闲内存，自己却不得不抛出内存溢出异常了。","categories":[],"tags":[]},{"title":"JVM内存分配和回收","slug":"JVM内存分配和回收","date":"2022-09-27T08:47:14.000Z","updated":"2022-11-22T01:27:53.367Z","comments":true,"path":"2022/09/27/JVM内存分配和回收/","link":"","permalink":"http://yoursite.com/2022/09/27/JVM%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%92%8C%E5%9B%9E%E6%94%B6/","excerpt":"","text":"对象的内存分配，就是在堆上分配（也可能经过 JIT 编译后被拆散为标量类型并间接在栈上分配），对象主要分配在新生代的 Eden 区上，少数情况下可能直接分配在老年代，分配规则不固定，取决于当前使用的垃圾收集器组合以及相关的参数配置。 以下列举几条最普遍的内存分配规则，供大家学习。 #对象优先在 Eden 分配 大多数情况下，对象在新生代 Eden 区中分配。当 Eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC。 👇Minor GC vs Major GC/Full GC： Minor GC：回收新生代（包括 Eden 和 Survivor 区域），因为 Java 对象大多都具备朝生夕灭的特性，所以 Minor GC 非常频繁，一般回收速度也比较快。 Major GC / Full GC：回收老年代，出现了 Major GC，经常会伴随至少一次的 Minor GC，但这并非绝对。Major GC 的速度一般会比 Minor GC 慢 10 倍 以上。 在 JVM 规范中，Major GC 和 Full GC 都没有一个正式的定义，所以有人也简单地认为 Major GC 清理老年代，而 Full GC 清理整个内存堆。 #大对象直接进入老年代 大对象是指需要大量连续内存空间的 Java 对象，如很长的字符串或数据。 一个大对象能够存入 Eden 区的概率比较小，发生分配担保的概率比较大，而分配担保需要涉及大量的复制，就会造成效率低下。 虚拟机提供了一个 -XX:PretenureSizeThreshold 参数，令大于这个设置值的对象直接在老年代分配，这样做的目的是避免在 Eden 区及两个 Survivor 区之间发生大量的内存复制。（还记得吗，新生代采用复制算法回收垃圾） #长期存活的对象将进入老年代 JVM 给每个对象定义了一个对象年龄计数器。当新生代发生一次 Minor GC 后，存活下来的对象年龄 +1，当年龄超过一定值时，就将超过该值的所有对象转移到老年代中去。 使用 -XXMaxTenuringThreshold 设置新生代的最大年龄，只要超过该参数的新生代对象都会被转移到老年代中去。 #动态对象年龄判定 如果当前新生代的 Survivor 中，相同年龄所有对象大小的总和大于 Survivor 空间的一半，年龄 &gt;= 该年龄的对象就可以直接进入老年代，无须等到 MaxTenuringThreshold 中要求的年龄。 #空间分配担保 JDK 6 Update 24 之前的规则是这样的： 在发生 Minor GC 之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间， 如果这个条件成立，Minor GC 可以确保是安全的； 如果不成立，则虚拟机会查看 HandlePromotionFailure 值是否设置为允许担保失败， 如果是，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小， 如果大于，将尝试进行一次 Minor GC，尽管这次 Minor GC 是有风险的； 如果小于，或者 HandlePromotionFailure 设置不允许冒险，那此时也要改为进行一次 Full GC。 JDK 6 Update 24 之后的规则变为： 只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小，就会进行 Minor GC，否则将进行 Full GC。 通过清除老年代中的废弃数据来扩大老年代空闲空间，以便给新生代作担保。 这个过程就是分配担保。 👇 总结一下有哪些情况可能会触发 JVM 进行 Full GC。 System.gc() 方法的调用 此方法的调用是建议 JVM 进行 Full GC，注意这只是建议而非一定，但在很多情况下它会触发 Full GC，从而增加 Full GC 的频率。通常情况下我们只需要让虚拟机自己去管理内存即可，我们可以通过 -XX:+ DisableExplicitGC 来禁止调用 System.gc()。 老年代空间不足 老年代空间不足会触发 Full GC 操作，若进行该操作后空间依然不足，则会抛出如下错误：java.lang.OutOfMemoryError: Java heap space 永久代空间不足 JVM 规范中运行时数据区域中的方法区，在 HotSpot 虚拟机中也称为永久代（Permanet Generation），存放一些类信息、常量、静态变量等数据，当系统要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，会触发 Full GC。如果经过 Full GC 仍然回收不了，那么 JVM 会抛出如下错误信息：java.lang.OutOfMemoryError: PermGen space CMS GC 时出现 promotion failed 和 concurrent mode failure promotion failed，就是上文所说的担保失败，而 concurrent mode failure 是在执行 CMS GC 的过程中同时有对象要放入老年代，而此时老年代空间不足造成的。 统计得到的 Minor GC 晋升到旧生代的平均大小大于老年代的剩余空间。","categories":[],"tags":[]},{"title":"JVM垃圾收集","slug":"JVM垃圾收集","date":"2022-09-27T08:45:46.000Z","updated":"2022-11-22T01:27:53.370Z","comments":true,"path":"2022/09/27/JVM垃圾收集/","link":"","permalink":"http://yoursite.com/2022/09/27/JVM%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86/","excerpt":"","text":"垃圾收集策略与算法 程序计数器、虚拟机栈、本地方法栈随线程而生，也随线程而灭；栈帧随着方法的开始而入栈，随着方法的结束而出栈。这几个区域的内存分配和回收都具有确定性，在这几个区域内不需要过多考虑回收的问题，因为方法结束或者线程结束时，内存自然就跟随着回收了。 而对于 Java 堆和方法区，我们只有在程序运行期间才能知道会创建哪些对象，这部分内存的分配和回收都是动态的，垃圾收集器所关注的正是这部分内存。 #判定对象是否存活 若一个对象不被任何对象或变量引用，那么它就是无效对象，需要被回收。 #引用计数法 在对象头维护着一个 counter 计数器，对象被引用一次则计数器 +1；若引用失效则计数器 -1。当计数器为 0 时，就认为该对象无效了。 引用计数算法的实现简单，判定效率也很高，在大部分情况下它都是一个不错的算法。但是主流的 Java 虚拟机里没有选用引用计数算法来管理内存，主要是因为它很难解决对象之间循环引用的问题。（虽然循环引用的问题可通过 Recycler 算法解决，但是在多线程环境下，引用计数变更也要进行昂贵的同步操作，性能较低，早期的编程语言会采用此算法。） 举个栗子 👉 对象 objA 和 objB 都有字段 instance，令 objA.instance = objB 并且 objB.instance = objA，由于它们互相引用着对方，导致它们的引用计数都不为 0，于是引用计数算法无法通知 GC 收集器回收它们。 # 可达性分析法 所有和 GC Roots 直接或间接关联的对象都是有效对象，和 GC Roots 没有关联的对象就是无效对象。 GC Roots 是指： Java 虚拟机栈（栈帧中的本地变量表）中引用的对象 本地方法栈中引用的对象 方法区中常量引用的对象 方法区中类静态属性引用的对象 GC Roots 并不包括堆中对象所引用的对象，这样就不会有循环引用的问题。 #引用的种类 判定对象是否存活与“引用”有关。在 JDK 1.2 以前，Java 中的引用定义很传统，一个对象只有被引用或者没有被引用两种状态，我们希望能描述这一类对象：当内存空间还足够时，则保留在内存中；如果内存空间在进行垃圾收集后还是非常紧张，则可以抛弃这些对象。很多系统的缓存功能都符合这样的应用场景。 在 JDK 1.2 之后，Java 对引用的概念进行了扩充，将引用分为了以下四种。不同的引用类型，主要体现的是对象不同的可达性状态reachable和垃圾收集的影响。 #强引用（Strong Reference） 类似 “Object obj = new Object()” 这类的引用，就是强引用，只要强引用存在，垃圾收集器永远不会回收被引用的对象。但是，如果我们错误地保持了强引用，比如：赋值给了 static 变量，那么对象在很长一段时间内不会被回收，会产生内存泄漏。 #软引用（Soft Reference） 软引用是一种相对强引用弱化一些的引用，可以让对象豁免一些垃圾收集，只有当 JVM 认为内存不足时，才会去试图回收软引用指向的对象。JVM 会确保在抛出 OutOfMemoryError 之前，清理软引用指向的对象。软引用通常用来实现内存敏感的缓存，如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。 #弱引用（Weak Reference） 弱引用的强度比软引用更弱一些。当 JVM 进行垃圾回收时，无论内存是否充足，都会回收只被弱引用关联的对象。 #虚引用（Phantom Reference） 虚引用也称幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响。它仅仅是提供了一种确保对象被 finalize 以后，做某些事情的机制，比如，通常用来做所谓的 Post-Mortem 清理机制。 #回收堆中无效对象 对于可达性分析中不可达的对象，也并不是没有存活的可能。 #判定 finalize() 是否有必要执行 JVM 会判断此对象是否有必要执行 finalize() 方法，如果对象没有覆盖 finalize() 方法，或者 finalize() 方法已经被虚拟机调用过，那么视为“没有必要执行”。那么对象基本上就真的被回收了。 如果对象被判定为有必要执行 finalize() 方法，那么对象会被放入一个 F-Queue 队列中，虚拟机会以较低的优先级执行这些 finalize()方法，但不会确保所有的 finalize() 方法都会执行结束。如果 finalize() 方法出现耗时操作，虚拟机就直接停止指向该方法，将对象清除。 #对象重生或死亡 如果在执行 finalize() 方法时，将 this 赋给了某一个引用，那么该对象就重生了。如果没有，那么就会被垃圾收集器清除。 任何一个对象的 finalize() 方法只会被系统自动调用一次，如果对象面临下一次回收，它的 finalize() 方法不会被再次执行，想继续在 finalize() 中自救就失效了。 #回收方法区内存 方法区中存放生命周期较长的类信息、常量、静态变量，每次垃圾收集只有少量的垃圾被清除。方法区中主要清除两种垃圾： 废弃常量 无用的类 #判定废弃常量 只要常量池中的常量不被任何变量或对象引用，那么这些常量就会被清除掉。比如，一个字符串 “bingo” 进入了常量池，但是当前系统没有任何一个 String 对象引用常量池中的 “bingo” 常量，也没有其它地方引用这个字面量，必要的话，&quot;bingo&quot;常量会被清理出常量池。 #判定无用的类 判定一个类是否是“无用的类”，条件较为苛刻。 该类的所有对象都已经被清除 加载该类的 ClassLoader 已经被回收 该类的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 一个类被虚拟机加载进方法区，那么在堆中就会有一个代表该类的对象：java.lang.Class。这个对象在类被加载进方法区时创建，在方法区该类被删除时清除。 #垃圾收集算法 学会了如何判定无效对象、无用类、废弃常量之后，剩余工作就是回收这些垃圾。常见的垃圾收集算法有以下几个： #标记-清除算法 标记的过程是：遍历所有的 GC Roots，然后将所有 GC Roots 可达的对象标记为存活的对象。 清除的过程将遍历堆中所有的对象，将没有标记的对象全部清除掉。与此同时，清除那些被标记过的对象的标记，以便下次的垃圾回收。 这种方法有两个不足： 效率问题：标记和清除两个过程的效率都不高。 空间问题：标记清除之后会产生大量不连续的内存碎片，碎片太多可能导致以后需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 #复制算法（新生代） 为了解决效率问题，“复制”收集算法出现了。它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块内存用完，需要进行垃圾收集时，就将存活者的对象复制到另一块上面，然后将第一块内存全部清除。这种算法有优有劣： 优点：不会有内存碎片的问题。 缺点：内存缩小为原来的一半，浪费空间。 为了解决空间利用率问题，可以将内存分为三块： Eden、From Survivor、To Survivor，比例是 8:1:1，每次使用 Eden 和其中一块 Survivor。回收时，将 Eden 和 Survivor 中还存活的对象一次性复制到另外一块 Survivor 空间上，最后清理掉 Eden 和刚才使用的 Survivor 空间。这样只有 10% 的内存被浪费。 但是我们无法保证每次回收都只有不多于 10% 的对象存活，当 Survivor 空间不够，需要依赖其他内存（指老年代）进行分配担保。 #分配担保 为对象分配内存空间时，如果 Eden+Survivor 中空闲区域无法装下该对象，会触发 MinorGC 进行垃圾收集。但如果 Minor GC 过后依然有超过 10% 的对象存活，这样存活的对象直接通过分配担保机制进入老年代，然后再将新对象存入 Eden 区。 #标记-整理算法（老年代） 标记：它的第一个阶段与标记-清除算法是一模一样的，均是遍历 GC Roots，然后将存活的对象标记。 整理：移动所有存活的对象，且按照内存地址次序依次排列，然后将末端内存地址以后的内存全部回收。因此，第二阶段才称为整理阶段。 这是一种老年代的垃圾收集算法。老年代的对象一般寿命比较长，因此每次垃圾回收会有大量对象存活，如果采用复制算法，每次需要复制大量存活的对象，效率很低。 #分代收集算法 根据对象存活周期的不同，将内存划分为几块。一般是把 Java 堆分为新生代和老年代，针对各个年代的特点采用最适当的收集算法。 新生代：复制算法 老年代：标记-清除算法、标记-整理算法","categories":[],"tags":[]},{"title":"Hotspot虚拟机的对象","slug":"Hotspot虚拟机的对象","date":"2022-09-27T08:43:35.000Z","updated":"2022-11-22T01:27:53.362Z","comments":true,"path":"2022/09/27/Hotspot虚拟机的对象/","link":"","permalink":"http://yoursite.com/2022/09/27/Hotspot%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%9A%84%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"对象的内存布局 在 HotSpot 虚拟机中，对象的内存布局分为以下 3 块区域： 对象头（Header） 实例数据（Instance Data） 对齐填充（Padding） #对象头 对象头记录了对象在运行过程中所需要使用的一些数据： 哈希码 GC 分代年龄 锁状态标志 线程持有的锁 偏向线程 ID 偏向时间戳 对象头可能包含类型指针，通过该指针能确定对象属于哪个类。如果对象是一个数组，那么对象头还会包括数组长度。 #实例数据 实例数据部分就是成员变量的值，其中包括父类成员变量和本类成员变量。 #对齐填充 用于确保对象的总长度为 8 字节的整数倍。 HotSpot VM 的自动内存管理系统要求对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 对齐填充并不是必然存在，也没有特别的含义，它仅仅起着占位符的作用。 #对象的创建过程 #类加载检查 虚拟机在解析.class文件时，若遇到一条 new 指令，首先它会去检查常量池中是否有这个类的符号引用，并且检查这个符号引用所代表的类是否已被加载、解析和初始化过。如果没有，那么必须先执行相应的类加载过程。 #为新生对象分配内存 对象所需内存的大小在类加载完成后便可完全确定，接下来从堆中划分一块对应大小的内存空间给新的对象。分配堆中内存有两种方式： 指针碰撞 如果 Java 堆中内存绝对规整（说明采用的是“复制算法”或“标记整理法”），空闲内存和已使用内存中间放着一个指针作为分界点指示器，那么分配内存时只需要把指针向空闲内存挪动一段与对象大小一样的距离，这种分配方式称为“指针碰撞”。 空闲列表 如果 Java 堆中内存并不规整，已使用的内存和空闲内存交错（说明采用的是标记-清除法，有碎片），此时没法简单进行指针碰撞， VM 必须维护一个列表，记录其中哪些内存块空闲可用。分配之时从空闲列表中找到一块足够大的内存空间划分给对象实例。这种方式称为“空闲列表”。 #初始化 分配完内存后，为对象中的成员变量赋上初始值，设置对象头信息，调用对象的构造函数方法进行初始化。 至此，整个对象的创建过程就完成了。 #对象的访问方式 所有对象的存储空间都是在堆中分配的，但是这个对象的引用却是在堆栈中分配的。也就是说在建立一个对象时两个地方都分配内存，在堆中分配的内存实际建立这个对象，而在堆栈中分配的内存只是一个指向这个堆对象的指针（引用）而已。 那么根据引用存放的地址类型的不同，对象有不同的访问方式。 #句柄访问方式 堆中需要有一块叫做“句柄池”的内存空间，句柄中包含了对象实例数据与类型数据各自的具体地址信息。 引用类型的变量存放的是该对象的句柄地址（reference）。访问对象时，首先需要通过引用类型的变量找到该对象的句柄，然后根据句柄中对象的地址找到对象。 #直接指针访问方式 引用类型的变量直接存放对象的地址，从而不需要句柄池，通过引用能够直接访问对象。但对象所在的内存空间需要额外的策略存储对象所属的类信息的地址。 需要说明的是，HotSpot 采用第二种方式，即直接指针方式来访问对象，只需要一次寻址操作，所以在性能上比句柄访问方式快一倍。但像上面所说，它需要额外的策略来存储对象在方法区中类信息的地址。","categories":[],"tags":[]},{"title":"JVM内存结构","slug":"JVM内存结构","date":"2022-09-27T08:42:07.000Z","updated":"2022-11-22T01:27:53.368Z","comments":true,"path":"2022/09/27/JVM内存结构/","link":"","permalink":"http://yoursite.com/2022/09/27/JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/","excerpt":"","text":"JVM 内存结构 Java 虚拟机的内存空间分为 5 个部分： 程序计数器 Java 虚拟机栈 本地方法栈 堆 方法区 JDK 1.8 同 JDK 1.7 比，最大的差别就是：元数据区取代了永久代。元空间的本质和永久代类似，都是对 JVM 规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元数据空间并不在虚拟机中，而是使用本地内存。 #程序计数器（PC 寄存器） #程序计数器的定义 程序计数器是一块较小的内存空间，是当前线程正在执行的那条字节码指令的地址。若当前线程正在执行的是一个本地方法，那么此时程序计数器为Undefined。 #程序计数器的作用 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制。 在多线程情况下，程序计数器记录的是当前线程执行的位置，从而当线程切换回来时，就知道上次线程执行到哪了。 #程序计数器的特点 是一块较小的内存空间。 线程私有，每条线程都有自己的程序计数器。 生命周期：随着线程的创建而创建，随着线程的结束而销毁。 是唯一一个不会出现 OutOfMemoryError 的内存区域。 #Java 虚拟机栈（Java 栈） #Java 虚拟机栈的定义 Java 虚拟机栈是描述 Java 方法运行过程的内存模型。 Java 虚拟机栈会为每一个即将运行的 Java 方法创建一块叫做“栈帧”的区域，用于存放该方法运行过程中的一些信息，如： 局部变量表 操作数栈 动态链接 方法出口信息 … #压栈出栈过程 当方法运行过程中需要创建局部变量时，就将局部变量的值存入栈帧中的局部变量表中。 Java 虚拟机栈的栈顶的栈帧是当前正在执行的活动栈，也就是当前正在执行的方法，PC 寄存器也会指向这个地址。只有这个活动的栈帧的本地变量可以被操作数栈使用，当在这个栈帧中调用另一个方法，与之对应的栈帧又会被创建，新创建的栈帧压入栈顶，变为当前的活动栈帧。 方法结束后，当前栈帧被移出，栈帧的返回值变成新的活动栈帧中操作数栈的一个操作数。如果没有返回值，那么新的活动栈帧中操作数栈的操作数没有变化。 由于 Java 虚拟机栈是与线程对应的，数据不是线程共享的（也就是线程私有的)，因此不用关心数据一致性问题，也不会存在同步锁的问题。 #局部变量表 定义为一个数字数组，主要用于存储方法参数、定义在方法体内部的局部变量，数据类型包括各类基本数据类型，对象引用，以及 return address 类型。 局部变量表容量大小是在编译期确定下来的。最基本的存储单元是 slot，32 位占用一个 slot，64 位类型（long 和 double）占用两个 slot。 对于 slot 的理解： JVM 虚拟机会为局部变量表中的每个 slot 都分配一个访问索引，通过这个索引即可成功访问到局部变量表中指定的局部变量值。 如果当前帧是由构造方法或者实例方法创建的，那么该对象引用 this，会存放在 index 为 0 的 slot 处，其余的参数表顺序继续排列。 栈帧中的局部变量表中的槽位是可以重复的，如果一个局部变量过了其作用域，那么其作用域之后申明的新的局部变量就有可能会复用过期局部变量的槽位，从而达到节省资源的目的。 在栈帧中，与性能调优关系最密切的部分，就是局部变量表，方法执行时，虚拟机使用局部变量表完成方法的传递局部变量表中的变量也是重要的垃圾回收根节点，只要被局部变量表中直接或间接引用的对象都不会被回收。 #操作数栈 栈顶缓存技术：由于操作数是存储在内存中，频繁的进行内存读写操作影响执行速度，将栈顶元素全部缓存到物理 CPU 的寄存器中，以此降低对内存的读写次数，提升执行引擎的执行效率。 每一个操作数栈会拥有一个明确的栈深度，用于存储数值，最大深度在编译期就定义好。32bit 类型占用一个栈单位深度，64bit 类型占用两个栈单位深度操作数栈。 并非采用访问索引方式进行数据访问，而是只能通过标准的入栈、出栈操作完成一次数据访问。 #方法的调用 静态链接：当一个字节码文件被装载进 JVM 内部时，如果被调用的目标方法在编译期可知，且运行时期间保持不变，这种情况下降调用方的符号引用转为直接引用的过程称为静态链接。 动态链接：如果被调用的方法无法在编译期被确定下来，只能在运行期将调用的方法的符号引用转为直接引用，这种引用转换过程具备动态性，因此被称为动态链接。 方法绑定 早期绑定：被调用的目标方法如果在编译期可知，且运行期保持不变。 晚期绑定：被调用的方法在编译期无法被确定，只能够在程序运行期根据实际的类型绑定相关的方法。 非虚方法：如果方法在编译期就确定了具体的调用版本，则这个版本在运行时是不可变的。这样的方法称为非虚方法静态方法，私有方法，final 方法，实例构造器，父类方法都是非虚方法，除了这些以外都是虚方法。 虚方法表：面向对象的编程中，会很频繁的使用动态分配，如果每次动态分配的过程都要重新在类的方法元数据中搜索合适的目标的话，就可能影响到执行效率，因此为了提高性能，JVM 采用在类的方法区建立一个虚方法表，使用索引表来代替查找。 每个类都有一个虚方法表，表中存放着各个方法的实际入口。 虚方法表会在类加载的链接阶段被创建，并开始初始化，类的变量初始值准备完成之后，JVM 会把该类的方法也初始化完毕。 方法重写的本质 找到操作数栈顶的第一个元素所执行的对象的实际类型，记做 C。如果在类型 C 中找到与常量池中描述符和简单名称都相符的方法，则进行访问权限校验。 如果通过则返回这个方法的直接引用，查找过程结束；如果不通过，则返回 java.lang.IllegalAccessError 异常。 否则，按照继承关系从下往上依次对 C 的各个父类进行上一步的搜索和验证过程。 如果始终没有找到合适的方法，则抛出 java.lang.AbstractMethodError 异常。 Java 中任何一个普通方法都具备虚函数的特征（运行期确认，具备晚期绑定的特点），C++ 中则使用关键字 virtual 来显式定义。如果在 Java 程序中，不希望某个方法拥有虚函数的特征，则可以使用关键字 final 来标记这个方法。 #Java 虚拟机栈的特点 运行速度特别快，仅仅次于 PC 寄存器。 局部变量表随着栈帧的创建而创建，它的大小在编译时确定，创建时只需分配事先规定的大小即可。在方法运行过程中，局部变量表的大小不会发生改变。 Java 虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。 StackOverFlowError 若 Java 虚拟机栈的大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度时，抛出 StackOverFlowError 异常。 OutOfMemoryError 若允许动态扩展，那么当线程请求栈时内存用完了，无法再动态扩展时，抛出 OutOfMemoryError 异常。 Java 虚拟机栈也是线程私有，随着线程创建而创建，随着线程的结束而销毁。 出现 StackOverFlowError 时，内存空间可能还有很多。 常见的运行时异常有： NullPointerException - 空指针引用异常 ClassCastException - 类型强制转换异 IllegalArgumentException - 传递非法参数异常 ArithmeticException - 算术运算异常 ArrayStoreException - 向数组中存放与声明类型不兼容对象异常 IndexOutOfBoundsException - 下标越界异常 NegativeArraySizeException - 创建一个大小为负数的数组错误异常 NumberFormatException - 数字格式异常 SecurityException - 安全异常 UnsupportedOperationException - 不支持的操作异常 #本地方法栈（C 栈） #本地方法栈的定义 本地方法栈是为 JVM 运行 Native 方法准备的空间，由于很多 Native 方法都是用 C 语言实现的，所以它通常又叫 C 栈。它与 Java 虚拟机栈实现的功能类似，只不过本地方法栈是描述本地方法运行过程的内存模型。 #栈帧变化过程 本地方法被执行时，在本地方法栈也会创建一块栈帧，用于存放该方法的局部变量表、操作数栈、动态链接、方法出口信息等。 方法执行结束后，相应的栈帧也会出栈，并释放内存空间。也会抛出 StackOverFlowError 和 OutOfMemoryError 异常。 如果 Java 虚拟机本身不支持 Native 方法，或是本身不依赖于传统栈，那么可以不提供本地方法栈。如果支持本地方法栈，那么这个栈一般会在线程创建的时候按线程分配。 #堆 #堆的定义 堆是用来存放对象的内存空间，几乎所有的对象都存储在堆中。 #堆的特点 线程共享，整个 Java 虚拟机只有一个堆，所有的线程都访问同一个堆。而程序计数器、Java 虚拟机栈、本地方法栈都是一个线程对应一个。 在虚拟机启动时创建。 是垃圾回收的主要场所。 堆可分为新生代（Eden 区：From Survior，To Survivor）、老年代。 Java 虚拟机规范规定，堆可以处于物理上不连续的内存空间中，但在逻辑上它应该被视为连续的。 关于 Survivor s0，s1 区: 复制之后有交换，谁空谁是 to。 不同的区域存放不同生命周期的对象，这样可以根据不同的区域使用不同的垃圾回收算法，更具有针对性。 堆的大小既可以固定也可以扩展，但对于主流的虚拟机，堆的大小是可扩展的，因此当线程请求分配内存，但堆已满，且内存已无法再扩展时，就抛出 OutOfMemoryError 异常。 Java 堆所使用的内存不需要保证是连续的。而由于堆是被所有线程共享的，所以对它的访问需要注意同步问题，方法和对应的属性都需要保证一致性。 #新生代与老年代 老年代比新生代生命周期长。 新生代与老年代空间默认比例 1:2：JVM 调参数，XX:NewRatio=2，表示新生代占 1，老年代占 2，新生代占整个堆的 1/3。 HotSpot 中，Eden 空间和另外两个 Survivor 空间缺省所占的比例是：8:1:1。 几乎所有的 Java 对象都是在 Eden 区被 new 出来的，Eden 放不了的大对象，就直接进入老年代了。 #对象分配过程 new 的对象先放在 Eden 区，大小有限制 如果创建新对象时，Eden 空间填满了，就会触发 Minor GC，将 Eden 不再被其他对象引用的对象进行销毁，再加载新的对象放到 Eden 区，特别注意的是 Survivor 区满了是不会触发 Minor GC 的，而是 Eden 空间填满了，Minor GC 才顺便清理 Survivor 区 将 Eden 中剩余的对象移到 Survivor0 区 再次触发垃圾回收，此时上次 Survivor 下来的，放在 Survivor0 区的，如果没有回收，就会放到 Survivor1 区 再次经历垃圾回收，又会将幸存者重新放回 Survivor0 区，依次类推 默认是 15 次的循环，超过 15 次，则会将幸存者区幸存下来的转去老年区 jvm 参数设置次数 : -XX:MaxTenuringThreshold=N 进行设置 频繁在新生区收集，很少在养老区收集，几乎不在永久区/元空间搜集 #Full GC /Major GC 触发条件 显示调用System.gc()，老年代的空间不够，方法区的空间不够等都会触发 Full GC，同时对新生代和老年代回收，FUll GC 的 STW 的时间最长，应该要避免 在出现 Major GC 之前，会先触发 Minor GC，如果老年代的空间还是不够就会触发 Major GC，STW 的时间长于 Minor GC #逃逸分析 #标量替换 标量不可在分解的量，java 的基本数据类型就是标量，标量的对立就是可以被进一步分解的量，而这种量称之为聚合量。而在 JAVA 中对象就是可以被进一步分解的聚合量 替换过程，通过逃逸分析确定该对象不会被外部访问，并且对象可以被进一步分解时，JVM 不会创建该对象，而会将该对象成员变量分解若干个被这个方法使用的成员变量所代替。这些代替的成员变量在栈帧或寄存器上分配空间。 对象和数组并非都是在堆上分配内存的 《深入理解 Java 虚拟机中》关于 Java 堆内存有这样一段描述：随着 JIT 编译期的发展与逃逸分析技术逐渐成熟，栈上分配,标量替换优化技术将会导致一些变化，所有的对象都分配到堆上也渐渐变得不那么&quot;绝对&quot;了。 这是一种可以有效减少 Java 内存堆分配压力的分析算法，通过逃逸分析，Java Hotspot 编译器能够分析出一个新的对象的引用的使用范围从而决定是否要将这个对象分配到堆上。 当一个对象在方法中被定义后，它可能被外部方法所引用，如作为调用参数传递到其他地方中，称为方法逃逸。 再如赋值给类变量或可以在其他线程中访问的实例变量，称为线程逃逸 使用逃逸分析，编译器可以对代码做如下优化： 同步省略：如果一个对象被发现只能从一个线程被访问到，那么对于这个对象的操作可以不考虑同步。 将堆分配转化为栈分配：如果一个对象在子程序中被分配，要使指向该对象的指针永远不会逃逸，对象可能是栈分配的候选，而不是堆分配。 分离对象或标量替换：有的对象可能不需要作为一个连续的内存结构存在也可以被访问到，那么对象的部分（或全部）可以不存储在内存，而是存储在 CPU 寄存器中。 12345678910public static StringBuffer createStringBuffer(String s1, String s2) &#123; StringBuffer s = new StringBuffer(); s.append(s1); s.append(s2); return s;&#125; s 是一个方法内部变量，上边的代码中直接将 s 返回，这个 StringBuffer 的对象有可能被其他方法所改变，导致它的作用域就不只是在方法内部，即使它是一个局部变量，但还是逃逸到了方法外部，称为方法逃逸。 还有可能被外部线程访问到，譬如赋值给类变量或可以在其他线程中访问的实例变量，称为线程逃逸。 在编译期间，如果 JIT 经过逃逸分析，发现有些对象没有逃逸出方法，那么有可能堆内存分配会被优化成栈内存分配。 jvm 参数设置，-XX:+DoEscapeAnalysis ：开启逃逸分析 ，-XX:-DoEscapeAnalysis ： 关闭逃逸分析 从 jdk 1.7 开始已经默认开始逃逸分析。 #TLAB TLAB 的全称是 Thread Local Allocation Buffer，即线程本地分配缓存区，是属于 Eden 区的，这是一个线程专用的内存分配区域，线程私有，默认开启的（当然也不是绝对的，也要看哪种类型的虚拟机） 堆是全局共享的，在同一时间，可能会有多个线程在堆上申请空间，但每次的对象分配需要同步的进行（虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性）但是效率却有点下降 所以用 TLAB 来避免多线程冲突，在给对象分配内存时，每个线程使用自己的 TLAB，这样可以使得线程同步，提高了对象分配的效率 当然并不是所有的对象都可以在 TLAB 中分配内存成功，如果失败了就会使用加锁的机制来保持操作的原子性 -XX:+UseTLAB使用 TLAB，-XX:+TLABSize 设置 TLAB 大小 #四种引用方式 强引用：创建一个对象并把这个对象赋给一个引用变量，普通 new 出来对象的变量引用都是强引用，有引用变量指向时永远不会被垃圾回收，jvm 即使抛出 OOM，可以将引用赋值为 null，那么它所指向的对象就会被垃圾回收。 软引用：如果一个对象具有软引用，内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。 弱引用：非必需对象，当 JVM 进行垃圾回收时，无论内存是否充足，都会回收被弱引用关联的对象。 虚引用：虚引用并不会决定对象的生命周期，如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。 #方法区 #方法区的定义 Java 虚拟机规范中定义方法区是堆的一个逻辑部分。方法区存放以下信息： 已经被虚拟机加载的类信息 常量 静态变量 即时编译器编译后的代码 #方法区的特点 线程共享。 方法区是堆的一个逻辑部分，因此和堆一样，都是线程共享的。整个虚拟机中只有一个方法区。 永久代。 方法区中的信息一般需要长期存在，而且它又是堆的逻辑分区，因此用堆的划分方法，把方法区称为“永久代”。 内存回收效率低。 方法区中的信息一般需要长期存在，回收一遍之后可能只有少量信息无效。主要回收目标是：对常量池的回收；对类型的卸载。 Java 虚拟机规范对方法区的要求比较宽松。 和堆一样，允许固定大小，也允许动态扩展，还允许不实现垃圾回收。 #运行时常量池 方法区中存放：类信息、常量、静态变量、即时编译器编译后的代码。常量就存放在运行时常量池中。 当类被 Java 虚拟机加载后， .class 文件中的常量就存放在方法区的运行时常量池中。而且在运行期间，可以向常量池中添加新的常量。如 String 类的 intern() 方法就能在运行期间向常量池中添加字符串常量。 #直接内存（堆外内存） 直接内存是除 Java 虚拟机之外的内存，但也可能被 Java 使用。 #操作直接内存 在 NIO 中引入了一种基于通道和缓冲的 IO 方式。它可以通过调用本地方法直接分配 Java 虚拟机之外的内存，然后通过一个存储在堆中的DirectByteBuffer对象直接操作该内存，而无须先将外部内存中的数据复制到堆中再进行操作，从而提高了数据操作的效率。 直接内存的大小不受 Java 虚拟机控制，但既然是内存，当内存不足时就会抛出 OutOfMemoryError 异常。 #直接内存与堆内存比较 直接内存申请空间耗费更高的性能 直接内存读取 IO 的性能要优于普通的堆内存 直接内存作用链： 本地 IO -&gt; 直接内存 -&gt; 本地 IO 堆内存作用链：本地 IO -&gt; 直接内存 -&gt; 非直接内存 -&gt; 直接内存 -&gt; 本地 IO 服务器管理员在配置虚拟机参数时，会根据实际内存设置-Xmx等参数信息，但经常忽略直接内存，使得各个内存区域总和大于物理内存限制，从而导致动态扩展时出现OutOfMemoryError异常。","categories":[],"tags":[]},{"title":"JVM命令","slug":"JVM命令","date":"2022-09-27T07:54:54.000Z","updated":"2023-04-23T03:31:28.277Z","comments":true,"path":"2022/09/27/JVM命令/","link":"","permalink":"http://yoursite.com/2022/09/27/JVM%E5%91%BD%E4%BB%A4/","excerpt":"","text":"1.分类 123456大体分为： 1.监控类工具：jps, jstat 2.故障排查工具: jinfo, jmap, jstack, jcmd, jhat, jhsdb 3.可视化工具:jconsole,visualvm,jhsdb 1.监控工具 1.jps主要监控进程 123456789101112131415参考文档： java 8 ：https:&#x2F;&#x2F;docs.oracle.com&#x2F;javase&#x2F;8&#x2F;docs&#x2F;technotes&#x2F;tools&#x2F;unix&#x2F;jps.html使用说明: 1. 直接键入 jps &#x2F;&#x2F;显示java进程 2. jps -m &#x2F;&#x2F;显示传入main方法参数 3. jps -ml &#x2F;&#x2F;显示传入main方法参数，同时显示启动类包名 4. jps -v &#x2F;&#x2F;显示传递给jvm的参数jps 命令有点类似 ps -ef | grep java 命令，但 jps 命令显示的格式更友好。 jps还可以查看远程服务器的jvm进程信息，需要一些配置，使用rmi协议 jps -l remote.domain &#x2F;&#x2F;端口1099 jps -l rmi:&#x2F;&#x2F;remote.comain:1231 2.jstat监控jvm各种运行状态 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667参考文档： java 8 : https:&#x2F;&#x2F;docs.oracle.com&#x2F;javase&#x2F;8&#x2F;docs&#x2F;technotes&#x2F;tools&#x2F;unix&#x2F;jstat.html使用说明: 格式：jstat [ generalOption | outputOptions vmid [ interval[s|ms] [ count ] ] generalOption可选参数如下： 1.class &#x2F;&#x2F;显示类加载器的统计信息 2.compiler &#x2F;&#x2F;显示有关jvm即时编译器行为的统计信息 3.gc | gcutil &#x2F;&#x2F;显示有关垃圾收集堆行为的统计信息 4.gccause &#x2F;&#x2F;显示引起垃圾收集事件的原因 ... outputOptions: 1.-t &#x2F;&#x2F;将时间戳显示为输出的第一列 2.-hn &#x2F;&#x2F;n为第几次，表示n次采样后输出一次列标题 vmid： &#x2F;&#x2F;进程的唯一标识 interval： &#x2F;&#x2F;间隔多长时间输出1次 count: &#x2F;&#x2F;输出多少次后退出例子： **jstat -gc -t -h3 26000 1000 10** 每隔1000毫秒输出一次gc统计信息，输出10次退出，同时将时间戳显示为输出的第一列，每3次 采样后输出一次列标题。 gc列标题含义(官网上都有详细说明): S0C：第一个幸存区的大小 S1C：第二个幸存区的大小 S0U：第一个幸存区的使用大小 S1U：第二个幸存区的使用大小 EC：eden的大小 EU：eden使用大小 OC：老年代大小 OU：老年代使用大小 MC：方法区大小 MU：方法区使用大小 CCSC:压缩类空间大小 CCSU:压缩类空间使用大小 YGC：年轻代垃圾回收次数 YGCT：年轻代垃圾回收消耗时间 FGC：老年代垃圾回收次数 FGCT：老年代垃圾回收消耗时间 GCT：垃圾回收消耗总时间使用jstat看G1的时候，s0永远是0： 因为G1的堆布局跟HotSpot VM里其它GC不一样——它只有一组逻辑上的survivor space， 而不像其它HotSpot GC一样有两段明确、固定的地址空间用作survivor space——所以用jstat 看G1的话肯定是survivor space 0显示0%，survivor space 1显示100%。这个是正常的。G1在初始化jstat用的计数器的时候就指定了s0永远是0: &#x2F;&#x2F; name &quot;generation.0.space.1&quot; &#x2F;&#x2F; See _old_space_counters for additional counters &#x2F;&#x2F; Set the arguments to indicate that this survivor space is not used. _from_counters &#x3D; new HSpaceCounters(&quot;s0&quot;, 1 &#x2F;* ordinal *&#x2F;, pad_capacity(0) &#x2F;* max_capacity *&#x2F;, pad_capacity(0) &#x2F;* init_capacity *&#x2F;, _young_collection_counters); &#x2F;&#x2F; name &quot;generation.0.space.2&quot; &#x2F;&#x2F; See _old_space_counters for additional counters _to_counters &#x3D; new HSpaceCounters(&quot;s1&quot;, 2 &#x2F;* ordinal *&#x2F;, pad_capacity(overall_reserved()) &#x2F;* max_capacity *&#x2F;, pad_capacity(survivor_space_committed()) &#x2F;* init_capacity *&#x2F;, _young_collection_counters); 来源R大帖子回复（https:&#x2F;&#x2F;hllvm-group.iteye.com&#x2F;group&#x2F;topic&#x2F;42352） 2.故障排查工具 1.jinfo主要用来查看与调整JVM参数 可以打印系统属性和JVM参数，同时支持部分参数动态修改！ 12345678910111213141516171819参考文档: java 8 : https:&#x2F;&#x2F;docs.oracle.com&#x2F;javase&#x2F;8&#x2F;docs&#x2F;technotes&#x2F;tools&#x2F;unix&#x2F;jinfo.html使用说明： jinfo [ option ] pid 查看例如： 1.jinfo -flags 进程id &#x2F;&#x2F;打印指定进程的VM参数 2.jinfo -sysprops 进程id &#x2F;&#x2F;只打印系统属性 3.jinfo -flag UseG1GC 进程id &#x2F;&#x2F;打印UseG1GC参数的值（判断是否开启G1） 4.jinfo -flag ThreadStackSize 进程id &#x2F;&#x2F;查看线程栈的指定大小 动态修改参数(可以达到不重启应用改变参数)： 1.使用如下命令显示出来的参数，基本上都是支持动态修改的： java -XX:+PrintFlagsInitial | grep manageable 如果显示的规则是true或者false： jinfo -flag +HeapDumpAfterFullGC 进程id &#x2F;&#x2F;+表示开，-表示关闭 如果显示的规则是数字，则以key,value形式: jinfo -flag MinHeapFreeRatio&#x3D;60 进程id &#x2F;&#x2F;修改MinHeapFreeRatio值为60 2.jmap全称Java Memory Map, 用来展示对象内存映射或堆内存详细信息 123456789101112131415161718192021222324参考文档: java 8 : https:&#x2F;&#x2F;docs.oracle.com&#x2F;javase&#x2F;8&#x2F;docs&#x2F;technotes&#x2F;tools&#x2F;unix&#x2F;jmap.html使用说明： jmap [options] pidoptions常用选项: 1.-clstats &#x2F;&#x2F;打印Java堆的类加载器统计信息 2.-dump:dump_options: &#x2F;&#x2F;转存java堆 live &#x2F;&#x2F;指定时，仅Dump活动对象；如果未指定，则转存堆中所有对象 format&#x3D;b: &#x2F;&#x2F;以hprof格式Dump堆 file&#x3D;filename: &#x2F;&#x2F;将堆Dump到filename 例如：jmap -dump:live,format&#x3D;b,file&#x3D;myDump.hprof pid &#x2F;&#x2F;可以不指定live 执行完命令后，可以在当前目录下找到文件，直接ll查看 3.-finalizerinfo: &#x2F;&#x2F;打印等待回收的对象 4.-histo[:live]: &#x2F;&#x2F;打印堆的直方图，如果指定live子选项，则仅统计活动对象扩展除了使用jmap存储堆内存，还可以： 1.使用-XX:+HeapDumpOnOutOfMemoryError &#x2F;&#x2F;OOM异常后自动生成堆Dump文件 2.使用-XX:+HeapDumpOnCtrlBreak,可以使用[Ctrl]+[Break], 让虚拟机生成堆Dump文件。 3.linux操作系统下，发送kill -3 pid命令 3.jstack用于打印当前虚拟机的线程快照 是对线程的Dump，也较Thread Dump 1234567891011121314151617181920参考文档： java 8 : https:&#x2F;&#x2F;docs.oracle.com&#x2F;javase&#x2F;8&#x2F;docs&#x2F;technotes&#x2F;tools&#x2F;unix&#x2F;jstack.html使用说明： jstack [-l][-e] &lt;pid&gt;options: -l 显示有关锁的额外信息 -e 展示有关线程的额外信息（比如分配了多少内存、定义了多少个类等等） 例如： 1.jstack pid &#x2F;&#x2F;不带任何参数，显示线程在干嘛 2.对比,输出到指定文件 jstack pid &gt; t1.txt jstack -l pid &gt; t2.txt &#x2F;&#x2F;带 -l 参数会显示锁额外信息， &#x2F;&#x2F;输出这个线程所持有的锁对象 jstack -l -e pid &gt; t3.txt &#x2F;&#x2F;多显示线程分配了多少内存、定义了多少个类直接看比较类，有可视化分析jstack dump出来的结果 4.jhat用来分析jmap生成的堆Dump文件 全程JVM Heap Analysis Tool， 12345不太建议使用，功能比较弱。可以使用： 1.VisualVm 2.Eclipse Memory Analyzer参考文档： java 8 : https:&#x2F;&#x2F;docs.oracle.com&#x2F;javase&#x2F;8&#x2F;docs&#x2F;technotes&#x2F;tools&#x2F;unix&#x2F;jhat.html 5.jcmd，用于将诊断命令发送到jvm 全程JVM Command 1234567参考文档： java 8 : https:&#x2F;&#x2F;docs.oracle.com&#x2F;javase&#x2F;8&#x2F;docs&#x2F;technotes&#x2F;tools&#x2F;unix&#x2F;jcmd.html很多功能类似和其它命令类似: 1.jcmd -l 列出本机上所有jvm进程 2.jcmd pid GC.heap_dump -all myheapdump.hprof &#x2F;&#x2F;类似jmap存储内存 3.jcmd pid GC.heap_info &#x2F;&#x2F;通知jvm做一次垃圾回收 3、一些好的jvm调优链接 3.1.G1调优官方指南必看： http://www.oracle.com/technetwork/articles/java/g1gc-1984535.html 3.2.不错的博客系列 jvm调优1-4： 1234https:&#x2F;&#x2F;www.cnblogs.com&#x2F;chiangchou&#x2F;p&#x2F;jvm-1.htmlhttps:&#x2F;&#x2F;www.cnblogs.com&#x2F;chiangchou&#x2F;p&#x2F;jvm-2.htmlhttps:&#x2F;&#x2F;www.cnblogs.com&#x2F;chiangchou&#x2F;p&#x2F;jvm-3.htmlhttps:&#x2F;&#x2F;www.cnblogs.com&#x2F;chiangchou&#x2F;p&#x2F;jvm-4.html 4、查看堆外内存使用量 jstat 命令可以用于监控JVM的堆外内存使用量。下面是一个示例命令，可以使用 jstat -gccapacity 命令来查看Java应用程序的堆外内存使用情况： 1jstat -gccapacity &lt;PID&gt; 其中，&lt;PID&gt; 是Java应用程序的进程ID。命令的输出将显示Java堆的容量和使用情况，包括堆外内存的使用量，例如： 12NGCMN NGCMX NGC S0C S1C EC OGCMN OGCMX OGC OC PGCMN PGCMX PGC PC YGC FGC110080.0 1753088.0 49152.0 1024.0 1024.0 12288.0 163840.0 2795520.0 163840.0 163840.0 8192.0 1048576.0 147456.0 8192.0 3 1 其中，NGCMN 表示新生代最小容量，NGCMX 表示新生代最大容量，NGC 表示当前新生代容量，S0C 和 S1C 分别表示新生代中两个survivor区的容量，EC 表示Eden区的容量，OGCMN 表示老年代最小容量，OGCMX 表示老年代最大容量，OGC 表示当前老年代容量，OC 表示Java堆中的对象总容量，PGCMN 和 PGCMX 分别表示永久代最小和最大容量，PGC 表示当前永久代容量，PC 表示Java堆中的PermGen总容量，YGC 表示Young GC次数，FGC 表示Full GC次数。 可以注意到，堆外内存使用量并没有直接显示在输出中。但是，通过计算堆外内存的使用量，我们可以从其他列中得出该信息。例如，可以通过以下公式计算堆外内存使用量： 12javaCopy codeHeap Memory Usage &#x3D; NGC + S0C + S1C + EC + OGCNon-Heap Memory Usage &#x3D; Total Memory - Heap Memory Usage 这将给出JVM的堆外内存使用量，以字节为单位。 ![img](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2738%27%20height=%2738%27/%3e) 直接使用命令计算 您可以使用下面的命令直接计算JVM堆外内存的使用量（假设您已经获取到了JVM进程的PID）： 12swiftCopy codejstat -gccapacity &lt;PID&gt; | tail -n 1 | awk &#39;&#123;print $16+$17+$18+$24&#125;&#39; 该命令将运行 jstat -gccapacity 命令并获取其输出，然后使用 tail -n 1 命令获取输出中的最后一行，最后使用 awk 命令将输出的多个列相加，以计算堆外内存使用量。 该命令的输出是堆外内存使用量（以字节为单位），例如： 12Copy code107256576 注意，该命令计算的是Java堆外内存的总使用量。如果您需要查看更细粒度的信息，例如Direct ByteBuffer的使用量，则需要使用其他工具或编写自己的代码来实现。","categories":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/categories/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}]},{"title":"分布式事务框架","slug":"分布式事务框架","date":"2022-09-20T12:13:37.000Z","updated":"2022-11-22T01:27:53.866Z","comments":true,"path":"2022/09/20/分布式事务框架/","link":"","permalink":"http://yoursite.com/2022/09/20/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%A1%86%E6%9E%B6/","excerpt":"","text":"ByteTCC、TCC-transaction、EasyTransaction 以及最近很火爆的 Seata，都是分布式事务框架，在介绍这些框架之前，必须要了解下TCC。 1、ByteTCC 2、TCC-transaction 3、Himly 4、EasyTransaction 5、Seata","categories":[{"name":"分布式事务","slug":"分布式事务","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"}],"tags":[{"name":"分布式事务","slug":"分布式事务","permalink":"http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"}]},{"title":"k8s的亲和性","slug":"k8s的亲和性","date":"2022-09-13T12:14:49.000Z","updated":"2022-11-22T01:27:53.669Z","comments":true,"path":"2022/09/13/k8s的亲和性/","link":"","permalink":"http://yoursite.com/2022/09/13/k8s%E7%9A%84%E4%BA%B2%E5%92%8C%E6%80%A7/","excerpt":"","text":"1、nodeSelector 1234567891011121314151617181920212223242526272829# 查看所有节点的标签kubectl get nodes --show-labelsk8s-master Ready control-plane,master 20d v1.22.5 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers=node1 Ready &lt;none&gt; 20d v1.22.5 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linuxnode2 Ready &lt;none&gt; 20d v1.22.5 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node2,kubernetes.io/os=linuxubuntu Ready &lt;none&gt; 19d v1.22.5 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=ubuntu,kubernetes.io/os=linux# 新增节点node1的标签为 :name=hfkubectl label nodes node1 name=hf# 使用apiVersion: v1kind: Podmetadata: labels: app: busybox-pod name: test-busyboxspec: containers: - command: - sleep - &quot;3600&quot; image: busybox imagePullPolicy: Always name: test-busybox nodeSelector: name: hf 2、亲和性和反亲和性 软策略：preferredDuringSchedulingIgnoredDuringExecution（满足更好，不满足也可以） 硬策略：requiredDuringSchedulingIgnoredDuringExecution（必须满足） 2.1 nodeAffinity 节点亲和性主要是用来控制 pod 要部署在哪些主机上，以及不能部署在哪些主机上的。","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"亲和性","slug":"亲和性","permalink":"http://yoursite.com/tags/%E4%BA%B2%E5%92%8C%E6%80%A7/"}]},{"title":"kubeapps","slug":"kubeapps","date":"2022-09-02T03:44:59.000Z","updated":"2022-11-22T01:27:53.692Z","comments":true,"path":"2022/09/02/kubeapps/","link":"","permalink":"http://yoursite.com/2022/09/02/kubeapps/","excerpt":"","text":"Kubeapps是Bitnami公司的一个项目，其目的是为Kubernetes的使用者们提供已经打包好的应用仪表盘，它拥有网页界面可以更方便的部署和管理k8s 原生应用。 1、安装 123456789101112131415kubectl create namespace kubeappshelm install kubeapps --namespace kubeapps bitnami/kubeapps# 查看进度kubectl get pods -w --namespace kubeapps # 账号和集群权限，这里创建的是管理员权限，如果在生产环境创建普通角色既可kubectl create --namespace kubeapps serviceaccount kubeapps-operatorkubectl create clusterrolebinding kubeapps-operator --clusterrole=cluster-admin --serviceaccount=default:kubeapps-operator# 获取token，后面登录需要kubectl get --namespace default secret kubeapps-operator-token -o go-template=&#x27;&#123;&#123;.data.token | base64decode&#125;&#125;&#x27;# 访问## 1、暴露访问端口,--address 0.0.0.0 可选，代表k8s集群外访问，8099端口也可以修改kubectl port-forward --address 0.0.0.0 --namespace kubeapps service/kubeapps 8099:80 ## 2、如果觉得转发端口麻烦，可以将service改成NodePort使用 IP:PORT访问kubectl edit -n kubeapps svc/kubeapps kubectl get svc -n kubeapps","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"kubeapps","slug":"kubeapps","permalink":"http://yoursite.com/tags/kubeapps/"}]},{"title":"k8s部署若干组件","slug":"k8s部署若干组件","date":"2022-09-01T03:54:39.000Z","updated":"2022-11-22T01:27:53.683Z","comments":true,"path":"2022/09/01/k8s部署若干组件/","link":"","permalink":"http://yoursite.com/2022/09/01/k8s%E9%83%A8%E7%BD%B2%E8%8B%A5%E5%B9%B2%E7%BB%84%E4%BB%B6/","excerpt":"","text":"运维工具 1234http://10.211.55.22:8888/kubepi/login admin kubepi12345http://10.211.55.22:30080/ admin Kuboard123https://10.211.55.26/ admin Rancher12345http://10.211.55.22:8099 token方式访问 （ kubectl get secret $(kubectl get serviceaccount kubeapps-operator -o jsonpath=&#x27;&#123;range .secrets[*]&#125;&#123;.name&#125;&#123;&quot;\\n&quot;&#125;&#123;end&#125;&#x27; | grep kubeapps-operator-token) -o jsonpath=&#x27;&#123;.data.token&#125;&#x27; -o go-template=&#x27;&#123;&#123;.data.token | base64decode&#125;&#125;&#x27; &amp;&amp; echo ） 主机规划 namespace： hf-local 节点 IP 操作系统 配置 master 10.211.55.22 Cetos7.9 2cup 4G内存 100G硬盘 node1 10.211.55.23 Cetos7.9 2cup 4G内存 100G硬盘 node2 10.211.55.24 Cetos7.9 2cup 4G内存 100G硬盘 node3 10.211.55.25 Cetos7.9 2cup 4G内存 100G硬盘 nfs 10.211.55.26 Cetos7.9 /data目录 1、Elasticsearch Kubernetes StatefulSet允许我们为Pod分配⼀个稳定的标识和持久化存储。 Elasticsearch需要稳定的存储来保证Pod在重新调度或者重启后的数据依然不变，所以我这里使⽤StatefulSet来管理Pod。","categories":[],"tags":[]},{"title":"若干有意思记录","slug":"若干有意思记录","date":"2022-08-30T05:06:19.000Z","updated":"2022-11-22T01:27:54.220Z","comments":true,"path":"2022/08/30/若干有意思记录/","link":"","permalink":"http://yoursite.com/2022/08/30/%E8%8B%A5%E5%B9%B2%E6%9C%89%E6%84%8F%E6%80%9D%E8%AE%B0%E5%BD%95/","excerpt":"","text":"影视 12https:&#x2F;&#x2F;www.libvio.me&#x2F;https:&#x2F;&#x2F;onelist.top&#x2F; 音乐 1无损下载：https:&#x2F;&#x2F;music.y444.cn 狗屁不同文档生成 1文章生成器：https:&#x2F;&#x2F;suulnnka.github.io&#x2F;BullshitGenerator&#x2F;index.html 在线小工具 1https:&#x2F;&#x2F;zxxgj.net&#x2F; 简历制作 12在线：https:&#x2F;&#x2F;huajian.smallpig.site&#x2F;GitHub地址：https:&#x2F;&#x2F;github.com&#x2F;huajian-pro&#x2F;resume-design 百度网盘不限速 12# 每天8次https:&#x2F;&#x2F;baidu.kinh.cc 在线摄像头 1https:&#x2F;&#x2F;voyeur-house.tv&#x2F;","categories":[],"tags":[]},{"title":"Prometheus服务自动发现","slug":"Prometheus服务自动发现","date":"2022-08-25T05:09:06.000Z","updated":"2022-11-22T01:27:53.427Z","comments":true,"path":"2022/08/25/Prometheus服务自动发现/","link":"","permalink":"http://yoursite.com/2022/08/25/Prometheus%E6%9C%8D%E5%8A%A1%E8%87%AA%E5%8A%A8%E5%8F%91%E7%8E%B0/","excerpt":"","text":"1、简介 prometheus配置文件 prometheus.yml 里配置需要监听的服务时，是按服务名写死的，如果后面增加了节点或者组件信息，就得手动修改此配置，并重启 promethues；那么能否动态的监听微服务呢？Prometheus 提供了多种动态服务发现的功能，这里以 consul 为例。引入consul之后，只需要在consul中维护监控组件配置，prometheus就能够动态发现配置了。 prometheus静态服务发现 123456789vim /usr/local/prometheus/prometheus.yml scrape_configs: - job_name: &#x27;prometheus&#x27; static_configs: - targets: [&#x27;localhost:9090&#x27;] # 采集node exporter监控数据 - job_name: &#x27;node&#x27; static_configs: - targets: [&#x27;localhost:9100&#x27;] 支持的多种发现机制 123451）static_configs: #静态服务发现2）file_sd_configs: #文件服务发现3）dns_sd_configs: DNS #服务发现4）kubernetes_sd_configs: #Kubernetes 服务发现5）consul_sd_configs: Consul #consul服务发现 2、consul 安装 安装后访问：http://127.0.0.1:8500 二进制安装 1234567## 下载wget https://releases.hashicorp.com/consul/1.0.0/consul_1.0.0_linux_amd64.zip?_ga=2.31706621.2141899075.1510636997-716462484.1510636997## 解压unzip consul_1.0.0_linux_amd64.zip./consul agent -server -ui -bootstrap-expect 1 -data-dir /tmp/consul &amp;# 查看启动状态 ./consul members docker容器安装 1$ docker run --name consul -d -p 8500:8500 consul 3、注册和注销服务 注册 12# 本机的node-export$ curl -X PUT -d &#x27;&#123;&quot;id&quot;: &quot;node-exporter&quot;,&quot;name&quot;: &quot;node-exporter-172.30.12.167&quot;,&quot;address&quot;: &quot;172.30.12.167&quot;,&quot;port&quot;: 9100,&quot;tags&quot;: [&quot;test&quot;],&quot;checks&quot;: [&#123;&quot;http&quot;: &quot;http://172.30.12.167:9100/metrics&quot;, &quot;interval&quot;: &quot;5s&quot;&#125;]&#125;&#x27; http://172.30.12.167:8500/v1/agent/service/register 注销 1$ curl -X PUT http://172.30.12.167:8500/v1/agent/service/deregister/node-exporter 4、配置prometheus自动服务发现 prometheus提前安装好了。 4.1 配置服务发现 12345# vim /usr/local/prometheus/prometheus.yml - job_name: &#x27;consul-prometheus&#x27; consul_sd_configs: - server: &#x27;172.30.12.167:8500&#x27; #Consul 的服务地址 services: [] 这样consul里配置所有的服务都会关联到consul-prometheus里面。我们需要根据不同的监控类型，分组存放。relabel_configs 可以解决这个问题，可以查看：http://prometheus-host:9090/targets 1234- job_name: &#39;consul-prometheus&#39; consul_sd_configs: - server: &#39;172.30.12.167:8500&#39; #Consul 的服务地址 services: [] 不过，我们会发现有如下几个问题： 有些必要的服务信息，也想要在标签中展示，该如何操作呢？ 如果需要自定义一些标签，例如 team、group、project 等关键分组信息，方便后边 alertmanager 进行告警规则匹配，该如何处理呢？ 所有 Consul 中注册的 Service 都会默认加载到 Prometheus 下配置的 consul_prometheus 组，如果有多种类型的 exporter，如何在 Prometheus 中配置分配给指定类型的组，方便直观的区别它们？ 4.2 自定义标签分类 Prometheus在拉取exporter的数据之后，会对数据标签进行编辑，允许用户通过relabel_configs对标签进行处理，包括修改、删除标签以及删除不必要的标签。 访问prometheus的target页面：http://prometheus-host:9090/targets consul元标签 123456789101112__meta_consul_address__meta_consul_dc__meta_consul_health__meta_consul_metadata_&lt;key&gt;__meta_consul_node__meta_consul_service_address__meta_consul_service_id__meta_consul_service_metadata_&lt;key&gt;__meta_consul_service_port__meta_consul_service__meta_consul_tagged_address_&lt;key&gt;__meta_consul_tags prometheus内置标签 123__address____metrics_path____scheme__ 上图的Before relabeling 是重新标记以前的，而 Labels 中是重新标记以后的。 配置 123456789101112scrape_configs: - job_name: &#x27;node&#x27; static_configs: - targets: [&#x27;127.0.0.1:9100&#x27;] relabel_configs: - source_labels: &#x27;[&#x27; &lt;labelname&gt; [, ...] &#x27;]&#x27; #从target数据中现有标签中选择值，source_label的值使用separator的配置进行连接，当与regex的正则表达式匹配时，进行action中配置的动作，如：替换、保留、删除等 separator: &lt;string&gt; | default = ; #连接source_label中的多个标签，默认值是&quot;;&quot; target_label: &lt;labelname&gt; #目标标签，当action配置为replace时，target_label是必须的 regex: &lt;regex&gt; | default = (.*) #通过正则表达式匹配提取值 modulus: &lt;uint64&gt; #获取source_labels的哈希值模数 replacement: &lt;string&gt; | default = $1 #如果正则表达式匹配到值的话，则用当前值进行替换 action: &lt;relabel_action&gt; | default = replace #对匹配到的标签进行的操作， 其中，action包含以下几种操作： replace：使用replacement的值替换被regex正则匹配到source_label； keep：保留被匹配到的标签的metric，删除未被匹配到标签的 metric; drop：删除被匹配到的标签的metric，保留未被匹配到标签的metric； hashmod：将target_label设置成source_label的modulus配置的hash值； labelmap：将regex匹配到的所有标签的名称配置成新的标签，值配置成新标签的值； labeldrop：将符合规则的标签删除，保留未被匹配的标签； labelkeep：将符合规则的标签保留，删除未被匹配的标签。 12345678910111213scrape_configs:- job_name: &#x27;consul-node&#x27; consul_sd_configs: - server: &#x27;172.16.168.10:8500&#x27; tags: - &quot;nodes&quot; refresh_interval: 3s relabel_configs: - source_labels: [__meta_consul_tags] regex: .*hf004.* action: keep - regex: __meta_consul_service_metadata_(.+) action: labelmap","categories":[{"name":"运维","slug":"运维","permalink":"http://yoursite.com/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"http://yoursite.com/tags/Prometheus/"}]},{"title":"NFS","slug":"NFS","date":"2022-08-23T08:19:06.000Z","updated":"2022-11-22T01:27:53.415Z","comments":true,"path":"2022/08/23/NFS/","link":"","permalink":"http://yoursite.com/2022/08/23/NFS/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243# 服务端安装$ sudo yum install nfs-utils# 启动 NFS 服务$ sudo systemctl enable rpcbind$ sudo systemctl enable nfs$ sudo systemctl start rpcbind$ sudo systemctl start nfs# 防火墙需要打开 rpc-bind 和 nfs 的服务$ sudo firewall-cmd --zone=public --permanent --add-service=&#123;rpc-bind,mountd,nfs&#125;# 配置共享目录$ sudo mkdir /data$ sudo chmod 755 /data$ vim /etc/exports/data/ *(rw,sync,no_root_squash,no_all_squash)#/data: 共享目录位置。#*: 客户端 IP 范围，* 代表所有，即没有限制，这里是10.211.55.0/24限定范围。#rw: 权限设置，可读可写。#sync: 同步共享目录。#no_root_squash: 可以使用 root 授权。#no_all_squash: 可以使用普通用户授权。# 重启nfs$ sudo systemctl restart nfs# 检查下本地共享目录showmount -e localhost# 客户端安装$ sudo yum install nfs-utils$ sudo systemctl enable rpcbind$ sudo systemctl start rpcbind# 测试链接（10.211.55.26 为nfs服务端ip）$ showmount -e 10.211.55.26Export list for 192.168.0.110:/data 10.211.55.0/24# 挂载本地/hf 到nfs$ mount -t nfs 10.211.55.26:/data /hf# 本地新建一个文件test$ touch test# nfs服务端口查看$ ls /datatest 2、使用nfs 2.1 静态 首先 nfs服务端新建文件夹 pv-1。 pv 创建一个storage为nfs的pv-1的pv：pv-nfs.yaml。 123456789101112131415apiVersion: v1kind: PersistentVolumemetadata: name: pv-1spec: storageClassName: nfs accessModes: - ReadWriteMany capacity: storage: 1Gi nfs: path: /data/pv-1 server: 10.211.55.26 12345$ kubectl apply -f pv-nfs.yaml$ kubectl apply -f pv-nfs.yaml$ kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpv-1 1Gi RWX Retain Available nfs 9s pvc 12345678910111213apiVersion: v1kind: PersistentVolumeClaimmetadata: name: nfs-static-pvcspec: storageClassName: nfs accessModes: - ReadWriteMany resources: requests: storage: 1Gi 1234$ kubectl apply -f pvc-static-nfs.yaml$ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEnfs-static-pvc Bound pv-1 1Gi RWX nfs 21s pod使用pvc 1234567891011121314151617181920apiVersion: v1kind: Podmetadata: name: nfs-static-pod-ngspec: volumes: - name: nfs-pvc-vol persistentVolumeClaim: claimName: nfs-static-pvc #pcv名字 containers: - name: nfs-pvc-test image: nginx:alpine ports: - containerPort: 80 volumeMounts: - name: nfs-pvc-vol mountPath: /tmp #容器内挂载目录，有数据了载nfs的/data会有 2.2 动态 PV 需要人工管理，必须要由系统管理员手动维护各种存储设备，再根据开发需求逐个创建 PV，而且 PV 的大小也很难精确控制，容易出现空间不足或者空间浪费的情况。在 Kubernetes 里“动态存储卷”的，它可以用 StorageClass 绑定一个 Provisioner 对象，而这个 Provisioner 就是一个能够自动管理存储、创建 PV 的应用，代替了原来系统管理员的手工劳动。 Kubernetes 里每类存储设备都有相应的 Provisioner 对象，对于 NFS 来说，它的 Provisioner 就是“NFS subdir external provisioner”，你可以在 GitHub 上找到这个项目（https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner）。 NFS Provisioner 也是以 Pod 的形式运行在 Kubernetes 里的，在 GitHub 的 deploy 目录里是部署它所需的 YAML 文件，一共有三个，分别是 rbac.yaml、class.yaml 和 deployment.yaml。 class.yaml StorageClass资源同PV一样，也不是命名空间级别。 1234567apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: nfs-clientprovisioner: k8s-sigs.io/nfs-subdir-external-provisioner # or choose another name, must match deployment&#x27;s env PROVISIONER_NAME&#x27;parameters: archiveOnDelete: &quot;false&quot; # 自动回收存储空间 不过这三个文件只是示例，想在我们的集群里真正运行起来还要修改其中的两个文件。 第一个要修改的是 rbac.yaml 它使用的是默认的 default 名字空间，应该把它改成其他的名字空间，避免与普通应用混在一起，你可以用“查找替换”的方式把它统一改成 kube-system。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263apiVersion: v1kind: ServiceAccountmetadata: name: nfs-client-provisioner namespace: default---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: nfs-client-provisioner-runnerrules: - apiGroups: [&quot;&quot;] resources: [&quot;nodes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;persistentvolumeclaims&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;] - apiGroups: [&quot;storage.k8s.io&quot;] resources: [&quot;storageclasses&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] - apiGroups: [&quot;&quot;] resources: [&quot;events&quot;] verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-client-provisionersubjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: defaultroleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisioner namespace: defaultrules: - apiGroups: [&quot;&quot;] resources: [&quot;endpoints&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisioner namespace: defaultsubjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: defaultroleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 第二个要修改的是 deployment.yaml 它要修改的地方比较多。首先要把namespace空间改成和 rbac.yaml 一样，比如是 kube-system，然后重点要修改 volumes 和 env 里的 IP 地址和共享目录名，必须和集群里的 NFS 服务器配置一样。 1234567891011121314151617181920212223242526272829303132333435363738apiVersion: apps/v1kind: Deploymentmetadata: name: nfs-client-provisioner labels: app: nfs-client-provisioner namespace: defaultspec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: k8s-sigs.io/nfs-subdir-external-provisioner - name: NFS_SERVER value: 10.211.55.26 # nfs 地址 - name: NFS_PATH value: /data #nfs 目录 volumes: - name: nfs-client-root nfs: server: 10.211.55.26 # nfs 地址 path: /data #nfs 目录 1234567$ kubectl apply -f rbac.yaml$ kubectl apply -f class.yaml$ kubectl apply -f deployment.yaml$ kubectl get scNAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGEnfs-client k8s-sigs.io/nfs-subdir-external-provisioner Delete Immediate false 10s 使用 123456789101112131415161718192021222324252627282930313233343536# pvc定义apiVersion: v1kind: PersistentVolumeClaimmetadata: name: nfs-dyn-10m-pvcspec: storageClassName: nfs-client accessModes: - ReadWriteMany resources: requests: storage: 10Mi# pod使用apiVersion: v1kind: Podmetadata: name: nfs-dyn-podspec: volumes: - name: nfs-dyn-10m-vol persistentVolumeClaim: claimName: nfs-dyn-10m-pvc containers: - name: nfs-dyn-test image: nginx:alpine ports: - containerPort: 80 volumeMounts: - name: nfs-dyn-10m-vol mountPath: /tmp","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"持久化卷","slug":"持久化卷","permalink":"http://yoursite.com/tags/%E6%8C%81%E4%B9%85%E5%8C%96%E5%8D%B7/"}]},{"title":"分布式存储","slug":"分布式存储","date":"2022-08-19T02:56:22.000Z","updated":"2022-11-22T01:27:53.867Z","comments":true,"path":"2022/08/19/分布式存储/","link":"","permalink":"http://yoursite.com/2022/08/19/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/","excerpt":"","text":"1、分布式存储 分布式存储是一种数据存储技术，通过网络使用企业中的每台机器上的磁盘空间，并将这些分散的存储资源构成一个虚拟的存储设备，数据分散的存储在企业的各个角落。根据存储方式，可以分为3种，如下： 块存储 ​ 就好比硬盘一样，直接挂在到主机， 一般用于主机的直接存储空间和数据库应用(MySQL)的存储。块存储(DAS/SAN)通常应用在某些专有的系统中，这类应用要求很高的随机读写性能和高可靠性，上面搭载的通常是 Oracle/DB2 这种传统数据库，连接通常是以 FC 光纤(8Gb/16Gb)为主，走光纤协议。如果要求稍低一些，也会出现基于千兆/万兆以太网的连接方式，MySQL 这种数据库就可能会使用 IP SAN，走 iSCSI 协议通常使用块存储的都是系统而非用户，并发访问不会很多，经常出现一套存储只服务一个应用系统，例如如交易系统，计费系统。典型行业如金融，制造，能源，电信等。 文件存储 文件存储(NAS)相对来说就更能兼顾多个应用和更多用户访问，同时提供方便的数据共享手段。在 PC 时代，数据共享也大多是用文件的形式，比如常见的的 FTP 服务，NFS 服务，Samba 共享这些都是属于典型的文件存储。几十个用户甚至上百用户的文件存储共享访问都可以用 NAS 存储加以解决。在中小企业市场，一两台 NAS 存储设备就能支撑整个 IT 部门了。CRM 系统，SCM 系统，OA 系统，邮件系统都可以使用 NAS 存储统统搞定。甚至在公有云发展的早几年，用户规模没有上来时，云存储的底层硬件也有用几套 NAS 存储设备就解决的，甚至云主机的镜像也有放在 NAS 存储上的例子文件存储的广泛兼容性和易用性，是这类存储的突出特点，但是从性能上来看，相对 SAN 就要低一些。NAS 存储基本上是以太网访问模式，普通千兆网，走 NFS/CIFS 协议。 对象存储 前面说到的块存储和文件存储，基本上都还是在专有的局域网络内部使用，而对象存储的优势场景却是互联网或者公网，主要解决海量数据，海量并发访问的需求。基于互联网的应用才是对象存储的主要适配（当然这个条件同样适用于云计算，基于互联网的应用最容易迁移到云上），基本所有成熟的公有云都提供了对象存储产品，不管是国内还是国外对象存储常见的适配应用如网盘、媒体娱乐，医疗 PACS，气象，归档等数据量超大而又相对 冷数据 和非在线处理的应用类型，这类应用单个数据大，总量也大，适合对象存储海量和易扩展的特点。网盘类应用也差不多，数据总量很大，另外还有并发访问量也大，支持 10 万级用户访问这种需求就值得单列一个项目了。归档类应用只是数据量大的 冷数据，并发访问的需求倒是不太突出。另外基于移动端的一些新兴应用也是适合的，智能手机和移动互联网普及的情况下，所谓 UGD（用户产生的数据，手机的照片视频）总量和用户数都是很大挑战。毕竟直接使用 HTTP get/put 就能直接实现数据存取，对移动应用来说还是有一定吸引力的对象存储的访问通常是在互联网，走 HTTP 协议，性能方面，单独看一个连接的是不高的（还要解决掉线断点续传之类的可靠性问题），主要强大的地方是支持的并发数量，聚合起来的性能带宽就非常可观了。 访问方式对比 块存储通常都是通过光纤网络连接，服务器/小机上配置 FC 光纤 HBA 卡，通过光纤交换机连接存储（IP SAN 可以通过千兆以太网，以 iSCSI 客户端连接存储），主机端以逻辑卷（Volume）的方式访问。连接成功后，应用访问存储是按起始地址，偏移量 Offset 的方法来访问的 文件存储通常只要是局域网内，千兆/百兆的以太网环境皆可。网线连上，服务器端通过操作系统内置的 NAS 客户端，如 NFS/CIFS/FTP 客户端挂载存储成为一个本地的文件夹后访问，只要符合 POXIS 标准，应用就可以用标准的 open，seek，write/read，close 这些方法对其访问操作 对象存储不在乎网络，而且它的访问比较有特色，只能存取删（put/get/delete），不能打开修改存盘。只能取下来改好后上传，去覆盖原对象。 2、连接方式 一般分布式存储的存储介质和系统分离，成为外挂式存储。存储介质和系统之间连接可以分为：直连化存储；网络化存储。 2.1 直连化存储（Direct-Attached Storage，简称DAS） 依赖服务器主机操作系统进行数据的IO读写和存储维护管理，数据备份和恢复要求占用服务器主机资源（包括CPU、系统IO等），数据流需要回流主机再到服务器连接着的磁带机（库），数据备份通常占用服务器主机资源20-30%。直连式存储与服务器主机之间的连接通道通常采用SCSI（Small Computer System Interface）连接，带宽为10MB/s、20MB/s、40MB/s、80MB/s等，一般SCSI会有IO瓶颈。 2.2 网络化存储（Fabric-Attached Storage，简称FAS） 存储区域网路（Storage Area Network，简称SAN） 通过光纤通道交换机连接存储阵列和服务器主机，建立专用于数据存储的区域网络。SAN经过十多年历史的发展，已经相当成熟，成为业界的事实标准（但各个厂商的光纤交换技术不完全相同，其服务器和SAN存储有兼容性的要求）。 网络接入存储（Network-Attached Storage，简称NAS） 采用网络（TCP/IP、ATM、FDDI）技术，通过网络交换机连接存储系统和服务器主机，建立专用于数据存储的存储私网。 2、种类对比 存储系统 Ceph Swift HDFS FastDFS Ambry MinIO 开发语言 C++ Python Java C Java Go 开源协议 LGPL Apache Apache GPL3 Apache Apache 存储方式 对象/文件/块 对象 文件 文件/块 对象 对象 在线扩容 支持 支持 支持 支持 支持 - 冗余备份 支持 支持 支持 支持 支持 - 单点故障 不存在 不存在 存在 不存在 不存在 - 易用性 一般 一般 一般 简单 简单 简单 跨集群 不支持 - 不支持 部分支持 不支持 - 适用场景 大中小文件 大中小文件 大中文件 中小文件 大中小文件 大中小文件","categories":[{"name":"分布式存储","slug":"分布式存储","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"分布式存储","slug":"分布式存储","permalink":"http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"}]},{"title":"harbor","slug":"harbor","date":"2022-08-12T06:28:51.000Z","updated":"2022-11-22T01:27:53.652Z","comments":true,"path":"2022/08/12/harbor/","link":"","permalink":"http://yoursite.com/2022/08/12/harbor/","excerpt":"","text":"1、安装 Linux上。 前提安装好docker和docker-compose，不再赘述。 新建/harbor目录，安装harbor。 github下载 1234567891011121314151617181920212223242526272829303132333435363738394041424344$ wget https://github.com/goharbor/harbor/releases/download/v2.4.3/harbor-offline-installer-v2.4.3.tgz$ tar -zxvf harbor-offline-installer-v2.4.3.tgz$ cp harbor.yml.tmpl harbor.yml$ vim harbor.yml# 输入服务器的地址(例如：127.0.0.1)hostname: docker.harbor.com# 下面是端口，根据自己的配置http: # port for http, default is 80. If https enabled, this port will redirect to https port port: 8111# 设置好harbor登录的密码，默认账户为adminharbor_admin_password: 123456# 如果需要https（建议启用）请配置好证书文件，不然就-------注释掉https: # https port for harbor, default is 443 port: 443 # The path of cert and key files for nginx certificate: /etc/certs/ca.crt private_key: /etc/certs/ca.key # 安装（会启动harbor）./install.sh --with-chartmuseum#访问http://127.0.0.1:8111# 启动/停止（安装目录下）sudo docker-compose up | down# docker 登录docker login ip：port输入用户名和密码后报错：Error response from daemon: Get &quot;https://192.168.90.203:8111/v2/&quot;: http: server gave HTTP response to HTTPS client$ vim /etc/docker/daemon.json 添加以下ip和端口，记得重启docker # &quot;insecure-registries&quot;:[&quot;ip:8111&quot;], &#123; &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;insecure-registries&quot;:[&quot;192.168.90.203:8111&quot;], &quot;registry-mirrors&quot;: [&quot;https://kn0t2bca.mirror.aliyuncs.com&quot;]&#125; 2、推送镜像 123docker tag SOURCE_IMAGE[:TAG] 192.168.90.203:8111/video/REPOSITORY[:TAG]docker push 192.168.90.203:8111/video/REPOSITORY[:TAG] 3、停止 1234cd /harbordocker-compose down# 开启helm charts的仓库./install.sh --with-chartmuseum","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"harbor","slug":"harbor","permalink":"http://yoursite.com/tags/harbor/"}]},{"title":"性能指标","slug":"性能指标","date":"2022-08-10T03:33:52.000Z","updated":"2022-11-22T01:27:54.012Z","comments":true,"path":"2022/08/10/性能指标/","link":"","permalink":"http://yoursite.com/2022/08/10/%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87/","excerpt":"","text":"1、ps ps -ef VS ps aux 123456789101112131415161718192021# bsd格式输出: UID , PID , PPID , C , STIME , TTY , TIME , CMD$ ps -ef | grep nginxroot 1980 1 0 Jun21 ? 00:00:00 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.confnginx 1983 1980 0 Jun21 ? 00:00:00 nginx: worker processdeploy 36920 33681 0 11:35 pts/4 00:00:00 grep nginx# 标准格式: USER , PID , %CPU , %MEM , VSZ , RSS , TTY , STAT , START , TIME , COMMAND$ ps aux | grep nginxroot 1980 0.0 0.0 29408 712 ? Ss Jun21 0:00 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.confnginx 1983 0.0 0.0 34252 1200 ? S Jun21 0:00 nginx: worker processdeploy 37070 0.0 0.0 14436 1100 pts/4 S+ 11:35 0:00 grep nginx# cpu占用top几$ ps aux | sort -k3nr | head -n 5# 内存占用top几$ ps aux | sort -k4nr | head -n 5# 进程启动&amp;运行时长$ ps -eo pid,lstart,etime | grep 1980 1980 Tue Jun 21 10:25:11 2022 50-01:23:59 2、top 123456789101112131415 # 查看服务进程id$ ps -ef | grep &#x27;MediaServer&#x27;hf 43741 29392 3 11:41 pts/6 00:00:00 ./MediaServerdeploy 43952 33681 0 11:41 pts/4 00:00:00 grep MediaServer# 通过进程id查看top$ top -p 43741top - 11:43:05 up 50 days, 1:18, 2 users, load average: 4.34, 4.73, 4.89Tasks: 1 total, 0 running, 1 sleeping, 0 stopped, 0 zombie%Cpu(s): 12.9 us, 2.4 sy, 0.0 ni, 84.7 id, 0.0 wa, 0.0 hi, 0.1 si, 0.0 stKiB Mem : 65735656 total, 25302812 free, 19633884 used, 20798960 buff/cacheKiB Swap: 2097148 total, 1563204 free, 533944 used. 45067136 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND43741 hf 20 0 7367764 25732 19680 S 5.3 0.0 0:05.49 main thread","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"性能","slug":"性能","permalink":"http://yoursite.com/tags/%E6%80%A7%E8%83%BD/"}]},{"title":"helm","slug":"helm","date":"2022-08-08T08:23:32.000Z","updated":"2022-11-22T01:27:53.653Z","comments":true,"path":"2022/08/08/helm/","link":"","permalink":"http://yoursite.com/2022/08/08/helm/","excerpt":"","text":"1、概述 Helm是k8s的包管理工具，类似Linux系统常用的 apt、yum等包管理工具。使用helm可以简化k8s应用部署。 Chart：一个 Helm 包，其中包含了运行一个应用所需要的镜像、依赖和资源定义等，还可能包含 Kubernetes 集群中的服务定义，类似 Homebrew 中的 formula、APT 的 dpkg 或者 Yum 的 rpm 文件。 Release：在 Kubernetes 集群上运行的 Chart 的一个实例。在同一个集群上，一个 Chart 可以安装很多次。每次安装都会创建一个新的 release。例如一个 MySQL Chart，如果想在服务器上运行两个数据库，就可以把这个 Chart 安装两次。每次安装都会生成自己的 Release，会有自己的 Release 名称。 Repository：用于发布和存储 Chart 的存储库。 Helm的Chart结构，比喻elasticsearch 1234567891011121314$ chart-test tree -L 3├── Chart.yaml --描述这个chart的基本信息，包括名字、描述信息、版本信息等。├── charts├── templates --用于存放部署使用的yaml文件模板，这里面的yaml都是通过各种判断、流程控制、引用变量去调用values中设置的变量信息，最后完成部署。│ ├── NOTES.txt│ ├── _helpers.tpl│ ├── deployment.yaml│ ├── hpa.yaml│ ├── ingress.yaml│ ├── service.yaml│ ├── serviceaccount.yaml│ └── tests│ └── test-connection.yaml└── values.yaml --用于存储templates目录中模板文件中用到的变量信息，也就是说template中的模板文件引用的是values.yaml中的变量。 Chart包构建的相关命令 12345创建chart：`helm create chart_name`将chart打包：`helm package chart_path`查看chart中yaml模板文件渲染信息：`helm get manifest release_name` 2、安装 heml安装 1brew install helm 添加仓库 1234567891011#阿里云helm repo add aliyun http://mirror.azure.cn/kubernetes/charts/# 微软helm add azure http://mirror.azure.cn/kubernetes/charts/# harbor， myharbor 自定义名字 ，http://192.168.90.203:8111/chartrepo固定的，third为harbor上的”私有“项目名字helm repo add --username=harbor --password=Harbor12345 203-harbor-third http://192.168.90.203:8111/chartrepo/third报错（待解决，可能是私有原因）：Error: looks like &quot;http://192.168.90.203:8111/chartrepo/third&quot; is not a valid chart repository or cannot be reached: failed to fetch http://192.168.90.203:8111/chartrepo/third/index.yaml : 401 Unauthorized# harbor，add 已有公开public项目libraryhelm repo add --username=harbor --password=Harbor12345 203-harbor http://192.168.90.203:8111/chartrepo/library 操作 12345678910111213# 已添加repo查看helm repo list# 会在已添加repo里搜索,-l 所有版本helm search repo mysql [-l]# 指定仓库搜zkhelm search repo bitnami -l | grep zookeeper# 查看详情helm show values aliyun/mysql# pull chart到本地helm pull bitnami/redis-clusterhelm pull bitnami/zookeeper --version 8.1.2# 安装到k8shelm install db aliyun/mysql push插件安装 123456789101112131415# 在线helm plugin install https://github.com/chartmuseum/helm-push# 离线(mac为例)下载上述包。放到helm plugin 目录下1、查看,HELM_PLUGINShelm envHELM_PLUGINS=&quot;/Users/hf/Library/helm/plugins&quot;mkdir helm-push cp helm-push_0.10.3_darwin_amd64.tar.gz helm-pushtar -zxvf helm-push_0.10.3_darwin_amd64.tar.gz -C helm-push2、查看插件helm listNAME VERSION DESCRIPTIONcm-push 0.10.3 Push chart package to ChartMuseum push 12# pushhelm cm-push redis-17.1.2.tgz 203-harbor --username=admin --password=Harbor12345","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"helm","slug":"helm","permalink":"http://yoursite.com/tags/helm/"}]},{"title":"k8s资源对象","slug":"k8s资源对象","date":"2022-08-04T11:31:33.000Z","updated":"2023-10-12T08:00:40.987Z","comments":true,"path":"2022/08/04/k8s资源对象/","link":"","permalink":"http://yoursite.com/2022/08/04/k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"1、资源对象分类 k8s作为云上的操作系统，它将一切事物都抽象为资源API对象，就像Linux一切是文件一样。API对象其遵循REST架构风格，组织并管理来这些资源及其对象，操作k8s实质就是操作资源对象，也就是通过标准的HTTP方法 (POST、PUT、PATCH、DELETE和GET)对资源进行增、删、改、 查等管理操作。 可以通过命令：kubectl api-resources 来查看k8s中资源对象，如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657NAME SHORTNAMES APIVERSION NAMESPACED KINDbindings v1 true Bindingcomponentstatuses cs v1 false ComponentStatusconfigmaps cm v1 true ConfigMapendpoints ep v1 true Endpointsevents ev v1 true Eventlimitranges limits v1 true LimitRangenamespaces ns v1 false Namespacenodes no v1 false Nodepersistentvolumeclaims pvc v1 true PersistentVolumeClaimpersistentvolumes pv v1 false PersistentVolumepods po v1 true Podpodtemplates v1 true PodTemplatereplicationcontrollers rc v1 true ReplicationControllerresourcequotas quota v1 true ResourceQuotasecrets v1 true Secretserviceaccounts sa v1 true ServiceAccountservices svc v1 true Servicemutatingwebhookconfigurations admissionregistration.k8s.io&#x2F;v1 false MutatingWebhookConfigurationvalidatingwebhookconfigurations admissionregistration.k8s.io&#x2F;v1 false ValidatingWebhookConfigurationcustomresourcedefinitions crd,crds apiextensions.k8s.io&#x2F;v1 false CustomResourceDefinitionapiservices apiregistration.k8s.io&#x2F;v1 false APIServicecontrollerrevisions apps&#x2F;v1 true ControllerRevisiondaemonsets ds apps&#x2F;v1 true DaemonSetdeployments deploy apps&#x2F;v1 true Deploymentreplicasets rs apps&#x2F;v1 true ReplicaSetstatefulsets sts apps&#x2F;v1 true StatefulSettokenreviews authentication.k8s.io&#x2F;v1 false TokenReviewlocalsubjectaccessreviews authorization.k8s.io&#x2F;v1 true LocalSubjectAccessReviewselfsubjectaccessreviews authorization.k8s.io&#x2F;v1 false SelfSubjectAccessReviewselfsubjectrulesreviews authorization.k8s.io&#x2F;v1 false SelfSubjectRulesReviewsubjectaccessreviews authorization.k8s.io&#x2F;v1 false SubjectAccessReviewhorizontalpodautoscalers hpa autoscaling&#x2F;v2 true HorizontalPodAutoscalercronjobs cj batch&#x2F;v1 true CronJobjobs batch&#x2F;v1 true Jobcertificatesigningrequests csr certificates.k8s.io&#x2F;v1 false CertificateSigningRequestleases coordination.k8s.io&#x2F;v1 true Leaseendpointslices discovery.k8s.io&#x2F;v1 true EndpointSliceevents ev events.k8s.io&#x2F;v1 true Eventflowschemas flowcontrol.apiserver.k8s.io&#x2F;v1beta2 false FlowSchemaprioritylevelconfigurations flowcontrol.apiserver.k8s.io&#x2F;v1beta2 false PriorityLevelConfigurationingressclasses networking.k8s.io&#x2F;v1 false IngressClassingresses ing networking.k8s.io&#x2F;v1 true Ingressnetworkpolicies netpol networking.k8s.io&#x2F;v1 true NetworkPolicyruntimeclasses node.k8s.io&#x2F;v1 false RuntimeClasspoddisruptionbudgets pdb policy&#x2F;v1 true PodDisruptionBudgetpodsecuritypolicies psp policy&#x2F;v1beta1 false PodSecurityPolicyclusterrolebindings rbac.authorization.k8s.io&#x2F;v1 false ClusterRoleBindingclusterroles rbac.authorization.k8s.io&#x2F;v1 false ClusterRolerolebindings rbac.authorization.k8s.io&#x2F;v1 true RoleBindingroles rbac.authorization.k8s.io&#x2F;v1 true Rolepriorityclasses pc scheduling.k8s.io&#x2F;v1 false PriorityClasscsidrivers storage.k8s.io&#x2F;v1 false CSIDrivercsinodes storage.k8s.io&#x2F;v1 false CSINodecsistoragecapacities storage.k8s.io&#x2F;v1 true CSIStorageCapacitystorageclasses sc storage.k8s.io&#x2F;v1 false StorageClassvolumeattachments storage.k8s.io&#x2F;v1 false VolumeAttachment 2、 资源对象类型 上面我们看到k8s内置很多资源对象，一般根据资源对象的功能作为标准，可以将对象分成几个大的类别：工作负载，发现负载，配置和存储，集群，元数据。如下也只是列举比较常用对象，和若干一些说明，其他对象使用需要参考官方文档使用。 2.1 工作负载类型 Pod：k8s调度最小单元，一切资源对象都是围绕它来的。是容器运行的地方，可以运行1-n个容器。 Deployment：Pod本身没有自愈、扩缩容，升级和回滚等能力，Deployment提供这些能力，保障护航Pod。 ReplicaSet：Deployment 底层使用ReplicaSet实现自愈、扩缩容，升级和回滚等。 StatefulSet：Deployment保障无状态的Pod，StatefulSet保障有状态的Pod，例如数据库mysql类型Pod。 DaemonSet：保障常驻的Pod，例如日志，监控扽Pod。 Job：为临时/周期运行类型容器的资源对象，这时候不适合创建Pod。 2.2 发现负载类型 Service ：Pod不稳定，每次都是新建，IP不固定，为了实现集群内服务互相访问一个资源对象。 Endpoint Ingress 2.3 配置和存储类型 ConfigMap：明文配置文件对象 Secret：秘问配置文件对象 PersistentVolumeClaim PersistentVolume 2.4 集群类型 Namespace Node Role ClusterRole RoleBinding ClusterRoleBinding 2.5 元数据类型 HorizontalPodAutoscaler 3、核心资源对象 参考：Kubernetes修炼手册。 3.1 Pod 在 Kubernetes 的世界中，最小单元是 Pod。一个 Pod 就是由一个或多个容器共享的运行环境。 单容器Pod，最简单的模型是一个 Pod 中仅包含一个容器的情况 多容器 Pod，“以应用为中心”的使用场景，就是用于承载被共同调度的紧耦合的工作负载（比如，集群中共享内存的两个容器，在被调度到不同的节点上时将无法正常运行的情况） Pod 实际上是一个名为 pause container 的特殊容器。没错，Pod 就是一种特殊容器的花哨称谓。这意味着，运行在 Pod 中的容器实际上是运行在容器中的。Pod(pause container)就是一些系统资源的集合，并且能够被其中运 行的容器继承和共享。 网络命名空间: IP 地址、端口范围、路由表… UTS 命名空间: 主机名。 IPC 命名空间: UNIX 域套接字(domain socket) 由此来说，Pod 中的所有容器都共享主机名、IP 地址、内存地址空间以及卷。 3.2 Deployment Pod 没有自愈能力，不能扩缩容，也不支持方便的升级和回滚。而 Deployment 可以。因此，建议绝大多数情况下采用 Deployment 来部署 Pod。一个 Deployment 对象只能管理一个 Pod 模板。例如，如果一个应用有 两个 Pod 模板— 前端(front-end)和目录服务(catalog service)，那么就需要两个 Deployment。 Deployment 使用 ReplicaSet 来提供自愈和扩缩容能力。可以理解为 Deployment 管理 ReplicaSet，ReplicaSet 管理 Pod。 3.2.1 自愈、扩缩容、升级、回滚 自愈：如果 Deployment 管理的 Pod 出现故障，那么它会被替换掉— 自愈。 扩容：如果 Deployment 管理的 Pod 承担的负载增加，那么可以轻松地扩展同样的 Pod 来处理负载— 扩容。 升级：零宕机的情况下实现了一种平滑的滚动升级。 回滚：零宕机的情况下实现了一种平滑的回滚。 自愈和扩缩容原理： Deployment 底层使用了一种名为 ReplicaSet 的对象来完成实际的自愈和扩缩容。ReplicaSet 的底层调谐循环能够定期检查集群中 Pod 的副本数是否达标。如果副本数不足，则增加;如果副本数过多，则销毁一些。 升级原理： 现在假设某人遇到了一个 Bug，并且需要部署一个新的镜像来完成修复。因此，他修改了同一个 Deployment 的 YAML 文件，将镜像版本更新并重新 POST 到 API Server。 这会在集群中注册新的期望状态，需要同样数量的 Pod，但全部都要运行新版的镜像。 为了达到这一点，Kubernetes 基于新镜像的 Pod 创建了一个新的 ReplicaSet。此时就有两个 ReplicaSet 了:一个是包含基于旧版镜像的 Pod，一个是新版本的 Pod。每次 Kubernetes 增加新 ReplicaSet(新版镜像)中的 Pod 数量的时候，都会相应地减少旧 ReplicaSet(旧 版镜像)中的 Pod 数量。从而，在零宕机的情况下实现了一种平滑的滚动升级。当然， 以后的更新也都可以重复这一过程— 只需要更新清单文件(应该保存在版本管理系统 中)即可。 回滚原理： 旧版的 ReplicaSet 已经暂停并且不再管理任何 Pod。然而，仍然保留了 所有的配置信息。这使回滚到前一版本成为可能。 3.2.1 实践 可以参考https://blog.csdn.net/jiangxiulilinux/article/details/115750640 修改deploy.yml 文件 执行：kubectl apply -f deploy.yml --record kubectl rollout history deployment hello-deploy（deployment的名字） 3.3 Service Pod 的 IP 地址是不可靠的。在某个 Pod 失效之 后，它会被一个拥有新的 IP 的 Pod 代替。Deployment 扩容也会引入拥有新 IP 的 Pod;而缩 容则会删除 Pod。这会导致大量的 IP 流失，因而 Pod 的 IP 地址是不可靠的。每一个 Service 都拥有固定的 IP 地址、固定的 DNS 名称，以及固定的端口。Service 利用 Label 来动态选择将流量转发至哪些 Pod。Service 可以将各个 Pod与客户端一侧，通过固定的 IP、DNS 和端口连接起来。 筛选Pod 由于 Service 的存在，这些 Pod 可以扩容或缩容，可以出现故障，也可以进行更新或回滚。当这些操作发生的时候，前方的 Service 能够监测到这些变化，并且更新其关联的健康 Pod 的列表。Service 与 Pod 之间是通过 Label 和 Label 筛选器(selector)松耦合在一起的. Service 和 Endpoint 随着 Pod 的时常进出(扩容和缩容、故障、滚动升级等)，Service 会动态更新其维护的 相匹配的健康 Pod 列表。具体来说，其中的匹配关系是通过 Label 筛选器和名为 Endpoint 对象的结构共同完成的。每一个 Service 在被创建的时候，都会得到一个关联的 Endpoint 对象。整个 Endpoint 对象其实就是一个动态的列表，其中包含集群中所有的匹配 Service Label 筛选器的健康 Pod。 ClusterIP类型 集群内部访问。 为Service默认类型，**ClusterIP类型的Service **拥有固定的 IP 地址和端口号，并且仅能够从集群内部访问得到。ClusterIP 与对应的 Service 名称一起被注册在集群内部的 DNS 服务中。集群中的 所有 Pod 都“知道”集群的 DNS 服务，故而所有的 Pod 都能够解析 Service 名称。 123Name:magic-sandboxClusterIP:172.12.5.17port:8080 假设我们创建了一个新的名为“magic-sandbox”的 Service，那么就会触发以下动作。 Kubernetes 会将 Service 的名字“magic-sandbox”和 ClusterIP 及端口一起注册列集群的 DNS 服务中。这个名字、ClusterIP 和端口号被确保长期不变，集群中所有的 Pod 都可以发送“服 务发现请求”(Service discovery request)到内部 DNS，并完成从“magic-sandbox”到 ClusterIP 的解析。集群中的 IPTABLES 或 IPVS 规则将确保发送至 ClusterIP 的流量被正确路由至相应 的 Pod。总而言之，只要 Pod(应用的微服务)知道 Service 的名称，就能够解析对应的 ClusterIP， 进而连接到所需的 Pod。 NodePort类型 集群外部访问。 ClusterIP 是默认的 Service 类型，并且它会在集群 DNS 中注册一个 DNS 名称、 一个虚拟 IP 和端口号。而 NodePort Service 在此基础上增加了另一个端口，这个用来从集群 外部访问到 Service 的端口叫作 NodePort。例如，一个NoPort类型Serive关键配置 1234Name:magic-sandboxClusterIP:172.12.5.17port:8080NodePort:30050 从集群外部，通过发送请求到集群中的任何一个节点的 IP 上的端口 30050 来访问它。使用 Label 将 Service 和 Pod 关联起来之后，该 Service 对象有一个可靠的 NodePort 与集群中的每一个节点映射:NodePort 的值在 每一个节点上都是相同的。这意味着，从集群外部，到达集群中的任何一个节点的 NodePort 的流量都可以直接到达应用(Pod)。 3.4 Ingress 根据前面对Service概念的说明，我们知道Service的表现形式为IP地 址和端口号(ClusterIP:Port)，即工作在TCP/IP层。而对于基于HTTP 的服务来说，不同的URL地址经常对应到不同的后端服务或者虚拟服务 器(Virtual Host)，这些应用层的转发机制仅通过Kubernetes的Service 机制是无法实现的。 3、资源对象配置清单 3.1 编写yaml 前面讲了k8s将它管控一切都抽象为API对象，并遵循HTTP协议，操作k8s实质就发送GET、POST、PUT等操作资源对象，这里具体操作对象需要有一个描述文件，一般k8s中使用yaml来描述。拿最重要Pod举例，基本格式如下： 12345678910111213apiVersion: v1 # API版本号，不同的对象可能会使用不同kind: Pod # 对象类型，Podmetadata: # 元数据 name: string # Pod名称，自定义 labels: # 标签（为了筛选出这个pod用的） env: demo # 标签键和值 owner: deploy # 标签键和值spec: # 资源内容的规范 containers: # 容器列表 - image: string # 容器镜像 name: string # 容器名称 ports: - containerPort: int # 容器端口 我们与HttpRequest做个类比：有Header和Body 2部分，Header一般是不变的，Body内容会变。Header相当于如下字段：apiVersion、kind、metadata。Body相当于如下字段，spec。然后我们使用 kubectl apply -f xxx.yml 去发布 YAML 来创建对象。 3.2 编写技巧 并不是所有对象都可以，最好办法是参考官网yaml配置。 这么多 API 对象，我们怎么知道该用什么 apiVersion、什么 kind？metadata、spec 里又该写哪些字段呢？可以有如下几种方式，简便我们的工作： 官方文档：https://kubernetes.io/docs/reference/kubernetes-api/ kubectl api-resources 命令，它会显示出资源对象相应的 API 版本和类型，比如 Pod 的版本是“v1”。 kubectl explain 命令，它相当于是 Kubernetes 自带的 API 文档，会给出对象字段的详细说明。例如： 1234kubectl explain podkubectl explain pod.metadatakubectl explain pod.speckubectl explain pod.spec.containers kubectl 的两个特殊参数 --dry-run=client 和 -o yaml。 前者是空运行，后者是生成 YAML 格式，结合起来使用就会让 kubectl 不会有实际的创建动作，而只生成 YAML 文件。例如： 1kubectl run ngx --image=nginx:alpine --dry-run=client -o yaml 123456kubectl create deployment nginx --image=nginx:alpine -o yaml --dry-run=client kubectl create StatefuleSet mysql --image=mysql:alpine -o yaml --dry-run=client # 命令获取任何一个对象的 YAML格式的配置清单，或者使用“kubectl get TYPE/NAME-o yamlkubectl get TYPE/NAME-o yaml","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"k8s资源对象","slug":"k8s资源对象","permalink":"http://yoursite.com/tags/k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/"}]},{"title":"c++","slug":"c","date":"2022-08-02T03:39:21.000Z","updated":"2022-11-22T01:27:53.453Z","comments":true,"path":"2022/08/02/c/","link":"","permalink":"http://yoursite.com/2022/08/02/c/","excerpt":"","text":"与Java语言相比，c语法略显复杂。归咎于c本身发展历程和语言处理对象有关。c从c语言发展而来，c语言事典型的面相过程编程语言，c加入了面相对象编程，但是又为了兼容c，语法变得复杂。c++目前主要战场还是在处理硬件交互，计算机视觉和游戏引擎方面，这几个方面都需要和底层操作系统和硬件打交道，因此它的处理过程很难像Java处理业务逻辑一样，都是对象的概念。对比之下我列举出2者主要有如下几个方面区别。 1、数据类型 Java中有8种基本数据类型，且在不同平台上所占存储空间不变（跨平台特性），如下： 数据类型 默认值 封装类 大小（位/字节） byte 0 Byte 1 char ‘/uoooo’(null) Character 2 short 0 Short 2 int 0 Integer 4 long 0L Long 8 float 0.0f Float 4 double 0.0 Double 8 boolean flase Boolean 1bit C++基本数据类型有7种。 数据类型 16位系统 32位系统 64位系统 bool 1bit 1bit 1bit char 1 1 1 short 2 2 2 int 2 4 4 long 4 4 8 float 4 4 4 double 8 8 8 ** 可以看到int、long会在不同位数机器上表现出占用空间大小不同。** C++ 允许在 char、int 和 double 数据类型前放置修饰符（signed，unsigned，long，short），改变基本类型的含义，让它更能满足各种情境的需求。规则如下： 修饰符 signed、unsigned、long 和 short 可应用于整型 int signed 和 unsigned 可应用于字符型char long 可应用于双精度型 double 修饰符 signed 和 unsigned 也可以作为 long 或 short 修饰符的前缀。例如：unsigned long int","categories":[{"name":"c++s","slug":"c-s","permalink":"http://yoursite.com/categories/c-s/"}],"tags":[{"name":"c++","slug":"c","permalink":"http://yoursite.com/tags/c/"}]},{"title":"cmake","slug":"cmake","date":"2022-07-28T03:33:13.000Z","updated":"2022-11-22T01:27:53.536Z","comments":true,"path":"2022/07/28/cmake/","link":"","permalink":"http://yoursite.com/2022/07/28/cmake/","excerpt":"","text":"","categories":[{"name":"c++","slug":"c","permalink":"http://yoursite.com/categories/c/"}],"tags":[{"name":"编译工具","slug":"编译工具","permalink":"http://yoursite.com/tags/%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7/"}]},{"title":"javaCV","slug":"javaCV","date":"2022-07-15T05:37:04.000Z","updated":"2022-11-22T01:27:53.661Z","comments":true,"path":"2022/07/15/javaCV/","link":"","permalink":"http://yoursite.com/2022/07/15/javaCV/","excerpt":"","text":"参考：http://zhangxuetu.cn/archives/2013 JavaCV通过JavaCPP调用了FFmpeg，并且对FFmpeg复杂的操作进行了封装，把视频处理分成了两大类：“帧抓取器”（FrameGrabber）和“帧录制器”（又叫“帧推流器”，FrameRecorder）以及用于存放音视频帧的Frame。流程： 视频源—-&gt;帧抓取器（FrameGabber） —-&gt;抓取视频帧（Frame）—-&gt;帧录制器（FrameRecorder）—-&gt;推流/录制—-&gt;流媒体服务/录像文件。 1、帧抓取器（FrameGrabber） 解码 封装了FFmpeg的检索流信息，自动猜测视频解码格式，音视频解码等具体API，并把解码完的像素数据（可配置像素格式）或音频数据保存到Frame中返回。 2、帧录制器/推流器（FrameRecorder） 编码 封装了FFmpeg的音视频编码操作和封装操作，把传参过来的Frame中的数据取出并进行编码、封装、发送等操作流程。 3、帧转换器（FrameConverter） FrameConverter封装了常用的转换操作，比如opencv与Frame的互转、java图像与Frame的互转以及安卓平台的Bitmap图像与Frame互转操作。 4、图像预览器（CancasFrame） CanvasFrame是用于预览Frame图像的工具类，但是这个工具类的gama值通常是有问题的，所以显示的图像可能会偏色，但是不影响最终图像的色彩。 5、帧（Frame） 存放解码后的视频图像像素和音频采样数据（如果没有配置FrameGrabber的像素格式和音频格式，那么默认解码后的视频格式是yuv420j，音频则是pcm采样数据）。里面**包含解码后：图像像素数据，大小（分辨率）、音频采样数据，音频采样率，音频通道（单声道、立体声等）**等数据。 Frame里面的一个字段opaque(翻译：不透明体)引用AVFrame、AVPacket、Mat等数据，也即是说，如果你是解码后获取的Frame，里面存放的属性找不到你需要的，需要从opaque属性中取需要的AVFrame原生属性。 AVFrame：ffmpeg解码的数据结构体（视频对应RGB/YUV像素数据，音频对应PCM采样数据） AVPacket：ffmpeg未解码数据结构体（视频对应H.264等码流数据，音频对应AAC/MP3等码流数据） Mat：ffmpeg解码后的数据类型是AVFrame，而Opencv的图像数据结构是Mat。 123456//获取视频解码后的图像像素，也就是说这时的Frame中的opaque存放的是AVFrameFrame frame = grabber.grabImage();//把Frame直接强制转换为AVFrameAVFrame avframe=(AVFrame)frame.opaque;long lastPts=avframe.pts();System.err.println(&quot;显示时间：&quot;+lastPts);","categories":[{"name":"javaCV","slug":"javaCV","permalink":"http://yoursite.com/categories/javaCV/"}],"tags":[{"name":"javaCV","slug":"javaCV","permalink":"http://yoursite.com/tags/javaCV/"}]},{"title":"ffmpeg图片转视频","slug":"ffmpeg图片转视频","date":"2022-07-11T03:01:07.000Z","updated":"2022-11-22T01:27:53.571Z","comments":true,"path":"2022/07/11/ffmpeg图片转视频/","link":"","permalink":"http://yoursite.com/2022/07/11/ffmpeg%E5%9B%BE%E7%89%87%E8%BD%AC%E8%A7%86%E9%A2%91/","excerpt":"","text":"https://magiclen.org/ffmpeg-image-sequence-encode/","categories":[{"name":"ffmpeg","slug":"ffmpeg","permalink":"http://yoursite.com/categories/ffmpeg/"}],"tags":[{"name":"ffmpeg","slug":"ffmpeg","permalink":"http://yoursite.com/tags/ffmpeg/"}]},{"title":"ffmpeg","slug":"ffmpeg","date":"2022-07-05T06:21:36.000Z","updated":"2022-11-22T01:27:53.570Z","comments":true,"path":"2022/07/05/ffmpeg/","link":"","permalink":"http://yoursite.com/2022/07/05/ffmpeg/","excerpt":"","text":"转载：https://www.ruanyifeng.com/blog/2020/01/ffmpeg.html 转载：https://www.jianshu.com/p/df3216a52e59 FFmpeg 本身是一个庞大的项目，包含许多组件和库文件，最常用的是它的命令行工具。本文介绍 FFmpeg 命令行如何处理视频，比桌面视频处理软件更简洁高效。 如果你还没安装，可以根据官方文档 先完成安装。 一、概念 介绍 FFmpeg 用法之前，需要了解一些视频处理的基本概念。 1.1 容器 视频文件本身其实是一个容器（container），里面包括了视频和音频，也可能有字幕等其他内容。 常见的容器格式有以下几种。一般来说，视频文件的后缀名反映了它的容器格式。 MP4 MKV WebM AVI 下面的命令查看 FFmpeg 支持的容器。 1$ ffmpeg -formats 1.2 编码格式 视频和音频都需要经过编码，才能保存成文件。不同的编码格式（CODEC），有不同的压缩率，会导致文件大小和清晰度的差异。 常用的视频编码格式如下。 H.262 H.264 H.265 上面的编码格式都是有版权的，但是可以免费使用。此外，还有几种无版权的视频编码格式。 VP8 VP9 AV1 常用的音频编码格式如下。 MP3 AAC 上面所有这些都是有损的编码格式，编码后会损失一些细节，以换取压缩后较小的文件体积。无损的编码格式压缩出来的文件体积较大，这里就不介绍了。 下面的命令可以查看 FFmpeg 支持的编码格式，视频编码和音频编码都在内。 1$ ffmpeg -codecs 1.3 编码器 编码器（encoders）是实现某种编码格式的库文件。只有安装了某种格式的编码器，才能实现该格式视频/音频的编码和解码。 以下是一些 FFmpeg 内置的视频编码器。 libx264：最流行的开源 H.264 编码器 NVENC：基于 NVIDIA GPU 的 H.264 编码器 libx265：开源的 HEVC 编码器 libvpx：谷歌的 VP8 和 VP9 编码器 libaom：AV1 编码器 音频编码器如下。 libfdk-aac aac 下面的命令可以查看 FFmpeg 已安装的编码器。 1$ ffmpeg -encoders 二、FFmpeg 的使用格式 FFmpeg 的命令行参数非常多，可以分成五个部分。 1$ ffmpeg &#123;1&#125; &#123;2&#125; -i &#123;3&#125; &#123;4&#125; &#123;5&#125; 上面命令中，五个部分的参数依次如下。 全局参数 输入文件参数 输入文件 输出文件参数 输出文件 参数太多的时候，为了便于查看，ffmpeg 命令可以写成多行。 123456$ ffmpeg \\[全局参数] \\[输入文件参数] \\-i [输入文件] \\[输出文件参数] \\[输出文件] 下面是一个例子。 123456$ ffmpeg \\-y \\ # 全局参数-c:a libfdk_aac -c:v libx264 \\ # 输入文件参数-i input.mp4 \\ # 输入文件-c:v libvpx-vp9 -c:a libvorbis \\ # 输出文件参数output.webm # 输出文件 上面的命令将 mp4 文件转成 webm 文件，这两个都是容器格式。输入的 mp4 文件的音频编码格式是 aac，视频编码格式是 H.264；输出的 webm 文件的视频编码格式是 VP9，音频格式是 Vorbis。 如果不指明编码格式，FFmpeg 会自己判断输入文件的编码。因此，上面的命令可以简单写成下面的样子。 1$ ffmpeg -i input.avi output.mp4 三、常用命令行参数 FFmpeg 常用的命令行参数如下。 -c：指定编码器 -c copy：直接复制，不经过重新编码（这样比较快） -c:v：指定视频编码器 -c:a：指定音频编码器 -i：指定输入文件 -an：去除音频流 -vn： 去除视频流 -preset：指定输出的视频质量，会影响文件的生成速度，有以下几个可用的值 ultrafast, superfast, veryfast, faster, fast, medium, slow, slower, veryslow。 -y：不经过确认，输出时直接覆盖同名文件。 四、常见用法 下面介绍 FFmpeg 几种常见用法。 4.1 查看文件信息 查看视频文件的元信息，比如编码格式和比特率，可以只使用-i参数。 1$ ffmpeg -i input.mp4 上面命令会输出很多冗余信息，加上-hide_banner参数，可以只显示元信息。 1$ ffmpeg -i input.mp4 -hide_banner 4.2 转换编码格式 转换编码格式（transcoding）指的是， 将视频文件从一种编码转成另一种编码。比如转成 H.264 编码，一般使用编码器libx264，所以只需指定输出文件的视频编码器即可。 1$ ffmpeg -i [input.file] -c:v libx264 output.mp4 下面是转成 H.265 编码的写法。 1$ ffmpeg -i [input.file] -c:v libx265 output.mp4 4.3 转换容器格式 转换容器格式（transmuxing）指的是，将视频文件从一种容器转到另一种容器。下面是 mp4 转 webm 的写法。 1$ ffmpeg -i input.mp4 -c copy output.webm 上面例子中，只是转一下容器，内部的编码格式不变，所以使用-c copy指定直接拷贝，不经过转码，这样比较快。 4.4 调整码率 调整码率（transrating）指的是，改变编码的比特率，一般用来将视频文件的体积变小。下面的例子指定码率最小为964K，最大为3856K，缓冲区大小为 2000K。 1234$ ffmpeg \\-i input.mp4 \\-minrate 964K -maxrate 3856K -bufsize 2000K \\output.mp4 4.5 改变分辨率（transsizing） 下面是改变视频分辨率（transsizing）的例子，从 1080p 转为 480p 。 1234$ ffmpeg \\-i input.mp4 \\-vf scale=480:-1 \\output.mp4 4.6 提取音频 有时，需要从视频里面提取音频（demuxing），可以像下面这样写。 1234$ ffmpeg \\-i input.mp4 \\-vn -c:a copy \\output.aac 上面例子中，-vn表示去掉视频，-c:a copy表示不改变音频编码，直接拷贝。 4.7 添加音轨 添加音轨（muxing）指的是，将外部音频加入视频，比如添加背景音乐或旁白。 123$ ffmpeg \\-i input.aac -i input.mp4 \\output.mp4 上面例子中，有音频和视频两个输入文件，FFmpeg 会将它们合成为一个文件。 4.8 截图 下面的例子是从指定时间开始，连续对1秒钟的视频进行截图。 12345$ ffmpeg \\-y \\-i input.mp4 \\-ss 00:01:24 -t 00:00:01 \\output_%3d.jpg 如果只需要截一张图，可以指定只截取一帧。 12345$ ffmpeg \\-ss 01:23:45 \\-i input \\-vframes 1 -q:v 2 \\output.jpg 上面例子中，-vframes 1指定只截取一帧，-q:v 2表示输出的图片质量，一般是1到5之间（1 为质量最高）。 4.9 裁剪 裁剪（cutting）指的是，截取原始视频里面的一个片段，输出为一个新视频。可以指定开始时间（start）和持续时间（duration），也可以指定结束时间（end）。 12$ ffmpeg -ss [start] -i [input] -t [duration] -c copy [output]$ ffmpeg -ss [start] -i [input] -to [end] -c copy [output] 下面是实际的例子。 12$ ffmpeg -ss 00:01:50 -i [input] -t 10.5 -c copy [output]$ ffmpeg -ss 2.5 -i [input] -to 10 -c copy [output] 上面例子中，-c copy表示不改变音频和视频的编码格式，直接拷贝，这样会快很多。 4.10 为音频添加封面 有些视频网站只允许上传视频文件。如果要上传音频文件，必须为音频添加封面，将其转为视频，然后上传。 下面命令可以将音频文件，转为带封面的视频文件。 12345$ ffmpeg \\-loop 1 \\-i cover.jpg -i input.mp3 \\-c:v libx264 -c:a aac -b:a 192k -shortest \\output.mp4 上面命令中，有两个输入文件，一个是封面图片cover.jpg，另一个是音频文件input.mp3。-loop 1参数表示图片无限循环，-shortest参数表示音频文件结束，输出视频就结束。 1. FFMPEG 目录及作用 libavcodec： 提供了一系列编码器的实现。 libavformat： 实现在流协议，容器格式及其本IO访问。 libavutil： 包括了hash器，解码器和各利工具函数。 libavfilter： 提供了各种音视频过滤器。 libavdevice： 提供了访问捕获设备和回放设备的接口。 libswresample： 实现了混音和重采样。 libswscale： 实现了色彩转换和缩放工能。 2.FFMPEG基本概念 在讲解 FFMPEG 命令之前，我们先要介绍一些音视频格式的基要概念。 音／视频流 在音视频领域，我们把一路音／视频称为一路流。如我们小时候经常使用VCD看港片，在里边可以选择粤语或国语声音，其实就是CD视频文件中存放了两路音频流，用户可以选择其中一路进行播放。 容器 我们一般把 MP4､ FLV、MOV等文件格式称之为容器。也就是在这些常用格式文件中，可以存放多路音视频文件。以 MP4 为例，就可以存放一路视频流，多路音频流，多路字幕流。 channel channel是音频中的概念，称之为声道。在一路音频流中，可以有单声道，双声道或立体声。 3.FFMPEG 命令 我们按使用目的可以将 FFMPEG 命令分成以下几类： 基本信息查询命令 录制 分解/复用 处理原始数据 滤镜 切割与合并 图／视互转 直播相关 除了 FFMPEG 的基本信息查询命令外，其它命令都按下图所示的流程处理音视频 截屏2021-09-25 下午9.10.45.png 先是解复用获取到编码的数据包，然后将编码的数据包传送给解码器（除非为数据流选择了流拷贝，请参阅进一步描述）。 解码器产生未压缩的帧（原始视频/ PCM音频/ …），可以通过滤波进一步处理（见下一节）。 在过滤之后，帧被传递到编码器，编码器并输出编码的数据包。 最后，这些传递给复用器，将编码的数据包写入输出文件。 默认情况下，ffmpeg只包含输入文件中每种类型（视频，音频，字幕）的一个流，并将其添加到每个输出文件中。 它根据以下标准挑选每一个的“最佳”：对于视频，它是具有最高分辨率的流，对于音频，它是具有最多channel的流，对于字幕，是第一个字幕流。 在相同类型的几个流相等的情况下，选择具有最低索引的流。 您可以通过使用-vn / -an / -sn / -dn选项来禁用某些默认设置。 要进行全面的手动控制，请使用-map选项，该选项禁用刚描述的默认设置。 3.1 基本信息查询命令 FFMPEG 可以使用下面的参数进行基本信息查询。例如，想查询一下现在使用的 FFMPEG 都支持哪些 filter，就可以用 ffmpeg -filters 来查询。详细参数说明如下： 参数 说明 -version 显示版本 -formats 显示可用的格式（包括设备）。 -demuxers 显示可用的demuxers。 -muxers 显示可用的muxers。 -devices 显示可用的设备。 -codecs 显示libavcodec已知的所有编解码器。 -decoders 显示可用的解码器。 -encoders 显示所有可用的编码器。 -bsfs 显示可用的比特流filter。 -protocols 显示可用的协议。 -filters 显示可用的libavfilter过滤器。 -pix_fmts 显示可用的像素格式。 -sample_fmts 显示可用的采样格式。 -layouts 显示channel名称和标准channel布局。 -colors 显示识别的颜色名称。 1ffmpeg -h encoder&#x3D;libfdk_aac 查询编译器libfdk_aac的信息 3.2命令基本格式及参数 FFMPEG 处理音视频时使用的命令格式与参数 12ffmpeg [global_options] &#123;[input_file_options] -i input_url&#125; ... &#123;[output_file_options] output_url&#125; ... ffmpeg 通过 -i 选项读取输任意数量的输入“文件”（可以是常规文件，管道，网络流，抓取设备等，并写入任意数量的输出“文件”。 原则上，每个输入/输出“文件”都可以包含任意数量的不同类型的视频流（视频/音频/字幕/附件/数据）。 流的数量和/或类型是由容器格式来限制。 选择从哪个输入进入到哪个输出将自动完成或使用 -map 选项。 要引用选项中的输入文件，您必须使用它们的索引（从0开始）。 例如。 第一个输入文件是0，第二个输入文件是1，等等。类似地，文件内的流被它们的索引引用。 例如。 2：3是指第三个输入文件中的第四个流 上面就是 FFMPEG 处理音视频的常用命令，下面是一些常用参数 3.2.1 主要参数 参数 说明 -f fmt（输入/输出） 强制输入或输出文件格式。 格式通常是自动检测输入文件，并从输出文件的文件扩展名中猜测出来，所以在大多数情况下这个选项是不需要的。 -i url（输入） 输入文件的网址 -y（全局参数） 覆盖输出文件而不询问。 -n（全局参数） 不要覆盖输出文件，如果指定的输出文件已经存在，请立即退出。 -c [：stream_specifier] codec（输入/输出，每个流） 选择一个编码器（当在输出文件之前使用）或解码器（当在输入文件之前使用时）用于一个或多个流。codec 是解码器/编码器的名称或 copy（仅输出）以指示该流不被重新编码。如：ffmpeg -i INPUT -map 0 -c:v libx264 -c:a copy OUTPUT -codec [：stream_specifier]编解码器（输入/输出，每个流） 同 -c -t duration（输入/输出） 当用作输入选项（在-i之前）时，限制从输入文件读取的数据的持续时间。当用作输出选项时（在输出url之前），在持续时间到达持续时间之后停止输出。 -ss位置（输入/输出） 当用作输入选项时（在-i之前），在这个输入文件中寻找位置。 请注意，在大多数格式中，不可能精确搜索，因此ffmpeg将在位置之前寻找最近的搜索点。 当转码和-accurate_seek被启用时（默认），搜索点和位置之间的这个额外的分段将被解码和丢弃。 当进行流式复制或使用-noaccurate_seek时，它将被保留。当用作输出选项（在输出url之前）时，解码但丢弃输入，直到时间戳到达位置。 -frames [：stream_specifier] framecount（output，per-stream） 停止在帧计数帧之后写入流。 -filter [：stream_specifier] filtergraph（output，per-stream） 创建由filtergraph指定的过滤器图，并使用它来过滤流。filtergraph是应用于流的filtergraph的描述，并且必须具有相同类型的流的单个输入和单个输出。在过滤器图形中，输入与标签中的标签相关联，标签中的输出与标签相关联。有关filtergraph语法的更多信息，请参阅ffmpeg-filters手册。 3.2.2 视频参数 参数 说明 -vframes num（输出） 设置要输出的视频帧的数量。对于-frames：v，这是一个过时的别名，您应该使用它。 -r [：stream_specifier] fps（输入/输出，每个流） 设置帧率（Hz值，分数或缩写）。作为输入选项，忽略存储在文件中的任何时间戳，根据速率生成新的时间戳。这与用于-framerate选项不同（它在FFmpeg的旧版本中使用的是相同的）。如果有疑问，请使用-framerate而不是输入选项-r。作为输出选项，复制或丢弃输入帧以实现恒定输出帧频fps。 -s [：stream_specifier]大小（输入/输出，每个流） 设置窗口大小。作为输入选项，这是video_size专用选项的快捷方式，由某些分帧器识别，其帧尺寸未被存储在文件中。作为输出选项，这会将缩放视频过滤器插入到相应过滤器图形的末尾。请直接使用比例过滤器将其插入到开头或其他地方。格式是’wxh’（默认 - 与源相同）。 -aspect [：stream_specifier] 宽高比（输出，每个流） 设置方面指定的视频显示宽高比。aspect可以是浮点数字符串，也可以是num：den形式的字符串，其中num和den是宽高比的分子和分母。例如“4：3”，“16：9”，“1.3333”和“1.7777”是有效的参数值。如果与-vcodec副本一起使用，则会影响存储在容器级别的宽高比，但不会影响存储在编码帧中的宽高比（如果存在）。 -vn（输出） 禁用视频录制。 -vcodec编解码器（输出） 设置视频编解码器。这是-codec：v的别名。 -vf filtergraph（输出） 创建由filtergraph指定的过滤器图，并使用它来过滤流。 3.2.3 音频参数 参数 说明 -aframes（输出） 设置要输出的音频帧的数量。这是-frames：a的一个过时的别名。 -ar [：stream_specifier] freq（输入/输出，每个流） 设置音频采样频率。对于输出流，它默认设置为相应输入流的频率。对于输入流，此选项仅适用于音频捕获设备和原始分路器，并映射到相应的分路器选件。 -ac [：stream_specifier]通道（输入/输出，每个流） 设置音频通道的数量。对于输出流，它默认设置为输入音频通道的数量。对于输入流，此选项仅适用于音频捕获设备和原始分路器，并映射到相应的分路器选件。 -an（输出） 禁用录音。 -acodec编解码器（输入/输出） 设置音频编解码器。这是-codec的别名：a。 -sample_fmt [：stream_specifier] sample_fmt（输出，每个流） 设置音频采样格式。使用-sample_fmts获取支持的样本格式列表。 -af filtergraph（输出） 创建由filtergraph指定的过滤器图，并使用它来过滤流。 3.3 录制 首先通过下面的命令查看一下 mac 上都有哪些设备。 1ffmpeg -f avfoundation -list_devices true -i &quot;&quot; 3.3.1 录屏 1ffmpeg -f avfoundation -i 1 -r 30 out.yuv -f 指定使用 avfoundation 采集数据。 -i 指定从哪儿采集数据，它是一个文件索引号。在我的MAC上，1代表桌面（可以通过上面的命令查询设备索引号）。 -r 指定帧率。按ffmpeg官方文档说-r与-framerate作用相同，但实际测试时发现不同。-framerate 用于限制输入，而-r用于限制输出。 注意，桌面的输入对帧率没有要求，所以不用限制桌面的帧率。其实限制了也没用。 3.3.2 录屏+声音 1ffmpeg -f avfoundation -i 1:0 -r 29.97 -c:v libx264 -crf 0 -c:a libfdk_aac -b:a 32k out.flv -i 1:0 冒号前面的 “1” 代表的屏幕索引号。冒号后面的&quot;0&quot;代表的声音索相号。 -c:v 与参数 -vcodec 一样，表示视频编码器。c 是 codec 的缩写，v 是video的缩写。 -crf 是 x264 的参数。 0 表式无损压缩。 -c:a 与参数 -acodec 一样，表示音频编码器。 -b:a 指定音频码率。 b 是 bitrate的缩写, a是 audio的缩与。 3.3.3 录视频 1ffmpeg -framerate 30 -f avfoundation -i 0 out.mp4 -framerate 限制视频的采集帧率。这个必须要根据提示要求进行设置，如果不设置就会报错。 -f 指定使用 avfoundation 采集数据。 -i 指定视频设备的索引号。 3.3.4 视频+音频 1ffmpeg -framerate 30 -f avfoundation -i 0:0 out.mp4 3.3.5 录音 1ffmpeg -f avfoundation -i :0 out.wav 3.3.6 录制音频裸数据 1ffmpeg -f avfoundation -i :0 -ar 44100 -f s16le out.pcm 3.4 分解与复用 流拷贝是通过将 copy 参数提供给-codec选项来选择流的模式。它使得ffmpeg省略了指定流的解码和编码步骤，所以它只能进行多路分解和多路复用。 这对于更改容器格式或修改容器级元数据很有用。 在这种情况下，上图将简化为： 图片1.png 由于没有解码或编码，速度非常快，没有质量损失。 但是，由于许多因素，在某些情况下可能无法正常工作。 应用过滤器显然也是不可能的，因为过滤器处理未压缩的数据 3.4.1 抽取音频流 1ffmpeg -i input.mp4 -acodec copy -vn out.aac acodec: 指定音频编码器，copy 指明只拷贝，不做编解码。 vn: v 代表视频，n 代表 no 也就是无视频的意思。 3.4.2 抽取视频流 1ffmpeg -i input.mp4 -vcodec copy -an out.h264 vcodec: 指定视频编码器，copy 指明只拷贝，不做编解码。 an: a 代表视频，n 代表 no 也就是无音频的意思。 3.4.3 转格式 1ffmpeg -i out.mp4 -vcodec copy -acodec copy out.flv 上面的命令表式的是音频、视频都直接 copy，只是将 mp4 的封装格式转成了flv。 3.4.4 音视频合并 1ffmpeg -i out.h264 -i out.aac -vcodec copy -acodec copy out.mp4 3.5 处理原始数据 3.5.1 提取YUV数据 12ffmpeg -i input.mp4 -an -c:v rawvideo -pixel_format yuv420p out.yuvffplay -s wxh out.yuv -c:v rawvideo 指定将视频转成原始数据 -pixel_format yuv420p 指定转换格式为yuv420p 3.5.2 YUV转H264 1ffmpeg -f rawvideo -pix_fmt yuv420p -s 320x240 -r 30 -i out.yuv -c:v libx264 -f rawvideo out.h264 3.5.3 提取PCM数据 12ffmpeg -i out.mp4 -vn -ar 44100 -ac 2 -f s16le out.pcmffplay -ar 44100 -ac 2 -f s16le -i out.pcm 3.5.4 PCM转WAV 12ffmpeg -f s16be -ar 8000 -ac 2 -acodec pcm_s16be -i input.raw output.wavffplay -f s16le -ar 8000 -ac 2 -acodec pcm_s16be tt5.wav 3.6 滤镜 在编码之前，ffmpeg可以使用libavfilter库中的过滤器处理原始音频和视频帧。 几个链式过滤器形成一个过滤器图形。 ffmpeg区分两种类型的过滤器图形：简单和复杂。 3.6.1 简单滤镜 简单的过滤器图是那些只有一个输入和输出，都是相同的类型。 在上面的图中，它们可以通过在解码和编码之间插入一个额外的步骤来表示： 图片2.png 简单的filtergraphs配置了per-stream-filter选项（分别为视频和音频使用-vf和-af别名）。 一个简单的视频filtergraph可以看起来像这样的例子： 图片3.png 请注意，某些滤镜会更改帧属性，但不会改变帧内容。 例如。 上例中的fps过滤器会改变帧数，但不会触及帧内容。 另一个例子是setpts过滤器。 3.6.2 复杂滤镜 复杂的过滤器图是那些不能简单描述为应用于一个流的线性处理链的过滤器图。 例如，当图形有多个输入和/或输出，或者当输出流类型与输入不同时，就是这种情况。 他们可以用下图来表示： 图片5.png 复杂的过滤器图使用-filter_complex选项进行配置。 请注意，此选项是全局性的，因为复杂的过滤器图形本质上不能与单个流或文件明确关联。 -lavfi选项等同于-filter_complex。 一个复杂的过滤器图的一个简单的例子是覆盖过滤器，它有两个视频输入和一个视频输出，包含一个视频叠加在另一个上面。 它的音频对应是amix滤波器 3.6.3 添加水印 1ffmpeg -i out.mp4 -vf &quot;movie=logo.png,scale=64:48[watermask];[in][watermask] overlay=30:10 [out]&quot; water.mp4 -vf中的 movie 指定logo位置。scale 指定 logo 大小。overlay 指定 logo 摆放的位置。 添加文字水印 1ffmpeg -i src.mp4 -vf &quot;drawtext=fontfile=simhei.ttf: text=&#x27;1&#x27;:x=300:y=500:fontsize=100:fontcolor=yellow:shadowy=2&quot; drawtext.mp4 添加本地时间水印 1ffmpeg -i src.mp4 -vf &quot;drawtext=fontsize=160:text=&#x27;%&#123;localtime\\:%T&#125;&#x27;&quot; -c:v libx264 -an -f mp4 output.mp4 -y https://www.jianshu.com/p/e4ad7e1e7ed5 3.6.4 删除水印 先通过 ffplay 找到要删除 LOGO 的位置 1ffplay -i test.flv -vf delogo=x=806:y=20:w=70:h=80:show=1 使用 delogo 滤镜删除 LOGO 1ffmpeg -i test.flv -vf delogo=x=806:y=20:w=70:h=80 output.flv 3.6.5 视频缩小一倍 1ffmpeg -i out.mp4 -vf scale=iw/2:-1 scale.mp4 -vf scale 指定使用简单过滤器 scale，iw/2:-1 中的 iw 指定按整型取视频的宽度。 -1 表示高度随宽度一起变化。 3.6.6 视频裁剪 1ffmpeg -i VR.mov -vf crop=in_w-200:in_h-200 vr_new.mp4 crop 格式：crop=out_w:out_h❌y out_w: 输出的宽度。可以使用 in_w 表式输入视频的宽度。 out_h: 输出的高度。可以使用 in_h 表式输入视频的高度。 x : X坐标 y : Y坐标 如果 x和y 设置为 0,说明从左上角开始裁剪。如果不写是从中心点裁剪。 3.6.7 倍速播放 1ffmpeg -i out.mp4 -filter_complex &quot;[0:v]setpts=0.5*PTS[v];[0:a]atempo=2.0[a]&quot; -map &quot;[v]&quot; -map &quot;[a]&quot; speed2.0.mp4 -filter_complex 复杂滤镜，[0:v]表示第一个（文件索引号是0）文件的视频作为输入。setpts=0.5*PTS表示每帧视频的pts时间戳都乘0.5 ，也就是差少一半。[v]表示输出的别名。音频同理就不详述了。 map 可用于处理复杂输出，如可以将指定的多路流输出到一个输出文件，也可以指定输出到多个文件。&quot;[v]&quot; 复杂滤镜输出的别名作为输出文件的一路流。上面 map的用法是将复杂滤镜输出的视频和音频输出到指定文件中。 3.6.8 对称视频 1ffmpeg -i out.mp4 -filter_complex &quot;[0:v]pad=w=2*iw[a];[0:v]hflip[b];[a][b]overlay=x=w&quot; duicheng.mp4 hflip 水平翻转 如果要修改为垂直翻转可以用vflip。 3.6.9 画中画 1ffmpeg -i out.mp4 -i out1.mp4 -filter_complex &quot;[1:v]scale=w=176:h=144:force_original_aspect_ratio=decrease[ckout];[0:v][ckout]overlay=x=W-w-10:y=0[out]&quot; -map &quot;[out]&quot; -movflags faststart new.mp4 3.6.10 录制画中画 12345ffmpeg -f avfoundation -i &quot;1&quot; -framerate 30 -f avfoundation -i &quot;0:0&quot; -r 30 -c:v libx264 -preset ultrafast -c:a libfdk_aac -profile:a aac_he_v2 -ar 44100 -ac 2 -filter_complex &quot;[1:v]scale=w=176:h=144:force_original_aspect_ratio=decrease[a];[0:v][a]overlay=x=W-w-10:y=0[out]&quot; -map &quot;[out]&quot; -movflags faststart -map 1:a b.mp4 3.6.11 多路视频拼接 1ffmpeg -f avfoundation -i &quot;1&quot; -framerate 30 -f avfoundation -i &quot;0:0&quot; -r 30 -c:v libx264 -preset ultrafast -c:a libfdk_aac -profile:a aac_he_v2 -ar 44100 -ac 2 -filter_complex &quot;[0:v]scale=320:240[a];[a]pad=640:240[b];[b][1:v]overlay=320:0[out]&quot; -map &quot;[out]&quot; -movflags faststart -map 1:a c.mp4 3.7 音视频的拼接与裁剪 3.7.1 裁剪 1ffmpeg -i out.mp4 -ss 00:00:00 -t 10 out1.mp4 -ss 指定裁剪的开始时间，精确到秒 -t 被裁剪后的时长。 3.7.2 合并 首先创建一个 inputs.txt 文件，文件内容如下： 123file &#x27;1.flv&#x27;file &#x27;2.flv&#x27;file &#x27;3.flv&#x27; 然后执行下面的命令： 1ffmpeg -f concat -i inputs.txt -c copy output.flv 3.7.3 hls切片 1ffmpeg -i out.mp4 -c:v libx264 -c:a libfdk_aac -strict -2 -f hls out.m3u8 -strict -2 指明音频使有AAC。 -f hls 转成 m3u8 格式。 3.8 视频图片互转 3.8.1 视频转JPEG 1ffmpeg -i test.flv -r 1 -f image2 image-%3d.jpeg 3.8.2 视频转gif 1ffmpeg -i out.mp4 -ss 00:00:00 -t 10 out.gif 3.8.3 图片转视频 1ffmpeg -f image2 -i image-%3d.jpeg images.mp4 3.9 直播相关 3.9.1 推流 1ffmpeg -re -i out.mp4 -c copy -f flv rtmp://server/live/streamName 3.9.2 拉流保存 1ffmpeg -i rtmp://server/live/streamName -c copy dump.flv 3.9.3 转流 1ffmpeg -i rtmp://server/live/originalStream -c:a copy -c:v copy -f flv rtmp://server/live/h264Stream 3.9.4 实时推流 1ffmpeg -framerate 15 -f avfoundation -i &quot;1&quot; -s 1280x720 -c:v libx264 -f flv rtmp://localhost:1935/live/room 3.10 ffplay 3.10.1 播放YUV 数据 1ffplay -s 192x144 -pix_fmt nv12 1.yuv 播放yuv数据，需要指定分辨率 -s 指定视频分辨率 -pix_fmt 指定像素格式 3.10.1 播放YUV中的 Y平面 1ffplay -s 640x480 -pix_fmt nv21 -vf extractplanes=&#x27;y&#x27; 1.yuv 播放pcm 1ffplay -ar 44100 -ac 2 -f s16le out.pcm","categories":[{"name":"ffmpeg","slug":"ffmpeg","permalink":"http://yoursite.com/categories/ffmpeg/"}],"tags":[{"name":"ffmpeg","slug":"ffmpeg","permalink":"http://yoursite.com/tags/ffmpeg/"}]},{"title":"Linux版本","slug":"Linux版本","date":"2022-07-01T08:52:57.000Z","updated":"2022-11-22T01:27:53.398Z","comments":true,"path":"2022/07/01/Linux版本/","link":"","permalink":"http://yoursite.com/2022/07/01/Linux%E7%89%88%E6%9C%AC/","excerpt":"","text":"1、查看版本 12345lsb_release -auname -acat /proc/versioncat /etc/issuecat /etc/*-release 2、分类 2.1 内核版本 12$ uname -aLinux ubuntu 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux ​ 内核版本指的是在Linus Torvalds领导下的开发小组开发出的系统内核的版本号，通常，内核版本号的第二位是偶数表示是稳定的版本，如2.6.25；是奇数表示有一些新的东西加入，是不稳定的测试版本，如2.5.6。Linux操作系统的核心就是它的内核，Linus Torvalds和他的小组在不断地开发和推出新内核。 2.2 发行版本 Linux的发行版本就是Linux内核再加上外围的实用程序组成的一个大软件。Linux的发行版本大体可以分为两类： 一类是社区组织维护的发行版本（Debian为代表，使用 .deb 和 .snap 的软件包，apt和dpkg进行包管理） 一类是商业公司维护的发行版本（RedHat为代表，使用 .rpm 和 flatpak 软件包，yum进行包管理） 3、发行版 Debian ​ Debian名字的由来 DebianGNU/Linux是由一个叫做伊恩·默多克（IanMurdock）在1993年发起的，他的名字以Ian开头，他太太的名字Debra开头三个字母是Deb。（老婆+自己名字）Debian是社区类Linux的典范，是迄今为止最遵循GNU规范的Linux系统，分为三个版本分支：stable(服务器版), testing(稳定版) 和unstable(测试版)。 Ubuntu ​ Ubuntu是基于Debian的unstable版本加强而来，可以这么说，Ubuntu就是一个拥有Debian所有的优点，以及自己所加强的优点的近乎完美的Linux桌面系统。共分三个版本: 基于Gnome的Ubuntu，基于KDE的Kubuntu以。基于Xfc的Xubuntu。更新较活跃。 Redhat 1994年3月，Linux1.0版正式发布，Marc Ewing [马克尤恩]成立了RedHat 软件公司，成为最著名的Linux分销商之一。 Fedora ​ Fedora和Redhat这两个Linux的发行版放联系很密切。Redhat 自9.0以后，不再发布桌面版的，而是把这个项目与开源社区合作，于是就有了Fedora 这个Linux 发行版。Fedora项目是由Red Hat 赞助，由开源社区与Red Hat 工程师合作开发的项目统称。稳定性一般！ centos ​ 全称是Community ENTerprise Operating System，社区企业操作系统，2004年是RHEL在源代码层面上的克隆版本(使用Redhat的内核)，对RHEL的主要修改是移除了一些不能自由使用的商标和闭源软件。和RHEL不同，CenOS可以免费试用，但无法得到红帽公司的技术支持。更新不活跃。 其他","categories":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/categories/linux/"}],"tags":[{"name":"版本","slug":"版本","permalink":"http://yoursite.com/tags/%E7%89%88%E6%9C%AC/"}]},{"title":"Mac命令","slug":"Mac命令","date":"2022-07-01T05:27:26.000Z","updated":"2022-11-22T01:27:53.399Z","comments":true,"path":"2022/07/01/Mac命令/","link":"","permalink":"http://yoursite.com/2022/07/01/Mac%E5%91%BD%E4%BB%A4/","excerpt":"","text":"查看mac上JDK路径 1&#x2F;usr&#x2F;libexec&#x2F;java_home -V mac 下ping IP和Port 12345678910ping 172.18.100.220nc -zvn -w 1 172.18.100.220 5007// 常用nc -z 172.18.100.220 5007-z 参数告诉netcat使用0输入输出模式，即连接成功后立即关闭连接，不进行数据交换- u udp端口-v 参数指使用冗余选项（译者注：即详细输出）-n 参数告诉netcat 不要使用DNS反向查询IP地址的域名-w 参数是超时参数，不加的话，可能一直不返回 打开不可见目录 如需要到/usr/local下去，打开Finder，然后使用command+shift+G，输入路径。","categories":[{"name":"命令","slug":"命令","permalink":"http://yoursite.com/categories/%E5%91%BD%E4%BB%A4/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://yoursite.com/tags/Mac/"}]},{"title":"k8s命令","slug":"k8s命令","date":"2022-06-28T03:08:20.000Z","updated":"2022-11-22T01:27:53.665Z","comments":true,"path":"2022/06/28/k8s命令/","link":"","permalink":"http://yoursite.com/2022/06/28/k8s%E5%91%BD%E4%BB%A4/","excerpt":"","text":"具体可见官方文档：https://kubernetes.io/zh-cn/docs/reference/kubectl/ 1、安装自动补全 Mac为例，其他类似，使用tab补全。 123456# 安装pip install kube-shell --user -U# 配置环境变量vim ~/.zshrcplugins=(kubectl)source &lt;(kubectl completion zsh) 2、命令 2.1 概述 格式：kubectl [command] [TYPE] [NAME] [flags] command 1create、get、describe、delete、expose、exec TYPE 声明command需要操作的资源类型，TYPE对大小写、单数、复数不敏感，支持缩写。 1234kubectl get pod kubectl get podskubectl get pokubectl get POD NAME 资源的名称，NAME是大小写敏感的。如果不指定某个资源的名称，则显示所有资源。 1kubectl get pods pod1 pod2 flags 不同的command，使用的flags不同，具体可见：https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands 2.2 常见命令 kubectl create - 使用一个文件或者标准输入创建资源。 12345# 使用exampe-service.yaml文件创建一个“service”对象$ kubectl create -f example-service.yaml# 使用example-controller.yaml文件创建一个&quot;replication&quot;对象$ kubectl create -f example-controller.yaml kubectl describe - 获取资源的详细状态，包括初始化中的资源。 123456789101112# 查看名为&lt;node-name&gt;的node节点详情$ kubectl describe nodes &lt;node-name&gt;# 查看名为&lt;pod-name&gt;的pod详情，包含pod的创建日志$ kubectl describe pods/&lt;pod-name&gt;# 查看所有由名为&lt;rc-name&gt;的replication管理的pod。# 注意: 任何由replication controller创建的pod，其名称前缀为replication名称。$ kubectl describe pods &lt;rc-name&gt;# 查看所有pods，但不包含未初始化的pods$ kubectl describe pods --include-uninitialized=false kubectl logs - 获取某个pod的日志 12345# 获取一个pod的日志快照$ kubectl logs &lt;pod-name&gt;# 获取一个pod的实时日志流，类似于linux的&#x27;tail -f&#x27;$ kubectl logs -f &lt;pod-name&gt; kubectl exec - 对pod中的容器执行命令 12345678# 从pod中获取运行&quot;date&quot;命令的输出，默认情况下，来自于pod中的第一个容器。$ kubectl exec &lt;pod-name&gt; date# 从pod中指定的容器中获取运行&quot;date&quot;命令的输出$ kubectl exec &lt;pod-name&gt; -c &lt;container-name&gt; date# 从pod中得到一个交互式tty(控制终端),并执行/bin/bash$ kubectl exec -ti &lt;pod-name&gt; /bin/bash","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"命令","slug":"命令","permalink":"http://yoursite.com/tags/%E5%91%BD%E4%BB%A4/"}]},{"title":"pod","slug":"pod","date":"2022-06-28T03:00:44.000Z","updated":"2022-11-22T01:27:53.741Z","comments":true,"path":"2022/06/28/pod/","link":"","permalink":"http://yoursite.com/2022/06/28/pod/","excerpt":"","text":"Kubernetes 使用 Pod 来管理容器，每个 Pod 可以包含一个或多个紧密关联的容器。 Pod 是一组紧密关联的容器集合，它们共享：PID、IPC、Network 和 UTS namespace，是 Kubernetes 调度的基本单位。Pod 内的多个容器共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"pod","slug":"pod","permalink":"http://yoursite.com/tags/pod/"}]},{"title":"分环境profile","slug":"分环境profile","date":"2022-06-10T03:21:54.000Z","updated":"2022-11-22T01:27:53.874Z","comments":true,"path":"2022/06/10/分环境profile/","link":"","permalink":"http://yoursite.com/2022/06/10/%E5%88%86%E7%8E%AF%E5%A2%83profile/","excerpt":"","text":"1、简述 开发项目，经常会有多个环境，我们会建几个配置文件。 2、命令指定 1java -jar demo.jar --spring.profiles.active=dev 3、配置文件指定 例如在application.yml中增加如下配置，指定使用dev环境配置。 123spring: profiles: active: dev 4、maven指定 maven里pom.xml 的profiles标签可以指定环境，并在idea里有窗口可以查看。 pom.xml 1234567891011121314151617181920&lt;profiles&gt; &lt;!-- 日常环境 --&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;properties&gt; &lt;profileActive&gt;dev&lt;/profileActive&gt; &lt;/properties&gt; &lt;activation&gt; &lt;!-- 设置默认激活这个配置 --&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;/profile&gt; &lt;!-- 发布环境 --&gt; &lt;profile&gt; &lt;id&gt;prod&lt;/id&gt; &lt;properties&gt; &lt;profileActive&gt;prod&lt;/profileActive&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;/profiles&gt; application.yml中增加如下配置(@profileActive@) 123spring: profiles: active: @profileActive@","categories":[{"name":"springBoot、maven","slug":"springBoot、maven","permalink":"http://yoursite.com/categories/springBoot%E3%80%81maven/"}],"tags":[{"name":"多环境","slug":"多环境","permalink":"http://yoursite.com/tags/%E5%A4%9A%E7%8E%AF%E5%A2%83/"}]},{"title":"kafka生产者","slug":"kafka生产者","date":"2022-06-07T11:44:12.000Z","updated":"2022-11-22T01:27:53.689Z","comments":true,"path":"2022/06/07/kafka生产者/","link":"","permalink":"http://yoursite.com/2022/06/07/kafka%E7%94%9F%E4%BA%A7%E8%80%85/","excerpt":"","text":"https://www.dvomu.com/bigdata/kafka/856.html 整个生产者客户端由两个线程协调工作，分别为main主线程和Sender线程。主线程中由KafkaProducer创建消息，然后会依次经过拦截器、序列化器、分区器，然后缓存到消息累加器，也称为RecordAccumulator。 而Sender线程则负责从RecordAccumulator中获取消息，然后发送给Kafka集群。 RecordAccumulator主要用来缓存消息，以便于Sender线程能够批量发送，这里有个生产者客户端参数buffer.memory来配置大小，默认32MB。 主线程过来的消息会被追加到RecordAccumulator的某个双端队列（Deque）中——对应分区号，队列的内容就是Deque，这里的ProduceeBatch包含一个或多个ProducerRecord。 RecordAccumulator内部还有个BufferPool，主要用来复用特定大小的ProducerBatch块，这个特定大小会通过batch.size指定，默认为16KB，例如说当有一条消息（ProducerRecord）流入RecordAccumulator，会先通过分区号寻找双端队列（如果没有则创建），再从队列的尾部获取一个ProducerBatch（如果没有则新建），查看ProducerBatch是否还可以装的下这个ProducerRecord，如果可以则写入，如果不可以则需创建个新的ProducerBatch。在新建ProducerBatch时会评估这个ProducerRecord的大小是否超过batch.size，也就是可复用ProducerBatch的大小，如果没超过，那么可以从BufferPool中拿个闲置的ProducerBatch来使用。如果超过，那么就按实际的大小创建ProducerBatch，这个ProducerBatch不会再被复用。 Sender从RecordAccumulator获取缓存的消息之后，会将原本的&lt;分区，Deque&gt;转换为&lt;Node，List&gt;，其中Node表示Kafka集群的broker节点，之后Sender还会进一步封装成&lt;Node，Request&gt;，这样就可以将Request发送给各个Node了。 请求从Sender线程发往Kafka之前还会保存到InFlightRequests中，InFlightRequests顾名思义——飞行途中的请求，主要作用是缓存已经发出去但还没有收到响应的请求，保存对象的具体形式为Map&lt;NodeId，Deque&gt;，这里有个重要的配置参数为max.in.flight.requests.per.connection，默认值为5，即每个连接最多只能缓存5个未收到响应的请求，超过这个数值之后便不能再往这个连接发送更多的请求了，除非有缓存的请求收到了响应。这里补充一点就是如果在需要保证消息顺序的场合建议把max.in.flight.requests.per.connection设为1，不过这样也会影响整体的吞吐量 通过比较各个Node的Deque的size来判断，哪个结点来不及处理消息，哪个结点处理消息很及时。 图为生产者将消息发送到Kafka集群的完整过程，详细说明如下： 第一步：在main线程中创建Producer对象，调用Send方法发送数据，发送数据过程中可能会经过拦截器（按需），通过kafka自带序列化器对数据进行序列话。 第二步：通过分区器将数据发送到缓存队列（缓存队列大小32兆，其中每个批次大小16k）。 第三步：缓存队列收到数据后会向内存池申请内存。 第四步：Sender线程主动向队列中拉取数据，拉取数据有2个条件，满足其一即可。 1）数据累计到batch.size大小才会发送数据，默认16K； 2）sender等待linger.ms设置的时间到了后就会发数据，单位ms，默认0ms，表示没有延迟； 第五步：Selector类似通道，负责连接队列与kafka集群，并将Sender中的请求发送到Kafka集群（broker节点最多缓存5个请求）。 第六步：Kafka集群收到请求后会将数据进行副本同步同时进行ACK相应，相应状态有3种： 1）0：生产者发送过来的数据，不需要等数据落盘应答； 2）1：生产者发送过来的数据，Leader收到数据后应答，不用等Follower落盘； 3）-1（all）：生产者发送过来的数据，Leader和ISR队列里面的所有节点收齐数据后再应答，即所有数据落盘后再应答。-1等同与all。 第七步：Selector收到响应后判断数据是否保存成功。 第八步：如果数据保存成功会清掉Sender中的数据，如果保存失败会进行重试请求保存，重试次数为int.MaxValue(可修改)。 第九步：Sender数据发送成功后同时会清理掉队列中的数据。 第十步：清理掉第三步申请的内存，将内存还回内存池。 1234567891011121314151617181920212223242526272829303132public class MyProducer &#123; public static void main(String[] args) &#123; Properties properties = new Properties(); properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;h1:9092,h2:9092&quot;); //指定对应的key和value虚拟化类型 properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); //1.创建kafka生产者 KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(properties); //2.发送数据 for (int i = 0; i &lt; 5; i++) &#123; // 2.1 异步发送 producer.send(new ProducerRecord&lt;&gt;(&quot;topicA&quot;, &quot;hello kafka&quot; + i)); // 2.1 异步发送带回调 //Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) producer.send(new ProducerRecord&lt;&gt;(&quot;topicA&quot;, &quot;hello kafka&quot; + i), new Callback() &#123; @Override public void onCompletion(RecordMetadata metadata, Exception e) &#123; if (e == null) &#123; System.out.println(&quot;主题:&quot; + metadata.topic() + &quot;,分区:&quot; + metadata.partition()); &#125; &#125; &#125;); //2.3 调用get()方法实现消息的同步发送。 producer.send(new ProducerRecord&lt;&gt;(&quot;topicA&quot;, &quot;hello kafka&quot; + i)).get(); &#125; System.out.println(&quot;发送完成&quot;); //3.关闭资源 producer.close(); &#125;&#125; 自定义分区 12345678910111213141516171819202122232425262728293031323334353637public class MyPartitioner implements Partitioner &#123; @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; int partition; if(value.toString().contains(&quot;order&quot;))&#123; partition = 0; &#125;else&#123; partition = 1; &#125; return partition; &#125; @Override public void close() &#123; &#125; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; &#125;&#125;//指定使用自定义分区properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG,&quot;com.dvomu.kafka.producer.custom.MyPartitioner&quot;);String msg = &quot;hello,fan&quot;;//发送数据producer.send(new ProducerRecord&lt;&gt;(&quot;topicA&quot;,msg), new Callback() &#123; @Override public void onCompletion(RecordMetadata metadata, Exception e) &#123; if(e==null)&#123; System.out.println(&quot;主题:&quot;+metadata.topic()+&quot;,分区:&quot;+metadata.partition()); &#125; &#125; &#125;);System.out.println(&quot;发送完成&quot;);","categories":[{"name":"中间件","slug":"中间件","permalink":"http://yoursite.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"}]},{"title":"minio","slug":"minio","date":"2022-05-27T08:30:28.000Z","updated":"2022-11-22T01:27:53.699Z","comments":true,"path":"2022/05/27/minio/","link":"","permalink":"http://yoursite.com/2022/05/27/minio/","excerpt":"","text":"1、安装 123wget https://dl.min.io/server/minio/release/linux-amd64/miniochmod +x minioMINIO_ROOT_USER=admin MINIO_ROOT_PASSWORD=password ./minio server /mnt/data --console-address &quot;:9001&quot;","categories":[{"name":"对象存储","slug":"对象存储","permalink":"http://yoursite.com/categories/%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"minio","slug":"minio","permalink":"http://yoursite.com/tags/minio/"}]},{"title":"grafana分享","slug":"grafana分享","date":"2022-05-24T07:14:46.000Z","updated":"2022-11-22T01:27:53.616Z","comments":true,"path":"2022/05/24/grafana分享/","link":"","permalink":"http://yoursite.com/2022/05/24/grafana%E5%88%86%E4%BA%AB/","excerpt":"","text":"日常中需要做一些服务器监控，我们会使用Prometheus+若干exporter 监控到指标数据，通过grafana展示，这样会很方面。但是业务需要看的指标具备很高的定制化，利用grafana现成的dashbord不满足情况，这种情况会自定义dashbord然后分享grafana页面连接，内嵌到应用里面。 但是，当你想将Grafana的Dashboard分享给别人，往往都需要登录，即使设置了只读账号，那还是会比较麻烦。在grafana里面可以设置匿名登录，具体操作如下： 1","categories":[{"name":"运维","slug":"运维","permalink":"http://yoursite.com/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"grafana","slug":"grafana","permalink":"http://yoursite.com/tags/grafana/"}]},{"title":"sip","slug":"sip","date":"2022-05-12T08:05:12.000Z","updated":"2022-11-22T01:27:53.752Z","comments":true,"path":"2022/05/12/sip/","link":"","permalink":"http://yoursite.com/2022/05/12/sip/","excerpt":"","text":"https://www.lmlphp.com/user/552/article/item/13133","categories":[{"name":"视频流","slug":"视频流","permalink":"http://yoursite.com/categories/%E8%A7%86%E9%A2%91%E6%B5%81/"}],"tags":[{"name":"信令","slug":"信令","permalink":"http://yoursite.com/tags/%E4%BF%A1%E4%BB%A4/"}]},{"title":"视频流","slug":"视频流","date":"2022-05-11T06:10:51.000Z","updated":"2022-11-22T01:27:54.221Z","comments":true,"path":"2022/05/11/视频流/","link":"","permalink":"http://yoursite.com/2022/05/11/%E8%A7%86%E9%A2%91%E6%B5%81/","excerpt":"","text":"1、流协议 M2V、 WAV、AC3 等后缀名的音视频文件，属 ES 流范畴。 M2P、MPG、MPEG 等后缀名的文件，属 PS 流。 MTS、M2TS、TS 等后缀名的文件，属 TS 流。 1.1 ES 流 (Elementary Stream) 也叫基本码流，包含视频、音频或数据的连续码流。分视频 ES 流，以及音频 ES 流等。 1.2 PES 流 (Packet Elementary Stream) 也叫打包的基本码流，是将基本的码流 ES 流根据需要分成长度不等的数据包，并加上包头就形成了打包的基本码流 PES 流。封装时不对其进行改变，只在前面添加头部，如私有头，解码时，将私有头剥掉，将原始 ES 码流送进解码器解码。PES 包是由固定包头，可选包头和负载三部分组成，其中固定包头固定 6 个字节；PES 包长度字段占位 16bit，最大值为 65536，因此一帧数据可能被分为多个 PES 包存放。 PTS、DTS PTS–PresentationTime Stamp（显示时间标记）表示显示单元出现在系统目标解码器（H.264、MJPEG 等）的时间。 DTS–Decoding Time Stamp（解码时间标记）表示将存取单元全部字节从解码缓存器移走的时间。 PTS/DTS 是打在 PES 包的包头里面的，这两个参数是解决音视频同步显示，防止解码器输入缓存上溢或下溢的关键。每一个 I（关键帧）、P（预测帧）、B（双向预测 帧）帧的包头都有一个 PTS 和 DTS，但 PTS 与 DTS 对于 B 帧不一样，无需标出 B 帧的 DTS，对于 I 帧和 P 帧，显示前一定要存储于视频解码器的重新排序缓存器中，经过延迟（重新排序）后再显示，所以一定要分别标明 PTS 和 DTS。 最后，根据 MPEG-2 的规范， 可以在下面最终生成的 2 种格式数据流中选择其一再封装。 1.3 TS 流 (Transport Stream) 也叫传输流，是由固定长度为 188 字节的包组成，含有独立时基的一个或多个 program, 一个 program 又可以包含多个视频、音频、和文字信息的 ES 流; 每个 ES 流会有不同的 PID 标示. 而又为了可以分析这些 ES 流, TS 有一些固定的 PID 用来间隔发送 program 和 ES 流信息的表格: PAT 和 PMT 表。适用于误码较多的环境。既使丢帧. 或者是从任意时段开始（播放），都能准确同步。 1.4 PS 流 (Program Stream) PS 流有一个结束码 MPEG_program_end_code：占位 32bit，其值为 0x000001B9，PS 流总是以 0x000001BA 开始，以 0x000001B9 结束，对于一个 PS 文件，有且只有一个结束码 0x000001B9，不过对于网传的 PS 流，则应该是没有结束码的。PS 包主要由固定包头，系统头，和 PES 包组成。目前的系统头部好像是没有用到的，所以对于系统头部的解析，我们一般只要先首先判断是否存在系统头（根据系统头的起始码 0x000001BB），然后我们读取系统头的头部长度, 即 header_length 部分，然后根据系统头部的长度，跳过 PS 系统头，进入下一个部分，即 PS 的 payload，PES 包；在固定包头和系统头之后，就是 PS 包的 payload，即 PES 包；若 PSM 存在，则第一个 PES 包即为 PSM。 1.5 PSM (program_stream_map) PSM 提供了对 PS 流中的原始流和他们之间的相互关系的描述信息；PSM 是作为一个 PES 分组出现，当 stream_id == 0xBC 时，说明此 PES 包是一个 PSM；PSM 是紧跟在系统头部后面的；PSM 是作为 PS 包的 payload 存在的； 解析 PS 包，要先找到 PS 包的的起始码 0x000001BA 位串，然后解析出系统头部字段，之后进入 PS 包的负载，判断是否有 PSM，根据 PSM 确定 payload 的 PES 包中所负载的 ES 流类型；然后再根据 ES 流类型和 ID 从 PES 包中解析出具体的 ES 流；解包过程则相反；若要从 PS 流中找出来帧类型，必须将 PS 包解析成 ES 并组成完整的帧，然后在帧数据开始根据 NAL 头来进行帧的类型的判断； PSM 只有在关键帧打包的时候，才会存在；IDR 包含了 SPS,PPS 和 I 帧；每个 IDR NALU 前一般都会包含 SPS、PPS 等 NALU，因此将 SPS、PPS、IDR 的 NALU 封装为一个 PS 包，包括 PS 头，PS system header，PSM，PES；所以一个 IDR NALU PS 包由外到内顺序是：PS header PS system header PSM PES。对于其它非关键帧的 PS 包，就简单多了，直接加上 PS 头和 PES 头就可以了。顺序为：PS header PES header h264raw data。以上是对只有视频 video 的情况，如果要把音频 Audio 也打包进 PS 封装，只需将数据加上 PES header 放到视频 PES 后就可以了。顺序如下：PS 包 = PS 头 PES(video) PES(audio)； Reference: https://blog.csdn.net/Guofengpu/article/details/52035557 https://blog.csdn.net/appledurian/article/details/73134558 http://bbs.ivideostar.com/forum.php?mod=viewthread&amp;tid=156 2、传输协议 2.1 RTP 实时传输协议 (Real-time Transport Protocol) RTP 协议详细说明了在互联网上传递音频和视频的标准数据包格式。它一开始被设计为一个多播协议，但后来被用在很多单播应用中。RTP 协议常用于流媒体系统（配合 RTSP 协议），视频会议和一键通（Push to Talk）系统（配合 H.323 或 SIP），使它成为 IP 电话产业的技术基础。RTP 协议和 RTP 控制协议 RTCP 一起使用，而且它是创建在 UDP 协议上的。 2.2 RTCP 实时传输控制协议 (RTP Control Protocol) RTCP 为 RTP 媒体流提供信道外（out-of-band）控制。RTCP 本身并不传输数据，但和 RTP 一起协作将多媒体数据打包和发送。RTCP 定期在流多媒体会话参加者之间传输控制数据。RTCP 收集相关媒体连接的统计信息，例如：传输字节数，传输分组数，丢失分组数，jitter，单向和双向网络延迟等等，网络应用程序即可利用 RTCP 的统计信息来控制传输的品质。RTP 使用一个 偶数 UDP port ；而 RTCP 则使用 RTP 的下一个 port，也就是一个奇数 port。 2.3 RTSP 实时流传输协议 (Real Time Streaming Protocol） RTSP 对流媒体提供了诸如暂停，快进等控制，而它本身并不传输数据，RTSP 的作用相当于流媒体服务器的远程控制。服务器端可以自行选择使用 TCP 或 UDP 来传送串流内容，它的语法和运作跟 HTTP 1.1 类似，但并不特别强调时间同步，所以比较能容忍网络延迟。应用程序对应的是 play, seek, pause, stop 等命令，RTSP 则是处理这些命令，在 UDP 传输时并使用 RTP(RTCP) 来完成。如果是 TCP 连接则不会使用 RTP(RTCP)。 协商过程 C-&gt;S:OPTION request // 询问 S 有哪些方法可用 S-&gt;C:OPTION response //S 回应信息中包括提供的所有可用方法 C-&gt;S:DESCRIBE request // 要求得到 S 提供的媒体初始化描述信息 S-&gt;C:DESCRIBE response //S 回应媒体初始化描述信息，主要是 sdp C-&gt;S:SETUP request // 设置会话的属性，以及传输模式，提醒 S 建立会话 S-&gt;C:SETUP response //S 建立会话，返回会话标识符，以及会话相关信息 C-&gt;S:PLAY request //C 请求播放 S-&gt;C:PLAY response //S 回应该请求的信息 Reference: https://blog.csdn.net/chen495810242/article/details/39207305 https://www.jianshu.com/p/4e3925f98e84 2.4 RTMP 实时消息传输协议 (Real Time Messaging Protocol) 该协议基于 TCP，是一个协议族，包括 RTMP 基本协议及 RTMPT/RTMPS/RTMPE 等多种变种。RTMP 是一种设计用来进行实时数据通信的网络协议，主要用来在 Flash/AIR 平台和支持 RTMP 协议的流媒体 / 交互服务器之间进行音视频和数据通信。支持该协议的软件包括 Adobe Media Server/Ultrant Media Server/red5 等。 2.5 HLS (HTTP Live Streaming) HLS (HTTP Live Streaming)是 Apple 的动态码率自适应技术。主要用于 PC 和 Apple 终端的音视频服务。包括一个 m3u(8)的索引文件，TS 媒体分片文件和 key 加密串文件。新型点播服务器系统，独创了内存缓存数据实时切片技术，不将 TS 切片文件存到磁盘，而是存在内存当中，这种技术使得服务器的磁盘上面不再会有 “数以吨计” 的文件碎片，极大提高了服务器运行的稳定性。同时，由于使用这种技术，使得终端请求数据时直接从服务器的内存中获取，极大提高了对终端数据请求的反应速度，优化了视频观看体验。 3、视频格式 3.1 H.264 H.264，同时也是 MPEG-4 第十部分，是由 ITU-T 视频编码专家组（VCEG）和 ISO/IEC 动态图像专家组（MPEG）联合组成的联合视频组（JVT，Joint Video Team）提出的高度压缩数字视频编解码器标准。 H.264 为解决不同应用中的网络传输的差异。定义了两层：视频编码层（VCL：Video Coding Layer）负责高效的视频内容表示，网络提取层（NAL：Network AbstractionLayer）负责以网络所要求的恰当的方式对数据进行打包和传送(如图所示： 标准的整体框架)。 市面上绝大多数的高清视频均是采用 H.264 的格式编码，它又分为四个最主要步骤，分别是流处理，逆变换，动态补偿，去方块滤波，这四步也是资源消耗的主要四个部分。 3.1.1 概念 H.264 是一次概念的革新，它打破常规，完全没有 I 帧、P 帧、B 帧的概念，也没有 IDR 帧的概念。对于 H.264 中出现的一些概念从大到小排序依次是：序列、图像、片组、片、NALU、宏块、亚宏块、块、像素。这里有几点值得说明： 在 H.264 协议中图像是个集合概念，顶场、底场、帧都可以称为图像（本文图像概念时都是集合概念）。因此我们可以知道，对于 H.264 协议来说，我们平常所熟悉的那些称呼，例如：I 帧、P 帧、B 帧等等，实际上都是我们把图像这个概念具体化和细小化了。我们在 H.264 里提到的 “帧” 通常就是指不分场的图像； 如果不采用 FMO（灵活宏块排序） 机制，则一幅图像只有一个片组； 如果不使用多个片，则一个片组只有一个片； 如果不采用 DP（数据分割）机制，则一个片就是一个 NALU，一个 NALU 也就是一个片。否则，一个片由 三个 NALU 组成（即标准 “表 7-1” 中 nal_unit_type 值为 2、3、4 的三个 NALU 属于一个片） 2: 编码条带数据分割块 A slice_data_partition_a_layer_rbsp() 3: 编码条带数据分割块 B slice_data_partition_b_layer_rbsp( ) 4: 编码条带数据分割块 C slice_data_partition_c_layer_rbsp( ) 一幅图像由 1～N 个片组组成，而每一个片组又由一个或若干个片组成一个片由一个 NALU 或三个 NALU（假如有数据分割）组成。图像解码过程中总是按照片进行解码，然后按照片组将解码宏块重组成图像。从这种意义上讲，片实际是最大的解码单元。 3.1.2 冗余处理 H.264 与以前的国际标准如 H.263 和 MPEG-4 相比，为达到高效的压缩，充分利用了各种冗余，统计冗余和视觉生理冗余。 1．统计冗余：频谱冗余（指色彩分量之间的相关性），空间冗余，还有时间冗余。这是视频压缩区别于静止图像的根本点，视频压缩主要利用时间冗余来实现大的压缩比。 2．视觉生理冗余 视觉生理冗余是由于人类的视觉系统（HVS）特性造成的，比如人眼对色彩分量的高频分量没有对亮度分量的高频分量敏感，对图像高频（即细节）处的噪声不敏感等。 针对这些冗余，视频压缩算法采用了不同的方法加以利用，但主要的考虑是集中在空间冗余和时间冗余上。H.264 也采用混合 (hybrid) 结构，即对空间冗余和时间冗余分别进行处理。对空间冗余，标准通过变换及量化达到消除的目的，这样编码的帧叫 I 帧；而时间冗余则是通过帧间预测，即运动估计和补偿来去除，这样编码的帧叫 P 帧或 B 帧。与以前标准不同的是，H.264 在编码 I 帧时，采用了帧内预测，然后对预测误差进行编码。这样就充分利用了空间相关性，提高了编码效率。H.264 帧内预测以 16x16 的宏块为基本单位。首先，编码器将与当前宏块同一帧的邻近像素作为参考，产生对当前宏块的预测值，然后对预测残差进行变换与量化，再对变换与量化后的结果做熵编码。熵编码的结果就可以形成码流了。由于在解码器端能够得到的参考数据都是经过反变换与反量化后的重建图像，因此为了使编解码一致，编码器端用于预测的参考数据就和解码器端一样，也是经过反变换与反量化后的重建图像。 Reference: https://blog.csdn.net/zhymxt/article/details/6654891 3.2 FLV (FlashVideo) FLV 是 FLASH VIDEO 的简称，FLV 流媒体格式是随着 Flash MX 的推出发展而来的视频格式。由于它形成的文件极小、加载速度极快，使得网络观看视频文件成为可能，它的出现有效地解决了视频文件导入 Flash 后，使导出的 SWF 文件体积庞大，不能在网络上很好的使用等问题。FLV 文件体积小巧，清晰的 FLV 视频 1 分钟在 1MB 左右，一部电影在 100MB 左右，是普通视频文件体积的 1/3。Flash 8 Video Encoder 支持转换的视频格式相当广泛，如：AVI、WMV、MPEG、ASF、MOV 等常见视频格式。","categories":[{"name":"视频流","slug":"视频流","permalink":"http://yoursite.com/categories/%E8%A7%86%E9%A2%91%E6%B5%81/"}],"tags":[{"name":"视频流","slug":"视频流","permalink":"http://yoursite.com/tags/%E8%A7%86%E9%A2%91%E6%B5%81/"}]},{"title":"视频网关","slug":"视频网关","date":"2022-05-10T05:37:14.000Z","updated":"2022-11-22T01:27:54.222Z","comments":true,"path":"2022/05/10/视频网关/","link":"","permalink":"http://yoursite.com/2022/05/10/%E8%A7%86%E9%A2%91%E7%BD%91%E5%85%B3/","excerpt":"","text":"一个基于国标GB28181协议视频平台，它整合了SIP信息服务和流媒体服务器，支持NAT穿透，支持海康、大华、宇视等品牌的IPC、NVR接入。支持国标级联，支持将不带国标功能的摄像机/直播流/直播推流转发到其他国标平台。 github地址：https://github.com/648540858/wvp-GB28181-pro 二次开发：http://demo1024.com/blog/gb28181-dev/ 虽然 WVP+ZLMediaKit+MediaServerUI 帮我实现了在线播放，但是还是有部分需求无法满足： MediaServerUI 无法对接 h265 码流的视频 部分视频设备是有 GPS 功能的，目前 wvp 不具备对接能力 未知 让 MediaServerUI 兼容 h265 这个是小问题，MediaServerUI 采用 @liveqing/liveplayer， 我们单独替换播放窗口的代码，改用 EasyPlayer.js 即可。 对 wvp-GB28181 二次开发 在对 wvp 二次开发前，必须要掌握两点： GB28181 2016 定义了些什么？ wvp-GB28181 的设计理念？项目结构？ wvp-GB28181 源码剖析 简介 WEB VIDEO PLATFORM 是一个基于 GB28181-2016 标准实现的网络视频平台， 负责实现核心信令与设备管理后台部分， 支持NAT穿透，支持海康、大华、宇视等品牌的IPC、NVR、DVR接入。 包结构设计 这个项目的基础包路径是 com.genersoft.iot.vmp，gb28181 的相关代码都是在这个域下开发的 genersoft 是作者本人的标识 iot 是物联网的意思，这体现了作者的架构观 vmp 是本期项目名称，我猜是 VIDEO MANAGE PLATFORM，WVP 后续还会有和 vmp 同级别的项目 对 vmp 包的理解 层级 设计理念 common 共性内容，如常量 conf 就是 spring Configuration，做变量映射、容器初始化、web 配置（比如我上面追加的跨域） gb28181 核心代码，实现 sip 服务器。下文有细节介绍 media.zlm 流媒体的实现，流媒体服务器被触发后（如无人观看断流）会回调这些接口，本项目适配的是 zlm storager 就是 dao 层，支持 jdbc 或 redis 两种切换 utils 辅助工具类 vmmanger 核心代码，实现 web 服务器。用户端需要的设备、通道、视频播放、历史记录等等功能 API 通常来说车载视频设备都会携带 GPS 模组， 在软件设计上，GPS 和视频都是走 gb28181 协议上载数据， 那么，我们基于 wvp 做 GPS 二次开发的话必须要调整 com.genersoft.iot.vmp.gb28181 的内容。 对 gb28181 包的理解 SipLayer 它实现了 SipListener 接口，代表负责监听SIP协议的应用客户端， 这个接口定义了接收处理从 SipProvider 提交的 SIP 事件消息， 是一个抽象的侦听线程。 这个类好比是 web 项目的启动类（DemoApplication.java）。 内部有两个核心方法，是对 SipListener 接口的实现，它是 sip 消息的入口和出口。 不难发现，无论是 processRequest 还是 processResponse ，内部具体实现都是 SIPProcessorFactory SIPProcessorFactory 它的设计思路是依照 gb28181 中定义消息类型 它是一个工厂类，用于生产 gb28181 中定义的接收、响应的消息类型 createRequestProcessor 处理请求：INVITE、REGISTER、SUBSCRIBE、ACK、BYE、CANCEL、MESSAGE、NOTIFY 分别对应如下 RequestProcessor createResponseProcessor 处理应答：INVITE、BYE、CANCEL 分别对应如下 ResponseProcessor 开发车载视频设备的 GPS 推送 在了解了请求和应答的代码结构后， 我们现在基于 gb28181 MESSAGE 通道编写 GPS 数据采集功能， 其实就是在这个位置追加 GPS 对应的 cmdType 识别即可。 在识别 gps cmdType 后，进入 processMessageGPS 方法， 通过 Element rootElement = getRootElement(evt); 获取消息体，然后执行字段提取。 通常来说，这个方法逻辑如下： 接收消息 解析消息 转发消息（通常来说， wvp 应该是做 GPS 转发平台，和业务系统解耦） return void （请求默认应答 200，在 MessageRequestProcessor.process 方法体最后几句代码已实现） 测试 下载 UDP 测试软件，传送门","categories":[{"name":"视频流","slug":"视频流","permalink":"http://yoursite.com/categories/%E8%A7%86%E9%A2%91%E6%B5%81/"}],"tags":[{"name":"视频网关","slug":"视频网关","permalink":"http://yoursite.com/tags/%E8%A7%86%E9%A2%91%E7%BD%91%E5%85%B3/"}]},{"title":"OSI模型","slug":"OSI模型","date":"2022-04-24T03:31:20.000Z","updated":"2022-11-22T01:27:53.416Z","comments":true,"path":"2022/04/24/OSI模型/","link":"","permalink":"http://yoursite.com/2022/04/24/OSI%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"OSI（Open System Interconnect），即开放式系统互联。 一般都叫OSI参考模型，是ISO（国际标准化组织）组织在1985年研究的网络互连模型。OSI模型偏重于理论，TCP/IP偏重于实战。 1、OSI七层模型 OSI划分为七层模型：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层。 1.1 物理层 ​ 物理层顾名思义就是最靠近物理传输设备的一层。物理媒介包括光纤，网线，等。改成的主要作用是实现相邻计算机间的比特流传输，尽可能屏蔽掉具体传输介质和物理设备的差异。尽量对上层也就是数据链路层屏蔽掉其不需要考虑的物理介质差异，对其提供统一的比特流传输调用方式。 物理层的主要功能：屏蔽物理媒介差异，为数据链路层提供统一的物理比特流传输能力。 数据单元：比特。 实例：光纤、网线、集线器、中继器、调制解调器等。 1.2 数据链路层 ​ 该层主要负责建立和管理不同计算机节点间的数据链路，并提供差错检测、封装成帧、透明传输的能力。数据链路层又分为两个层：媒体访问控制子层（MAC）和逻辑链路控制子层（LLC）。 媒体访问控制子层（MAC） ​ MAC子层的主要任务是解决共享型网络中多用户对信道竞争的问题，完成网络介质的访问控制。实现这个功能的是集线器。用集线器组网，检查计算机与计算机之间有没有冲突，避免冲突的协议叫CSMA/CD协议。 逻辑链路控制子层（LLC） ​ 主要任务是建立和维护网络连接和链路控制。 数据链路层的主要功能：将不可靠的物理信道变成无差错的、能可靠传输数据帧的数据链路，即：数据的差错检验、封装成帧、透明传输。 数据单元：帧 实例：网卡、MAC地址、以太网、交换机 1.3 网络层 网络层的主要功能：通过IP地址，实现网络寻址，即IP寻址，通过路由算法进行最优的网络路由。 数据单元：数据包 1.4 传输层 ​ 上述三层实现了互联网，也就是实现了两台机器间的互联互通。但是一台计算机上往往有好多应用程序，端口是用来区分不同应用程序的方式，每个应用程序都有各自的端口。很简单的一个道理，QQ用户能给微信用户发送即时消息吗？不能，为什么？这就是传输层的作用。传输层的作用是为上层协议提供端到端的可靠和透明的数据传输服务。包括处理差错控制和流量控制等。该层向上层应用屏蔽了底层通信细节。上层应用只需要按照传输层的规范，向传输层提交数据传输任务，其余的事情不需要上层应用关系。我们常见的TCP/IP协议中的TCP就作用在这一层。 传输层的主要功能：传输层的作用是为上层协议提供端到端的可靠和透明的数据传输服务，并提供差错控制和流量控制等功能。 数据单元：称作数据包（packets）。 实例：TCP、UDP 1.5 会话层 ​ 会话层就是负责建立、管理和终止表示层实体之间的通信会话。该层的通信由不同设备中的应用程序之间的服务请求和响应组成。会话层是什么？给你一个通俗易懂的答案：假如你了解session+cookie机制，相信你就明白了，这种session+cookie的机制就是会话层的实现。 1.6 表示层 ​ 如上所述，该层主要作用是数据格式的编码和转换。有点抽象？HTTP请求头/响应头 Content-Type：application/json; charset=utf-8 。这就是规定双方协商的数据格式： application/json; 和编码格式： charset=utf-8; 1.7 应用层 ​ OSI应用层是最靠近上层开发者的一层，这一层就是将通信模型定制化成一个协议，比如适合于超文本传输的协议HTTP，具备安全性传输的HTTPS，还有一些比如FTP，POP3，SMTP等。这一层可以这样理解，将下面几层根据场景具象，形成的一个提供给上层开发者应用的协议层。 2、TCP/IP TCP/IP协议是四层模型：应用层、传输层、网络层、网络接口层。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"osi","slug":"osi","permalink":"http://yoursite.com/tags/osi/"}]},{"title":"系统监控","slug":"系统监控","date":"2022-04-22T07:06:53.000Z","updated":"2022-11-22T01:27:54.215Z","comments":true,"path":"2022/04/22/系统监控/","link":"","permalink":"http://yoursite.com/2022/04/22/%E7%B3%BB%E7%BB%9F%E7%9B%91%E6%8E%A7/","excerpt":"","text":"https://songjiayang.gitbooks.io/prometheus/content/introduction/ https://www.kancloud.cn/willseecloud/prometheus/1904334 https://www.cnblogs.com/gered/p/13475933.html#autoid-1-0-0 https://songjiayang.gitbooks.io/prometheus/content/introduction/ 1、Prometheus 1.1 架构 1.1.1 prometheus 工作流程可以总结如下 Prometheus服务器周期性地或在设定的时间段内，可以通过以下 方式获取内容。 从已经配置好的job或者exporter中拉取metric。 接收从Pushgateway推送过来的metric。 从其他的Prometheus服务器中拉取metric。 Prometheus服务器获取到数据后存储在本地(也可以选择远端存 储)，通过一定规则对数据进行清理和整理，并且把结果存储到新的 时间序列中。 Prometheus服务器定时查询已经定义好的规则，若发现满足定义 的触发条件，便将alert信息推送至已配置好的Alertmanager。 Alertmanager收到alert信息后，根据配置文件对接收到的alert信息 进行处理(聚合、去重、降噪)，然后将它们转换为网页、电子邮件 和其他通知方式发出告警。 最后通过PromQL或其他API可视化地展示收集的数据，例如自 带Prometheus Web UI、Grafana可视化收集查询数据等。 Prometheus主要有四大组件Prometheus Server、Push gateway、Exporters和Alertmanager，分别如下： Prometheus Server：负责从Exporter拉取和存储监控数据，根据告警规则产生告警并发送给Alertmanager，并提供一套灵活的查询语言PromQL 121. Exporters&#x2F;Jobs：Prometheus的数据采集组件，负责收集目标对象（host, container…）的性能数据，并通过HTTP接口提供给Prometheus Server。支持数据库、硬件、消息中间件、存储系统、http服务器、jmx等。2. Short-lived jobs：瞬时任务的场景，无法通过pull方式拉取，需要使用push方式，与PushGateway搭配使用 PushGateway：应对部分push场景的组件可选组件，这部分监控数据先推送到Push Gateway上，然后再由Prometheus Server端拉取 。（使用场景：export存在时间短，server没pull就没了；server和agent网络不通，通过gateway做中介） Alertmanager：从Prometheus server端接收到alerts后，会基于PromQL的告警规则分析数据，如果满足PromQL定义的规则，则会产生一条告警，并发送告警信息到Alertmanager，Alertmanager则是根据配置处理告警信息并发送。常见的接收方式有：电子邮件，pagerduty，OpsGenie, webhook 等。 Service Discovery：Prometheus支持多种服务发现机制：文件、DNS、Consul、Kubernetes、OpenStack、EC2等等。基于服务发现的过程是通过第三方提供的接口，Prometheus查询到需要监控的Target列表，然后轮训这些Target获取监控数据。 1.1.2 prometheus metrics 四种数据类型 采集器存储的四种数据类型如下：（K/V 形式） Counter Counter用于累计值，例如记录请求次数、任务完成数、错误发生次数。一直增加，不会减少。重启进程后，会被重置。 例如：http_response_total{method=”GET”,endpoint=”/api/tracks”} 100，10秒后抓取http_response_total {method=”GET”,endpoint=”/api/tracks”} 100。 Gauge Gauge常规数值，例如 温度变化、内存使用变化。可变大，可变小。重启进程后，会被重置。 例如： memory_usage_bytes{host=”master-01″} 100 &lt; 抓取值、memory_usage_bytes{host=”master-01″} 30、 memory_usage_bytes{host=”master-01″} 50、memory_usage_bytes{host=”master-01″} 80 &lt; 抓取值。 Histogram Histogram（直方图）可以理解为柱状图的意思，常用于跟踪事件发生的规模。 例如：请求耗时、响应大小。它特别之处是可以对记录的内 容进行分组，提供count和sum全部值的功能。 例如：{小于10=5次，小于20=1次，小于30=2次}，count=7次，sum=7次的求和值。 Summary Summary和Histogram十分相似，常用于跟踪事件发生的规模，例如：请求耗时、响应大小。同样提供 count 和 sum 全部值的功能。 例如：count=7次，sum=7次的值求值。 它提供一个quantiles的功能，可以按%比划分跟踪的结果。例如：quantile取值0.95，表示取采样值里面的95%数据。 1.2 监控指标 123#HELP node_cpu_seconds_total Seconds the cpus spent in each mode.#TYPE node_cpu_seconds_total counternode_cpu_seconds_total&#123;cpu&#x3D;&quot;0&quot;,mode&#x3D;&quot;idle&quot;&#125; 1.40181324e+06 一条指标：指标名+&#123;标签键值对&#125;+值 #HELP开头的行用来说明下面指标 node_cpu_seconds_total的意义: 指标node_cpu_seconds_total用来表示CPU在每种模式下工作的秒数，而大括号里的称之为标签，以键值对的形式出现。 比如这里的cpu=&quot;0&quot;表示CPU的第0个核心，mode=&quot;idle&quot;表示工作模式为idle，也就是没被任何程序使用的空闲模式。 #TYPE开始的第二行表示指标的数值类型为counter类型。counter类型表示只增长的类型，也就是只会增加不会减少的值，且数值只能是正整数。大数值会启用科学计数法。 指标node_cpu_seconds_total&#123;cpu=&quot;0&quot;,mode=&quot;idle&quot;&#125; 1.40181324e+06表示节点的CPU0的空闲时间共计1.40181324e+06秒。数值从系统开机时算起，重启归零。 2、安装 2.1 go环境安装 12345678910111213#检查linux内核版本，下载对应gocat /proc/version# 例如我下载tar -xf go1.18.1.linux-amd64.tar -C /usr/localvim /etc/profileexport GO111MODULE=onexport GOROOT=/usr/local/goexport GOPATH=/home/gopathexport PATH=$PATH:$GOROOT/bin:$GOPATH/bin# 环境变量生效source /etc/profile# go版本go version 2.2 关闭防火墙 12345systemctl status firewalld.service systemctl stop firewalld.service systemctl disable firewalld.service sed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/selinux/configcat /etc/selinux/config 2.3 grafana+prometheus+node_exporter+alertmanager安装 下载 1234wget https://github.com/prometheus/prometheus/releases/download/v2.27.0-rc.0/prometheus-2.27.0-rc.0.linux-amd64.tar.gzwget https://github.com/prometheus/node_exporter/releases/download/v1.1.2/node_exporter-1.1.2.linux-amd64.tar.gzwget https://github.com/prometheus/alertmanager/releases/download/v0.22.0-rc.0/alertmanager-0.22.0-rc.0.linux-amd64.tar.gzwget https://dl.grafana.com/oss/release/grafana-7.5.5-1.x86_64.rpm grafana 默认端口：3000 默认日志：var/log/grafana/grafana.log 默认持久化文件：/var/lib/grafana/grafana.db web默认账户密码：admin/admin 访问地址：http://127.0.0.1:3000 12345678910111213141516171819202122232425262728# rpm安装wget https://dl.grafana.com/oss/release/grafana-8.0.5-1.x86_64.rpmyum install grafana-8.0.5-1.x86_64.rpmsystemctl start grafana-server&amp;&amp;systemctl enable grafana-server# 二进制包安装wget https://dl.grafana.com/oss/release/grafana-8.0.5.linux-amd64.tar.gztar -zxvf grafana-8.0.5.linux-amd64.tar.gz -C /optcd /opt &amp;&amp; mv grafana-8.0.5.linux-amd64 grafana#启动nohup /opt/grafana/bin/grafana-server &amp;cat &lt;&lt;EOF &gt;/usr/lib/systemd/system/grafana-server.service[Unit]Description=grafanaDocumentation=https://grafana.com/After=network.target[Service]Type=simpleExecStart=/opt/grafana/bin/grafana-server --homepath /opt/grafanaRestart=on-failure[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl start grafana-serversystemctl enable grafana-server prometheus 配置文件：/usr/local/prometheus/prometheus.yml 访问地址：http://localhost:9090 1234567891011121314151617181920212223242526272829303132tar xzf prometheus-2.26.0.linux-amd64.tar.gz -C /usr/local/cd /usr/local/mv prometheus-2.26.0.linux-amd64 prometheusmkdir -p /data/prometheus/prometheus/datavim /lib/systemd/system/prometheus.service[Unit]Description= PrometheusAfter=network.target[Service]Type=simpleUser=root #注意这里是设置prometheus的属主和属组，如果之前改为了prometheus或者其他用户记得修改，为了方便我直接使用的root#这里要注意路径！另外prometheus不是重复的，而是启动程序。--web.listen-address=:9876 自定义web访问端口ExecStart=/usr/local/prometheus/prometheus --config.file=/usr/local/prometheus/prometheus.yml --storage.tsdb.path=/data/prometheus/prometheus/data ExecReload=/bin/kill -HUP $MAINPIDRestart=on-failure[Install]WantedBy=multi-user.target# 检查配置文件（若有修改）/usr/local/prometheus/promtool check config /usr/local/prometheus/prometheus.ymlsystemctl daemon-reloadsystemctl enable prometheus.servicesystemctl start prometheus.servicesystemctl status prometheus.service# 查看日志journalctl -f journalctl -u prometheus.service node_exporter（被监控机器） 1234567891011121314151617181920212223242526272829303132333435363738tar -xzf node_exporter-1.1.2.linux-amd64.tar.gz -C /usr/local/cd /usr/local/mv node_exporter-1.1.2.linux-amd64 node_exportervim /lib/systemd/system/node_exporter.service[Unit]Description=node-exporter[Service]ExecStart=/usr/local/node_exporter/node_exporter[Install]WantedBy=multi-user.targetsystemctl daemon-reloadsystemctl start node_exportersystemctl enable node_exportersystemctl status node_exporter此时就可以访问到metric页面了：http://localhost:9100#为了能够让Prometheus Server能够从当前node exporter获取到监控数据，这里需要修改Prometheus配置文件。编辑prometheus.yml并在scrape_configs节点下添加以下内容:vim /usr/local/prometheus/prometheus.yml scrape_configs: - job_name: &#x27;prometheus&#x27; static_configs: - targets: [&#x27;localhost:9090&#x27;] # 采集node exporter监控数据 - job_name: &#x27;node&#x27; static_configs: - targets: [&#x27;localhost:9100&#x27;] systemctl restart prometheus.service # 访问prometheus：http://localhost:9090，输入up查询，可以i看到2条信息如下：up&#123;instance=&quot;localhost:9100&quot;, job=&quot;node&quot;&#125;up&#123;instance=&quot;localhost:9876&quot;, job=&quot;prometheus&quot;&#125; altermanager 123456789101112131415161718tar xzf alertmanager-0.22.0-rc.0.linux-amd64.tar.gz -C /usr/local/cd /usr/local/mv alertmanager-0.22.0-rc.0.linux-amd64 alertmanagervim /usr/lib/systemd/system/alertmanager.service[Unit]Description=https://prometheus.io[Service]ExecStart=/usr/local/alertmanager/alertmanager --config.file=/usr/local/alertmanager/alertmanager.yml --storage.path=/usr/local/alertmanager/data[Install]WantedBy=multi-user.targetsystemctl daemon-reloadsystemctl start alertmanager.servicesystemctl enable alertmanager.servicesystemctl status alertmanager.service 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647全局配置（global）：用于定义一些全局的公共参数，如全局的SMTP配置，Slack配置等内容；模板（templates）：用于定义告警通知时的模板，如HTML模板，邮件模板等；告警路由（route）：根据标签匹配，确定当前告警应该如何处理；接收人（receivers）：接收人是一个抽象的概念，它可以是一个邮箱也可以是微信，Slack或者Webhook等，接收人一般配合告警路由使用；抑制规则（inhibit_rules）：合理设置抑制规则可以减少垃圾告警的产生# 全局配置项global: resolve_timeout: 5m #处理超时时间，默认为5min smtp_smarthost: &#x27;smtp.sina.com:25&#x27; # 邮箱smtp服务器代理 smtp_from: &#x27;******@sina.com&#x27; # 发送邮箱名称 smtp_auth_username: &#x27;******@sina.com&#x27; # 邮箱名称 smtp_auth_password: &#x27;******&#x27; # 邮箱密码或授权码 wechat_api_url: &#x27;https://qyapi.weixin.qq.com/cgi-bin/&#x27; # 企业微信地址# 定义模板信心templates: - &#x27;template/*.tmpl&#x27;# 定义路由树信息route: group_by: [&#x27;alertname&#x27;] # 报警分组依据 group_wait: 10s # 最初即第一次等待多久时间发送一组警报的通知 group_interval: 10s # 在发送新警报前的等待时间 repeat_interval: 1m # 发送重复警报的周期 对于email配置中，此项不可以设置过低，否则将会由于邮件发送太多频繁，被smtp服务器拒绝 receiver: &#x27;email&#x27; # 发送警报的接收者的名称，以下receivers name的名称# 定义警报接收者信息receivers: - name: &#x27;email&#x27; # 警报 email_configs: # 邮箱配置 - to: &#x27;******@163.com&#x27; # 接收警报的email配置 html: &#x27;&#123;&#123; template &quot;test.html&quot; . &#125;&#125;&#x27; # 设定邮箱的内容模板 headers: &#123; Subject: &quot;[WARN] 报警邮件&quot;&#125; # 接收邮件的标题 webhook_configs: # webhook配置 - url: &#x27;http://127.0.0.1:5001&#x27; send_resolved: true wechat_configs: # 企业微信报警配置 - send_resolved: true to_party: &#x27;1&#x27; # 接收组的id agent_id: &#x27;1000002&#x27; # (企业微信--&gt;自定应用--&gt;AgentId) corp_id: &#x27;******&#x27; # 企业信息(我的企业--&gt;CorpId[在底部]) api_secret: &#x27;******&#x27; # 企业微信(企业微信--&gt;自定应用--&gt;Secret) message: &#x27;&#123;&#123; template &quot;test_wechat.html&quot; . &#125;&#125;&#x27; # 发送消息模板的设定# 一个inhibition规则是在与另一组匹配器匹配的警报存在的条件下，使匹配一组匹配器的警报失效的规则。两个警报必须具有一组相同的标签。 inhibit_rules: - source_match: severity: &#x27;critical&#x27; target_match: severity: &#x27;warning&#x27; equal: [&#x27;alertname&#x27;, &#x27;dev&#x27;, &#x27;instance&#x27;] 1）repeat_interval配置项，对于email来说，此项不可以设置过低，否则将会由于邮件发送太多频繁，被smtp服务器拒绝 2）企业微信注册地址：https://work.weixin.qq.com 上述配置的email、webhook和wechat三种报警方式。目前Alertmanager所有的报警方式有以下几个方面： 1234567email_confighipchat_configpagerduty_configpushover_configslack_configopsgenie_configvictorops_config nvidia_gpu_exporter_0.5.0_linux_x86_64.tar.gz 1234567891011121314151617tar -zxvf nvidia_gpu_exporter_0.5.0_linux_x86_64.tar.gz -C /usr/local/binsudo vim /etc/systemd/system/nvidia_gpu_exporter.service[Unit]Description=nvidia_gpu_exporter[Service]ExecStart=/usr/local/bin/nvidia_gpu_exporter[Install]WantedBy=multi-user.targetsudo systemctl daemon-reloadsudo systemctl start nvidia_gpu_exporter.servicesudo systemctl enable nvidia_gpu_exporter.servicesudo systemctl status nvidia_gpu_exporter.service 基于consul服务发现 https://www.mafeifan.com/DevOps/Prometheus/18.基于Consul的服务发现.html#四-prometheus配置","categories":[{"name":"运维","slug":"运维","permalink":"http://yoursite.com/categories/%E8%BF%90%E7%BB%B4/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"http://yoursite.com/tags/Prometheus/"}]},{"title":"mysql锁","slug":"mysql锁","date":"2022-04-20T08:09:18.000Z","updated":"2022-11-22T01:27:53.704Z","comments":true,"path":"2022/04/20/mysql锁/","link":"","permalink":"http://yoursite.com/2022/04/20/mysql%E9%94%81/","excerpt":"","text":"1、mysql 锁分类 2、粒度 2.1 表锁 MYISAM引擎默认支持的锁。InnoDB也支持。 读锁 加锁的session可以读（写报错），其他session只能读，写阻塞。 12lock table 表名字 read;unlock tables; 写锁 加锁的session可以读写，其他session读写都阻塞。 12lock table 表名字 write;unlock tables; 即，读锁会阻塞写，但是不会阻塞读。而写锁则会把读和写都阻塞。 2.2 行锁 InnoDB存储引擎支持，开销大，加锁慢，会出现死锁，锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 通过检查innodb_row_lock状态变量来分析系统上的行锁的争夺情况. 123456789mysql&gt; show status like&#x27;innodb_row_lock%&#x27;;+-------------------------------+-------+| Variable_name | Value |+-------------------------------+-------+| Innodb_row_lock_current_waits | 0 |----&gt;当前正在等待锁定的数量| Innodb_row_lock_time | 0 |----&gt;从系统启动到现在锁定总时间长度(等待总时长)| Innodb_row_lock_time_avg | 0 |----&gt;每次等待所花平均时间(平均等待时长)| Innodb_row_lock_time_max | 0 |----&gt;从系统启动到现在等待最长的一次所花时间| Innodb_row_lock_waits | 0 |----&gt;系统启动后到现在总共等待的次数(等待总次数) Mysql 支持的事务隔离级别 123456可重复读：repeatable-read、读未提交：read-uncommitted读已提交：read-committed、串行化：serializable# 查看使用的事务隔离级别show variables like &#x27;tx_isolation&#x27;;# 设置事务隔离级别set tx_isolation=&#x27;repeatable-read&#x27;; 开启事务 1234# 方式1begin&#x3D;start teansaction# 方式2SET autocommit &#x3D; 0; 模拟死锁 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166# session-1begin;1、select * from account where id=4 for update;3、select * from account where id=5 for update;# session-2begin;2、select * from account where id=5 for update;4、select * from account where id=4 for update; ERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transactionmysql&gt; # 查看死锁信息 show engine innodb status; | InnoDB | |=====================================2022-04-20 18:40:23 0x70000f163000 INNODB MONITOR OUTPUT=====================================Per second averages calculated from the last 31 seconds-----------------BACKGROUND THREAD-----------------srv_master_thread loops: 8 srv_active, 0 srv_shutdown, 45950 srv_idlesrv_master_thread log flush and writes: 45958----------SEMAPHORES----------OS WAIT ARRAY INFO: reservation count 4OS WAIT ARRAY INFO: signal count 4RW-shared spins 0, rounds 16, OS waits 2RW-excl spins 0, rounds 3, OS waits 0RW-sx spins 0, rounds 0, OS waits 0Spin rounds per wait: 16.00 RW-shared, 3.00 RW-excl, 0.00 RW-sx------------------------LATEST DETECTED DEADLOCK------------------------2022-04-20 18:35:31 0x70000f1a7000*** (1) TRANSACTION:TRANSACTION 65820, ACTIVE 38 sec starting index readmysql tables in use 1, locked 1LOCK WAIT 3 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 5, OS thread handle 123145555423232, query id 120 localhost root statisticsselect * from account where id=5 for update*** (1) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 1054 page no 3 n bits 80 index PRIMARY of table `test`.`account` trx id 65820 lock_mode X locks rec but not gap waitingRecord lock, heap no 3 PHYSICAL RECORD: n_fields 5; compact format; info bits 0 0: len 4; hex 80000005; asc ;; 1: len 6; hex 000000010108; asc ;; 2: len 7; hex a800000d400110; asc @ ;; 3: len 6; hex 68616e6d6569; asc hanmei;; 4: len 4; hex 80003e80; asc &gt; ;;*** (2) TRANSACTION:TRANSACTION 65821, ACTIVE 28 sec starting index readmysql tables in use 1, locked 13 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 6, OS thread handle 123145555701760, query id 121 localhost root statisticsselect * from account where id=4 for update*** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 1054 page no 3 n bits 80 index PRIMARY of table `test`.`account` trx id 65821 lock_mode X locks rec but not gapRecord lock, heap no 3 PHYSICAL RECORD: n_fields 5; compact format; info bits 0 0: len 4; hex 80000005; asc ;; 1: len 6; hex 000000010108; asc ;; 2: len 7; hex a800000d400110; asc @ ;; 3: len 6; hex 68616e6d6569; asc hanmei;; 4: len 4; hex 80003e80; asc &gt; ;;*** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 1054 page no 3 n bits 80 index PRIMARY of table `test`.`account` trx id 65821 lock_mode X locks rec but not gap waitingRecord lock, heap no 2 PHYSICAL RECORD: n_fields 5; compact format; info bits 0 0: len 4; hex 80000004; asc ;; 1: len 6; hex 000000010107; asc ;; 2: len 7; hex a7000001450110; asc E ;; 3: len 5; hex 6c696c6569; asc lilei;; 4: len 4; hex 800001c2; asc ;;*** WE ROLL BACK TRANSACTION (2)------------TRANSACTIONS------------Trx id counter 65822Purge done for trx&#x27;s n:o &lt; 65820 undo n:o &lt; 0 state: running but idleHistory list length 0LIST OF TRANSACTIONS FOR EACH SESSION:---TRANSACTION 421964162468536, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 65820, ACTIVE 330 sec3 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 5, OS thread handle 123145555423232, query id 124 localhost root startingshow engine innodb status--------FILE I/O--------I/O thread 0 state: waiting for i/o request (insert buffer thread)I/O thread 1 state: waiting for i/o request (log thread)I/O thread 2 state: waiting for i/o request (read thread)I/O thread 3 state: waiting for i/o request (read thread)I/O thread 4 state: waiting for i/o request (read thread)I/O thread 5 state: waiting for i/o request (read thread)I/O thread 6 state: waiting for i/o request (write thread)I/O thread 7 state: waiting for i/o request (write thread)I/O thread 8 state: waiting for i/o request (write thread)I/O thread 9 state: waiting for i/o request (write thread)Pending normal aio reads: [0, 0, 0, 0] , aio writes: [0, 0, 0, 0] , ibuf aio reads:, log i/o&#x27;s:, sync i/o&#x27;s:Pending flushes (fsync) log: 0; buffer pool: 0525 OS file reads, 170 OS file writes, 76 OS fsyncs0.00 reads/s, 0 avg bytes/read, 0.00 writes/s, 0.00 fsyncs/s-------------------------------------INSERT BUFFER AND ADAPTIVE HASH INDEX-------------------------------------Ibuf: size 1, free list len 0, seg size 2, 0 mergesmerged operations: insert 0, delete mark 0, delete 0discarded operations: insert 0, delete mark 0, delete 0Hash table size 34673, node heap has 2 buffer(s)Hash table size 34673, node heap has 0 buffer(s)Hash table size 34673, node heap has 0 buffer(s)Hash table size 34673, node heap has 0 buffer(s)Hash table size 34673, node heap has 0 buffer(s)Hash table size 34673, node heap has 0 buffer(s)Hash table size 34673, node heap has 0 buffer(s)Hash table size 34673, node heap has 2 buffer(s)0.00 hash searches/s, 0.00 non-hash searches/s---LOG---Log sequence number 1798777235Log flushed up to 1798777235Pages flushed up to 1798777235Last checkpoint at 17987772260 pending log flushes, 0 pending chkp writes51 log i/o&#x27;s done, 0.00 log i/o&#x27;s/second----------------------BUFFER POOL AND MEMORY----------------------Total large memory allocated 137428992Dictionary memory allocated 118358Buffer pool size 8191Free buffers 7675Database pages 512Old database pages 209Modified db pages 0Pending reads 0Pending writes: LRU 0, flush list 0, single page 0Pages made young 0, not young 00.00 youngs/s, 0.00 non-youngs/sPages read 471, created 41, written 1020.00 reads/s, 0.00 creates/s, 0.00 writes/sNo buffer pool page gets since the last printoutPages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/sLRU len: 512, unzip_LRU len: 0I/O sum[0]:cur[0], unzip sum[0]:cur[0]--------------ROW OPERATIONS--------------0 queries inside InnoDB, 0 queries in queue0 read views open inside InnoDBProcess ID=125, Main thread ID=123145550036992, state: sleepingNumber of rows inserted 6, updated 0, deleted 0, read 350.00 inserts/s, 0.00 updates/s, 0.00 deletes/s, 0.00 reads/s----------------------------END OF INNODB MONITOR OUTPUT============================ 2.3 共享锁 select … lock in share mode; 2.3.1 共享锁事务之间的读取 session1: 12start transaction/ begin;select * from test where id = 1 lock in share mode; session2: 12start transaction/ begin;select * from test where id = 1 lock in share mode; 此时 session1 和 session2 都可以正常获取结果，那么再加入 session3 排他锁读取尝试 session3: 12start transaction/ begin;select * from test where id = 1 for update; 在 session3 中则无法获取数据，直到超时或其它事物 commit 1Lock wait timeout exceeded; try restarting transaction 2.3.2 共享锁之间的更新 当 session1 执行了修改语句，session1: 12# 可以很多获取执行结果。update test set name = &#x27;kkkk&#x27; where id = 1; 当 session2 再次执行修改 id=1 的语句时，session2: 1update test set name = &#x27;zzz&#x27; where id = 1; 就会出现死锁或者锁超时，错误如下： 123Deadlock found when trying to get lock; try restarting transaction# 或者：Lock wait timeout exceeded; try restarting transaction 必须等到 session1 完成 commit 动作后，session2 才会正常执行，如果此时多个 session 并发执行，可想而知出现死锁的几率将会大增。session3 则更不可能! 2.3.3 结论 允许其它事务也增加共享锁读取 不允许其它事物增加排他锁 (for update) 当事务同时增加共享锁时候，事务的更新必须等待先执行的事务 commit 后才行，如果同时并发太大可能很容易造成死锁 2.4 排他锁 当一个事物加入排他锁后，不允许其他事务加共享锁或者排它锁读取，更加不允许其他事务修改加锁的行。 2.4.1 排他锁不同事务读 session1 select … for update; 12start transaction;select * from test where id = 1 for update; session2 12start transaction;select * from test where id = 1 for update; 当 session1 执行完成后，再次执行 session2，此时 session2 也会卡住，无法立刻获取查询的数据。直到出现超时 1Lock wait timeout exceeded; try restarting transaction session3 加入共享锁尝试 1select * from test where id = 1 lock in share mode; 结果也是如此，和 session2 一样，超时或等待 session1 commit 1Lock wait timeout exceeded; try restarting transaction 2.4.2 排他锁不同事务写 session1 1update test set name = 123 where id = 1; session2 1update test set name = &#x27;s2&#x27; where id = 1; 则会卡住直接超时或 session1 commit, 才会正常吐出结果 !","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"内存管理","slug":"内存管理","date":"2022-04-12T02:18:25.000Z","updated":"2022-11-22T01:27:53.860Z","comments":true,"path":"2022/04/12/内存管理/","link":"","permalink":"http://yoursite.com/2022/04/12/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","excerpt":"","text":"虚拟内存 如果你是电子相关专业的，肯定在大学里捣鼓过单片机。 单片机是没有操作系统的，所以每次写完代码，都需要借助工具把程序烧录进去，这样程序才能跑起来。 另外，单片机的 CPU 是直接操作内存的「物理地址」。 在这种情况下，要想在内存中同时运行两个程序是不可能的。如果第一个程序在 2000 的位置写入一个新的值，将会擦掉第二个程序存放在相同位置上的所有内容，所以同时运行两个程序是根本行不通的，这两个程序会立刻崩溃。 操作系统是如何解决这个问题呢？ 这里关键的问题是这两个程序都引用了绝对物理地址，而这正是我们最需要避免的。 我们可以把进程所使用的地址「隔离」开来，即让操作系统为每个进程分配独立的一套「虚拟地址」，人人都有，大家自己玩自己的地址就行，互不干涉。但是有个前提每个进程都不能访问物理地址，至于虚拟地址最终怎么落到物理内存里，对进程来说是透明的，操作系统已经把这些都安排的明明白白了。 进程的中间层 操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。 如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样不同的进程运行的时候，写入的是不同的物理地址，这样就不会冲突了。 于是，这里就引出了两种地址的概念： 我们程序所使用的内存地址叫做虚拟内存地址（Virtual Memory Address） 实际存在硬件里面的空间地址叫物理内存地址（Physical Memory Address）。 操作系统引入了虚拟内存，进程持有的虚拟地址会通过 CPU 芯片中的内存管理单元（MMU）的映射关系，来转换变成物理地址，然后再通过物理地址访问内存，如下图所示： 虚拟地址寻址 操作系统是如何管理虚拟地址与物理地址之间的关系？ 主要有两种方式，分别是内存分段和内存分页，分段是比较早提出的，我们先来看看内存分段。 内存分段（有连续内存空间） 程序是由若干个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。不同的段是有不同的属性的，所以就用分段（*Segmentation*）的形式把这些段分离出来。 分段机制下，虚拟地址和物理地址是如何映射的？ 分段机制下的虚拟地址由两部分组成，段选择子和段内偏移量。 内存分段-寻址的方式 段选择子就保存在段寄存器里面。段选择子里面最重要的是段号，用作段表的索引。段表里面保存的是这个段的基地址、段的界限和特权等级等。 虚拟地址中的段内偏移量应该位于 0 和段界限之间，如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。 在上面，知道了虚拟地址是通过段表与物理地址进行映射的，分段机制会把程序的虚拟地址分成 4 个段，每个段在段表中有一个项，在这一项找到段的基地址，再加上偏移量，于是就能找到物理内存中的地址，如下图： 内存分段-虚拟地址与物理地址 如果要访问段 3 中偏移量 500 的虚拟地址，我们可以计算出物理地址为，段 3 基地址 7000 + 偏移量 500 = 7500。 分段的办法很好，解决了程序本身不需要关心具体的物理内存地址的问题，但它也有一些不足之处： 第一个就是内存碎片的问题。 第二个就是内存交换的效率低的问题。 接下来，说说为什么会有这两个问题。 我们先来看看，分段为什么会产生内存碎片的问题？ 我们来看看这样一个例子。假设有 1G 的物理内存，用户执行了多个程序，其中： 游戏占用了 512MB 内存 浏览器占用了 128MB 内存 音乐占用了 256 MB 内存。 这个时候，如果我们关闭了浏览器，则空闲内存还有 1024 - 512 - 256 = 256MB。 如果这个 256MB 不是连续的，被分成了两段 128 MB 内存，这就会导致没有空间再打开一个 200MB 的程序。 内存碎片的问题 这里的内存碎片的问题共有两处地方： 外部内存碎片，也就是产生了多个不连续的小物理内存，导致新的程序无法被装载； 内部内存碎片，程序所有的内存都被装载到了物理内存，但是这个程序有部分的内存可能并不是很常使用，这也会导致内存的浪费； 针对上面两种内存碎片的问题，解决的方式会有所不同。 解决外部内存碎片的问题就是内存交换。 可以把音乐程序占用的那 256MB 内存写到硬盘上，然后再从硬盘上读回来到内存里。不过再读回的时候，我们不能装载回原来的位置，而是紧紧跟着那已经被占用了的 512MB 内存后面。这样就能空缺出连续的 256MB 空间，于是新的 200MB 程序就可以装载进来。 这个内存交换空间，在 Linux 系统里，也就是我们常看到的 Swap 空间，这块空间是从硬盘划分出来的，用于内存与硬盘的空间交换。 再来看看，分段为什么会导致内存交换效率低的问题？ 对于多进程的系统来说，用分段的方式，内存碎片是很容易产生的，产生了内存碎片，那不得不重新 Swap 内存区域，这个过程会产生性能瓶颈。 因为硬盘的访问速度要比内存慢太多了，每一次内存交换，我们都需要把一大段连续的内存数据写到硬盘上。 所以，如果内存交换的时候，交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。 为了解决内存分段的内存碎片和内存交换效率低的问题，就出现了内存分页。 内存分页 分段的好处就是能产生连续的内存空间，但是会出现内存碎片和内存交换的空间太大的问题。 要解决这些问题，那么就要想出能少出现一些内存碎片的办法。另外，当需要进行内存交换的时候，让需要交换写入或者从磁盘装载的数据更少一点，这样就可以解决问题了。这个办法，也就是内存分页（Paging）。 分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小。这样一个连续并且尺寸固定的内存空间，我们叫页（Page）。在 Linux 下，每一页的大小为 4KB。 虚拟地址与物理地址之间通过页表来映射，如下图： 内存映射 页表实际上存储在 CPU 的内存管理单元 （MMU） 中，于是 CPU 就可以直接通过 MMU，找出要实际要访问的物理内存地址。 而当进程访问的虚拟地址在页表中查不到时，系统会产生一个缺页异常，进入系统内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。 分页是怎么解决分段的内存碎片、内存交换效率低的问题？ 由于内存空间都是预先划分好的，也就不会像分段会产生间隙非常小的内存，这正是分段会产生内存碎片的原因。而采用了分页，那么释放的内存都是以页为单位释放的，也就不会产生无法给进程使用的小内存。 如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉，也就是暂时写在硬盘上，称为换出（Swap Out）。一旦需要的时候，再加载进来，称为换入（Swap In）。所以，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，内存交换的效率就相对比较高。 换入换出 更进一步地，分页的方式使得我们在加载程序的时候，不再需要一次性都把程序加载到物理内存中。我们完全可以在进行虚拟内存和物理内存的页之间的映射之后，并不真的把页加载到物理内存里，而是只有在程序运行中，需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。 分页机制下，虚拟地址和物理地址是如何映射的？ 在分页机制下，虚拟地址分为两部分，页号和页内偏移。页号作为页表的索引，页表包含物理页每页所在物理内存的基地址，这个基地址与页内偏移的组合就形成了物理内存地址，见下图。 内存分页寻址 总结一下，对于一个内存地址转换，其实就是这样三个步骤： 把虚拟内存地址，切分成页号和偏移量； 根据页号，从页表里面，查询对应的物理页号； 直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。 下面举个例子，虚拟内存中的页通过页表映射为了物理内存中的页，如下图： 虚拟页与物理页的映射 这看起来似乎没什么毛病，但是放到实际中操作系统，这种简单的分页是肯定是会有问题的。 简单的分页有什么缺陷吗？ 有空间上的缺陷。 因为操作系统是可以同时运行非常多的进程的，那这不就意味着页表会非常的庞大。 在 32 位的环境下，虚拟地址空间共有 4GB，假设一个页的大小是 4KB（2^12），那么就需要大约 100 万 （2^20） 个页，每个「页表项」需要 4 个字节大小来存储，那么整个 4GB 空间的映射就需要有 4MB 的内存来存储页表。 这 4MB 大小的页表，看起来也不是很大。但是要知道每个进程都是有自己的虚拟地址空间的，也就说都有自己的页表。 那么，100 个进程的话，就需要 400MB 的内存来存储页表，这是非常大的内存了，更别说 64 位的环境了。 多级页表 要解决上面的问题，就需要采用的是一种叫作多级页表（Multi-Level Page Table）的解决方案。 在前面我们知道了，对于单页表的实现方式，在 32 位和页大小 4KB 的环境下，一个进程的页表需要装下 100 多万个「页表项」，并且每个页表项是占用 4 字节大小的，于是相当于每个页表需占用 4MB 大小的空间。 我们把这个 100 多万个「页表项」的单级页表再分页，将页表（一级页表）分为 1024 个页表（二级页表），每个表（二级页表）中包含 1024 个「页表项」，形成二级分页。如下图所示： 二级分页 你可能会问，分了二级表，映射 4GB 地址空间就需要 4KB（一级页表）+ 4MB（二级页表）的内存，这样占用空间不是更大了吗？ 当然如果 4GB 的虚拟地址全部都映射到了物理内存上的话，二级分页占用空间确实是更大了，但是，我们往往不会为一个进程分配那么多内存。 其实我们应该换个角度来看问题，还记得计算机组成原理里面无处不在的局部性原理么？ 每个进程都有 4GB 的虚拟地址空间，而显然对于大多数程序来说，其使用到的空间远未达到 4GB，因为会存在部分对应的页表项都是空的，根本没有分配，对于已分配的页表项，如果存在最近一定时间未访问的页表，在物理内存紧张的情况下，操作系统会将页面换出到硬盘，也就是说不会占用物理内存。 如果使用了二级分页，一级页表就可以覆盖整个 4GB 虚拟地址空间，但如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表。做个简单的计算，假设只有 20% 的一级页表项被用到了，那么页表占用的内存空间就只有 4KB（一级页表） + 20% * 4MB（二级页表）= 0.804MB，这对比单级页表的 4MB 是不是一个巨大的节约？ 那么为什么不分级的页表就做不到这样节约内存呢？我们从页表的性质来看，保存在内存中的页表承担的职责是将虚拟地址翻译成物理地址。假如虚拟地址在页表中找不到对应的页表项，计算机系统就不能工作了。所以页表一定要覆盖全部虚拟地址空间，不分级的页表就需要有 100 多万个页表项来映射，而二级分页则只需要 1024 个页表项（此时一级页表覆盖到了全部虚拟地址空间，二级页表在需要时创建）。 我们把二级分页再推广到多级页表，就会发现页表占用的内存空间更少了，这一切都要归功于对局部性原理的充分应用。 对于 64 位的系统，两级分页肯定不够了，就变成了四级目录，分别是： 全局页目录项 PGD（Page Global Directory）； 上层页目录项 PUD（Page Upper Directory）； 中间页目录项 PMD（Page Middle Directory）； 页表项 PTE（Page Table Entry）； 四级目录 TLB 多级页表虽然解决了空间上的问题，但是虚拟地址到物理地址的转换就多了几道转换的工序，这显然就降低了这俩地址转换的速度，也就是带来了时间上的开销。 程序是有局部性的，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域。 程序的局部性 我们就可以利用这一特性，把最常访问的几个页表项存储到访问速度更快的硬件，于是计算机科学家们，就在 CPU 芯片中，加入了一个专门存放程序最常访问的页表项的 Cache，这个 Cache 就是 TLB（Translation Lookaside Buffer） ，通常称为页表缓存、转址旁路缓存、快表等。 地址转换 在 CPU 芯片里面，封装了内存管理单元（Memory Management Unit）芯片，它用来完成地址转换和 TLB 的访问与交互。 有了 TLB 后，那么 CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的页表。 TLB 的命中率其实是很高的，因为程序最常访问的页就那么几个。 段页式内存管理 内存分段和内存分页并不是对立的，它们是可以组合起来在同一个系统中使用的，那么组合起来后，通常称为段页式内存管理。 段页式地址空间 段页式内存管理实现的方式： 先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制； 接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页； 这样，地址结构就由段号、段内页号和页内位移三部分组成。 用于段页式地址变换的数据结构是每一个程序一张段表，每个段又建立一张页表，段表中的地址是页表的起始地址，而页表中的地址则为某页的物理页号，如图所示： 段页式管理中的段表、页表与内存的关系 段页式地址变换中要得到物理地址须经过三次内存访问： 第一次访问段表，得到页表起始地址； 第二次访问页表，得到物理页号； 第三次将物理页号与页内位移组合，得到物理地址。 可用软、硬件相结合的方法实现段页式地址变换，这样虽然增加了硬件成本和系统开销，但提高了内存的利用率。 Linux 内存管理 那么，Linux 操作系统采用了哪种方式来管理内存呢？ 在回答这个问题前，我们得先看看 Intel 处理器的发展历史。 早期 Intel 的处理器从 80286 开始使用的是段式内存管理。但是很快发现，光有段式内存管理而没有页式内存管理是不够的，这会使它的 X86 系列会失去市场的竞争力。因此，在不久以后的 80386 中就实现了对页式内存管理。也就是说，80386 除了完成并完善从 80286 开始的段式内存管理的同时还实现了页式内存管理。 但是这个 80386 的页式内存管理设计时，没有绕开段式内存管理，而是建立在段式内存管理的基础上，这就意味着，页式内存管理的作用是在由段式内存管理所映射而成的地址上再加上一层地址映射。 由于此时由段式内存管理映射而成的地址不再是“物理地址”了，Intel 就称之为“线性地址”（也称虚拟地址）。于是，段式内存管理先将逻辑地址映射成线性地址，然后再由页式内存管理将线性地址映射成物理地址。 Intel X86 逻辑地址解析过程 这里说明下逻辑地址和线性地址： 程序所使用的地址，通常是没被段式内存管理映射的地址，称为逻辑地址； 通过段式内存管理映射的地址，称为线性地址，也叫虚拟地址； 逻辑地址是「段式内存管理」转换前的地址，线性地址则是「页式内存管理」转换前的地址。 了解完 Intel 处理器的发展历史后，我们再来说说 Linux 采用了什么方式管理内存？ Linux 内存主要采用的是页式内存管理，但同时也不可避免地涉及了段机制。 这主要是上面 Intel 处理器发展历史导致的，因为 Intel X86 CPU 一律对程序中使用的地址先进行段式映射，然后才能进行页式映射。既然 CPU 的硬件结构是这样，Linux 内核也只好服从 Intel 的选择。 但是事实上，Linux 内核所采取的办法是使段式映射的过程实际上不起什么作用。也就是说，“上有政策，下有对策”，若惹不起就躲着走。 Linux 系统中的每个段都是从 0 地址开始的整个 4GB 虚拟空间（32 位环境下），也就是所有的段的起始地址都是一样的。这意味着，Linux 系统中的代码，包括操作系统本身的代码和应用程序代码，所面对的地址空间都是线性地址空间（虚拟地址），这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被用于访问控制和内存保护。 我们再来瞧一瞧，Linux 的虚拟地址空间是如何分布的？ 在 Linux 操作系统中，虚拟地址空间的内部又被分为内核空间和用户空间两部分，不同位数的系统，地址空间的范围也不同。比如最常见的 32 位和 64 位系统，如下所示： 用户空间与内存空间 通过这里可以看出： 32 位系统的内核空间占用 1G，位于最高处，剩下的 3G 是用户空间； 64 位系统的内核空间和用户空间都是 128T，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。 再来说说，内核空间与用户空间的区别： 进程在用户态时，只能访问用户空间内存； 只有进入内核态后，才可以访问内核空间的内存； 虽然每个进程都各自有独立的虚拟内存，但是每个虚拟内存中的内核地址，其实关联的都是相同的物理内存。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。 每个进程的内核空间都是一致的 接下来，进一步了解虚拟空间的划分情况，用户空间和内核空间划分的方式是不同的，内核空间的分布情况就不多说了。 我们看看用户空间分布的情况，以 32 位系统为例，我画了一张图来表示它们的关系： 虚拟内存空间划分 通过这张图你可以看到，用户空间内存，从低到高分别是 7 种不同的内存段： 程序文件段，包括二进制可执行代码； 已初始化数据段，包括静态常量； 未初始化数据段，包括未初始化的静态变量； 堆段，包括动态分配的内存，从低地址开始向上增长； 文件映射段，包括动态库、共享内存等，从低地址开始向上增长（跟硬件和内核版本有关）； 栈段，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 8 MB。当然系统也提供了参数，以便我们自定义大小； 在这 7 个内存段中，堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 malloc() 或者 mmap() ，就可以分别在堆和文件映射段动态分配内存。 总总结结 为了在多进程环境下，使得进程之间的内存地址不受影响，相互隔离，于是操作系统就为每个进程独立分配一套虚拟地址空间，每个程序只关心自己的虚拟地址就可以，实际上大家的虚拟地址都是一样的，但分布到物理地址内存是不一样的。作为程序，也不用关心物理地址的事情。 每个进程都有自己的虚拟空间，而物理内存只有一个，所以当启用了大量的进程，物理内存必然会很紧张，于是操作系统会通过内存交换技术，把不常使用的内存暂时存放到硬盘（换出），在需要的时候再装载回物理内存（换入）。 那既然有了虚拟地址空间，那必然要把虚拟地址「映射」到物理地址，这个事情通常由操作系统来维护。 那么对于虚拟地址与物理地址的映射关系，可以有分段和分页的方式，同时两者结合都是可以的。 内存分段是根据程序的逻辑角度，分成了栈段、堆段、数据段、代码段等，这样可以分离出不同属性的段，同时是一块连续的空间。但是每个段的大小都不是统一的，这就会导致内存碎片和内存交换效率低的问题。 于是，就出现了内存分页，把虚拟空间和物理空间分成大小固定的页，如在 Linux 系统中，每一页的大小为 4KB。由于分了页后，就不会产生细小的内存碎片。同时在内存交换的时候，写入硬盘也就一个页或几个页，这就大大提高了内存交换的效率。 再来，为了解决简单分页产生的页表过大的问题，就有了多级页表，它解决了空间上的问题，但这就会导致 CPU 在寻址的过程中，需要有很多层表参与，加大了时间上的开销。于是根据程序的局部性原理，在 CPU 芯片中加入了 TLB，负责缓存最近常被访问的页表项，大大提高了地址的转换速度。 Linux 系统主要采用了分页管理，但是由于 Intel 处理器的发展史，Linux 系统无法避免分段管理。于是 Linux 就把所有段的基地址设为 0，也就意味着所有程序的地址空间都是线性地址空间（虚拟地址），相当于屏蔽了 CPU 逻辑地址的概念，所以段只被用于访问控制和内存保护。 另外，Linxu 系统中虚拟空间分布可分为用户态和内核态两部分，其中用户态的分布：代码段、全局变量、BSS、函数栈、堆内存、映射区。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://yoursite.com/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://yoursite.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"直接内存","slug":"直接内存","date":"2022-04-12T01:37:39.000Z","updated":"2022-11-22T01:27:54.200Z","comments":true,"path":"2022/04/12/直接内存/","link":"","permalink":"http://yoursite.com/2022/04/12/%E7%9B%B4%E6%8E%A5%E5%86%85%E5%AD%98/","excerpt":"","text":"对比 堆内存 直接内存 申请 快 慢 读写 慢 快 1234567891011ByteBuffer buffer = ByteBuffer.allocateDirect(500);//分配500个字节的DirectBufferfor (int i = 0; i &lt; 100000; i ++) &#123; for (int j = 0; j &lt; 99; j ++) &#123; buffer.putInt(j); //向DirectBuffer写入数据 &#125; buffer.flip(); for (int j = 0; j &lt; 99; j ++) &#123; buffer.get(); //从DirectBuffer中读取数据 &#125; buffer.clear();&#125; DirectByteBuffer 直接内存地址：Java NIO中的DirectByteBuffer自身是一个Java对象，在Java堆中，对象中保存一个long类型的字段address，记录着malloc()申请到的直接内存地址。 直接内存回收：DirectByteBuffer中有一个内部静态类Deallocator，实现了Runnable接口，并维护一个对DirectByteBuffer对象的虚引用(PhantomReference)。当DirectByteBuffer对象被GC回收时，Deallocator会通过虚引用得到通知，创建一个线程释放该DirectByteBuffer对象malloc申请的直接内存空间。 注意事项 创建和销毁比普通Buffer慢。 虽然DirectByteBuffer的传输速度很快，但是创建和销毁比普通Buffer慢。因此DirectByteBuffer不适合只是短时使用需要频繁创建和销毁的场合。 使用直接内存要设置-XX:MaxDirectMemorySize指定最大大小。 直接内存不受GC管理，而基于DirectByteBuffer对象的自动回收过程并不稳定，如DirectByteBuffer对象被MinorGC经过MinorGC进入老年代，但是由于堆内存充足，迟迟没有触发Full GC，DirectByteBuffer将不会被回收，其申请的直接内存也就不会被释放，最终造成直接内存的OutOfMemoryError。","categories":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/categories/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}]},{"title":"零拷贝","slug":"零拷贝","date":"2022-04-11T08:32:17.000Z","updated":"2022-11-22T01:27:54.230Z","comments":true,"path":"2022/04/11/零拷贝/","link":"","permalink":"http://yoursite.com/2022/04/11/%E9%9B%B6%E6%8B%B7%E8%B4%9D/","excerpt":"","text":"1、前置知识 1.1 虚拟地址（虚拟存储器） 内存有限，用户程序很多，不能同时给连续的物理存储，因此在每一个进程开始创建的时候，都会分配一个虚拟存储器（就是一段虚拟地址）然后通过虚拟地址和物理地址的映射表来获取真实数据。 对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方） 对64位操作系统而言，它的寻址空间（虚拟存储空间）为128T，实质没有（主板限制）（2的64次方） 1.2 cpu指令集 CPU指令其实也是分权限的，主要分为Ring0~Ring3，用户态拥有最低权限Ring3，而内核具有最高权限Ring0。 1.3 内核空间 CPU 可以执行任何指令。运行的代码也不受任何的限制，可以自由地访问任何有效地址，也可以直接进行端口的访问。 对32位操作系统，一般1GB空间大小。 1.4 用户空间 被执行的代码要受到 CPU 的诸多检查，它们只能访问映射其地址空间的页表项中规定的在用户态下可访问页面的虚拟地址。 对32位操作系统，最大为3GB空间大小。 1.5 DMA（Direct Memory Access） 外部设备不通过CPU而直接与系统内存交换数据的接口技术。 技术：内存和外设直接存取数据这种内存访问的计算机技术 硬件模块：DMA控制逻辑器 2、零拷贝 磁盘可以说是计算机系统最慢的硬件之一，读写速度相差内存 10 倍以上，所以针对优化磁盘的技术非常的多，比如零拷贝、直接 I/O、异步 I/O 等等，这些优化的目的就是为了提高系统的吞吐量，另外操作系统内核中的磁盘高速缓存区，可以有效的减少磁盘的访问次数。 这次，我们就以「文件传输」作为切入点，来分析 I/O 工作方式，以及如何优化传输文件的性能。 正文 为什么要有 DMA 技术? 在没有 DMA 技术前，I/O 的过程是这样的： CPU 发出对应的指令给磁盘控制器，然后返回； 磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到磁盘控制器的内部缓冲区中，然后产生一个中断； CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 CPU 是无法执行其他任务的。 为了方便你理解，我画了一副图： 可以看到，整个数据的传输过程，都要需要 CPU 亲自参与搬运数据的过程，而且这个过程，CPU 是不能做其他事情的。 简单的搬运几个字符数据那没问题，但是如果我们用千兆网卡或者硬盘传输大量数据的时候，都用 CPU 来搬运的话，肯定忙不过来。 计算机科学家们发现了事情的严重性后，于是就发明了 DMA 技术，也就是直接内存访问（*Direct Memory Access*） 技术。 什么是 DMA 技术？简单理解就是，在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务。 那使用 DMA 控制器进行数据传输的过程究竟是什么样的呢？下面我们来具体看看。 具体过程： 用户进程调用 read 方法，向操作系统发出 I/O 请求，请求读取数据到自己的内存缓冲区中，进程进入阻塞状态； 操作系统收到请求后，进一步将 I/O 请求发送 DMA，然后让 CPU 执行其他任务； DMA 进一步将 I/O 请求发送给磁盘； 磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知自己缓冲区已满； DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，此时不占用 CPU，CPU 可以执行其他任务； 当 DMA 读取了足够多的数据，就会发送中断信号给 CPU； CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷贝到用户空间，系统调用返回； 可以看到， 整个数据传输的过程，CPU 不再参与数据搬运的工作，而是全程由 DMA 完成，但是 CPU 在这个过程中也是必不可少的，因为传输什么数据，从哪里传输到哪里，都需要 CPU 来告诉 DMA 控制器。 早期 DMA 只存在在主板上，如今由于 I/O 设备越来越多，数据传输的需求也不尽相同，所以每个 I/O 设备里面都有自己的 DMA 控制器。 传统的文件传输有多糟糕？ 如果服务端要提供文件传输的功能，我们能想到的最简单的方式是：将磁盘上的文件读取出来，然后通过网络协议发送给客户端。 传统 I/O 的工作方式是，数据读取和写入是从用户空间到内核空间来回复制，而内核空间的数据是通过操作系统层面的 I/O 接口从磁盘读取或写入。 代码通常如下，一般会需要两个系统调用： 12read(file, tmp_buf, len);write(socket, tmp_buf, len); 代码很简单，虽然就两行代码，但是这里面发生了不少的事情。 首先，期间共发生了 4 次用户态与内核态的上下文切换，因为发生了两次系统调用，一次是 read() ，一次是 write()，每次系统调用都得先从用户态切换到内核态，等内核完成任务后，再从内核态切换回用户态。 上下文切换到成本并不小，一次切换需要耗时几十纳秒到几微秒，虽然时间看上去很短，但是在高并发的场景下，这类时间容易被累积和放大，从而影响系统的性能。 其次，还发生了 4 次数据拷贝，其中两次是 DMA 的拷贝，另外两次则是通过 CPU 拷贝的，下面说一下这个过程： 第一次拷贝，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。 第二次拷贝，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。 第三次拷贝，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。 第四次拷贝，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的。 我们回过头看这个文件传输的过程，我们只是搬运一份数据，结果却搬运了 4 次，过多的数据拷贝无疑会消耗 CPU 资源，大大降低了系统性能。 这种简单又传统的文件传输方式，存在冗余的上文切换和数据拷贝，在高并发系统里是非常糟糕的，多了很多不必要的开销，会严重影响系统性能。 所以，要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数。 如何优化文件传输的性能？ 先来看看，如何减少「用户态与内核态的上下文切换」的次数呢？ 读取磁盘数据的时候，之所以要发生上下文切换，这是因为用户空间没有权限操作磁盘或网卡，内核的权限最高，这些操作设备的过程都需要交由操作系统内核来完成，所以一般要通过内核去完成某些任务的时候，就需要使用操作系统提供的系统调用函数。 而一次系统调用必然会发生 2 次上下文切换：首先从用户态切换到内核态，当内核执行完任务后，再切换回用户态交由进程代码执行。 所以，要想减少上下文切换到次数，就要减少系统调用的次数。 再来看看，如何减少「数据拷贝」的次数？ 在前面我们知道了，传统的文件传输方式会历经 4 次数据拷贝，而且这里面，「从内核的读缓冲区拷贝到用户的缓冲区里，再从用户的缓冲区里拷贝到 socket 的缓冲区里」，这个过程是没有必要的。 因为文件传输的应用场景中，在用户空间我们并不会对数据「再加工」，所以数据实际上可以不用搬运到用户空间，因此用户的缓冲区是没有必要存在的。 如何实现零拷贝？ 零拷贝技术实现的方式通常有 2 种： mmap + write sendfile 下面就谈一谈，它们是如何减少「上下文切换」和「数据拷贝」的次数。 mmap + write 在前面我们知道，read() 系统调用的过程中会把内核缓冲区的数据拷贝到用户的缓冲区里，于是为了减少这一步开销，我们可以用 mmap() 替换 read() 系统调用函数。 12buf = mmap(file, len);write(sockfd, buf, len); mmap() 系统调用函数会直接把内核缓冲区里的数据「映射」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。 具体过程如下： 应用进程调用了 mmap() 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区； 应用进程再调用 write()，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据； 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。 我们可以得知，通过使用 mmap() 来代替 read()， 可以减少一次数据拷贝的过程。 但这还不是最理想的零拷贝，因为仍然需要通过 CPU 把内核缓冲区的数据拷贝到 socket 缓冲区里，而且仍然需要 4 次上下文切换，因为系统调用还是 2 次。 sendfile 在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 sendfile()，函数形式如下： 12#include &lt;sys/socket.h&gt;ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); 它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。 首先，它可以替代前面的 read() 和 write() 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。 其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。如下图： 但是这还不是真正的零拷贝技术，如果网卡支持 SG-DMA（The Scatter-Gather Direct Memory Access）技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。 你可以在你的 Linux 系统通过下面这个命令，查看网卡是否支持 scatter-gather 特性： 12$ ethtool -k eth0 | grep scatter-gatherscatter-gather: on 于是，从 Linux 内核 2.4 版本开始起，对于支持网卡支持 SG-DMA 技术的情况下， sendfile() 系统调用的过程发生了点变化，具体过程如下： 第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里； 第二步，缓冲区描述符和数据长度传到 socket 缓冲区，这样网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就减少了一次数据拷贝； 所以，这个过程之中，只进行了 2 次数据拷贝，如下图： 这就是所谓的零拷贝（*Zero-copy*）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。。 零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运。 所以，总体来看，零拷贝技术可以把文件传输的性能提高至少一倍以上。 使用零拷贝技术的项目 事实上，Kafka 这个开源项目，就利用了「零拷贝」技术，从而大幅提升了 I/O 的吞吐率，这也是 Kafka 在处理海量数据为什么这么快的原因之一。 如果你追溯 Kafka 文件传输的代码，你会发现，最终它调用了 Java NIO 库里的 transferTo 方法： 1234@Overridepublic long transferFrom(FileChannel fileChannel, long position, long count) throws IOException &#123; return fileChannel.transferTo(position, count, socketChannel);&#125; 如果 Linux 系统支持 sendfile() 系统调用，那么 transferTo() 实际上最后就会使用到 sendfile() 系统调用函数。 曾经有大佬专门写过程序测试过，在同样的硬件条件下，传统文件传输和零拷拷贝文件传输的性能差异，你可以看到下面这张测试数据图，使用了零拷贝能够缩短 65% 的时间，大幅度提升了机器传输数据的吞吐量。 数据来源于：https://developer.ibm.com/articles/j-zerocopy/ 另外，Nginx 也支持零拷贝技术，一般默认是开启零拷贝技术，这样有利于提高文件传输的效率，是否开启零拷贝技术的配置如下： 12345http &#123;... sendfile on...&#125; sendfile 配置的具体意思: 设置为 on 表示，使用零拷贝技术来传输文件：sendfile ，这样只需要 2 次上下文切换，和 2 次数据拷贝。 设置为 off 表示，使用传统的文件传输技术：read + write，这时就需要 4 次上下文切换，和 4 次数据拷贝。 当然，要使用 sendfile，Linux 内核版本必须要 2.1 以上的版本。 PageCache 有什么作用？ 回顾前面说道文件传输过程，其中第一步都是先需要先把磁盘文件数据拷贝「内核缓冲区」里，这个「内核缓冲区」实际上是磁盘高速缓存（*PageCache*）。 由于零拷贝使用了 PageCache 技术，可以使得零拷贝进一步提升了性能，我们接下来看看 PageCache 是如何做到这一点的。 读写磁盘相比读写内存的速度慢太多了，所以我们应该想办法把「读写磁盘」替换成「读写内存」。于是，我们会通过 DMA 把磁盘里的数据搬运到内存里，这样就可以用读内存替换读磁盘。 但是，内存空间远比磁盘要小，内存注定只能拷贝磁盘里的一小部分数据。 那问题来了，选择哪些磁盘数据拷贝到内存呢？ 我们都知道程序运行的时候，具有「局部性」，所以通常，刚被访问的数据在短时间内再次被访问的概率很高，于是我们可以用 PageCache 来缓存最近被访问的数据，当空间不足时淘汰最久未被访问的缓存。 所以，读磁盘数据的时候，优先在 PageCache 找，如果数据存在则可以直接返回；如果没有，则从磁盘中读取，然后缓存 PageCache 中。 还有一点，读取磁盘数据的时候，需要找到数据所在的位置，但是对于机械磁盘来说，就是通过磁头旋转到数据所在的扇区，再开始「顺序」读取数据，但是旋转磁头这个物理动作是非常耗时的，为了降低它的影响，PageCache 使用了「预读功能」。 比如，假设 read 方法每次只会读 32 KB 的字节，虽然 read 刚开始只会读 0 ～ 32 KB 的字节，但内核会把其后面的 32～64 KB 也读取到 PageCache，这样后面读取 32～64 KB 的成本就很低，如果在 32～64 KB 淘汰出 PageCache 前，进程读取到它了，收益就非常大。 所以，PageCache 的优点主要是两个： 缓存最近被访问的数据； 预读功能； 这两个做法，将大大提高读写磁盘的性能。 但是，在传输大文件（GB 级别的文件）的时候，PageCache 会不起作用，那就白白浪费 DMA 多做的一次数据拷贝，造成性能的降低，即使使用了 PageCache 的零拷贝也会损失性能 这是因为如果你有很多 GB 级别文件需要传输，每当用户访问这些大文件的时候，内核就会把它们载入 PageCache 中，于是 PageCache 空间很快被这些大文件占满。 另外，由于文件太大，可能某些部分的文件数据被再次访问的概率比较低，这样就会带来 2 个问题： PageCache 由于长时间被大文件占据，其他「热点」的小文件可能就无法充分使用到 PageCache，于是这样磁盘读写的性能就会下降了； PageCache 中的大文件数据，由于没有享受到缓存带来的好处，但却耗费 DMA 多拷贝到 PageCache 一次； 所以，针对大文件的传输，不应该使用 PageCache，也就是说不应该使用零拷贝技术，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，这样在高并发的环境下，会带来严重的性能问题。 大文件传输用什么方式实现？ 那针对大文件的传输，我们应该使用什么方式呢？ 我们先来看看最初的例子，当调用 read 方法读取文件时，进程实际上会阻塞在 read 方法调用，因为要等待磁盘数据的返回，如下图： 具体过程： 当调用 read 方法时，会阻塞着，此时内核会向磁盘发起 I/O 请求，磁盘收到请求后，便会寻址，当磁盘数据准备好后，就会向内核发起 I/O 中断，告知内核磁盘数据已经准备好； 内核收到 I/O 中断后，就将数据从磁盘控制器缓冲区拷贝到 PageCache 里； 最后，内核再把 PageCache 中的数据拷贝到用户缓冲区，于是 read 调用就正常返回了。 对于阻塞的问题，可以用异步 I/O 来解决，它工作方式如下图： 它把读操作分为两部分： 前半部分，内核向磁盘发起读请求，但是可以不等待数据就位就可以返回，于是进程此时可以处理其他任务； 后半部分，当内核将磁盘中的数据拷贝到进程缓冲区后，进程将接收到内核的通知，再去处理数据； 而且，我们可以发现，异步 I/O 并没有涉及到 PageCache，所以使用异步 I/O 就意味着要绕开 PageCache。 绕开 PageCache 的 I/O 叫直接 I/O，使用 PageCache 的 I/O 则叫缓存 I/O。通常，对于磁盘，异步 I/O 只支持直接 I/O。 前面也提到，大文件的传输不应该使用 PageCache，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache。 于是，在高并发的场景下，针对大文件的传输的方式，应该使用「异步 I/O + 直接 I/O」来替代零拷贝技术。 直接 I/O 应用场景常见的两种： 应用程序已经实现了磁盘数据的缓存，那么可以不需要 PageCache 再次缓存，减少额外的性能损耗。在 MySQL 数据库中，可以通过参数设置开启直接 I/O，默认是不开启； 传输大文件的时候，由于大文件难以命中 PageCache 缓存，而且会占满 PageCache 导致「热点」文件无法充分利用缓存，从而增大了性能开销，因此，这时应该使用直接 I/O。 另外，由于直接 I/O 绕过了 PageCache，就无法享受内核的这两点的优化： 内核的 I/O 调度算法会缓存尽可能多的 I/O 请求在 PageCache 中，最后「合并」成一个更大的 I/O 请求再发给磁盘，这样做是为了减少磁盘的寻址操作； 内核也会「预读」后续的 I/O 请求放在 PageCache 中，一样是为了减少对磁盘的操作； 于是，传输大文件的时候，使用「异步 I/O + 直接 I/O」了，就可以无阻塞地读取文件了。 所以，传输文件的时候，我们要根据文件的大小来使用不同的方式： 传输大文件的时候，使用「异步 I/O + 直接 I/O」； 传输小文件的时候，则使用「零拷贝技术」； 在 nginx 中，我们可以用如下配置，来根据文件的大小来使用不同的方式： 12345location /video/ &#123; sendfile on; aio on; directio 1024m; &#125; 当文件大小大于 directio 值后，使用「异步 I/O + 直接 I/O」，否则使用「零拷贝技术」。 总结 早期 I/O 操作，内存与磁盘的数据传输的工作都是由 CPU 完成的，而此时 CPU 不能执行其他任务，会特别浪费 CPU 资源。 于是，为了解决这一问题，DMA 技术就出现了，每个 I/O 设备都有自己的 DMA 控制器，通过这个 DMA 控制器，CPU 只需要告诉 DMA 控制器，我们要传输什么数据，从哪里来，到哪里去，就可以放心离开了。后续的实际数据传输工作，都会由 DMA 控制器来完成，CPU 不需要参与数据传输的工作。 传统 IO 的工作方式，从硬盘读取数据，然后再通过网卡向外发送，我们需要进行 4 上下文切换，和 4 次数据拷贝，其中 2 次数据拷贝发生在内存里的缓冲区和对应的硬件设备之间，这个是由 DMA 完成，另外 2 次则发生在内核态和用户态之间，这个数据搬移工作是由 CPU 完成的。 为了提高文件传输的性能，于是就出现了零拷贝技术，它通过一次系统调用（sendfile 方法）合并了磁盘读取与网络发送两个操作，降低了上下文切换次数。另外，拷贝数据都是发生在内核中的，天然就降低了数据拷贝的次数。 Kafka 和 Nginx 都有实现零拷贝技术，这将大大提高文件传输的性能。 零拷贝技术是基于 PageCache 的，PageCache 会缓存最近访问的数据，提升了访问缓存数据的性能，同时，为了解决机械硬盘寻址慢的问题，它还协助 I/O 调度算法实现了 IO 合并与预读，这也是顺序读比随机读性能好的原因。这些优势，进一步提升了零拷贝的性能。 需要注意的是，零拷贝技术是不允许进程对文件内容作进一步的加工的，比如压缩数据再发送。 另外，当传输大文件时，不能使用零拷贝，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，并且大文件的缓存命中率不高，这时就需要使用「异步 IO + 直接 IO 」的方式。 在 Nginx 里，可以通过配置，设定一个文件大小阈值，针对大文件使用异步 IO 和直接 IO，而对小文件使用零拷贝。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://yoursite.com/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://yoursite.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"kafka基础","slug":"kafka基础","date":"2022-03-28T06:52:26.000Z","updated":"2022-11-22T01:27:53.684Z","comments":true,"path":"2022/03/28/kafka基础/","link":"","permalink":"http://yoursite.com/2022/03/28/kafka%E5%9F%BA%E7%A1%80/","excerpt":"","text":"1、基本概念 名称 解释 Broker 消息中间件处理节点，⼀个Kafka节点就是⼀个broker，⼀个或者多个Broker可以组成⼀个Kafka集群 Topic Kafka根据topic对消息进⾏归类，发布到Kafka集群的每条消息都需要指定⼀个topic Producer 消息⽣产者，向Broker发送消息的客户端 Consumer 消息消费者，从Broker读取消息的客户端 ConsumerGroup 每个Consumer属于⼀个特定的Consumer Group，⼀条消息可以被多个不同的Consumer Group消费，但是⼀个Consumer Group中只能有⼀个Consumer能够消费该消息 Partition 物理上的概念，⼀个topic可以分为多个partition，每个partition内部消息是有序的 2、安装 安装jdk 安装zk docker安装 123456789101112# dockerdocker run --name myKafka \\-p 9092:9092 \\-e KAFKA_BROKER_ID&#x3D;0 \\-e KAFKA_ZOOKEEPER_CONNECT&#x3D;10.0.3.249:2181 \\-e KAFKA_ADVERTISED_LISTENERS&#x3D;PLAINTEXT:&#x2F;&#x2F;10.0.3.249:9092 \\-e KAFKA_LISTENERS&#x3D;PLAINTEXT:&#x2F;&#x2F;0.0.0.0:9092 \\-d wurstmeister&#x2F;kafka# docker-compose 官网下载kafka的压缩包:http://kafka.apache.org/downloads 解压缩至如下路径 1/usr/local/kafka/ 修改配置文件：/usr/local/kafka/ kafka_2.12-3.1.0/config/server.properties 12345678#broker.id属性在kafka集群中必须要是唯一broker.id= 0#kafka部署的机器ip和提供服务的端口号listeners=PLAINTEXT://192.168.65.60:9092#kafka的消息存储文件log.dir=/usr/local/data/kafka-logs#kafka连接zookeeper的地址zookeeper.connect= 127.0.0.1:2181 核心配置项 Property Default Description broker.id 0 每个broker都可以⽤⼀个唯⼀的⾮负整数id进⾏标识；这个id可以作为broker的“名字”，你可以选择任意你喜欢的数字作为id，只要id是唯⼀的即可。 log.dirs /tmp/kafka-logs kafka存放数据的路径。这个路径并不是唯⼀的，可以是多个，路径之间只需要使⽤逗号分隔即可；每当创建新partition时，都会选择在包含最少partitions的路径下进⾏。 listeners PLAINTEXT://192.168.65.60:9092 server接受客户端连接的端⼝，ip配置kafka本机ip即可 zookeeper.connect 127.0.0.1:2181 zooKeeper连接字符串的格式为：hostname:port，此处hostname和port分别是ZooKeeper集群中某个节点的host和port；zookeeper如果是集群，连接⽅式为hostname1:port1, hostname2:port2,hostname3:port3 log.retention.hours 168 每个⽇志⽂件删除之前保存的时间。默认数据保存时间对所有topic都⼀样。 num.partitions 1 创建topic的默认分区数 default.replication.factor 1 ⾃动创建topic的默认副本数量，建议设置为⼤于等于2 min.insync.replicas 1 当producer设置acks为-1时，min.insync.replicas指定replicas的最⼩数⽬（必须确认每⼀个repica的写数据都是成功的），如果这个数⽬没有达到，producer发送消息会产⽣异常 delete.topic.enable false 是否允许删除主题 启动 1./kafka-server-start.sh -daemon../config/server.properties 验证 12# 查看zkls /brokers/ids/ 3、命令行操作 旧版（&lt; v2.2）–zookeeper zkhost:2181 新版（&gt;= v2.2）–bootstrap-server brokerhost:9092 新建topic 1./kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --replication-factor 1 --partitions 1 --topic topicName 查看所有topic 1./kafka-topics.sh --list --bootstrap-server 127.0.0.1:9092 删除 1./kafka-topics.sh --bootstrap-server 127.0.0.1:9092 --delete --topic topicName 发送 12345678# 不指定key./kafka-console-producer.sh --bootstrap-server 127.0.0.1:9092 --topic topicName&gt;Hello Kafka!&gt;你好 kafka!# 指定key./kafka-console-producer.sh --bootstrap-server 127.0.0.1:9092 --topic topicName --property parse.key=true&gt;Lei Li Hello Kafka!&gt;Meimei Han 你好 kafka! 消费 123456789# 从最后一条消息的偏移量+1开始消费./kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic topicName # 从头开始消费./kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic topicName --from-beginning# 单播消费（一个消息只能被一个消费组内一个消费者消费）./kafka-console-consumer.sh --bootstrap-server 10.31.167.10:9092 --consumer-property group.id=testGroup --topic topicName# 多播消费（一个消费可以被多个消费组内多个消费者消费）./kafka-console-consumer.sh --bootstrap-server 10.31.167.10:9092 --consumer-property group.id=testGroup1 --topic topicName./kafka-console-consumer.sh --bootstrap-server 10.31.167.10:9092 --consumer-property group.id=testGroup2 --topic topicName 4、存储机制 Topic 是逻辑上的概念 Partition 是物理上的概念 每个 Partition 对应于一个 log 文件 log文件（每个 Partition ）由若干段segment组成 每个segment由：xxx.log 和yyy.index 例如读取offset=368776的message，需要通过下面2个步骤查找。 第一步查找segment file ，其中00000000000000000000.index表示最开始的文件，起始偏移量(offset)为0.第二个文件00000000000000368769.index的消息量起始偏移量为368770 = 368769 + 1.同样，第三个文件00000000000000737337.index的起始偏移量为737338=737337 + 1，其他后续文件依次类推，以起始偏移量命名并排序这些文件，只要根据offset 二分查找文件列表，就可以快速定位到具体文件。 当offset=368776时定位到00000000000000368769.index|log 第二步通过segment file查找message 通过第一步定位到segment file，当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和00000000000000368769.log的物理偏移地址，然后再通过00000000000000368769.log顺序查找直到offset=368776为止。 5、高性能 分区（并行处理） ​ 一个主题下的分区可以分布在集群的不同broker上面，也就是说，一个主题可以横跨多个broker。这样的话，生产者在指定主题（可以指定也可以不指定分区）发送消息的时候，Kafka会将消息分发至不同的分区，如果这些分区不在同一个broker上，就相当于并发的写入多台broker，性能自然要比写入单台broker要高。 顺序读写磁盘 随机IO：当读取第一个block时,要经历寻道,旋转延迟,传输三个步骤才能读取完这个block的数据.而对于下一个block,如果它在磁盘的某个位置,访问它会同样经历寻道,旋转,延时,传输才能读取完这个block的数据, 我们把这种方式的IO叫做随机IO。 顺序IO：block的起始扇区刚好在我刚才访问的block的后面,磁头就能立刻遇到.不需等待,直接传输.这种IO就叫顺序IO PageCache加速读写 12345I/O Scheduler 会将连续的小块写组装成大块的物理写从而提高性能**I/O Scheduler 会尝试将一些写操作重新按顺序排好，从而减少磁盘头的移动时间**充分利用所有空闲内存（非 JVM 内存）。如果使用应用层 Cache（即 JVM 堆内存），会增加 GC 负担**读操作可直接在 Page Cache 内进行。如果消费和生产速度相当，甚至不需要通过物理磁盘（直接通过 Page Cache）交换数据**如果进程重启，JVM 内的 Cache 会失效，但 Page Cache 仍然可用** 磁盘文件 ==DMAcopy（DMA direct memory access）=&gt; 页缓存 ==CPUcopy=&gt; 用户空间缓存 ==CPUcopy=&gt; Socket缓存 ==DMAcopy=&gt;&gt; 网卡 零拷贝 磁盘文件 ==DMAcopy（DMA direct memory access）=&gt; 【页缓存并共享作为用户空间缓存】 ==CPUcopy=&gt; Socket缓存 ==DMAcopy=&gt;&gt; 网卡","categories":[{"name":"中间件","slug":"中间件","permalink":"http://yoursite.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"}]},{"title":"本地安装组件备忘","slug":"本地安装组件备忘","date":"2022-03-28T03:31:44.000Z","updated":"2022-11-22T01:27:54.024Z","comments":true,"path":"2022/03/28/本地安装组件备忘/","link":"","permalink":"http://yoursite.com/2022/03/28/%E6%9C%AC%E5%9C%B0%E5%AE%89%E8%A3%85%E7%BB%84%E4%BB%B6%E5%A4%87%E5%BF%98/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"ik分词器","slug":"ik分词器","date":"2022-03-18T08:27:30.000Z","updated":"2022-11-22T01:27:53.660Z","comments":true,"path":"2022/03/18/ik分词器/","link":"","permalink":"http://yoursite.com/2022/03/18/ik%E5%88%86%E8%AF%8D%E5%99%A8/","excerpt":"","text":"1、安装 从github官网下载对应es版本的ik分词器，https://github.com/medcl/elasticsearch-analysis-ik/releases 下载完成后解压插件到es的安装目录的plugins目录下，注意修改plugins目录权限为读，最好就是777权限。重启es。 2、查看分词 标准分词器 123456789101112131415161718192021222324252627282930313233343536373839POST http://localhost:9200/_analyze&#123;&quot;analyzer&quot;:&quot;standard&quot;,&quot;text&quot;:&quot;北京大学&quot;&#125;# 返回&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;北&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 1, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;京&quot;, &quot;start_offset&quot;: 1, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;大&quot;, &quot;start_offset&quot;: 2, &quot;end_offset&quot;: 3, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;学&quot;, &quot;start_offset&quot;: 3, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot;: 3 &#125; ]&#125; ik_smart 关键字最粗粒度分词，比如我们知道“北京大学”是一个词，不需要分词。具体还是需要测试看看！ 123456789101112131415161718POST http://localhost:9200/_analyze&#123;&quot;analyzer&quot;:&quot;ik_smart&quot;,&quot;text&quot;:&quot;北京大学&quot;&#125;# 返回&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;北京大学&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 0 &#125; ]&#125; ik_max_word 关键字最细粒度分词，也就是说将关键词尽量拆分。 1234567891011121314151617181920212223242526272829303132333435363738POST http://localhost:9200/_analyze&#123;&quot;analyzer&quot;:&quot;ik_max_word&quot;,&quot;text&quot;:&quot;北京大学&quot;&#125;# 返回&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;北京大学&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;北京大&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 3, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;北京&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;大学&quot;, &quot;start_offset&quot;: 2, &quot;end_offset&quot;: 4, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 3 &#125; ]&#125; 3、设置分词器 ES分词器主要有两种情况会被使用： 创建索引，指定analyzer（ES先检查是否设置了analyzer字段，如果没定义就用ES预设的），不能改变！！ 查询时，指定search_analyzer（ES查询时会先检查是否设置了search_analyzer字段，如果没有设置，还会去检查创建索引时是否指定了analyzer，还是没有还设置才会去使用ES预设的） 1234567891011curl -XPOST http://localhost:9200/index/_mapping -H &#x27;Content-Type:application/json&#x27; -d&#x27;&#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_smart&quot; &#125; &#125;&#125;&#x27; 4、官方分词器 查看有已经安装哪些插件（分词器） 123456# shell./bin/elasticsearch-plugin list# url http://127.0.0.1:9200/_cat/plugins# kibanaGET /_cat/plugins 官方内置分词器：https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html 官方内置分词插件（可以安装为分词器）：https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis.html 5、英文Analyzer 官方的内置的Standard Analyzer 官方的内置的Simple Analyzer 6、中文 Analyzer 官方的ICU插件 官方SmartCN插件 HanLP IK Pinyin ansj jieba jcseg 7、自定义分词器 参考：https://www.elastic.co/guide/en/elasticsearch/reference/7.16/analysis-custom-analyzer.html 分词器（analyzer）由三部分组成：字符过滤器（CharacterFilters）、分词器（Tokenizer）和词元过滤器（TokenFilters）。自定义需要： zero or more character filters a tokenizer zero or more token filters. 123456789101112131415161718192021222324252627# 自定义分词器PUT my-index-000001&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;my_custom_analyzer&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;tokenizer&quot;: &quot;standard&quot;, &quot;char_filter&quot;: [ &quot;html_strip&quot; ], &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;asciifolding&quot; ] &#125; &#125; &#125; &#125;&#125;# 查看POST my-index-000001/_analyze&#123; &quot;analyzer&quot;: &quot;my_custom_analyzer&quot;, &quot;text&quot;: &quot;Is this &lt;b&gt;déjà vu&lt;/b&gt;?&quot;&#125;","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}]},{"title":"yum","slug":"yum","date":"2022-03-03T02:26:58.000Z","updated":"2022-11-22T01:27:53.855Z","comments":true,"path":"2022/03/03/yum/","link":"","permalink":"http://yoursite.com/2022/03/03/yum/","excerpt":"","text":"1、介绍 Yum(全称为 Yellow dogUpdater, Modified)是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。 使用 yum 安装软件时至少需要一个 yum 源。yum 源既可以使用网络 yum 源，也可以将本地光盘作为 yum 源。接下来就给大家介绍配置网络 yun 源，个人习惯阿里云的源。 网络 yum 源配置文件位于 /etc/yum.repos.d/ 目录下，文件扩展名为”.repo”（只要扩展名为 “.repo” 的文件都是 yum 源的配置文件）, 通常情况下我们关注 CentOS-Base.repo 文件。 123456789101112-rw-r--r-- 1 root root 1050 10月 3 2017 epel-testing.repo-rw-r--r-- 1 root root 951 10月 3 2017 epel.repo-rw-r--r-- 1 root root 2523 6月 16 2018 CentOS-Base.repo -----重点关注！-rw-r--r-- 1 root root 84 2月 14 2019 centos.repo-rw-r--r-- 1 root root 2640 3月 16 2020 docker-ce.repo-rw-r--r-- 1 root root 616 4月 8 2020 CentOS-x86_64-kernel.repo-rw-r--r-- 1 root root 7577 4月 8 2020 CentOS-Vault.repo-rw-r--r-- 1 root root 1331 4月 8 2020 CentOS-Sources.repo-rw-r--r-- 1 root root 630 4月 8 2020 CentOS-Media.repo-rw-r--r-- 1 root root 314 4月 8 2020 CentOS-fasttrack.repo-rw-r--r-- 1 root root 649 4月 8 2020 CentOS-Debuginfo.repo-rw-r--r-- 1 root root 1309 4月 8 2020 CentOS-CR.repo 切换yum源 123456789# 1、备份sudo zip CentOS-Base.repo.zip CentOS-Base.repo# 2、删除sudo rm Centos-Base.repo# 3、下载阿里云源头sudo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo# 4、清理 yum 并生成缓存yum clean allyum makecache 2、命令 2.1 查询 yum list：查询所有已安装和可安装的软件包 yum list [包名]：查询执行软件包的安装情况 yum search [关键字]：从 yum 源服务器上查找与关键字相关的所有软件包 yum info [包名]：查询执行软件包的详细信息 2.2 安装 yum -y install 包名 2.3 升级 升级所有包：yum -y update升级所有包 特定软件包：yum -y update [包] 2.4 卸载 12# remove erase相等关系yum remove 包名（可以多个）===== yum erase 包名（可以多个）","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]},{"title":"canal初体验","slug":"canal初体验","date":"2022-03-01T05:59:58.000Z","updated":"2022-11-22T01:27:53.454Z","comments":true,"path":"2022/03/01/canal初体验/","link":"","permalink":"http://yoursite.com/2022/03/01/canal%E5%88%9D%E4%BD%93%E9%AA%8C/","excerpt":"","text":"1、介绍 https://github.com/alibaba/canal。 canal [kə’næl]，译意为水道/管道/沟渠，主要用途是基于 MySQL 数据库增量日志解析，提供增量数据订阅和消费。这也意味着它不合适进行之前有初始化数据的同步。 1.1 mysql 主备原理： MySQL master 将数据变更写入二进制日志( binary log, 其中记录叫做二进制日志事件binary log events，可以通过 show binlog events 进行查看) MySQL slave 将 master 的 binary log events 拷贝到它的中继日志(relay log) MySQL slave 重放 relay log 中事件，将数据变更反映它自己的数据 1.2 canal工作原理 canal 模拟 MySQL slave 的交互协议，伪装自己为 MySQL slave ，向 MySQL master 发送dump 协议 MySQL master 收到 dump 请求，开始推送 binary log 给 slave (即 canal ) canal 解析 binary log 对象(原始为 byte 流) 2、安装 对于自建 MySQL , 需要先开启 Binlog 写入功能，配置 binlog-format 为 ROW 模式，my.cnf (/etc/my.cnf)中配置如下： 1234[mysqld]log-bin=mysql-bin # 开启 binlogbinlog-format=ROW # 选择 ROW 模式server_id=71 # 配置 MySQL replaction 需要定义，不要和 canal 的 slaveId 重复 授权 canal 链接 MySQL 账号具有作为 MySQL slave 的权限, 如果已有账户可直接 grant 1234CREATE USER canal IDENTIFIED BY &#39;canal&#39;; GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO &#39;canal&#39;@&#39;%&#39;;-- GRANT ALL PRIVILEGES ON *.* TO &#39;canal&#39;@&#39;%&#39; ;FLUSH PRIVILEGES; 下载 1wget https://github.com/alibaba/canal/releases/download/canal-1.1.5/canal.deployer-1.1.5.tar.gz 解压缩 123456789mkdir /tmp/canaltar zxvf canal.deployer-1.1.5.tar.gz -C /tmp/canalcd /tmp/canaltree -L 1├── bin├── conf├── lib├── logs└── plugin 配置 12345678910111213141516171819vim conf&#x2F;example&#x2F;instance.properties## mysql serverIdcanal.instance.mysql.slaveId &#x3D; 1234#position info，需要改成自己的数据库信息canal.instance.master.address &#x3D; 127.0.0.1:3306 canal.instance.master.journal.name &#x3D; canal.instance.master.position &#x3D; canal.instance.master.timestamp &#x3D; #canal.instance.standby.address &#x3D; #canal.instance.standby.journal.name &#x3D;#canal.instance.standby.position &#x3D; #canal.instance.standby.timestamp &#x3D; #username&#x2F;password，需要改成自己的数据库信息canal.instance.dbUsername &#x3D; canal canal.instance.dbPassword &#x3D; canalcanal.instance.defaultDatabaseName &#x3D;canal.instance.connectionCharset &#x3D; UTF-8#table regexcanal.instance.filter.regex &#x3D; .\\*\\\\\\\\..\\* 启动 1sh bin/startup.sh 3、同步 目前官方提供了一个canal-adapter工程，内置了若干种可以同步目的数据源，例如：Kafka、Hbase、ES等。但是他们都是基于增量同步的，且需要大量配置，比较麻烦，这里我们需求很明确，具体描述如下： 从mysql同步到elasticsearch 支持增量和全量同步 支持横表、纵表同步 工程：pump-server 一个可以支持从Mysql到Elasticsearch的canal客户端同步项目，可以对mysql业务库中的增、删、改操作进行近乎实时的同步，且提供手动rest接口进行单个表全量同步。 3.1、原理 mysql开启biglog，且binlog-format为ROW模式，canal服务端监听binlog，pump-server作为canal客户端，订阅canal服务端监听到的binlog数据，内部使用spring的spring的事件发布机制，解耦程序处理增、删、改的数据，同步到Elasticsearch中。 3.2 配置 application.properties 123456789101112131415161718192021222324spring.application.name=pumpserver.port=8796# mysql数据库配置(全量同步es会使用)spring.datasource.driver-class-name=com.mysql.jdbc.Driverspring.datasource.url=jdbc:mysql://127.0.0.1:3306?characterSet=utf8mb4&amp;useSSL=falsespring.datasource.username=rootspring.datasource.password=rootspring.datasource.dbcp2.max-idle=10spring.datasource.dbcp2.min-idle=5spring.datasource.dbcp2.initial-size=2spring.datasource.dbcp2.validation-query=SELECT 1spring.datasource.dbcp2.test-while-idle=truemybatis.config-location=classpath:mybatis/mybatis-config.xmlmybatis.mapper-locations=classpath:mybatis/mapper/*.xml# canal配置canal.host=127.0.0.1canal.port=11111canal.destination=examplecanal.username=canal.password=# elasticsearch配置elasticsearch.cluster=docker-clusterelasticsearch.host=127.0.0.1elasticsearch.port=9300 db2es.properties 123456# 需要被同步到es的库&amp;表信息db2es.mapping.[dbname.tablename]=dbname.tablename# 表主键（代表业务上记录为一条的唯一字段）db2es.primaryKey.[dbname.tablename]=primaryField#是否是纵表db2es.isVertical.[dbname.tablename]=true 3.3 全量同步 URL：http://ip:port/sync/table method: POST 参数 参数信息需要在db2es.properties里配置过！ 1234&#123; &quot;database&quot;: &quot;hf&quot;, &quot;table&quot;: &quot;bas_event&quot;&#125; 3.4 规范和工具类 表主键id 横表、纵表都增加主键id，且为自增或者雪花算法生成值！ 纵表模版 12345678CREATE TABLE &#96;tmp_vertical&#96; ( &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#39;表主键，没有业务含义&#39;, &#96;identification&#96; bigint(20) NOT NULL COMMENT &#39;业务上记录的唯一标识字段&#39;, &#96;attr&#96; varchar(100) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;字段英文名&#39;, &#96;attr_value&#96; varchar(100) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;字段值&#39;, PRIMARY KEY (&#96;id&#96;), UNIQUE KEY &#96;id&#96; (&#96;id&#96;)) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;1 DEFAULT CHARSET&#x3D;utf8mb4 COLLATE&#x3D;utf8mb4_unicode_ci COMMENT&#x3D;&#39;示例纵表结构&#39;; 工具类依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.dtwave.industry.pony&lt;/groupId&gt; &lt;artifactId&gt;ipony-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; 查询纵表全部字段SQL 12345678910111213141516# APIVerticalTableUtil.getSelectItemSql(tableName);# 返回sqlSELECT 1 AS `isGroupBy`, &#x27;identification&#x27; AS `fieldName`UNION ALLSELECT 0 AS `isGroupBy`, MAX(ATTR) AS `fieldName`FROM TMP_VERTICALGROUP BY ATTR # mysql里执行结果（此纵表有5个字段，identification为主记录字段，另外为age、hobby、name、sex） 1 identification0 age0 hobby0 name0 sex 纵转横表查询数据SQL 123456789101112131415# APIVerticalTableUtil.getHorizontalSql(SqlObject sqlObject); # 返回sql SELECT identification AS `identification`, MAX(if(attr = &#x27;age&#x27;, attr_value, NULL)) AS `age`, MAX(if(attr = &#x27;hobby&#x27;, attr_value, NULL)) AS `hobby`, MAX(if(attr = &#x27;name&#x27;, attr_value, NULL)) AS `name`, MAX(if(attr = &#x27;sex&#x27;, attr_value, NULL)) AS `sex` FROM hf.tmp_vertical [ where identification=102 ]GROUP BY identification # mysql里执行结果identification age hobby name sex 101 18 unknow jim 男 索引和类型 1234mysql同步后，在es里面的索引和类型格式：dbname_tableName、tableName例如：test（库）user（表） ---&gt; test_user、useres查询：GET /test_user/user/_search 4、测试pump-server 项目信息暂在本地。 新建表 12345678910111213141516171819202122232425262728293031323334353637383940CREATE TABLE &#96;tmp_vertical&#96; ( &#96;id&#96; int(11) NOT NULL AUTO_INCREMENT COMMENT &#39;表主键，没有业务含义&#39;, &#96;identification&#96; int(11) NOT NULL COMMENT &#39;业务上记录的唯一标识字段&#39;, &#96;attr&#96; varchar(100) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;字段英文名&#39;, &#96;attr_value&#96; varchar(100) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;字段值&#39;, PRIMARY KEY (&#96;id&#96;), UNIQUE KEY &#96;id&#96; (&#96;id&#96;)) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;1 DEFAULT CHARSET&#x3D;utf8mb4 COLLATE&#x3D;utf8mb4_unicode_ci COMMENT&#x3D;&#39;临时测试纵表&#39;;CREATE TABLE &#96;tmp_horizonal&#96; ( &#96;id&#96; int(11) NOT NULL AUTO_INCREMENT, &#96;type&#96; varchar(32) COLLATE utf8mb4_unicode_ci DEFAULT NULL, &#96;identity&#96; varchar(32) COLLATE utf8mb4_unicode_ci DEFAULT NULL, &#96;value&#96; varchar(128) COLLATE utf8mb4_unicode_ci DEFAULT NULL, &#96;invalid&#96; char(1) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;N&#39; COMMENT &#39;Y(无效)N(有效)&#39;, PRIMARY KEY (&#96;id&#96;)) ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;1 DEFAULT CHARSET&#x3D;utf8mb4 COLLATE&#x3D;utf8mb4_unicode_ci COMMENT&#x3D;&#39;临时测试横表&#39;;INSERT INTO &#96;tmp_horizonal&#96; (&#96;id&#96;, &#96;type&#96;, &#96;identity&#96;, &#96;value&#96;, &#96;invalid&#96;)VALUES (177, &#39;planStatus&#39;, &#39;0&#39;, &#39;未生效&#39;, &#39;N&#39;), (178, &#39;planStatus&#39;, &#39;1&#39;, &#39;已生效&#39;, &#39;N&#39;), (179, &#39;planStatus&#39;, &#39;2&#39;, &#39;暂停&#39;, &#39;N&#39;), (180, &#39;planStatus&#39;, &#39;3&#39;, &#39;已结束&#39;, &#39;N&#39;), (181, &#39;planStatus&#39;, &#39;-1&#39;, &#39;待完善&#39;, &#39;N&#39;), (182, &#39;planGroup&#39;, &#39;1&#39;, &#39;默认分组&#39;, &#39;N&#39;), (183, &#39;planRepeat&#39;, &#39;1&#39;, &#39;永不&#39;, &#39;N&#39;), (184, &#39;planRepeat&#39;, &#39;2&#39;, &#39;每天&#39;, &#39;N&#39;), (185, &#39;planRepeat&#39;, &#39;3&#39;, &#39;每周&#39;, &#39;N&#39;), (186, &#39;planRepeat&#39;, &#39;4&#39;, &#39;每月&#39;, &#39;N&#39;), (187, &#39;planTime&#39;, &#39;MINUTE&#39;, &#39;分钟&#39;, &#39;N&#39;), (188, &#39;planTime&#39;, &#39;HOUR&#39;, &#39;小时&#39;, &#39;N&#39;), (189, &#39;planTime&#39;, &#39;DAY&#39;, &#39;天&#39;, &#39;N&#39;);INSERT INTO &#96;tmp_vertical&#96; (&#96;id&#96;, &#96;identification&#96;, &#96;attr&#96;, &#96;attr_value&#96;)VALUES (5, 101, &#39;sex&#39;, &#39;男&#39;), (6, 101, &#39;name&#39;, &#39;jim&#39;), (7, 101, &#39;hobby&#39;, &#39;unknow&#39;), (8, 101, &#39;age&#39;, &#39;18&#39;); 使用API全量同步 123456POST http://ip:8796/sync/table&#123; &quot;database&quot;:&quot;yourdbname&quot;, &quot;table&quot;:&quot;tmp_horizonal&quot;&#125; kibana查看ES里数据 1GET yourdbname&#x2F;tmp_horizonal&#x2F;_search 其他增删改不演示了","categories":[{"name":"canal","slug":"canal","permalink":"http://yoursite.com/categories/canal/"}],"tags":[{"name":"canal","slug":"canal","permalink":"http://yoursite.com/tags/canal/"}]},{"title":"k8s安装","slug":"k8s安装","date":"2022-02-22T02:35:09.000Z","updated":"2022-11-22T01:27:53.666Z","comments":true,"path":"2022/02/22/k8s安装/","link":"","permalink":"http://yoursite.com/2022/02/22/k8s%E5%AE%89%E8%A3%85/","excerpt":"","text":"1、环境规划 1.1 集群类型 主要分成2类：一主多从；多主多从（搭建烦烦，适合生产环境）。 1.2 安装方式 主流有：kubeadm、minikube、二进制包 minikube：一个用于快速搭建单节点的k8s工具包。 kubeadm：一个用于快速搭建k8s的集群工具。 二进制包：从官网下载每个组件的二进制包，依次安装，可对k8s各个组件更加有效了解。 1.3 主机规划 节点 IP 操作系统 配置 master 10.211.55.51 Cetos7.9 2cup 8G内存 100G硬盘 node1 10.211.55.52 Cetos7.9 2cup 8G内存 100G硬盘 node2 10.211.55.53 Cetos7.9 2cup 8G内存 100G硬盘 1.4 安装docker 切换yum为阿里（每台机器）：wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 安装指定版本docker（每台机器）：yum -y install --setopt=obsoletes=0 docker-ce-18.06.3.ce-3.el7 docker 默认使用cgroupfs，k8s建议使用systemd替代cgroupfs。因此修改/etc/docker/daemon.json（没有就新建） 12345678910&#123; &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;registry-mirrors&quot;: [&quot;https://kn0t2bca.mirror.aliyuncs.com&quot;], &quot;runtimes&quot;: &#123; &quot;nvidia&quot;: &#123; &quot;path&quot;: &quot;nvidia-container-runtime&quot;, &quot;runtimeArgs&quot;: [] &#125; &#125;&#125; 启动docker 12systemctl restart dockersystemctl enable docker 1.5 系统设置 123456789101112131415systemctl stop firewalld &amp;&amp; systemctl disable firewalldsetenforce 0echo 1 &gt; /proc/sys/net/bridge/bridge-nf-call-iptablesecho 1 &gt; /proc/sys/net/bridge/bridge-nf-call-ip6tablesecho 1 &gt; /proc/sys/net/ipv4/ip_forwardsysctl -w net.bridge.bridge-nf-call-iptables=1vim /etc/sysctl.confnet.ipv4.ip_forward=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1sysctl -pswapoff -a 1.6 安装k8s组件 切换k8s国内镜像源 12345678# vim /etc/yum.repos.d/kubernetes.repo ，添加下面配置[kubernetes]name=kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enable=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun/kubernetes/yum/doc/rpm-package-key.gpg 安装kubeadm、kubelet、kubectl 1yum install --setopt=obsolets=0 kubeadm-1.17.4-0 kubelet-1.17.4-0 kubectl-1.17.4-0 -y 配置kubelet的cgroup 123# vim /etc/sysconfig/kubelet,添加下面配置KUBELET_CGROUP_ARGS=&quot;--cgroup-driver=systemd&quot;KUBE_PROXY_MODE=&quot;ipvs&quot; 设置kubelet开启自启动 1systemctl enable kubelet 1.6 集群初始化(master节点执行) 123456kubeadm init \\--kubernetes-version=v1.17.4 \\--image-repository=registry.aliyuncs.com/google_containers \\--pod-network-cidr=10.244.0.0/16 \\--service-cidr=10.96.0.0/12 \\--apiserver-advertise-address=10.211.55.51 1234567mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/configAlternatively, if you are the root user, you can run:export KUBECONFIG=/etc/kubernetes/admin.conf 123# node节点运行kubeadm join 10.211.55.51:6443 --token a100ku.c03og6fpjuifpoxb \\ --discovery-token-ca-cert-hash sha256:925a15700fe45862441991bac19c66bd04255f831b043e97af3184242acc571f 1.7 集群网络插件安装(master节点执行) k8s支持多种网络插件，比如：flannel、calico、canal等。 flannel 123# 获取flannel配置文件(/etc/hosts里新增 199.232.96.133 raw.githubusercontent.com)wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlkubectl apply -f kube-flannel.yml calico 123456789101112131415curl -O https://docs.projectcalico.org/manifests/calico.yaml# 修改1# 找到这2项目，去掉注释，让后修改网络地址为init里pod网段- name: CALICO_IPV4POOL_CIDR value: &quot;10.244.0.0/16 &quot; # 修改2 指定 IP_AUTODETECTION_METHOD 中的 interface 为网卡名即可，支持通配符。 - name: CLUSTER_TYPEvalue: &quot;k8s,bgp&quot;# 这里新增- name: IP_AUTODETECTION_METHODvalue: &quot;interface=(eth0|eno1) # 应用 kubectl apply -f calico.yaml 1kubeadm join 192.168.90.102:6443 --token cvlagm.1fg1h0llsgkwsqqd --discovery-token-ca-cert-hash sha256:4a75f2ca7949d3bf441c42a736d512bb2ebf3ab4d3614eff3a9684f01c4ba371","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/tags/k8s/"}]},{"title":"k8s组件","slug":"k8s组件","date":"2022-02-22T01:45:41.000Z","updated":"2022-11-22T01:27:53.670Z","comments":true,"path":"2022/02/22/k8s组件/","link":"","permalink":"http://yoursite.com/2022/02/22/k8s%E7%BB%84%E4%BB%B6/","excerpt":"","text":"一个k8s集群由控制节点（master）、工作节点（node）构成，每个节点都会安装不同的组件。 master：集群的控制平面，负责集群的决策（管理） 1234ApiServer:资源唯一入口，接受用户的输入命令，提供认证、授权、API注册和发现等机制，只有 apiserver 会连接 ETCD。Scheduler：负责集群资源调度，按照预定的调度策略将pod调度到响应的node节点上。ControllerManager：负责维护集群的状态，比如程序部署安排，故障检测、自动扩展、滚动更新等。Etcd：负存储集群中各种资源对象的信息。 node：集群的数据平面，负责为容器提供运行环境。 123Kubelet：负责维护容器的生命周期，即通过docker来创建、更新、销毁容器。KubeProxy：负责提供集群内部的服务发现和负载均衡，即主要负责网络的打通，早期利用 iptables，现在使用 ipvs技术。Docker：负责节点上容器的各种操作。 k8s架构 下面，以部署一个nginx服务说明k8s集群各个组件调用关系： 首先要明确，一旦k8s环境启动之后，master和node都会将自身的信息存储到etcd数据库中。 一个nginx服务的安装请求会首先被发送到master的apiServer组件上。 apiServer组件调用scheduler组件来决策到底应该把这个服务安装到哪个node上。scheduler从etcd中读取各个node的信息，按照一定算法选择，并告知apiServer。 apiServer调用controllerManager去调度node节点按照ngxin kublet接受到指令后，会通知docker，然后由docker启动一个nginx的pod（pod是k8s最小操作单元，一个容器必须跑在pod中）。 一个nginx服务就运行了，如果需要访问nginx，就需要通过kubeProxy来对pod产生访问的代理，这样外界用户就可以访问集群的nginx服务了。","categories":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/tags/k8s/"}]},{"title":"磁盘管理","slug":"磁盘管理","date":"2022-02-21T01:46:28.000Z","updated":"2022-11-22T01:27:54.201Z","comments":true,"path":"2022/02/21/磁盘管理/","link":"","permalink":"http://yoursite.com/2022/02/21/%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86/","excerpt":"","text":"1、raid 英文：Redundant Arrays of Independent Disk 中文：冗余磁盘阵列 实现方式：软raid、硬raid、软硬结合raid。软 RAID 所有功能均有操作系统和 CPU 来完成，没有独立的 RAID 控制 / 处理芯片和 I/O 处理芯片，效率自然最低。硬 RAID 配备了专门的 RAID 控制 / 处理芯片和 I/O 处理芯片以及阵列缓冲，不占用 CPU 资源，但成本很高。软硬混合 RAID 具备 RAID 控制 / 处理芯片，但缺乏 I/O 处理芯片，需要 CPU 和驱动程序来完成，性能和成本 在软 RAID 和硬 RAID 之间 级别：0—6共7种。 1.1 数据条带 ​ 当多个进程同时访问磁盘时，会出现磁盘冲突。大多数磁盘系统都对访问次数(每秒的I/O操作)和数据传输率(每秒传输的数据量)有限制。当达到这些限制时，后面需要访问磁盘的进程就需要等待，这就是所谓的磁盘冲突 。磁盘条带化是指利用条带化技术就是将一块连续的数据分成很多小部分并把它们分别存储到不同磁盘上去。影响条带化效果的因素有两个，一是条带大小（stripesize），即数据被切成的小数据块的大小，另一个条带宽度（stripe width），即数据被存储到多少块硬盘上。 1.2 数据校验 ​ 数据条带通过并发性来大幅提高性能，然而对数据安全性、可靠性未作考虑。镜像具有高安全性、高读性能，但冗余开销太昂贵。数据校验是一种冗余技术，它用校验数据来提供数据的安全，可以检测数据错误，并在能力允许的前提下进行数据重构。 ​ 采用数据校验时， RAID 要在写入数据同时进行校验计算，并将得到的校验数据存储在 RAID 成员磁盘中。校验数据可以集中保存在某个磁盘或分散存储在多个不同磁盘中，甚至校验数据也可以分块，不同 RAID 等级实现各不相同。当其中一部分数据出错时，就可以对剩余数据和校验数据进行反校验计算重建丢失的数据。校验技术相对于镜像技术的优势在于节省大量开销，但由于每次数据读写都要进行大量的校验运算，对计算机的运算速度要求很高，必须使用硬件 RAID 控制器。在数据重建恢复方面，检验技术比镜像技术复杂得多且慢得多。 1.3 raid级别 raid等级 raid0 raid1 raid5 raid6 raid10 容错冗余能力 无 高 较高 高 高 读性能 高 低 高 高 高 随机写性能 高 低 一般 低 一般 连续读写性 高 低 低 低 一般 磁盘利用率 100% 50% (n-1)/n (n-2)/n 50% 最少硬盘数量 1 2 3 4 4 raid0 ​ 把所有的磁盘通过条带化组成的一个大硬盘，数据因此也实现了并发读写，成倍地提高了读写IO性能，同时磁盘利用率也达到了100%，但是由于没有丝毫的冗余能力，一旦阵列中出现磁盘损坏，数据将无法恢复，通常适用于一些对数据安全不太高但是对读写速度要求较高的场所。 raid1 ​ 侧重于数据的安全性，通过完全的镜像来做到数据的完全容错能力，通常也被叫做镜像，但是这也带来了高额的成本，通过下图可以看出磁盘空间利用率仅仅只有50%。 raid10 2快硬盘组成raid1，2组raid1组成raid0。 raid5 ​ 把数据和相对应的奇偶校验信息存储到组成RAID5的各个磁盘上，并且奇偶校验信息和相对应的数据分别存储于不同的磁盘上，其中任意N-1块磁盘上都存储完整的数据，也就是说有相当于一块磁盘容量的空间用于存储奇偶校验信息。 raid6 同RAID5最大的区别就是在RAID5的基础上除了具有P校验位以外,还加入了第2个校验位Q位。 2、两种分区形式 磁盘分区形式 支持最大磁盘容量 支持分区数量 Linux分区工具 主启动记录分区（MBR） 2 TB 4个主分区 、3个主分区和1个扩展分区 MBR分区包含主分区和扩展分区，其中扩展分区里面可以包含若干个逻辑分区。扩展分区不可以直接使用，需要划分成若干个逻辑分区才可以使用。以创建6个分区为例，以下两种分区情况供参考： 3个主分区，1个扩展分区，其中扩展分区中包含3个逻辑分区。 1个主分区，1个扩展分区，其中扩展分区中包含5个逻辑分区。 以下两种工具均可以使用：fdisk工具parted工具 全局分区表（GPT, Guid Partition Table） 18 EB1 EB = 1048576 TB 不限制分区数量GPT格式下没有主分区、扩展分区以及逻辑分区之分。是UEFI标准的一部分，主板必须要支持UEFI标准 parted工具 查看分区是GPT还是MBR 1$ fdisk -l 1$ parted -l 2.1 MBR分区和挂载 详情见：Linux挂载磁盘章节 2.2 gpt分区和挂载 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485$ parted /dev/sdbGNU Parted 3.1Using /dev/sdbWelcome to GNU Parted! Type &#x27;help&#x27; to view a list of commands.# 1、设置磁盘标签类型为GPT(parted) mklabel gpt################################分区命令格式说明################################mkpart PART-TYPE [FS-TYPE] START ENDPART-TYPE(分区类型)primary主分区logical逻辑分区extended扩展分区FS-TYPE(文件系统类型)ext4ext3ext2xfs其他......START设定磁盘分区起始点；可以为0，numberMiB/GiB/TiB；0设定当前分区的起始点为磁盘的第一个扇区；1G设定当前分区的起始点为磁盘的1G处开始；END设定磁盘分区结束点；-1设定当前分区的结束点为磁盘的最后一个扇区；10G设定当前分区的结束点为磁盘的10G处；# 2、将/dev/sdb整个空间分给同一个分区(parted) mkpart primary 0 -1 Warning: The resulting partition is not properly aligned for best performance.Ignore/Cancel? I(parted) p Model: AVAGO AVAGO (scsi)Disk /dev/sdb: 18.0TBSector size (logical/physical): 512B/4096BPartition Table: gptDisk Flags: Number Start End Size File system Name Flags 1 17.4kB 18.0TB 18.0TB primary(parted) q Information: You may need to update /etc/fstab.# 3、格式化分区（因为整个/dev/sdb只分了一个区，则这个分区名默认会分配为/dev/sdb1；使用mkfs命令将/dev/sdb1分区格式化为ext4）$ mkfs -t ext4 /dev/sdb1 mke2fs 1.42.9 (28-Dec-2013)/dev/sdb1 alignment is offset by 244736 bytes.This may result in very poor performance, (re)-partitioning suggested.Filesystem label=OS type: LinuxBlock size=4096 (log=2)Fragment size=4096 (log=2)Stride=64 blocks, Stripe width=64 blocks274659328 inodes, 4394530311 blocks219726515 blocks (5.00%) reserved for the super userFirst data block=0134111 block groups32768 blocks per group, 32768 fragments per group2048 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, 102400000, 214990848, 512000000, 550731776, 644972544, 1934917632, 2560000000, 3855122432Allocating group tables: doneWriting inode tables: doneCreating journal (32768 blocks): doneWriting superblocks and filesystem accounting information: done# 4、设置分区lable（可选择）$ e2label /dev/sdb1 /gfsdata01# 5、新建将挂载的目录$ mkdir /gfsdata01# 6、挂载$ mount /dev/sdb1 /gfsdata01# 7、开机自动挂载$ echo &#x27;/dev/sdb1 /gfsdata01 ext4 defaults 0 0&#x27; &gt;&gt;/etc/fstab 一些注意事项： 在 fstab里使用的设备名字，最好使用uuid，可以通过下面命令查看： 1blkid [设备] 开机自动挂载，最好放在/etc/rc.local 12vim /etc/rc.localmount /dev/sdb1 /gfsdata01","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"磁盘管理","slug":"磁盘管理","permalink":"http://yoursite.com/tags/%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86/"}]},{"title":"nio的buffer","slug":"nio的buffer","date":"2022-02-15T03:50:56.000Z","updated":"2022-11-22T01:27:53.726Z","comments":true,"path":"2022/02/15/nio的buffer/","link":"","permalink":"http://yoursite.com/2022/02/15/nio%E7%9A%84buffer/","excerpt":"","text":"转载/参考：https://www.cnblogs.com/robothy/p/14223249.html 1、Buffer java.nio.Buffer 是一个接口，有 7 个重要的子类。每个类的实例化都需要使用allocate(int capacity)来实例化。Buffer 中有 3 个重要的属性 capacity, limit, position。 2、读写切换 allocate 12// position =0,limit=capacityCharBuffer buf = CharBuffer.allocate(10); put 12// 每写入一个 char 数据，position 就移动 1 位。buf.put(&#x27;A&#x27;).put(&#x27;B&#x27;).put(&#x27;C&#x27;).put(&#x27;D&#x27;).put(&#x27;E&#x27;).put(&#x27;F&#x27;).put(&#x27;G&#x27;); flip() 12// 切换成读模式，移动limit=position，position=0。buf.flip(); get() 1234// position移动System.out.println(buf.get()); // 输出：ASystem.out.println(buf.get()); // 输出：BSystem.out.println(buf.get()); // 输出：C mark、reset 12345// 当前的position赋值给markbuf.mark();System.out.println(buf.get()); // 输出：D// position回退到之前的mark位置buf.reset(); compact 123// 读到一半，需要切换到写模式，将没有读取的移到0开始位置，position=limit-oldposition，limit=capacitybuf.compact();System.out.println(Arrays.toString(buf.array())); // 输出：[D, E, F, G, E, F, G, , , ] clear 12// position =0,limit=capacitybuf.clear(); 3、总结 Buffer 是一个缓冲数据的容器，其内部结构是一个固定长度的数组，支持 7 种数据的存取。 Buffer 是非线程安全的，多线程并发访问一个 Buffer 时应该进行同步。 Buffer 中并没有明确限制读取模式中Buffer 只能读取，写入模式中只能写入。读写模式切换图如下：","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"nio","slug":"nio","permalink":"http://yoursite.com/tags/nio/"}]},{"title":"nio","slug":"nio","date":"2022-02-15T02:12:34.000Z","updated":"2022-11-22T01:27:53.725Z","comments":true,"path":"2022/02/15/nio/","link":"","permalink":"http://yoursite.com/2022/02/15/nio/","excerpt":"","text":"网络编程再Java里面有BIO和NIO。 1、BIO 客户端 1234567891011@Slf4jpublic class BioClient &#123; public static void main(String[] args) throws Exception &#123; try (Socket socket = new Socket(&quot;127.0.0.1&quot;, 8000)) &#123; for (int i = 0; i &lt; 5; i++) &#123; socket.getOutputStream().write((LocalDateTime.now() + &quot;: hi-&quot; + i).getBytes()); &#125; &#125; log.info(&quot;Job finished!&quot;); &#125;&#125; 服务端 1234567891011121314151617@Slf4jpublic class BioServer &#123; public static void main(String[] args) throws IOException &#123; try (ServerSocket serverSocket = new ServerSocket(8000)) &#123; while (true) &#123; try (Socket socket = serverSocket.accept(); InputStream inputStream = socket.getInputStream()) &#123; int len; byte[] buffer = new byte[1024]; while ((len = inputStream.read(buffer)) != -1) &#123; log.info(new String(buffer, 0, len)); &#125; &#125; &#125; &#125; &#125;&#125; 2、NIO 客户端 1234567891011@Slf4jpublic class NioClient &#123; public static void main(String[] args) throws IOException &#123; SocketChannel sc = SocketChannel.open(); sc.connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 8000)); for (int i = 0; i &lt; 5; i++) &#123; sc.write(Charset.defaultCharset().encode(&quot;1&quot;)); &#125; log.info(&quot;Job finished!&quot;); &#125;&#125; 服务端 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132@Slf4jpublic class NioServer &#123; public static void main(String[] args) throws IOException &#123;// block();// nonblock();// select(); &#125; private static void select() throws IOException &#123; ServerSocketChannel ssc = ServerSocketChannel.open().bind(new InetSocketAddress(8000)); Selector selector = Selector.open(); ssc.configureBlocking(false); ssc.register(selector, SelectionKey.OP_ACCEPT); // 阻塞等待 while (true) &#123; // 一般情况会阻塞，但是若其上注册的channel有发生事件（当时注册的事件），但是在程序中又没有处理，就会变成非阻塞 int select = selector.select(); if (select &gt; 0) &#123; Iterator&lt;SelectionKey&gt; iterator = selector.selectedKeys().iterator(); while (iterator.hasNext()) &#123; SelectionKey key = iterator.next(); // 拿到后必须溢出，否在下次还会拿到，但是key iterator.remove(); if (key.isAcceptable()) &#123; // 拿到的就是上面的ssc，同一个对象// ServerSocketChannel ssc2 = ((ServerSocketChannel) key.channel()); SocketChannel sc = null; try &#123; sc = ssc.accept(); if (sc == null) &#123; // 从selector.keys 集合里面移除 key.cancel(); break; &#125; sc.configureBlocking(false); sc.register(selector, SelectionKey.OP_READ); &#125; catch (IOException e) &#123; e.printStackTrace(); // 从selector.keys 集合里面移除 key.cancel(); &#125; &#125; if (key.isReadable()) &#123; SocketChannel sc = (SocketChannel) key.channel(); System.out.println(&quot;read....&quot;); ByteBuffer byteBuffer = ByteBuffer.allocate(150); int read = sc.read(byteBuffer); if (read == -1) &#123; key.cancel(); &#125; else &#123; byteBuffer.flip(); CharBuffer charBuffer = Charset.defaultCharset().decode(byteBuffer); System.out.println(charBuffer); &#125; &#125; &#125; &#125; &#125; &#125; /** * 非阻塞模式 * * @throws IOException */ private static void nonblock() throws IOException &#123; ServerSocketChannel ssc = ServerSocketChannel.open().bind(new InetSocketAddress(8000)); ssc.configureBlocking(false); List&lt;SocketChannel&gt; channels = new ArrayList&lt;&gt;(); ByteBuffer byteBuffer = ByteBuffer.allocate(10); while (true) &#123; log.info(&quot;准备连接&quot;); SocketChannel channel = ssc.accept(); if (null != channel) &#123; log.info(&quot;连接成功，channel:&quot; + channel); channel.configureBlocking(false); channels.add(channel); channels.forEach(ch -&gt; &#123; try &#123; log.info(&quot;准备开始读&quot;); int read = ch.read(byteBuffer); if (read &gt; 0) &#123; log.info(&quot;读了字节，read:&quot; + read); byteBuffer.flip(); CharBuffer charBuffer = Charset.defaultCharset().decode(byteBuffer); log.info(charBuffer.toString()); byteBuffer.clear(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; &#125; &#125; /** * 阻塞模式，同bio * * @throws IOException */ private static void block() throws IOException &#123; ServerSocketChannel ssc = ServerSocketChannel.open().bind(new InetSocketAddress(9000)); List&lt;SocketChannel&gt; channels = new ArrayList&lt;&gt;(); ByteBuffer byteBuffer = ByteBuffer.allocate(10); while (true) &#123; log.info(&quot;准备连接&quot;); // 阻塞 SocketChannel channel = ssc.accept(); log.info(&quot;连接成功，channel:&quot; + channel); channels.add(channel); channels.forEach(ch -&gt; &#123; try &#123; log.info(&quot;准备开始读&quot;); // 阻塞 int read = ch.read(byteBuffer); log.info(&quot;读了字节，read:&quot; + read); if (read &gt; 0) &#123; log.info(&quot;读了字节，read:&quot; + read); byteBuffer.flip(); CharBuffer charBuffer = Charset.defaultCharset().decode(byteBuffer); log.info(charBuffer.toString()); byteBuffer.clear(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;); &#125; &#125; 这里需要注意下，在selector模式中，有2个keys需要正确使用： Set&lt;SelectionKey&gt; keys = selector.keys() channel注册后会，自动加入到这个set中，只会在key.cancel或者channel取消注册才会移除。 Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys() 在有事件发生后，自动加入到这个set中，并不会自动移除，一般需要在使用后手动的移除。","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"nio","slug":"nio","permalink":"http://yoursite.com/tags/nio/"}]},{"title":"netty的handler","slug":"netty的handler","date":"2022-02-14T10:02:13.000Z","updated":"2022-11-22T01:27:53.719Z","comments":true,"path":"2022/02/14/netty的handler/","link":"","permalink":"http://yoursite.com/2022/02/14/netty%E7%9A%84handler/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"netty的Future和Promise","slug":"netty的Future和Promise","date":"2022-02-14T09:11:51.000Z","updated":"2022-11-22T01:27:53.715Z","comments":true,"path":"2022/02/14/netty的Future和Promise/","link":"","permalink":"http://yoursite.com/2022/02/14/netty%E7%9A%84Future%E5%92%8CPromise/","excerpt":"","text":"Future可以说是一个“菜蓝子”，线程间数据传递容器。在JDK和Netty中都有，是在线程提交后，程序自动给我们生成返回的。 Netty的Promise和Future类似，也是线程间数据传递容器，但需要手动新建，构造方法是传递线程（eventLoop）进去。 123456789101112131415161718192021222324252627282930313233public static void main(String[] args) throws ExecutionException, InterruptedException &#123; /** * JDK 里的Future */ ExecutorService executorService = Executors.newFixedThreadPool(2); Future&lt;String&gt; future = executorService.submit(() -&gt; &quot;jdk future&quot;); String jdkStr = future.get(); System.out.println(jdkStr); executorService.shutdown(); /** * Netty 里的Future */ EventLoop eventLoop = new NioEventLoopGroup().next(); // netty里的线程 io.netty.util.concurrent.Future&lt;String&gt; nettyFuture = eventLoop.submit(() -&gt; &quot;netty future&quot;); // 同步获取结果(get 方法) System.out.println(&quot;sys get: &quot; + nettyFuture.get()); // 同步获取结果（添加listener后再get） nettyFuture.addListener(new GenericFutureListener&lt;io.netty.util.concurrent.Future&lt;? super String&gt;&gt;() &#123; @Override public void operationComplete(io.netty.util.concurrent.Future&lt;? super String&gt; future) throws Exception &#123; System.out.println(&quot;listener get: &quot; + nettyFuture.get()); &#125; &#125;); /** * Netty 里的Promise */ DefaultPromise&lt;String&gt; promise = new DefaultPromise&lt;&gt;(eventLoop); new Thread(() -&gt; &#123; promise.setSuccess(&quot;netty promise&quot;); &#125;).start(); System.out.println(promise.get()); eventLoop.shutdownGracefully(); &#125;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://yoursite.com/tags/netty/"}]},{"title":"netty的channel","slug":"netty的channel","date":"2022-02-14T08:14:14.000Z","updated":"2022-11-22T01:27:53.716Z","comments":true,"path":"2022/02/14/netty的channel/","link":"","permalink":"http://yoursite.com/2022/02/14/netty%E7%9A%84channel/","excerpt":"","text":"ChannelFuture 服务端：ChannelFuture channelFuture = server.bind(port); 客户端：ChannelFuture channelFuture = bootstrap.connect(address, port); ChannelFuture是Channel异步IO操作的结果。可以从其中获取channel。 1Channel channel=channelFuture.channel(); Netty中的所有IO操作都是异步的。这意味着任何IO调用都将立即返回，而不能保证所请求的IO操作在调用结束时完成。相反，将返回一个带有ChannelFuture的实例，该实例将提供有关IO操作的结果或状态的信息。 客户端和服务端口的bind和connect都是异步非阻塞。即，bind/connect的线程（1）和真正bind/connect的线程（2）为不同，且（1）可以在调用完api后不阻塞，可以做自己的事。我们可以有2种方式处理这种异步非阻塞的bind和连接。 12345678910111213141516#1、 (1)线程同步等待（2）完成，自己处理逻辑ChannelFuture channelFuture = server.bind(port).sync;ChannelFuture channelFuture = bootstrap.connect(address, port).sync;##2、 (1)线程异步调用，然后放手给线程(3)来处理逻辑，即NioEventLoop的一个实例channelFuture.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) throws Exception &#123; // dosth（1:重连【connect()自己实现】；2：获取channel发送信息） if (!future.isSuccess()) &#123; channelFuture.channel().eventLoop().schedule(() -&gt; connect(), 1, TimeUnit.SECONDS); &#125; Channel channel = future.channel(); channel.writeAndFlush(&quot;123&quot;); &#125; &#125;);","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://yoursite.com/tags/netty/"}]},{"title":"netty心跳","slug":"netty心跳","date":"2022-02-11T01:41:01.000Z","updated":"2022-11-22T01:27:53.712Z","comments":true,"path":"2022/02/11/netty心跳/","link":"","permalink":"http://yoursite.com/2022/02/11/netty%E5%BF%83%E8%B7%B3/","excerpt":"","text":"在Netty中IdleStateHandler主要用来检测远端是否存活，IdleStateHandler实现对三种心跳的检测，分别是： 1）readerIdleTime：读超时时间 2）writerIdleTime：写超时时间 3）allIdleTime：所有类型的超时时间 实现思路：服务端设置读超时，客户端设置写超时，具备断线重连。具体为客户端每隔n秒往server发信息，服务端在m秒内没有读到客户端信息，则触发读超时，计数器加1，达到阀值关闭通道。这里需要注意：m&gt;n，不然server读永远是超时的了！ 1、服务端 1.1 server 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798import com.dtwave.industry.uac.provider.metadata.route.service.UacRouteService;import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.*;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.SocketChannel;import io.netty.channel.socket.nio.NioServerSocketChannel;import io.netty.handler.codec.string.StringDecoder;import io.netty.handler.codec.string.StringEncoder;import io.netty.handler.timeout.IdleStateHandler;import lombok.RequiredArgsConstructor;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Value;import org.springframework.boot.CommandLineRunner;import org.springframework.cloud.gateway.route.RouteDefinition;import org.springframework.core.annotation.Order;import org.springframework.stereotype.Component;import java.util.concurrent.TimeUnit;@Slf4j@Order(1)@Component@RequiredArgsConstructor(onConstructor = @__(@Autowired))public class HeartbeatServer implements CommandLineRunner &#123; /** * 业务service，在心跳handler里处理一些业务逻辑，可以自定义 * 我这里是sprngCloud网关路由信息操作service */ private final UacRouteService&lt;RouteDefinition&gt; uacRouteService; /** * 端口 */ @Value(&quot;$&#123;heartbeat.server.port:60100&#125;&quot;) private int heartbeatServerPort; /** * 读超时时间 */ @Value(&quot;$&#123;heartbeat.read.timeout:5&#125;&quot;) private int readTimeout; /** * 读（超时）空闲的次数阀值（溢出就关闭通道） */ @Value(&quot;$&#123;heartbeat.channel.close.timeout-count:3&#125;&quot;) private int idleCloseCount; private static final EventLoopGroup BOSS_GROUP = new NioEventLoopGroup(); private static final EventLoopGroup WORK_GROUP = new NioEventLoopGroup(); /** * 1、JVM 钩子，优雅关闭netty * 2、开启netty服务端 * * @param args * @throws Exception */ @Override public void run(String... args) throws Exception &#123; Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; &#123; try &#123; WORK_GROUP.shutdownGracefully().sync(); BOSS_GROUP.shutdownGracefully().sync(); &#125; catch (Exception e) &#123; Thread.currentThread().interrupt(); e.printStackTrace(); &#125; &#125;)); start(); &#125; private ChannelFuture start() &#123; ServerBootstrap server = new ServerBootstrap().group(BOSS_GROUP, WORK_GROUP) .option(ChannelOption.SO_BACKLOG, 128) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel socketChannel) &#123; ChannelPipeline pipeline = socketChannel.pipeline(); pipeline.addLast(new IdleStateHandler(readTimeout, 0,0)); pipeline.addLast(&quot;encoder&quot;, new StringEncoder()); pipeline.addLast(&quot;decoder&quot;, new StringDecoder()); pipeline.addLast(&quot;handler&quot;, new HeartbeatServerHandler(idleCloseCount, uacRouteService)); &#125; &#125;); ChannelFuture channelFuture = server.bind(heartbeatServerPort); channelFuture.addListener((ChannelFutureListener) future -&gt; &#123; if (!future.isSuccess()) &#123; log.warn(&quot;Failed to start heartbeat server, start reconnection after 3 seconds!&quot;); channelFuture.channel().eventLoop().schedule(this::start, 3L, TimeUnit.SECONDS); &#125; &#125;); return channelFuture; &#125;&#125; 1.2 handler 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102import com.dtwave.industry.uac.provider.metadata.route.service.UacRouteService;import io.netty.channel.ChannelFutureListener;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInboundHandlerAdapter;import io.netty.handler.timeout.IdleStateEvent;import io.netty.util.Attribute;import io.netty.util.AttributeKey;import lombok.extern.slf4j.Slf4j;import org.springframework.cloud.gateway.route.RouteDefinition;@Slf4jpublic class HeartbeatServerHandler extends ChannelInboundHandlerAdapter &#123; /** * 读（超时）空闲阀值 */ private final int idleCloseCount; /** * 当前读（超时）空闲次数 */ private int currentIdleCount = 0; /** * 业务service，处理一些业务逻辑 */ private final UacRouteService&lt;RouteDefinition&gt; uacRouteService; /** * client 发送过来心跳信息读取后长度，可以自定义 */ private static final int INSTANCE_INFO_LENGTH = 3; private static final String INSTANCE_INFO_KEY = &quot;instanceInfo&quot;; public HeartbeatServerHandler(int idleCloseCount, UacRouteService&lt;RouteDefinition&gt; uacRouteService) &#123; this.idleCloseCount = idleCloseCount; this.uacRouteService = uacRouteService; &#125; @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123; super.userEventTriggered(ctx, evt); if (evt instanceof IdleStateEvent) &#123; IdleStateEvent idleStateEvent = (IdleStateEvent) evt; switch (idleStateEvent.state()) &#123; case READER_IDLE: currentIdleCount++; // 读空闲超过阀值，关闭通道 if (currentIdleCount &gt; idleCloseCount) &#123; log.warn(String.format(&quot;%s ===&gt; server,&quot; + &quot; %d idle channel close!&quot;, ctx.channel().remoteAddress(), idleCloseCount)); ctx.close(); &#125; break; case WRITER_IDLE: case ALL_IDLE: ctx.close(); break; default: &#125; &#125; &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; super.channelRead(ctx, msg); // 回复一条给client ctx.writeAndFlush(&quot;healthy&quot;).addListener(ChannelFutureListener.CLOSE_ON_FAILURE); // 读空闲次数重置 currentIdleCount = 0; // 拿到心跳消息，处理业务 String[] instanceInfo = String.valueOf(msg).split(&quot;&amp;&quot;); Attribute&lt;String[]&gt; instanceInfoKey = ctx.channel().attr(AttributeKey.valueOf(INSTANCE_INFO_KEY)); if (instanceInfoKey.get() == null &amp;&amp; instanceInfo.length == INSTANCE_INFO_LENGTH) &#123; uacRouteService.add(instanceInfo[0], instanceInfo[1], instanceInfo[2]); instanceInfoKey.set(instanceInfo); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; Attribute&lt;String[]&gt; instanceInfoKey = ctx.channel().attr(AttributeKey.valueOf(INSTANCE_INFO_KEY)); String[] instanceInfo = instanceInfoKey.get(); if (instanceInfo != null &amp;&amp; instanceInfo.length == INSTANCE_INFO_LENGTH) &#123; // 业务处理 uacRouteService.del(instanceInfo[0], instanceInfo[1], instanceInfo[2]); &#125; log.warn(ctx.channel().remoteAddress() + &quot; ===&gt; server, channel inactive!&quot;); super.channelUnregistered(ctx); &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; log.info(ctx.channel().remoteAddress() + &quot; ===&gt; server, channel active&quot;); super.channelActive(ctx); &#125;&#125; 2、客户端 2.1 server 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import io.netty.bootstrap.Bootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelFutureListener;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelPipeline;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.SocketChannel;import io.netty.channel.socket.nio.NioSocketChannel;import io.netty.handler.codec.string.StringDecoder;import io.netty.handler.codec.string.StringEncoder;import io.netty.handler.timeout.IdleStateHandler;import org.springframework.beans.factory.annotation.Value;import org.springframework.boot.CommandLineRunner;import lombok.extern.slf4j.Slf4j;import java.util.concurrent.TimeUnit;@Slf4jpublic class HeartbeatClient implements CommandLineRunner &#123; /** * 服务端ip */ private final String serverHost; /** * 服务端口 */ private final int serverPort; /** * 写（超时）空闲时间阀值 */ private final int writeTimeout; /** * 心跳包内容 */ private final String heartbeatPacket; private static final NioEventLoopGroup LOOP_GROUP = new NioEventLoopGroup(); public HeartbeatClient(String host, int port, int rate, String heartbeatPacket) &#123; this.serverHost = host; this.serverPort = port; this.writeTimeout = rate; this.heartbeatPacket = heartbeatPacket == null ? &quot;heartbeat&quot; : heartbeatPacket; &#125; public String getHeartbeatPacket() &#123; return heartbeatPacket; &#125; @Override public void run(String... args) throws Exception &#123; Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; &#123; try &#123; LOOP_GROUP.shutdownGracefully().sync(); &#125; catch (Exception e) &#123; Thread.currentThread().interrupt(); e.printStackTrace(); &#125; &#125;)); connect(); &#125; public ChannelFuture connect() &#123; Bootstrap client = new Bootstrap(); client.group(LOOP_GROUP).channel(NioSocketChannel.class).handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new IdleStateHandler(0, writeTimeout, 0)); pipeline.addLast(&quot;encoder&quot;, new StringEncoder()); pipeline.addLast(&quot;decoder&quot;, new StringDecoder()); pipeline.addLast(&quot;handler&quot;, new HeartbeatClientHandler(new HeartbeatClient(serverHost,serverPort,writeTimeout,&quot;心跳包内容&quot;))); &#125; &#125;); return client.connect(serverHost, serverPort).addListener((ChannelFutureListener) future -&gt; &#123; if (!future.isSuccess()) &#123; log.warning(&quot;Failed to start gateway heartbeat client, restart after 2 seconds!&quot;); future.channel().eventLoop().schedule(this::connect, 2L, TimeUnit.SECONDS); &#125; &#125;); &#125;&#125; 2.2 handler 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import io.netty.channel.ChannelFutureListener;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInboundHandlerAdapter;import io.netty.handler.timeout.IdleStateEvent;import lombok.extern.slf4j.Slf4j;import java.util.concurrent.TimeUnit;@Slf4jpublic class HeartbeatClientHandler extends ChannelInboundHandlerAdapter &#123; private final HeartbeatClient client; public HeartbeatClientHandler(HeartbeatClient client) &#123; this.client = client; &#125; @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123; super.userEventTriggered(ctx, evt); if (evt instanceof IdleStateEvent) &#123; IdleStateEvent idleStateEvent = (IdleStateEvent) evt; switch (idleStateEvent.state()) &#123; case WRITER_IDLE: ctx.writeAndFlush(client.getHeartbeatPacket()).addListener(ChannelFutureListener.CLOSE_ON_FAILURE); break; case READER_IDLE: case ALL_IDLE: ctx.close(); break; default: &#125; &#125; &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; super.channelRead(ctx, msg); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; log.warning(ctx.channel().remoteAddress() + &quot; ===&gt; client, channel inactive, reconnection after 1 seconds!&quot;); ctx.channel().eventLoop().schedule(client::connect, 1L, TimeUnit.SECONDS); super.channelInactive(ctx); &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; log.info(ctx.channel().remoteAddress() + &quot; ===&gt; client, channel active&quot;); super.channelActive(ctx); &#125;&#125;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://yoursite.com/tags/netty/"}]},{"title":"Socket操作","slug":"Socket操作","date":"2022-01-25T06:29:01.000Z","updated":"2022-11-22T01:27:53.449Z","comments":true,"path":"2022/01/25/Socket操作/","link":"","permalink":"http://yoursite.com/2022/01/25/Socket%E6%93%8D%E4%BD%9C/","excerpt":"","text":"1、阻塞IO通信 1234567891011121314151617181920212223242526public class BioClient &#123; public static void main(String[] args) throws Exception &#123; Socket socket = new Socket(); socket.connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 8080)); socket.getOutputStream().write(&quot;hello server&quot;.getBytes()); &#125;&#125;public class BioServer &#123; public static void main(String[] args) throws Exception &#123; ServerSocket serverSocket = new ServerSocket(); serverSocket.bind(new InetSocketAddress(8080)); Socket accept = serverSocket.accept(); InputStream inputStream = accept.getInputStream(); // 循环读 byte[] bt = new byte[1024]; int len = 0; int temp; while ((temp = inputStream.read()) != -1) &#123; bt[len] = (byte) temp; len++; &#125; inputStream.close(); System.out.println(new String(bt, 0, len)); &#125;&#125; 2、非阻塞IO通信","categories":[{"name":"网络编程","slug":"网络编程","permalink":"http://yoursite.com/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"socket","slug":"socket","permalink":"http://yoursite.com/tags/socket/"}]},{"title":"IO操作","slug":"IO操作","date":"2022-01-21T03:04:55.000Z","updated":"2022-11-22T01:27:53.363Z","comments":true,"path":"2022/01/21/IO操作/","link":"","permalink":"http://yoursite.com/2022/01/21/IO%E6%93%8D%E4%BD%9C/","excerpt":"","text":"1、类图","categories":[],"tags":[{"name":"IO","slug":"IO","permalink":"http://yoursite.com/tags/IO/"}]},{"title":"代码片段","slug":"代码片段","date":"2022-01-20T09:10:38.000Z","updated":"2022-11-22T01:27:53.858Z","comments":true,"path":"2022/01/20/代码片段/","link":"","permalink":"http://yoursite.com/2022/01/20/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5/","excerpt":"","text":"1、使用JDK原生API发Http请求 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// GETURL url = new URL(fullUrl);HttpURLConnection conn = (HttpURLConnection) url.openConnection();conn.setConnectTimeout(10000);conn.connect();if (conn.getResponseCode() == HttpURLConnection.HTTP_OK) &#123; InputStream is = conn.getInputStream(); ByteArrayOutputStream arrayOutputStream = new ByteArrayOutputStream(); int len; byte[] bytes = new byte[1024]; while ((len = is.read(bytes)) != -1) &#123; arrayOutputStream.write(bytes, 0, len); arrayOutputStream.flush(); &#125;is.close();arrayOutputStream.close(); //POSTprivate static JSONObject post(byte[] bytes) &#123; StringBuilder result = new StringBuilder(); try &#123; String url = &quot;xxxxxxx&quot;; URL realUrl = new URL(url); URLConnection conn = realUrl.openConnection(); conn.setRequestProperty(&quot;accept&quot;, &quot;*/*&quot;); conn.setRequestProperty(&quot;connection&quot;, &quot;Keep-Alive&quot;); conn.setRequestProperty(&quot;user-agent&quot;, &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)&quot;); conn.setRequestProperty(&quot;Content-Type&quot;, &quot;application/octet-stream&quot;); conn.setDoOutput(true); conn.setDoInput(true); DataOutputStream dos = new DataOutputStream(conn.getOutputStream()); dos.write(bytes); dos.flush(); BufferedReader in = new BufferedReader(new InputStreamReader(conn.getInputStream())); String line; while ((line = in.readLine()) != null) &#123; result.append(line); &#125; JSONObject jsonObject = JSONObject.parseObject(String.valueOf(result)); if ((Integer) jsonObject.get(&quot;error_code&quot;) != 0) &#123; throw new RuntimeException((String) jsonObject.get(&quot;error_msg&quot;)); &#125; return jsonObject; &#125; catch (Exception e) &#123; e.getStackTrace(); System.out.println(&quot;异常,&quot; + e.getMessage()); &#125; return null; &#125; 2、从图片获取bytes 1234567891011121314151617181920212223public static byte[] main(String[] args) throws Exception &#123; File f = new File(&quot;/Users/hf/IdeaProjects/lanxun/idun/idun-provider/src/main/resources/1.JPG&quot;); ByteArrayOutputStream bos = new ByteArrayOutputStream((int) f.length()); BufferedInputStream in = null; try &#123; in = new BufferedInputStream(new FileInputStream(f)); int buf_size = 1024; byte[] buffer = new byte[buf_size]; int len; while (-1 != (len = in.read(buffer, 0, buf_size))) &#123; bos.write(buffer, 0, len); &#125; byte[] bytes = bos.toByteArray(); return bytes; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; bos.close(); assert in != null; in.close(); &#125; return null;&#125; 3、判断文件是否是指定格式的图片 123456789101112131415161718192021222324private static boolean isFitPicture(File file) &#123; if (file == null) &#123; return false; &#125; // 获得文件后缀名 String filename = file.getName(); if (filename.lastIndexOf(&quot;.&quot;) == -1) &#123; return false; &#125; String tmpName = filename.substring(filename.lastIndexOf(&quot;.&quot;) + 1); // 声明图片后缀名数组 String[][] imageArray = &#123; &#123;&quot;jpg&quot;, &quot;0&quot;&#125;, &#123;&quot;png&quot;, &quot;1&quot;&#125;, &#123;&quot;bmp&quot;, &quot;2&quot;&#125;, &#123;&quot;pdf&quot;, &quot;3&quot;&#125;, &#123;&quot;tiff&quot;, &quot;4&quot;&#125;, &#125;; // 遍历名称数组 for (String[] strings : imageArray) &#123; if (strings[0].equals(tmpName.toLowerCase())) &#123; return true; &#125; &#125; return false;&#125; 4、byte转base64 1234//byte[]转base64 public static String byte2Base64StringFun(byte[] b) &#123; return Base64.getEncoder().encodeToString(b); &#125; 5、shell/脚本调用 12345678910List&lt;String&gt; commend &#x3D; new ArrayList&lt;&gt;(); commend.add(&quot;rm&quot;); commend.add(&quot;-rf&quot;); commend.add(relativePath); commend.add(relativePath2); ProcessBuilder builder &#x3D; new ProcessBuilder(); builder.command(commend); Process process &#x3D; builder.start(); process.waitFor(); process.destroy();","categories":[{"name":"代码片段","slug":"代码片段","permalink":"http://yoursite.com/categories/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5/"}],"tags":[{"name":"代码片段","slug":"代码片段","permalink":"http://yoursite.com/tags/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5/"}]},{"title":"springCloud路由","slug":"springCloud路由","date":"2022-01-20T02:11:29.000Z","updated":"2022-11-22T01:27:53.809Z","comments":true,"path":"2022/01/20/springCloud路由/","link":"","permalink":"http://yoursite.com/2022/01/20/springCloud%E8%B7%AF%E7%94%B1/","excerpt":"","text":"springCloud版本为2.1.1.RELEASE 1、RouteDefinitionLocator All Known Subinterfaces: RouteDefinitionRepository All Known Implementing Classes: CachingRouteDefinitionLocator, CompositeRouteDefinitionLocator, DiscoveryClientRouteDefinitionLocator, InMemoryRouteDefinitionRepository, PropertiesRouteDefinitionLocator 默认从PropertiesRouteDefinitionLocator读取路由配置 12345678910111213141516171819spring: cloud: gateway: routes: - id: setstatus_route uri: https://example.org filters: - name: SetStatus args: status: 401 - id: setstatusshortcut_route uri: https://example.org filters: - SetStatus=401 #id：我们自定义的路由 ID，保持唯一#uri：目标服务地址#predicates：路由条件，Predicate 接受一个输入参数，返回一个布尔值结果。该接口包含多种默认方法来将 Predicate 组合成其他复杂的逻辑（比如：与，或，非）。#filters：过滤规则 开启自动发现（zk、redis等作为路由承载时候） 1spring.cloud.gateway.discovery.locator.enabled=true Gateway 执行流程如下 2、动态路由 sc默认使用配置文件配置路由定义信息，定义后很难修改，需要实现服务加入到网关后，可以动态感知服务上下架。大概思路为：客户端加入/下线网关（实质就是操作路由信息），路由信息在增、删、改3种操作后发布路由刷新事件，sc会调用获取路由信息接口（我们自己实现的路由定位器）更新springCloud内存中路由信息，达到动态路由需求。业务逻辑图如下： 假若客户端没有使用sc基于zk做服务发现，那我们实现动态路由可以使用netty心跳或者路由信息service的http接口调用形式实现动态路由，这里以netty心跳实现，技术流程图如下： 主要的类有： 路由定位器：zk、mysql定位器，实现sc的路由定位器接口 路由操作service：增删改查 netty心跳：客户端和服务端 2.1 路由定位器 123456public interface UacRouteLocator extends RouteDefinitionLocator &#123; /** * 刷新路由 */ void refreshRoutes();&#125; 123456789101112131415161718192021222324252627282930313233@Component@ConditionalOnProperty( name = &quot;spring.cloud.zookeeper.enabled&quot;, havingValue = &quot;true&quot;)@RequiredArgsConstructor(onConstructor = @__(@Autowired))public class ZkUacRouteLocator implements UacRouteLocator, ApplicationEventPublisherAware &#123; private final ZkClientUtil zkClientUtil; private ApplicationEventPublisher publisher; @Value(&quot;$&#123;spring.cloud.zookeeper.discovery.root&#125;&quot;) private String rootPath; @Override public Flux&lt;RouteDefinition&gt; getRouteDefinitions() &#123; List&lt;RegistrationInstance&gt; registrationInstances = zkClientUtil.getChildrenLoop(rootPath, new HashMap&lt;&gt;(16)).entrySet().stream().filter(entry -&gt; entry.getKey() .split(&quot;/&quot;).length == 4).map(entry -&gt; JSON.parseObject(entry.getValue(), RegistrationInstance.class)).collect(Collectors.toList()); return Flux.fromIterable(CommonUacRouteService .getInstanceId2RouteByInstances(registrationInstances).values()); &#125; @Override public void setApplicationEventPublisher(ApplicationEventPublisher applicationEventPublisher) &#123; this.publisher = applicationEventPublisher; &#125; @Override public void refreshRoutes() &#123; this.publisher.publishEvent(new RefreshRoutesEvent(this)); &#125;&#125; 12345678910111213141516171819202122232425262728@Component@ConditionalOnProperty( name = &quot;spring.cloud.zookeeper.enabled&quot;, havingValue = &quot;false&quot;)@RequiredArgsConstructor(onConstructor = @__(@Autowired))public class MySqlUacRouteLocator implements UacRouteLocator, ApplicationEventPublisherAware &#123; private final IScRoutesService iScRoutesService; private ApplicationEventPublisher publisher; @Override public Flux&lt;RouteDefinition&gt; getRouteDefinitions() &#123; return Flux.fromIterable(CommonUacRouteService.getInstanceId2RouteByScRoutes( iScRoutesService.list()).values()); &#125; @Override public void setApplicationEventPublisher(ApplicationEventPublisher applicationEventPublisher) &#123; this.publisher = applicationEventPublisher; &#125; @Override public void refreshRoutes() &#123; this.publisher.publishEvent(new RefreshRoutesEvent(this)); &#125;&#125; 2.2 路由操作service 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public interface UacRouteService&lt;T extends RouteDefinition&gt; &#123; /** * 获取服务实例Id * * @param serviceId * @param host * @param port * @return */ default String getServiceInstanceId(String serviceId, String host, String port) &#123; return SecureUtil.md5(String.format(&quot;%s&amp;%s&amp;%s&quot;, serviceId, host, port)); &#125; /** * 添加服务路由 * * @param serviceId * @param host * @param port * @return */ RouteDefinition add(String serviceId, String host, String port); /** * 删除服务路由 * * @param serviceId * @param host * @param port * @return */ boolean del(String serviceId, String host, String port); /** * 更新服务路由 * * @param serviceId * @param host * @param port * @return */ boolean update(String serviceId, String host, String port); /** * 搜索服务路由 * * @param serviceId * @return */ List&lt;T&gt; select(String serviceId);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172@Service@ConditionalOnProperty( name = &quot;spring.cloud.zookeeper.enabled&quot;, havingValue = &quot;true&quot;)@RequiredArgsConstructor(onConstructor = @__(@Autowired))public class ZkUacRouteServiceImpl implements UacRouteService&lt;RouteDefinition&gt; &#123; private final ZkClientUtil zkClientUtil; private final RegistrationInstance tmpInstance; private final UacRouteLocator uacRouteLocator; private static final String INSTANCE_PATH_TEMPLATE = &quot;%s/%s/%s&quot;; @Value(&quot;$&#123;spring.cloud.zookeeper.discovery.root&#125;&quot;) private String rootPath; @Override public RouteDefinition add(String serviceId, String host, String port) &#123; String serviceInstanceId = getServiceInstanceId(serviceId, host, port); RouteDefinition routeDefinition = new RouteDefinition(); RegistrationInstance actualInstance = new RegistrationInstance(); BeanUtils.copyProperties(tmpInstance, actualInstance); actualInstance.setName(serviceId); actualInstance.setId(serviceInstanceId); actualInstance.setAddress(host); actualInstance.setPort(Integer.parseInt(port)); RegistrationInstance.PayloadBean payloadBean = new RegistrationInstance.PayloadBean(); payloadBean.set_$Class244(payloadBean.get_$Class244()); payloadBean.setId(serviceInstanceId); payloadBean.setName(serviceId); actualInstance.setPayload(payloadBean); actualInstance.setRegistrationTimeUtc(OffsetDateTime.now(ZoneOffset.UTC).toInstant().toEpochMilli()); String jsonString = JSON.toJSONString(actualInstance); String data = jsonString.replace(&quot;$Class244&quot;, &quot;@class&quot;); zkClientUtil.createPerNodeLoop(String.format(INSTANCE_PATH_TEMPLATE, rootPath, serviceId, serviceInstanceId), data); uacRouteLocator.refreshRoutes(); return routeDefinition; &#125; @Override public boolean del(String serviceId, String host, String port) &#123; zkClientUtil.deleteNode(getPath(serviceId, host, port)); uacRouteLocator.refreshRoutes(); return true; &#125; @Override public boolean update(String serviceId, String host, String port) &#123; del(serviceId, host, port); add(serviceId, host, port); return true; &#125; @Override public List&lt;RouteDefinition&gt; select(String serviceId) &#123; String path = serviceId == null ? rootPath : rootPath + &quot;/&quot; + serviceId; Set&lt;String&gt; serviceIds = zkClientUtil.getChildrenLoop(path, new HashMap&lt;&gt;(16)).entrySet().stream().filter(entry -&gt; entry.getKey() .split(&quot;/&quot;).length == 4).map(entry -&gt; JSON.parseObject(entry.getValue(), RegistrationInstance.class)).map(RegistrationInstance::getName).collect(Collectors.toSet()); return serviceIds.stream().map(CommonUacRouteService::createRouteDefinitionByServiceId). collect(Collectors.toList()); &#125; private String getPath(String serviceId, String host, String port) &#123; return String.format(INSTANCE_PATH_TEMPLATE, rootPath, serviceId, getServiceInstanceId(serviceId, host, port)); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273@Service@ConditionalOnProperty( name = &quot;spring.cloud.zookeeper.enabled&quot;, havingValue = &quot;false&quot;)@Transactional(rollbackFor = Exception.class)@RequiredArgsConstructor(onConstructor = @__(@Autowired))public class MySqlUacRouteServiceImpl implements UacRouteService&lt;RouteDefinition&gt; &#123; private final IScRoutesService iScRoutesService; private final UacRouteLocator uacRouteLocator; @Override public RouteDefinition add(String serviceId, String host, String port) &#123; String serviceInstanceId = getServiceInstanceId(serviceId, host, port); RouteDefinition routeDefinition = createRouteDefinitionByServiceId(serviceId); ScRoutes scRoutes = new ScRoutes(); scRoutes.setAppCode(serviceId); scRoutes.setUri(routeDefinition.getUri().toString()); scRoutes.setInstanceId(serviceInstanceId); scRoutes.setHost(host); scRoutes.setPort(port); scRoutes.setPredicates(JSONUtil.toJsonStr(routeDefinition.getPredicates())); scRoutes.setFilters(JSONUtil.toJsonStr(routeDefinition.getFilters())); iScRoutesService.saveOrUpdate(scRoutes, new UpdateWrapper&lt;ScRoutes&gt;().lambda().eq(ScRoutes::getInstanceId, serviceInstanceId)); uacRouteLocator.refreshRoutes(); return routeDefinition; &#125; @Override public boolean del(String serviceId, String host, String port) &#123; iScRoutesService.remove(Wrappers.lambdaQuery(ScRoutes.class).eq(ScRoutes::getInstanceId, getServiceInstanceId(serviceId, host, port))); uacRouteLocator.refreshRoutes(); return true; &#125; @Override @Transactional(rollbackFor = Exception.class) public boolean update(String serviceId, String host, String port) &#123; del(serviceId, host, port); add(serviceId, host, port); return true; &#125; @Override public List&lt;RouteDefinition&gt; select(String serviceId) &#123; List&lt;ScRoutes&gt; scRoutes = serviceId == null ? iScRoutesService.list() : iScRoutesService.list(Wrappers .lambdaQuery(ScRoutes.class).eq(ScRoutes::getAppCode, serviceId)); return new ArrayList&lt;&gt;(CommonUacRouteService.getInstanceId2RouteByScRoutes(scRoutes).values()); &#125; /** * 动态负载查询 * @return */ @Bean public ServerList&lt;Server&gt; mySqlServerList() &#123; return new ServerList&lt;Server&gt;() &#123; @Override public List&lt;Server&gt; getInitialListOfServers() &#123; return Collections.emptyList(); &#125; @Override public List&lt;Server&gt; getUpdatedListOfServers() &#123; return iScRoutesService.list().stream().map(server -&gt; new Server(server.getHost(), Integer.parseInt(server.getPort()))).collect(Collectors.toList()); &#125; &#125;; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class CommonUacRouteService &#123; private CommonUacRouteService() &#123; &#125; public static Map&lt;String, RouteDefinition&gt; getInstanceId2RouteByInstances (Collection&lt;RegistrationInstance&gt; registrationInstances) &#123; return registrationInstances.stream().collect( Collectors.toMap(RegistrationInstance::getId, registrationInstance -&gt; createRouteDefinitionByServiceId(registrationInstance.getName()), (a, b) -&gt; a)); &#125; public static Map&lt;String, RouteDefinition&gt; getInstanceId2RouteByScRoutes (Collection&lt;ScRoutes&gt; scRoutes) &#123; return scRoutes.stream() .collect(Collectors.toMap(ScRoutes::getInstanceId, scRoute -&gt; &#123; String code = scRoute.getAppCode(); RouteDefinition definition = new RouteDefinition(); definition.setId(code); try &#123; definition.setUri(new URI(scRoute.getUri())); &#125; catch (URISyntaxException e) &#123; e.printStackTrace(); &#125; definition.setPredicates(JSONUtil.toList(scRoute.getPredicates(), PredicateDefinition.class)); definition.setFilters(JSONUtil.toList(scRoute.getFilters(), FilterDefinition.class)); return definition; &#125;, (a, b) -&gt; a)); &#125; public static RouteDefinition createRouteDefinitionByServiceId(String serviceId) &#123; RouteDefinition definition = new RouteDefinition(); definition.setId(serviceId); try &#123; definition.setUri(new URI(&quot;lb://&quot; + serviceId)); &#125; catch (URISyntaxException e) &#123; e.printStackTrace(); &#125; PredicateDefinition predicateDefinition = new PredicateDefinition(); Map&lt;String, String&gt; pArgs = new HashMap&lt;&gt;(1); pArgs.put(&quot;1&quot;, &quot;/iuac/&quot; + serviceId + &quot;_api/**&quot;); predicateDefinition.setName(&quot;Path&quot;); predicateDefinition.setArgs(pArgs); definition.setPredicates(Collections.singletonList(predicateDefinition)); FilterDefinition filterDefinition = new FilterDefinition(); Map&lt;String, String&gt; fArgs = new HashMap&lt;&gt;(1); filterDefinition.setName(&quot;StripPrefix&quot;); fArgs.put(&quot;_genkey_0&quot;, &quot;2&quot;); filterDefinition.setArgs(fArgs); definition.setFilters(Collections.singletonList(filterDefinition)); return definition; &#125;&#125; 2.3 netty心跳 客户端 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114public class HeartbeatClient &#123; private static final Logger LOG = Logger.getLogger(HeartbeatClient.class.toString()); private final String host; private final int port; private final int rate; private final String heartbeatPacket; private static final NioEventLoopGroup LOOP_GROUP = new NioEventLoopGroup(); static &#123; Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; &#123; try &#123; LOOP_GROUP.shutdownGracefully().sync(); &#125; catch (Exception e) &#123; Thread.currentThread().interrupt(); e.printStackTrace(); &#125; &#125;)); &#125; public HeartbeatClient(String host, int port, int rate, String heartbeatPacket) &#123; this.host = host; this.port = port; this.rate = rate; this.heartbeatPacket = heartbeatPacket == null ? &quot;heartbeat&quot; : heartbeatPacket; &#125; public String getHeartbeatPacket() &#123; return heartbeatPacket; &#125; public ChannelFuture connect() &#123; HeartbeatClient connectionClient = this; Bootstrap client = new Bootstrap(); client.group(LOOP_GROUP).channel(NioSocketChannel.class).handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new IdleStateHandler(0, rate, 0)); pipeline.addLast(&quot;encoder&quot;, new StringEncoder()); pipeline.addLast(&quot;decoder&quot;, new StringDecoder()); pipeline.addLast(&quot;handler&quot;, new HeartbeatClientHandler(connectionClient)); &#125; &#125;); return client.connect(host, port).addListener((ChannelFutureListener) future -&gt; &#123; if (!future.isSuccess()) &#123; LOG.warning(&quot;Failed to start gateway heartbeat client, restart after 2 seconds!&quot;); future.channel().eventLoop().schedule(this::connect, 2L, TimeUnit.SECONDS); &#125; &#125;); &#125; public String getHost() &#123; return host; &#125; public int getPort() &#123; return port; &#125;&#125;public class HeartbeatClientHandler extends ChannelInboundHandlerAdapter &#123; private static final Logger LOG = Logger.getLogger(HeartbeatClientHandler.class.toString()); private final HeartbeatClient client; public HeartbeatClientHandler(HeartbeatClient client) &#123; this.client = client; &#125; @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123; super.userEventTriggered(ctx, evt); if (evt instanceof IdleStateEvent) &#123; IdleStateEvent idleStateEvent = (IdleStateEvent) evt; switch (idleStateEvent.state()) &#123; case WRITER_IDLE: ctx.writeAndFlush(client.getHeartbeatPacket()).addListener(ChannelFutureListener.CLOSE_ON_FAILURE); break; case READER_IDLE: case ALL_IDLE: ctx.close(); break; default: &#125; &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; LOG.warning(getServerInfo() + &quot; !==&gt; client channel inactive, reconnection after 1 seconds!&quot;); ctx.channel().eventLoop().schedule(client::connect, 1L, TimeUnit.SECONDS); super.channelInactive(ctx); &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; LOG.info(getServerInfo() + &quot; ===&gt; client channel active!&quot;); ctx.writeAndFlush(client.getHeartbeatPacket()).addListener(ChannelFutureListener.CLOSE_ON_FAILURE); super.channelActive(ctx); &#125; private String getServerInfo() &#123; return client.getHost() + &quot;:&quot; + client.getPort(); &#125;&#125; 服务端 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@Slf4j@Order(1)@Component@RequiredArgsConstructor(onConstructor = @__(@Autowired))public class HeartbeatServer implements CommandLineRunner &#123; private final UacRouteService&lt;RouteDefinition&gt; uacRouteService; @Value(&quot;$&#123;gateway.heartbeat.server.port:60100&#125;&quot;) private int heartbeatServerPort; @Value(&quot;$&#123;gateway.heartbeat.channel.close.timeout-count:3&#125;&quot;) private int heartbeatTimeoutCount; private static final EventLoopGroup BOSS_GROUP = new NioEventLoopGroup(); private static final EventLoopGroup WORK_GROUP = new NioEventLoopGroup(); @Override public void run(String... args) throws Exception &#123; Runtime.getRuntime().addShutdownHook(new Thread(() -&gt; &#123; try &#123; BOSS_GROUP.shutdownGracefully().sync(); WORK_GROUP.shutdownGracefully().sync(); &#125; catch (Exception e) &#123; Thread.currentThread().interrupt(); e.printStackTrace(); &#125; &#125;)); start(); &#125; private ChannelFuture start() &#123; ServerBootstrap server = new ServerBootstrap().group(BOSS_GROUP, WORK_GROUP) .option(ChannelOption.SO_BACKLOG, 128) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel socketChannel) &#123; ChannelPipeline pipeline = socketChannel.pipeline(); pipeline.addLast(new IdleStateHandler(4, 0, 0)); pipeline.addLast(&quot;encoder&quot;, new StringEncoder()); pipeline.addLast(&quot;decoder&quot;, new StringDecoder()); pipeline.addLast(&quot;handler&quot;, new HeartbeatServerHandler(heartbeatTimeoutCount, uacRouteService)); &#125; &#125;); ChannelFuture channelFuture = server.bind(heartbeatServerPort); channelFuture.addListener((ChannelFutureListener) future -&gt; &#123; if (!future.isSuccess()) &#123; log.warn(&quot;Failed to start gateway heartbeat server, start reconnection after 3 seconds!&quot;); channelFuture.channel().eventLoop().schedule(this::start, 3L, TimeUnit.SECONDS); &#125; &#125;); return channelFuture; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980@Slf4jpublic class HeartbeatServerHandler extends ChannelInboundHandlerAdapter &#123; private int currentIdleCount = 0; private final int idleCloseCount; private final UacRouteService&lt;RouteDefinition&gt; uacRouteService; private static final int INSTANCE_INFO_LENGTH = 3; private static final String INSTANCE_INFO_KEY = &quot;instanceInfo&quot;; /** * @param idleCloseCount 空闲次数(默认3次)，超过该次数，触发close */ public HeartbeatServerHandler(int idleCloseCount, UacRouteService&lt;RouteDefinition&gt; uacRouteService) &#123; this.idleCloseCount = idleCloseCount; this.uacRouteService = uacRouteService; &#125; @Override public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123; super.userEventTriggered(ctx, evt); if (evt instanceof IdleStateEvent) &#123; IdleStateEvent idleStateEvent = (IdleStateEvent) evt; switch (idleStateEvent.state()) &#123; case READER_IDLE: currentIdleCount++; if (currentIdleCount &gt; idleCloseCount) &#123; log.warn(JSONUtil.toJsonStr(getInstanceInfo(ctx)) + &quot;!==&gt; server channel idle!&quot;); ctx.close(); &#125; break; case WRITER_IDLE: case ALL_IDLE: ctx.close(); break; default: &#125; &#125; &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; super.channelRead(ctx, msg); currentIdleCount = 0; String[] instanceInfo = String.valueOf(msg).split(&quot;&amp;&quot;); if (instanceInfo.length == INSTANCE_INFO_LENGTH &amp;&amp; !ctx.channel().hasAttr(AttributeKey.valueOf(INSTANCE_INFO_KEY))) &#123; ctx.channel().attr(AttributeKey.valueOf(INSTANCE_INFO_KEY)).set(instanceInfo); uacRouteService.add(instanceInfo[0], instanceInfo[1], instanceInfo[2]); log.info(JSONUtil.toJsonStr(instanceInfo) + &quot; ===&gt; server channel active!&quot;); &#125; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) &#123; cause.printStackTrace(); ctx.close(); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; super.channelUnregistered(ctx); List&lt;String&gt; instanceInfo = getInstanceInfo(ctx); if (CollUtil.isNotEmpty(instanceInfo)) &#123; uacRouteService.del(instanceInfo.get(0), instanceInfo.get(1), instanceInfo.get(2)); log.warn(JSONUtil.toJsonStr(instanceInfo) + &quot; !==&gt; server channel inactive!&quot;); &#125; &#125; private List&lt;String&gt; getInstanceInfo(ChannelHandlerContext ctx) &#123; if (ctx.channel().hasAttr(AttributeKey.valueOf(INSTANCE_INFO_KEY))) &#123; Attribute&lt;String[]&gt; instanceInfoKey = ctx.channel().attr(AttributeKey.valueOf(INSTANCE_INFO_KEY)); return Arrays.asList(instanceInfoKey.get()); &#125; return Collections.emptyList(); &#125;&#125;","categories":[{"name":"springCloud","slug":"springCloud","permalink":"http://yoursite.com/categories/springCloud/"}],"tags":[{"name":"路由","slug":"路由","permalink":"http://yoursite.com/tags/%E8%B7%AF%E7%94%B1/"}]},{"title":"Reactor模式","slug":"Reactor模式","date":"2022-01-17T06:57:26.000Z","updated":"2022-11-22T01:27:53.434Z","comments":true,"path":"2022/01/17/Reactor模式/","link":"","permalink":"http://yoursite.com/2022/01/17/Reactor%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"Reactor模式由Reactor线程、Handlers处理器2大角色组成。 Reactor线程职责：负责响应IO事件，并且分发到Handlers处理器。 Handlers处理器：非阻塞的执行月处理逻辑。 同步阻塞：点奶餐，在柜台前等 同步非阻塞：点奶餐，在旁边座位上等，观望奶茶是否好了 异步非阻塞：点奶茶，在旁边座位上等，有点餐提醒设备提醒你奶茶好了。 1、传统的多线程IO 阻塞的处理请求 123456while(true) &#123; // 阻塞，接受连接 socket=accept(); // 读取数据、业务处理、写入结果 handle(socket);&#125; 2、Reactor模式 单Reactor单线程 一个餐厅里只有一个既是前台也是服务员的人，负责接待客人，也负责把客人点的菜下达给厨师。 缺点：一个client的handler阻塞就会导致所有的client的Handler都被阻塞了，也会导致注册事件也无法处理。 场景：在处理Handler比较快速完成时候。 单Reactor多线程 餐厅里有一个前台，多个服务员。前台只负责接待客人，服务员只负责服务客人。 多Reactor多线程 餐厅里有多个前台和多个服务员，前台只负责接待客人，服务员只负责服务客人。 3、Netty Netty同时支持Reactor的单线程、多线程和主从多线程模型，在不同的应用中通过启动参数的配置来启动不同的线程模型。 Reactor单线程 1234private EventLoopGroup group = new NioEventLoopGroup();ServerBootstrap bootstrap = new ServerBootstrap() .group(group) .childHandler(new HeartbeatInitializer()); Reactor多线程 12345private EventLoopGroup boss = new NioEventLoopGroup(1);private EventLoopGroup work = new NioEventLoopGroup();ServerBootstrap bootstrap = new ServerBootstrap() .group(boss,work) .childHandler(new HeartbeatInitializer()); Reactor主从多线程 12345private EventLoopGroup boss = new NioEventLoopGroup();private EventLoopGroup work = new NioEventLoopGroup();ServerBootstrap bootstrap = new ServerBootstrap() .group(boss,work) .childHandler(new HeartbeatInitializer());","categories":[{"name":"Reactor","slug":"Reactor","permalink":"http://yoursite.com/categories/Reactor/"}],"tags":[{"name":"Reactor","slug":"Reactor","permalink":"http://yoursite.com/tags/Reactor/"}]},{"title":"mysql存储引擎","slug":"mysql存储引擎","date":"2022-01-14T01:36:27.000Z","updated":"2022-11-22T01:27:53.702Z","comments":true,"path":"2022/01/14/mysql存储引擎/","link":"","permalink":"http://yoursite.com/2022/01/14/mysql%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/","excerpt":"","text":"原子性(Atomicity) 事务是一个原子操作单元,其对数据的修改,要么全都执行,要么全都不执行。 一个大的事务操作中，有多个子业务操作，要么都成功，要么都失败 一致性(Consistent) 在事务开始和完成时,数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改,以保持数据的完整性;事务结束时,所有的内部数据结构(如B树索引或双向链表)也都必须是正确的。 一个事务操作中，最终的数据结果状态要么都成功，要么都失败 隔离性(Isolation) 数据库系统提供一定的隔离机制,保证事务在不受外部并发操作影响的“独立”环境执行。这意味着事务处理过程中的中间状态对外部是不可见的,反之亦然。 多个事务操作，相互之间互不影响 持久性(Durable) 事务完成之后,它对于数据的修改是永久性的,即使出现系统故障也能够保持。 事务在提交之后，这个是不能在被修改的 事务并发带来的问题：更新丢失（update lost）、脏读（Dirty Read）、不可重复读（Non-Repeatable Read）、幻读（phantom read）。 并发带来的问题 说明 更新丢失（增删改） 整体说明会带来增删改有问题 脏读（读） 读到未提交的修改（强调读到未提交） 不可重复读（读） 多次读取不一致，可能是提交/未提交的数据（较大事务，强调是多次读一条数据） 幻读（读） 多次读取不一致，可能是提交/未提交的数据（较大事务，强调是多次读一批数据） SQL92定义的4个事务隔离级别如下：读未提交（read uncommitted）、可重复读（repeated read）、读已提交（read committed）、串行化（serializable）。 事务隔离级别 mysql实现 读未提交 select不加锁 可重复读（mysql 默认） MVCC实现 读已提交 MVCC实现 串行化 加共享锁 Spring事务隔离级别 spring事务隔离级别以及和MySQL事务的隔离级别是一模一样的。 1、Serializable ：最严格的级别，事务串⾏执⾏，资源消耗最⼤； 2、REPEATABLE READ ：保证了⼀个事务不会修改已经由另⼀个事务读取但未提交（回滚）的数据。避免了“脏读取”和“不可重复读取”的情况，但是带来了更多的性能损失。 3、READ COMMITTED :⼤多数主流数据库的默认事务等级，保证了⼀个事务不会读到另⼀个并⾏事务已修改但未提交的数据，避免了“脏读取”。该级别适⽤于⼤多数系统，在保证性能的情况下，又保证了一定的安全性。 4、Read Uncommitted ：保证了读取过程中不会读取到⾮法数据。 上⾯的解释其实每个定义都有⼀些拗⼝，其中涉及到⼏个术语：脏读、不可重复读、幻读。 这⾥解释⼀下： 脏读 :所谓的脏读，其实就是读到了别的事务回滚前的脏数据。⽐如事务B执⾏过程中修改了数据X，在未提交前，事务A读取了X，⽽事务B却回滚了，这样事务A就形成了脏读。 不可重复读 ：不可重复读字⾯含义已经很明了了，⽐如事务A⾸先读取了⼀条数据，然后执⾏逻辑的时候，事务B将这条数据改变了，然后事务A再次读取的时候，发现数据不匹配了，就是所谓的不可重复读了。 幻读 ：⼩的时候数⼿指，第⼀次数⼗10个，第⼆次数是11个，怎么回事？产⽣幻觉了？ 幻读也是这样⼦，事务A⾸先根据条件索引得到10条数据，然后事务B改变了数据库⼀条数据，导致也 符合事务A当时的搜索条件，这样事务A再次搜索发现有11条数据了，就产⽣了幻读。 Spring事务传播机制及应用场景 1） PROPAGATION_REQUIRED ，Spring默认的事务传播级别，使⽤该级别的特点是，如果上下⽂中已经存在事务，那么就加⼊到事务中执⾏，如果当前上下⽂中不存在事务，则新建事务执⾏。所以这个级别通常能满⾜处理⼤多数的业务场景。 2）PROPAGATION_SUPPORTS ，从字⾯意思就知道，supports，⽀持，该传播级别的特点是，如果上下⽂存在事务，则⽀持事务加⼊事务，如果没有事务，则使⽤⾮事务的⽅式执⾏。所以说，并⾮所有的包在transactionTemplate.execute中的代码都会有事务⽀持。这个通常是⽤来处理那些并⾮原⼦性的⾮核⼼业务逻辑操作。应⽤场景较少。 3）PROPAGATION_MANDATORY ， 该级别的事务要求上下⽂中必须要存在事务，否则就会抛出异常！配置该⽅式的传播级别是有效的控制上下⽂调⽤代码遗漏添加事务控制的保证⼿段。⽐如⼀段代码不能单独被调⽤执⾏，但是⼀旦被调⽤，就必须有事务包含的情况，就可以使⽤这个传播级别。 4）PROPAGATION_REQUIRES_NEW ，从字⾯即可知道，new，每次都要⼀个新事务，该传播级别的特点是，每次都会新建⼀个事务，并且同时将上下⽂中的事务挂起，执⾏当前新建事务完成以后，上下⽂事务恢复再执⾏。 这是⼀个很有⽤的传播级别，举⼀个应⽤场景：现在有⼀个发送100个红包的操作，在发送之前，要做⼀些系统的初始化、验证、数据记录操作，然后发送100封红包，然后再记录发送⽇志，发送⽇志要求100%的准确，如果⽇志不准确，那么整个⽗事务逻辑需要回滚。 怎么处理整个业务需求呢？就是通过这个PROPAGATION_REQUIRES_NEW 级别的事务传播控制就可以完成。发送红包的⼦事务不会直接影响到⽗事务的提交和回滚。 5）PROPAGATION_NOT_SUPPORTED ，这个也可以从字⾯得知，not supported ，不⽀持，当前级别的特点就是上下⽂中存在事务，则挂起事务，执⾏当前逻辑，结束后恢复上下⽂的事务。 这个级别有什么好处？可以帮助你将事务极可能的缩⼩。我们知道⼀个事务越⼤，它存在的⻛险也就越多。所以在处理事务的过程中，要保证尽可能的缩⼩范围。举⼀个应⽤场景：⽐如⼀段代码，是每次逻辑操作都必须调⽤的，⽐如循环1000次的某个⾮核⼼业务逻辑操作。这样的代码如果包在事务中，势必造成事务太⼤，导致出现⼀些难以考虑周全的异常情况。所以这个事务这个级别的传播级别就派上⽤场了。⽤当前级别的事务模板抱起来就可以了。 6）PROPAGATION_NEVER ，该事务更严格，上⾯⼀个事务传播级别只是不⽀持⽽已，有事务就挂起，⽽PROPAGATION_NEVER传播级别要求上下⽂中不能存在事务，⼀旦有事务，就抛出runtime异 常，强制停⽌执⾏！这个级别上辈⼦跟事务有仇。 7）PROPAGATION_NESTED ，字⾯也可知道，nested，嵌套级别事务。该传播级别特征是，如果上下⽂中存在事务，则嵌套事务执⾏，如果不存在事务，则新建事务。 Spring事务传播和隔离级别配置 @Transactional(propagation=Propagation.REQUIRED,rollbackFor=Exception.class,timeout=1,isolation=Isolation.DEFAULT) 事务的传播性：@Transactional(propagation=Propagation.REQUIRED) 如果有事务, 那么加入事务, 没有的话新建一个(默认情况下) 事务的超时性：@Transactional(timeout=30) //默认是30秒 事务的隔离级别：@Transactional(isolation = Isolation.READ_UNCOMMITTED) 回滚指定异常类：@Transactional(rollbackFor={RuntimeException.class,Exception.class}) 只读：@Transactional(readOnly=true)该属性用于设置当前事务是否为只读事务，设置为true表示只读 1、InnoDB InnoDB 存储引擎是第三方公司开发的，InnoDB 牺牲了存储和查 询的效率 ， 支持事务安全 ，支持自动增长列。 1.1 支持的事务 read-uncommitted、repeated-read、read-committed、serializable。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"mysql系统语句","slug":"mysql系统语句","date":"2022-01-14T01:31:28.000Z","updated":"2022-11-22T01:27:53.702Z","comments":true,"path":"2022/01/14/mysql系统语句/","link":"","permalink":"http://yoursite.com/2022/01/14/mysql%E7%B3%BB%E7%BB%9F%E8%AF%AD%E5%8F%A5/","excerpt":"","text":"当前mysq支持的存储引擎 1show engines \\g;","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"greenplum常用sql","slug":"greenplum常用sql","date":"2022-01-12T02:26:36.000Z","updated":"2022-11-22T01:27:53.620Z","comments":true,"path":"2022/01/12/greenplum常用sql/","link":"","permalink":"http://yoursite.com/2022/01/12/greenplum%E5%B8%B8%E7%94%A8sql/","excerpt":"","text":"一、查看schema下所有表的分布键信息 1234567891011121314151617181920212223242526272829303132SELECTaaa.nspname AS &quot;模式名&quot;,aaa.relname AS &quot;表名&quot;,aaa.table_comment AS &quot;中文表名&quot;,ccc.attname AS &quot;分布键&quot;FROM ( SELECT aa.oid, obj_description (aa.oid) AS table_comment, aa.relname, bb.localoid, bb.attrnums, regexp_split_to_table ( array_to_string (bb.attrnums, &#x27;,&#x27;), &#x27;,&#x27; ) att, dd.nspname FROM pg_class aa --原数据信息 最重要的表！ LEFT JOIN gp_distribution_policy bb ON bb.localoid = aa.oid --分布键表LEFT JOIN pg_namespace dd ON dd.oid = aa.relnamespace --模式LEFT JOIN pg_inherits hh ON aa.oid = hh.inhrelid --继承表 WHERE dd.nspname = &#x27;public&#x27; -- 替换成需要的模式AND hh.inhrelid IS NULL ) aaaLEFT JOIN pg_attribute ccc ON ccc.attrelid = aaa.oidAND ccc.attnum = aaa.attWHEREccc.attnum &gt; 0ORDER BYaaa.relname; 二、 数据库运行状态查询管理 greenplum查询正在运行的sql，session 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162-- 方法1：SELECT tt.procpid, -- pid usename user_name, -- 执行的用户 backend_start, -- 会话开始时间 query_start, -- 查询开始时间 waiting, -- 是否等待执行 now() - query_start AS current_query_time, -- 累计执行时间 now() - backend_start AS current_session_time, current_query, client_addr , datnameFROM pg_stat_activity tt WHERE current_query != &#x27;&lt;IDLE&gt;&#x27;ORDER BY current_query_time DESC;-- 方法2（通过视图查）SELECT procpid, START, now() - START AS lap, current_query, -- count() over() count_num, t2.rolname,t3.rsqname, ipFROM ( SELECT backendid, pg_stat_get_backend_userid(S.backendid) as uid, pg_stat_get_backend_client_addr(S.backendid) as ip, pg_stat_get_backend_pid (S.backendid) AS procpid, pg_stat_get_backend_activity_start (S.backendid) AS START, pg_stat_get_backend_activity (S.backendid) AS current_query FROM ( SELECT pg_stat_get_backend_idset () AS backendid ) AS S ) AS t1 left join pg_authid t2 on t1.uid=t2.oid left join pg_resqueue t3 on t2.rolresqueue=t3.oidWHERE current_query!= &#x27;&lt;IDLE&gt;&#x27;ORDER BY lap DESC;-- 方法3（限定了角色和资源队列，查当前账号正在查询的语句）SELECT rolname, rsqname, pid, GRANTED, current_query, datnameFROM pg_roles t1, gp_toolkit.gp_resqueue_status t2 , pg_locks t3 , pg_stat_activity t4 WHERE t1.rolresqueue = t3.objidAND t3.objid=t2.queueidand t4.procpid=t3.pid 终止执行的sql 12# 根据上述的方法3查出运行sql的pidselect pg_terminate_backend(pid); 查看greemplum资源队列状态 1SELECT * FROM gp_toolkit.gp_resqueue_status; 查看greemplum资源队列锁 1SELECT * FROM gp_toolkit.gp_locks_on_resqueue WHERE lorwaiting=&#x27;true&#x27;; 查看greemplum资源队列优先级 1select * from gp_toolkit.gp_resq_priority_statement; 查看greemplum所有连接 类似mysql SHOW PROCESSLIST 12# 所有状态的连接select * from pg_stat_activity; greemplum磁盘使用，通过SQL查看Greenplum中用了多少空间 1select datname,pg_size_pretty(pg_database_size(datname)) from pg_database; 查看greemplum节点状态 12select * from gp_segment_configuration tt; -- 所有节点select * from gp_segment_configuration tt where tt.status=&#x27;d&#x27;; --所有状态为down的接待你 节点故障等历史信息 1select * from gp_configuration_history tt order by 1 desc ; 数据倾斜 123456789101112131415SELECT t1.gp_segment_id, t1.count_tatol, round(t1.count_tatol-(AVG(t1.count_tatol) over()) ,0)FROM ( SELECT gp_segment_id, COUNT (*) count_tatol FROM adm_channel GROUP BY gp_segment_id ) t1 order by 3 greemplum表或索引大小 （占用空间） 12select pg_size_pretty(pg_relation_size(&#x27;gp_test&#x27;));select pg_size_pretty(pg_total_relation_size(&#x27;gp_test&#x27;)); greemplum查看指定数据库大小（占用空间） 1select pg_size_pretty(pg_database_size(&#x27;postgres&#x27;)); greemplum所有数据库大小（占用空间） 1select datname,pg_size_pretty(pg_database_size(datname)) from pg_database; 查看表greemplum数据分布情况 1select gp_segment_id,count(*) from 表名字 group by gp_segment_id order by 1; 三、查源数据 查看greemplum数据表更新时间 12345678SELECT *FROM pg_stat_last_operation, pg_classWHERE objid = oidAND relname =&#x27;表名&#x27;; 通过sql 获取greemplum指定表分布键 12345SELECT string_agg(att.attname,&#x27;,&#x27; order by attrnums) as distribution FROM gp_distribution_policy a,pg_attribute att WHERE a.localoid =&#x27;public.adm_channel&#x27;::regclass and a.localoid = att.attrelid and att.attnum = any(a.attrnums); 通过sql获取 greemplum指定表字段和字段类型 12345678910111213 SELECT attname, typname FROM pg_attribute INNER JOIN pg_class ON pg_attribute.attrelid = pg_class.oid INNER JOIN pg_type ON pg_attribute.atttypid = pg_type.oid INNER JOIN pg_namespace on pg_class.relnamespace=pg_namespace.oid -- WHERE pg_attribute.attnum &gt; 0 AND attisdropped &lt;&gt; &#x27;t&#x27; AND pg_namespace.nspname=&#x27;模式名&#x27; AND pg_class.relname= &#x27;表名字&#x27;order by pg_attribute.attnum 显示哪些没有统计信息且可能需要ANALYZE的表 1SELECT * from gp_toolkit.gp_stats_missing ; 显示在系统表中被标记为掉线的Segment的信息 1SELECT * from gp_toolkit.gp_pgdatabase_invalid;","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"greenplum","slug":"greenplum","permalink":"http://yoursite.com/tags/greenplum/"}]},{"title":"dockerfile","slug":"Dockerfile","date":"2022-01-11T01:57:20.000Z","updated":"2022-11-22T01:27:53.354Z","comments":true,"path":"2022/01/11/Dockerfile/","link":"","permalink":"http://yoursite.com/2022/01/11/Dockerfile/","excerpt":"","text":"Dockerfile是一个文本格式的配置文件，用户可以使用 Dockerfile来快速创建自定义的镜像。 1、结构 Dockerfile文件主体由4部分组成：基础镜像信息、维护者信息、镜像操作指令、容器启动执行指令。 12345678910111213141516# 格式基础镜像信息 FROM维护者信息 MAINTAINER镜像操作指令 RUN、COPY、ADD、EXPOSE、WORKDIR、ONBUILD、USER、VOLUME、ENV等容器启动时执行指令 CMD、ENTRYPOINT# 基础镜像信息FROM ubuntu:xeniel# 维护者信息LABEL maintainer docker user&lt;docker user@email.com&gt;# 镜像操作指令RUN echo &quot;deb http://archive.ubuntu.com/ubuntu/ xeniel main universe &quot; &gt;&gt; / etc/apt/sources.listRUN apt-get update &amp;&amp; apt-get install -y nginx RUN echo &quot;\\ndaemon off&quot; &gt;&gt; /etc/nginx/nginx.conf# 容器启动执行指令CMD /usr/sbin/nginx 2、指令说明 FROM：从哪里导入 USER：用什么用户起 ENV：设置环境变量 RUN： 修改时区成中国时区’Asia/Shanghai’ WORKDIR：指定工作目录，这里指定的是之前ENV指定的WWW 目录，即是/usr/share/nginx/html ADD：添加指定的东西进去，可以是远程url &amp; 会解压缩 EXPOSE：暴露端口 CMD：指令的首要目的在于为启动的容器指定默认要运行的程序，程序运行结束，容器也就结束 分类 指令 说明 配置指令 ARG 定义创建过程中使用的变量 配置指令 FROM 指定所创建镜像的基础镜像 配置指令 LABEL 为生成的镜像添加运输局标签信息 配置指令 EXPOSE 声明镜像内服务监听的端口 配置指令 ENV 指定环境变量 配置指令 ENTRYPOINT 指定镜像的默认入口命令 配置指令 VOLUME 创建一个数据卷挂载点 配置指令 USER 指定运行容器时用户名或者UID 配置指令 WORKDIR 指定工作目录 配置指令 ONBUILD 创建子镜像时指定自动执行的操作命令 配置指令 STOPSIGNAL 指定退出的信号值 配置指令 HEALTHCHECK 配置所启动容器如何进行健康检查 配置指令 SHELL 指定默认shell类型 操作指令 RUN 运行指定命令 操作指令 CMD 启动容器时指定默认执行的命令 操作指令 ADD 添加内容到镜像，可以是远程url &amp; 会解压缩 操作指令 COPY 复制内容到镜像（常用，不会解压） 2.1 配置指令 ARG 定义创建镜像过程中使用的变量。在执行 docker build时可以通过-build-arg[=]来为变量赋值。 当镜像编译成功后， ARG 指定的变量将不再存在 (ENV 指定的变量将在镜像中保留) 。Docker 内置了一些镜像创建变量，用户可以直接使用而无须声明，包括(不区分大小写) HTTP_PROXY、 HTTPS_PROXY、 FTP_PROXY、 NO_PROXY。 1234# 格式ARG &lt;name&gt;[=&lt;default value&gt;]# 例子ARG VERSION=9.3 FROM 指定所创建镜像的基础镜像 。任何 Dockerfile 中第一条指令必须为 FROM指令。 并且，如果在同一个 Dockerfile 中创建多个镜像时，可以使用多个 FROM 指令(每个镜像一次) 。为了保证镜像精简，可以选用体积较小的镜像如 Alpin巳或 Debian作为基础镜像。 123456# 格式FROM &lt;image&gt; [AS &lt;name&gt;]FROM &lt;image&gt;:&lt;tag&gt; [AS &lt;name&gt;] FROM &lt;image&gt;®&lt;digest&gt; [AS &lt;name&gt;]# 例子FROM debian:$&#123;VERSION&#125; LABEL LABEL 指令可以为生成的镜像添 加元数据标签信息 。这些信息可以用来辅助过滤出特定镜像 。格式为： 123456# 格式LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ...# 例子LABEL version=&quot;1.0.0&quot;LABEL author=&quot;xxx@github&quot; date=&quot;2022-01-01&quot; LABEL description=&quot;hello&quot; EXPOSE 声明镜像内服务监听的端口。**注意：**该指令只是起到声明作用，并不会自动完成端口映射 。如果要映射端口出来，在启动容器时可以使用 -P(大写) 参数( Docker 主机会自动分配一个宿主 机的临时端口)或- p HOST_PORT:CONTAINER_PORT 参数(具体指定所映射的本地端口) 。 1234# 格式EXPOSE &lt;port&gt; [&lt;port&gt;/&lt;protocol&gt; ... ] # 例子EXPOSE 22 80 8443 ENV 指定环境变量 ，在镜像生成过程中会被后续 RUN指令使用 ，在镜像启动的容器中也会存在。 123456# 格式ENV &lt;key&gt;&lt;value&gt; 或者 ENV&lt;key&gt;=&lt;value&gt;...# 例子ENV APP_VERS工ON=l.0.0ENV APP_HOME=/usr/local/app ENV PATH $PATH:/usr/local/bin 指令指定的环境变量在运行时可以被覆盖掉，如 docker run --env &lt;key&gt;=&lt;value&gt; built_image 。 注意当一条 ENV 指令中同时为多个环境变量赋值并且值也是从环境变量读取时，会为变量都赋值后再更新。 如下面的指令， 123# 最终结果为 key1=value1 key2=value2:ENV key1=value2ENV key1=value1 key2=$&#123;key1) ENTRYPOINT 指定镜像的默认入口命令，该入口命令会在启动容器时作为根命令执行，所有传入值作为该命令的参数 。每个 Dockerfile 中只能有一个 ENTRYPOINT，当指定多个时，只有最后一个起效。 在运行时，可以被-- entrypoint 参数覆盖掉，如docker run --entrypoint。 1234567# 格式ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] // exec 模式的写法，注意需要使用双引号。(常用)ENTRYPOINT command param1 param2 // shell 模式的写法。# 例子ENTRYPOINT [&quot;curl&quot;, &quot;-s&quot;, &quot;https://www.baidu.com&quot;] ENTRYPOINT [&quot;/user/sbin/nginx&quot;]docker run -it image -g &quot;daemo off&quot; --&gt; -g &quot;daemo off&quot; //会被当成参数传递给ENTRYPOINT VOLUME 创建一个数据卷挂载点 。 运行容器时可以从本地主机或其他容器挂载数据卷，一般用来存放数据库和需要保持的数据等。 1VOLUME [&quot;/data&quot;] USER 指定运行容器时的用户名或 UID。使用USER指定用户后，Dockerfile中其后的命令RUN、CMD、ENTRYPOINT都将使用该用户。镜像构建完成后，通过docker run运行容器时，可以通过-u参数来覆盖所指定的用户。 123456789# 格式USER userUSER user:groupUSER uidUSER uid:gidUSER user:gidUSER uid:group# 例子USER www WORKDIR 为后续的 RUN、 CMD、 ENTRYPOINT 指令配置工作目录 。 1234567# 格式WORKDIR /path/to/workdir# 例子（最终路径为/a/b/c ，为了避免出错，推荐WORKDIR指令中只使用绝对路径）WORKDIR /a WORKDIR b WORKDIR c RUN pwd ONBUILD 指定当基于所生成镜像创建子镜像时，自动执行的操作指令。ONBUILD 指令在创建专门用于自动编译、检查等操作的基础镜像时，十分有用 。 123456789101112#格式ONBUILD [INSTRUCTION]# 例子--父镜像parentImageFROM ubuntu...ONBUILD ADD ./var/www...--------------------------------------# 例子--子镜像childImage子镜像childImage FROM parentImage... STOPSIGNAL 指定所创建镜像启动的容器接收退出的信号值 :STOPSIGNAL signal HEALTHCHECK 配置所启动容器如何进行健康检查(如何判断健康与否)，自 Docker1.12开始支持，HEALTHCHECK 只可以出现一次，如果写了多个，只有最后一个生效。当在一个镜像指定了 HEALTHCHECK 指令后，用其启动容器，初始状态会为 starting，在 HEALTHCHECK 指令检查成功后变为 healthy，如果连续一定次数失败，则会变为 unhealthy。 12345678# 格式HEALTHCHECK [OPTIONS] CMD command //根据所执行命令返回值是否为 0 来判断;HEALTHCHECK NONE //禁止基础镜像中的健康检查 。# 例子FROM nginxRUN apt-get update &amp;&amp; apt-get install -y curl &amp;&amp; rm -rf /var/lib/apt/lists/*HEALTHCHECK --interval=5s --timeout=3s \\ CMD curl -fs http://localhost/ || exit 1 OPTIONS为： --interval=&lt;间隔&gt;：两次健康检查的间隔，默认为 30 秒； --timeout=&lt;时长&gt;：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒； --retries=&lt;次数&gt;：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。 SHELL 覆盖命令（CMD ENTRYPOINT等）的shell模式所使用的默认shell。Linux的默认shell是[“/bin/sh”, “-c”]，Windows的是[“cmd”, “/S”, “/C”] 1234# 格式SHELL [&quot;executable&quot; , &quot;parameters&quot;]# 例子SHELL [&quot;/bin/sh&quot;,&quot;-c&quot;] 2.2 操作指令 RUN 123456# 格式RUN &lt;command&gt; (shell模式，在/bin/sh -c 中执行)RUN [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] (使用 exec 执行，不会启动 shell 环境。会被解析为JSON数组，参数必须是双引号)# 例子RUN apt-get update RUN [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo hello&quot;] CMD 指定启动容器时默认执行的命令。每个Dockerfile只能有一条CMD命令。 如果指定了多条命令，只有最后一条会被执行。 如果用户启动容器时候手动指定了运行的命令(作为 run命令的参数)，则会覆盖掉CMD 指定的命令 。 12345678# 格式CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] (exec模式，常用)CMD command param1 param2 (shell模式，在/bin/sh -c 中执行，提供给需要交互的应用)CMD [&quot;param1&quot;,&quot;param2&quot;] (提供给ENTRYPOINT的默认参数)# 例子CMD [&quot;echo&quot;,&quot;hello&quot;] CMD [&quot;/usr/bin/wc&quot;,&quot;--help&quot;]CMD echo &quot;hello&quot;ENTRYPOINT [ &quot;top&quot;,&quot;-b&quot; ] \\ CMD [ &quot;-c&quot; ] ADD COPY的高级版，将主机上的资源复制或加入到容器镜像中。 12345678910111213141516171819# 格式ADD &lt;src&gt; &lt;dest&gt;# 例子ADD test1.txt test1.txtADD test1.txt test1.txt.bakADD test1.txt /mydir/ADD data1 data1ADD data2 data2ADD zip.tar /myzipADD http://xxx/data.tar.xz .src的值* Dockerfile所在目录的一个相对路径(文件或目录)* 一个 tar 文件(自动解压为目录) * 一个远程的URLdest值* 可以是镜像内绝对路径* 对于工作目录( WORK_DIR)的相对路径 * 目标路径目录不存在时，会自动创建 COPY 将主机上的资源复制或加入到容器镜像中（推荐使用）。 123456789101112# 格式COPY &lt;src&gt; &lt;dest&gt;# 例子COPY test.txt .src的值* Dockerfile所在目录的一个相对路径(文件或目录)dest值* 可以是镜像内绝对路径* 对于工作目录( WORK_DIR)的相对路径 * 目标路径目录不存在时，会自动创建 使用当前目录的 Dockerfile 创建镜像，标签为 runoob/ubuntu:v1。 1docker build -t runoob&#x2F;ubuntu:v1 . 使用URL github.com/creack/docker-firefox 的 Dockerfile 创建镜像。 1docker build github.com&#x2F;creack&#x2F;docker-firefox 也可以通过 -f Dockerfile 文件的位置： 1$ docker build -f &#x2F;path&#x2F;to&#x2F;a&#x2F;Dockerfile .","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"Type","slug":"Type","date":"2022-01-04T07:16:03.000Z","updated":"2022-11-22T01:27:53.450Z","comments":true,"path":"2022/01/04/Type/","link":"","permalink":"http://yoursite.com/2022/01/04/Type/","excerpt":"","text":"Type type在java.lang.reflect包中，数据较高级接口。它在JDK中关注的有5个子接口： Class ：普通的Java类 ParameterizedType: 参数化类型，类似 : Foo&lt;T&gt;、List&lt;String&gt;、Map&lt;String,Object&gt;等 GenericArrayType: 泛型数组，类似：T[]、List&lt;Stirng&gt;[]，即含有 T 、U、或者&lt;&gt; TypeVariable: 类型变量，类似：List&lt;T&gt; T中的T WildcardType：通配符类型，类似: 如 `?` ， `? extends Number` ，或 `? super Integer 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465 public class Model&lt;T&gt; &#123; // Class String s; // ParameterizedType List&lt;String&gt; list; // GenericArrayType T[] array; // TypeVariable T t; // WildcardType Class&lt;? extends Number&gt; num; &#125;@Testpublic void theClass() throws NoSuchFieldException, SecurityException &#123; Field f = Model.class.getDeclaredField(&quot;s&quot;); Type type = f.getGenericType(); assertTrue(type instanceof Class&lt;?&gt;); &#125; @Test public void theParameterizedType() throws NoSuchFieldException, SecurityException &#123; Field f = Model.class.getDeclaredField(&quot;list&quot;); Type type = f.getGenericType(); assertTrue(type instanceof ParameterizedType); ParameterizedType parameterizedType = (ParameterizedType) type; assertEquals(String.class, parameterizedType.getActualTypeArguments()[0]); &#125; @Test public void theGenericArrayType() throws NoSuchFieldException, SecurityException &#123; Field f = Model.class.getDeclaredField(&quot;array&quot;); Type type = f.getGenericType(); assertTrue(type instanceof GenericArrayType); &#125; @Test public void theTypeVariable() throws NoSuchFieldException, SecurityException &#123; Field f = Model.class.getDeclaredField(&quot;t&quot;); Type type = f.getGenericType(); assertTrue(type instanceof TypeVariable); TypeVariable typeVariable = (TypeVariable) type; assertEquals(1, typeVariable.getBounds().length); assertEquals(Model.class, typeVariable.getGenericDeclaration()); assertEquals(Object.class, typeVariable.getBounds()[0]); assertEquals(&quot;T&quot;, typeVariable.getTypeName()); &#125; @Test public void theWildcardType() throws NoSuchFieldException, SecurityException &#123; Field f = Model.class.getDeclaredField(&quot;num&quot;); Type type = f.getGenericType(); assertTrue(type instanceof ParameterizedType); ParameterizedType parameterizedType = (ParameterizedType) type; Type subType = parameterizedType.getActualTypeArguments()[0]; assertTrue(subType instanceof WildcardType); WildcardType wildcardType = (WildcardType) subType; assertEquals(&quot;? extends java.lang.Number&quot;, wildcardType.getTypeName()); Type[] upperBounds = wildcardType.getUpperBounds(); Type[] lowerBounds = wildcardType.getLowerBounds(); assertEquals(1, upperBounds.length); assertEquals(0, lowerBounds.length); assertEquals(Number.class, upperBounds[0]); &#125;","categories":[{"name":"反射","slug":"反射","permalink":"http://yoursite.com/categories/%E5%8F%8D%E5%B0%84/"}],"tags":[{"name":"Type","slug":"Type","permalink":"http://yoursite.com/tags/Type/"}]},{"title":"json框架","slug":"json框架","date":"2022-01-04T02:12:25.000Z","updated":"2022-11-22T01:27:53.664Z","comments":true,"path":"2022/01/04/json框架/","link":"","permalink":"http://yoursite.com/2022/01/04/json%E6%A1%86%E6%9E%B6/","excerpt":"","text":"1、fastjson 1.1 依赖 12345 &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.79&lt;/version&gt;&lt;/dependency&gt; 1.2 三大对象 JSON 功能：将Java对象转成JSON字符串；将JSON字符串转成Java对象。 1234567891011User user = new User();user.setId(UUID.randomUUID().getMostSignificantBits());user.setUserName(&quot;fan&quot;);user.setAge(11);// 1 toJSONString(Object object)String userJson = JSON.toJSONString(user);// 2 parseObject(String text, Class&lt;T&gt; clazz)User user1 = JSON.parseObject(userJson, User.class);// 3 parseArray(String text, Class&lt;T&gt; clazz) String jsonStringArray = &quot;[&#123;\\&quot;age\\&quot;:3,\\&quot;userName\\&quot;:\\&quot;kit\\&quot;,\\&quot;id\\&quot;:123456789&#125;,&#123;\\&quot;age\\&quot;:4,\\&quot;userName\\&quot;:\\&quot;jim\\&quot;,\\&quot;id\\&quot;:123456788&#125;]&quot;;List&lt;User&gt; users = JSON.parseArray(jsonStringArray, User.class); JSONObject 功能：JSON对象(JSONObject)中的数据都是以key-value形式出现，它实现了Map接口，使用Map就没多大的区别（因为它底层实际上就是操作Map)，常用的方法： 12345678910111213JSONObject jsonObject = new JSONObject();// 0 toJSONString()String jsonString = jsonObject.toJSONString();// 1 put(k,v)jsonObject.put(&quot;age&quot;, 11);// 2 get(k)Integer age = (Integer) jsonObject.get(&quot;age&quot;);// 3 containsKey(k)boolean exit = jsonObject.containsKey(&quot;age&quot;);// 4 containsValue(k)boolean exitVal = jsonObject.containsValue(11);// 5 remove(k)，返回移除key的valObject remove = jsonObject.remove(&quot;age1&quot;); JSONArray 12345678910111213141516171819List&lt;User&gt; list = new ArrayList&lt;&gt;();User user = new User();user.setId(DtWaveIdWorker.getId());user.setAge(11);user.setUserName(&quot;kit&quot;);list.add(user);User user2 = new User();user2.setId(DtWaveIdWorker.getId());user2.setAge(12);user2.setUserName(&quot;jim&quot;);list.add(user2);// 1 str -&gt; jsonArray | list -&gt; jsonArrayJSONArray jsonArray = JSONArray.parseArray(JSON.toJSONString(list));// 2 jsonArray -&gt; strString jsonString = jsonArray.toJSONString();// 3 jsonArray -&gt; listList&lt;User&gt; userList = JSONObject.parseArray(jsonArray.toJSONString(), User.class);","categories":[{"name":"json","slug":"json","permalink":"http://yoursite.com/categories/json/"}],"tags":[{"name":"json","slug":"json","permalink":"http://yoursite.com/tags/json/"}]},{"title":"greenplum镜像","slug":"greenplum镜像","date":"2021-12-21T07:37:28.000Z","updated":"2022-11-22T01:27:53.637Z","comments":true,"path":"2021/12/21/greenplum镜像/","link":"","permalink":"http://yoursite.com/2021/12/21/greenplum%E9%95%9C%E5%83%8F/","excerpt":"","text":"1、组镜像 默认的镜像方式 每台主机镜像在另外一个主机上 单个主机失效后，后备主机活动主segment数量翻倍 2、散布镜像 初始化集群时候加上参数- S，gpinitsystem -c gpinitsystem_config -h seg_hosts -S 每台主机镜像分散在剩下的主机上 每台主机的镜像数&lt;主机数时候选择使用 3、自定义镜像 不是gp提供的默认镜像扩展方式，初始化时候使用组镜像初始化，后续使用gpmovemirrors自定义镜像位置。 查看当前主segment和镜像segment的位置 1234567891011121314151617181920212223242526272829303132333435SELECT dbid, content, address, port, replication_port, fselocation as datadir FROM gp_segment_configuration, pg_filespace_entry WHERE dbid=fsedbid AND content &gt; -1 ORDER BY dbid; dbid | content | address | port | replication_port | datadir------+---------+---------+-------+------------------+------------------------------- 2 | 0 | seg1 | 33000 | 36000 | /data/gpdata1/gpdatap1/gpseg0 3 | 1 | seg1 | 33001 | 36001 | /data/gpdata1/gpdatap2/gpseg1 4 | 2 | seg2 | 33000 | 36000 | /data/gpdata1/gpdatap1/gpseg2 5 | 3 | seg2 | 33001 | 36001 | /data/gpdata1/gpdatap2/gpseg3 6 | 4 | seg3 | 33000 | 36000 | /data/gpdata1/gpdatap1/gpseg4 7 | 5 | seg3 | 33001 | 36001 | /data/gpdata1/gpdatap2/gpseg5 8 | 6 | seg4 | 33000 | 36000 | /data/gpdata1/gpdatap1/gpseg6 9 | 7 | seg4 | 33001 | 36001 | /data/gpdata1/gpdatap2/gpseg7 10 | 0 | seg2 | 34000 | 35000 | /data/gpdata1/gpdatam1/gpseg0 11 | 1 | seg2 | 34001 | 35001 | /data/gpdata1/gpdatam2/gpseg1 12 | 2 | seg3 | 34000 | 35000 | /data/gpdata1/gpdatam1/gpseg2 13 | 3 | seg3 | 34001 | 35001 | /data/gpdata1/gpdatam2/gpseg3 14 | 4 | seg4 | 34000 | 35000 | /data/gpdata1/gpdatam1/gpseg4 15 | 5 | seg4 | 34001 | 35001 | /data/gpdata1/gpdatam2/gpseg5 16 | 6 | seg1 | 34000 | 35000 | /data/gpdata1/gpdatam1/gpseg6 17 | 7 | seg1 | 34001 | 35001 | /data/gpdata1/gpdatam2/gpseg7(16 rows)# 主seg和镜像seg对应关系其中dbid=2 和dbid=10其中dbid=3 和dbid=11其中dbid=4 和dbid=12其中dbid=5 和dbid=13其中dbid=6 和dbid=14其中dbid=7 和dbid=15其中dbid=8 和dbid=16其中dbid=9 和dbid=17 镜像移动文件 1234567# mirror_config_file 文件内容格式filespaceOrder=[filespace1_fsname[:filespace2_fsname:...]old_address:port:fselocation new_address:port:replication_port:fselocation[:fselocation:...]# 示例（dbid=10 移到seg3； dbid=11 移到seg4上； port:replication_port不要和已有的重复 ）filespaceOrder=seg2:34000:/data/gpdata1/gpdatam1/gpseg0 seg3:34002:35002:/data/gpdata1/gpdatam1/gpseg0seg2:34001:/data/gpdata1/gpdatam2/gpseg1 seg4:34003:35003:/data/gpdata1/gpdatam1/gpseg1 操作 1gpmovemirrors -i mirror_config_file 查看结果 1234567891011121314151617181920212223postgres=# SELECT dbid, content, address, port,postgres-# replication_port, fselocation as datadirpostgres-# FROM gp_segment_configuration, pg_filespace_entrypostgres-# WHERE dbid=fsedbid AND content &gt; -1postgres-# ORDER BY dbid; dbid | content | address | port | replication_port | datadir------+---------+---------+-------+------------------+------------------------------- 2 | 0 | seg1 | 33000 | 36000 | /data/gpdata1/gpdatap1/gpseg0 3 | 1 | seg1 | 33001 | 36001 | /data/gpdata1/gpdatap2/gpseg1 4 | 2 | seg2 | 33000 | 36000 | /data/gpdata1/gpdatap1/gpseg2 5 | 3 | seg2 | 33001 | 36001 | /data/gpdata1/gpdatap2/gpseg3 6 | 4 | seg3 | 33000 | 36000 | /data/gpdata1/gpdatap1/gpseg4 7 | 5 | seg3 | 33001 | 36001 | /data/gpdata1/gpdatap2/gpseg5 8 | 6 | seg4 | 33000 | 36000 | /data/gpdata1/gpdatap1/gpseg6 9 | 7 | seg4 | 33001 | 36001 | /data/gpdata1/gpdatap2/gpseg7 10 | 0 | seg3 | 34002 | 35002 | /data/gpdata1/gpdatam1/gpseg0 ----&gt;移动后 11 | 1 | seg4 | 34003 | 35003 | /data/gpdata1/gpdatam1/gpseg1 ----&gt;移动后 12 | 2 | seg3 | 34000 | 35000 | /data/gpdata1/gpdatam1/gpseg2 13 | 3 | seg3 | 34001 | 35001 | /data/gpdata1/gpdatam2/gpseg3 14 | 4 | seg4 | 34000 | 35000 | /data/gpdata1/gpdatam1/gpseg4 15 | 5 | seg4 | 34001 | 35001 | /data/gpdata1/gpdatam2/gpseg5 16 | 6 | seg1 | 34000 | 35000 | /data/gpdata1/gpdatam1/gpseg6 17 | 7 | seg1 | 34001 | 35001 | /data/gpdata1/gpdatam2/gpseg7 4、为集群添加镜像 4.1 初始化时候添加 打开配置文件里如下属性 1234567891011# vim gpinitsystem_config...MIRROR_PORT_BASE=43000REPLICATION_PORT_BASE=41000MIRROR_REPLICATION_PORT_BASE=51000# 一个主机上设置几个mirror就填写几个，注意和主机上的primary个数要一样declare -a MIRROR_DATA_DIRECTORY=(/gpdata/mir1 /gpdata/mir2)... 初始化 12# 加上参数 -S 就初始化为散布镜像gpinitsystem -c gpinitsystem_config -h seg_hosts [-S] 4.2 初始化后添加 1234567891011# 1 生成镜像文件gpaddmirrors -o ./addmirrorscat addmirrors# 2 看看文件里内容，自动生成的filespaceOrder=mirror0=0:sdw1-1:52001:53001:54001:/gpdata/mir1/gp0mirror1=1:sdw1-2:52002:53002:54002:/gpdata/mir2/gp1mirror2=2:sdw2-1:52001:53001:54001:/gpdata/mir1/gp2mirror3=3:sdw2-2:52002:53002:54002:/gpdata/mir2/gp3# 执行添加镜像gpaddmirrors -i addmirrors","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"greenplum","slug":"greenplum","permalink":"http://yoursite.com/tags/greenplum/"}]},{"title":"greenplum资源队列","slug":"greenplum资源队列","date":"2021-12-20T09:52:29.000Z","updated":"2022-11-22T01:27:53.636Z","comments":true,"path":"2021/12/20/greenplum资源队列/","link":"","permalink":"http://yoursite.com/2021/12/20/greenplum%E8%B5%84%E6%BA%90%E9%98%9F%E5%88%97/","excerpt":"","text":"超级管理员是不受资源队列限制的 1、介绍 在Greenplum 4.x之后的版本中，加入了资源队列的概念。资源负载管理是为了限制系统中活动的SQL对使用资源的消耗，避免由于SQL将系统资源(如CPU、I/O、内存)耗尽而造成 系统缓慢或崩溃。资源队列可以限制活动SQL的个数，以及SQL 各种消耗的大小。所有的数据库 用户都必须对应一个资源队列，如果配置具体的资源队列，默认的资源队列是pg_default。 2、限制 在GP中资源对接可以限制： 活动SQL数量 能够消耗最大内存 SQL优先级 SQL的cost值 2.1 内存 如果一个资源队列中限制了最大使用内存是2000MB，同时设置了同时执行的SQL数为10个，那么每一个SQL最多使用的内存就是200MB。同时，每个SQL消耗的内存，不能大于 statement_mem参数中设置的内存大小。当一个SQL运行的时候， 这个内存大小就会被分配出来，直到SQL执行结束后才释放。 2.2 CPU CPU优先级管理，每一个资源队列中，都有一个对应的CPU 优先级。CPU的优先级有三个等级。 adhoc，低优先级。 reporting，高优先级。 executive，最高优先级。 2.3 受限SQL SELECT、SELECT INTO、CREATE TABLE AS SELECT、 DECLARE CURSOR。如果将参数 resource_select_only设置成off，那么INSERT、UPDATE、 DELETE语句也会被限制在队列中。 3、使用 新建/修改/删除 创建Resource Queue时，ACTIVE_STATEMENTS 和 MAX_COST两个属性中必须指定一个，否则创建无法成功。 1234567891011121314151617# 新建/修改CREATE |ALTER RESOURCE QUEUE name WITH (queue_attribute=value [, ... ])# with 语句可有以下种方式 ACTIVE_STATEMENTS=integer [ MAX_COST=float [COST_OVERCOMMIT=&#123;TRUE|FALSE&#125;] ] [ MIN_COST=float ] [ PRIORITY=&#123;MIN|LOW|MEDIUM|HIGH|MAX&#125; ] [ MEMORY_LIMIT=&#x27;memory_units&#x27; ] MAX_COST=float [ COST_OVERCOMMIT=&#123;TRUE|FALSE&#125; ] [ ACTIVE_STATEMENTS=integer ] [ MIN_COST=float ] [ PRIORITY=&#123;MIN|LOW|MEDIUM|HIGH|MAX&#125; ] [ MEMORY_LIMIT=&#x27;memory_units&#x27; ]# 删除DROP RESOURCE QUEUE name; 属性 说明 ACTIVE_STATEMENTS 用于指定Resource Queue在某个时间点最多的活跃查询（正在执行的查询）数量。 MEMORY_LIMIT 用于指定在单个计算节点（segment）上Resource Queue所有查询最多可以使用的内存。 单位可以为KB、MB、GB。默认值为-1，表示没有限制。 MAX_COST 用于指定Resource Queue查询代价的最大值，默认值为-1，表示没有限制。 说明 这里的查询代价是指云原生数据仓库PostgreSQL版优化器估算出来的查询代价。 COST_OVERCOMMIT 该参数需要设置MAX_COST参数。 当COST_OVERCOMMIT为true，查询代价大于MAX_COST的查询可以在系统空闲的时候执行。当COST_OVERCOMMIT为false，查询代价大于MAX_COST的查询将会被拒绝执行。 MIN_COST 用于指定Resource Queue最小的查询代价，当查询代价小于MIN_COST的查询，将不会排队等待而是会立即被执行。 PRIORITY 用于指定Resource Queue的优先级，优先级高的查询将会被分配更多的CPU资源用于执行。 MINLOWMEDIUMHIGHMAX默认值为MEDIUM。 新建 123456789101112// 限制同一时刻最大的活动SQL数=3CREATE RESOURCE QUEUE adhoc WITH (ACTIVE_STATEMENTS=3);// 限制同一时刻最大的活动SQL数=3，队列总内存限制2000MBCREATE RESOURCE QUEUE myqueue WITH (ACTIVE_STATEMENTS=20,MEMORY_LIMIT=&#x27;2000MB&#x27;);// 对单个sql单独处理内存SET statement_mem=&#x27;2GB&#x27;;SELECT * FROM my_big_table WHERE column=&#x27;value&#x27; ORDER BY id;RESET statement_mem;// 设置最大costCREATE RESOURCE QUEUE webuser WITH (MAX_COST=10000.0);// 设置CPU级别（MIN|LOW|MEDIUM|HIGH|MAX）ALTER RESOURCE QUEUE adhoc WITH (PRIORITY=LOW); 查看 1234567891011121314151617// 查看资源队列情况postgres=# select * from pg_resqueue_attributes; rsqname | resname | ressetting | restypid------------+-------------------+------------+---------- pg_default | active_statements | 20 | 1 pg_default | max_cost | -1 | 2 pg_default | min_cost | 0 | 3 pg_default | cost_overcommit | 0 | 4 pg_default | priority | medium | 5 pg_default | memory_limit | -1 | 6 // 查看现有的资源队列使用情况 postgres=# select * from pg_resqueue_status; rsqname | rsqcountlimit | rsqcountvalue | rsqcostlimit | rsqcostvalue | rsqwaiters | rsqholders------------+---------------+---------------+--------------+--------------+------------+------------ pg_default | 20 | 0 | -1 | | 0 | 0(1 row) 用户指定资源队列-创建/修改/删除 1234567891011a) 创建用户 并指定默认资源队create role aquery with login RESOURCE QUEUE adhoc;b）修改角色资源队列alter role etl resource queue adhoc;c) 恢复角色到使用默认的资源队列ALTER ROLE etl RESOURCE QUEUE none; d) 删除角色资源队列alter role etl resource queue none; 其他 12345// 查看角色使用的资源队列postgres=# SELECT rolname, rsqname FROM pg_roles, gp_toolkit.gp_resqueue_status WHERE pg_roles.rolresqueue=gp_toolkit.gp_resqueue_status.queueid; rolname | rsqname---------+------------ gpadmin | pg_default","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"greenplum","slug":"greenplum","permalink":"http://yoursite.com/tags/greenplum/"}]},{"title":"Linux挂载磁盘","slug":"linux挂载磁盘","date":"2021-12-17T08:29:27.000Z","updated":"2022-11-22T01:27:53.693Z","comments":true,"path":"2021/12/17/linux挂载磁盘/","link":"","permalink":"http://yoursite.com/2021/12/17/linux%E6%8C%82%E8%BD%BD%E7%A3%81%E7%9B%98/","excerpt":"","text":"注意：挂载操作会清空数据，请确认挂载盘无数据或者未使用，以centos7为操作系统 步骤（5步）：查看–分区–&gt;格式化（某个分区）–&gt;挂载（某个分区）–&gt;开机自挂载（写入/etc/fstab） 1、查看 df -Th 查看已挂载情况 1234567891011$ df -ThFilesystem Type Size Used Avail Use% Mounted ondevtmpfs devtmpfs 7.8G 0 7.8G 0% &#x2F;devtmpfs tmpfs 7.8G 0 7.8G 0% &#x2F;dev&#x2F;shmtmpfs tmpfs 7.8G 17M 7.8G 1% &#x2F;runtmpfs tmpfs 7.8G 0 7.8G 0% &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;dev&#x2F;mapper&#x2F;vg_root-lv_root xfs 50G 18G 32G 37% &#x2F;&#x2F;dev&#x2F;sdb1 xfs 500G 51G 450G 11% &#x2F;data&#x2F;dev&#x2F;sda2 xfs 509M 143M 367M 29% &#x2F;boottmpfs tmpfs 1.6G 0 1.6G 0% &#x2F;run&#x2F;user&#x2F;530tmpfs tmpfs 1.6G 0 1.6G 0% &#x2F;run&#x2F;user&#x2F;531 fdisk -l 查看磁盘（挂载&amp;未挂载） 如果磁盘下面有类似，说明该磁盘未挂载：Disk /dev/sdc doesn’t contain a valid partition table；磁盘下面没有类似于：sdb1 sdb2 123456789101112131415161718$ sudo fdisk -lDisk &#x2F;dev&#x2F;sda: 64.4 GB, 64424509440 bytes255 heads, 63 sectors&#x2F;track, 7832 cylinders, total 125829120 sectorsUnits &#x3D; 扇区 of 1 * 512 &#x3D; 512 bytesSector size (logical&#x2F;physical): 512 bytes &#x2F; 512 bytesI&#x2F;O size (minimum&#x2F;optimal): 512 bytes &#x2F; 512 bytesDisk identifier: 0x00000000 设备 启动 起点 终点 块数 Id 系统&#x2F;dev&#x2F;sda1 1 125829119 62914559+ ee GPTDisk &#x2F;dev&#x2F;sdb: 53.7 GB, 53687091200 bytes255 heads, 63 sectors&#x2F;track, 6527 cylinders, total 104857600 sectorsUnits &#x3D; 扇区 of 1 * 512 &#x3D; 512 bytesSector size (logical&#x2F;physical): 512 bytes &#x2F; 512 bytesI&#x2F;O size (minimum&#x2F;optimal): 512 bytes &#x2F; 512 bytesDisk identifier: 0x00000000Disk &#x2F;dev&#x2F;sdb doesn&#39;t contain a valid partition table &lt;&#x3D;&#x3D;未挂载 lsblk 查看磁盘（挂载&amp;未挂载） MOUNTPOINT 没有数据就是未挂载 12345678910$ lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 50G 0 disk &lt;&#x3D;&#x3D;sda 有3个分区├─sda1 8:1 0 1M 0 part├─sda2 8:2 0 512M 0 part &#x2F;boot└─sda3 8:3 0 49.5G 0 part └─vg_root-lv_root 253:0 0 49.5G 0 lvm &#x2F;sdb 8:16 0 500G 0 disk &lt;&#x3D;&#x3D;sdb 有1个分区└─sdb1 8:17 0 500G 0 part &#x2F;datasr0 11:0 1 1024M 0 rom 2、分区 开始 123456789$ fdisk &#x2F;dev&#x2F;sdb &lt;&#x3D;&#x3D;仔细看，不要加上数字喔！The number of cylinders for this disk is set to 5005.There is nothing wrong with that, but this is larger than 1024,and could in certain setups cause problems with:1) software that runs at boot time (e.g., old versions of LILO)2) booting and partitioning software from other OSs (e.g., DOS FDISK, OS&#x2F;2 FDISK)Command (m for help): &lt;&#x3D;&#x3D;等待你的输入！ 帮助 123456789101112131415161718Command (m for help): m &lt;&#x3D;&#x3D; 输入 m 后，就会看到底下这些命令介绍Command action a toggle a bootable flag b edit bsd disklabel c toggle the dos compatibility flag d delete a partition &lt;&#x3D;&#x3D;删除一个partition l list known partition types m print this menu n add a new partition &lt;&#x3D;&#x3D;新增一个partition o create a new empty DOS partition table p print the partition table &lt;&#x3D;&#x3D;在屏幕上显示分割表 q quit without saving changes &lt;&#x3D;&#x3D;不储存离开fdisk程序 s create a new empty Sun disklabel t change a partition&#39;s system id u change display&#x2F;entry units v verify the partition table w write table to disk and exit &lt;&#x3D;&#x3D;将刚刚的动作写入分割表 x extra functionality (experts only) 查看当前盘分区情况 123456789101112131415Command (m for help): p &lt;&#x3D;&#x3D; 这里可以输出目前磁盘的状态Disk &#x2F;dev&#x2F;hdc: 41.1 GB, 41174138880 bytes &lt;&#x3D;&#x3D;这个磁盘的文件名与容量255 heads, 63 sectors&#x2F;track, 5005 cylinders &lt;&#x3D;&#x3D;磁头、扇区与磁柱大小Units &#x3D; cylinders of 16065 * 512 &#x3D; 8225280 bytes &lt;&#x3D;&#x3D;每个磁柱的大小 Device Boot Start End Blocks Id System&#x2F;dev&#x2F;hdc1 * 1 13 104391 83 Linux&#x2F;dev&#x2F;hdc2 14 1288 10241437+ 83 Linux&#x2F;dev&#x2F;hdc3 1289 1925 5116702+ 83 Linux&#x2F;dev&#x2F;hdc4 1926 5005 24740100 5 Extended&#x2F;dev&#x2F;hdc5 1926 2052 1020096 82 Linux swap &#x2F; Solaris# 装置文件名 启动区否 开始磁柱 结束磁柱 1K大小容量 磁盘分区槽内的系统Command (m for help): q 操作 依次输入 123456789n，回车p，回车1，回车回车w 3、格式化 123mkfs -t xfs &#x2F;dev&#x2F;sdb1或者mkfs.xfs -f &#x2F;dev&#x2F;sdb1 4、挂载&amp;卸载 12$ mount &#x2F;dev&#x2F;sdb1 &#x2F;data$ umount -v &#x2F;dev&#x2F;sdb1 或者 umount -v &#x2F;data 5、开机挂载 1$ echo &quot;&#x2F;dev&#x2F;sdb1 &#x2F;data xfs defaults 0 0&quot; &gt;&gt; &#x2F;etc&#x2F;fstab","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"挂载","slug":"挂载","permalink":"http://yoursite.com/tags/%E6%8C%82%E8%BD%BD/"}]},{"title":"greenplum安装节点计算","slug":"greenplum安装节点计算","date":"2021-12-17T06:32:51.000Z","updated":"2022-11-22T01:27:53.619Z","comments":true,"path":"2021/12/17/greenplum安装节点计算/","link":"","permalink":"http://yoursite.com/2021/12/17/greenplum%E5%AE%89%E8%A3%85%E8%8A%82%E7%82%B9%E8%AE%A1%E7%AE%97/","excerpt":"","text":"参考：https://gp-docs-cn.github.io/docs/best_practices/sysconfig.html 配置修改后请：gpstop -r 1、文件系统格式 XFS，使用挂载项：rw,noatime,inode64 123456789101112131415# 演示示例# 格式化为xfsmkfs.xfs /dev/dev_nameN# 挂载mount -o rw,nodev,nobarrier,noatime,inode64,allocsize=16M -t xfs /dev/dev_nameN /target# 开机挂载/dev/dev_nameN /target xfs rw,nodev,nobarrier,noatime,inode64,allocsize=16M 0 0# 查看挂载详情[gpadmin@master ~]$ tail -n 5 /etc/mtabmqueue /dev/mqueue mqueue rw,relatime 0 0/dev/sdb1 /data xfs rw,noatime,attr2,inode64,allocsize=16384k,noquota 0 0/dev/sda2 /boot xfs rw,relatime,attr2,inode64,noquota 0 0tmpfs /run/user/530 tmpfs rw,nosuid,nodev,relatime,size=1626600k,mode=700,uid=530,gid=530 0 0tmpfs /run/user/531 tmpfs rw,nosuid,nodev,relatime,size=1626600k,mode=700,uid=531,gid=531 0 0 2、 端口 注意：若部署多个gp在机器上，请修改gpinitsystem_config里面的port为不同。 123# 初始化文件（gpinitsystem_config）里的各种port应在下面的区间里[gp5admin@master conf]$ cat /proc/sys/net/ipv4/ip_local_port_range1025 65535 3、I/O配置 12345678910111213#在含有数据目录的设备上，blockdev预读尺寸应该被设置为16384。[gp5admin@master conf]$ sudo /sbin/blockdev --getra /dev/sdb16384#应该为所有数据目录设备设置死线IO调度器。[gp5admin@master conf]$ sudo cat /sys/block/sdb/queue/schedulernoop [deadline] cfq#应该在/etc/security/limits.conf文件中增加OS文件和进程的最大数量。[gp5admin@master conf]$ tail -6f /etc/security/limits.conf* soft nofile 65536* soft core unlimited* hard nofile 65536* soft nproc 131072* hard nproc 131072 4、 内存设置 前置介绍：vm.overcommit_memory &amp; vm.overcommit_ratio 12345678内核对内存分配的一种策略，可取值有0 1 2 如果设置为0，申请的内存无法满足时（根据内部算法），则会触发OOM。如果设置为1，申请的内存无法满足时（根据内部算法），部分会触发OOM，部分触发重启。如果设置为2，申请的内存无法满足时，则禁止分配。那阈值是多少，由内部算法决定。它是通过内核参数vm.overcommit_ratio或vm.overcommit_kbytes间接设置的，公式如下：CommitLimit = (RAM * vm.overcommit_ratio / 100) + Swapovercommit_ratio默认为50，如有特殊需求，可以自己修改。公式如下：vm.overcommit_ratio = (RAM - 0.026 * gp_vmem) / RAM &lt;=====gp_vmem 下面介绍 4.1 OS内存和共享内存配置 1234567[gp5admin@master conf]$ vim /etc/sysctl.confkernel.shmmax = 500000000kernel.shmmni = 4096kernel.shmall = 4000000000vm.overcommit_memory = 2vm.overcommit_ratio = 95... 4.2 检查配置是否合理 1234567[gp5admin@master conf]$ gpcheck -h seg1...20211217:15:08:32:016762 gpcheck:master:gp5admin-[ERROR]:-GPCHECK_ERROR host(None): utility will not check all settings when run as non-root user20211217:15:08:32:016762 gpcheck:master:gp5admin-[ERROR]:-GPCHECK_ERROR host(seg1): XFS filesystem on device /dev/sdb1 has 5 XFS mount options and 4 are expected20211217:15:08:32:016762 gpcheck:master:gp5admin-[ERROR]:-GPCHECK_ERROR host(seg1): XFS filesystem on device /dev/sdb1 is missing the recommended mount option &#x27;allocsize=16m&#x27;...20211217:15:08:32:016762 gpcheck:master:gp5admin-[INFO]:-gpcheck completing... 4.3 seg节点可用主机内存（gp_vmem） 12# SWAP和RAM单位（GB）gp_vmem = ((SWAP + RAM) – (7.5GB + 0.05 * RAM)) / 1.7 4.4 最大活跃主segment个数（max_acting_primary_segments） 12344个seg主机1个seg主机上有：8 primary 8 mirror，失效一个seg主机：若mirror为group方式：单主最大运行的primary个数为16若mirror为spread方式：单主最大运行的primary个数为11 4.5 单个segment，所有活动可以消耗的内存（gp_vmem_protect_limit） 查看 12345[gpadmin@master ~]$ gpconfig -s gp_vmem_protect_limitValues on all segments are consistentGUC : gp_vmem_protect_limitMaster value: 8192Segment value: 8192 计算 1gp_vmem_protect_limit = gp_vmem / max_acting_primary_segments 设置 1gpconfig -c gp_vmem_protect_limit -v 46933 4.6 失控查询终止（runaway_detector_activation_percent） 失控查询终止在Greenplum数据库4.3.4中被引入，它能防止内存不足的问题。runaway_detector_activation_percent系统参数控制触发查询终止的gp_vmem_protect_limit内存利用率。默认它被设置为90%。 查看 12345[gpadmin@master ~]$ gpconfig -s runaway_detector_activation_percentValues on all segments are consistentGUC : runaway_detector_activation_percentMaster value: 90Segment value: 90 设置 1gpconfig -c runaway_detector_activation_percent -v 50 4.7 单个segment，查询语句可以消耗的内存（statement_mem） 查看 12345[gpadmin@master ~]$ gpconfig -s statement_memValues on all segments are consistentGUC : statement_memMaster value: 125MBSegment value: 125MB 计算 123gp_statement_mem=(gp_vmem_protect_limit * 0.9) / max_expected_concurrent_queries# 如果gp_vmem_protect_limit被设置为8GB（8192MB），对于40个并发查询：gp_statement_mem=(8192MB * 0.9) / 40 = 184MB 设置 1gpconfig -c statement_mem -v 1408MB 4.8 单个查询允许使用的临时溢出文件（工作文件）的最大数量（gp_workfile_limit_files_per_query） 不建议设置，10万个够大，真出现不足，优先优化sql。 查看 12345[gpadmin@master ~]$ gpconfig -s gp_workfile_limit_files_per_queryValues on all segments are consistentGUC : gp_workfile_limit_files_per_queryMaster value: 100000Segment value: 100000 设置 1gpconfig -c gp_workfile_limit_files_per_query -v 200000 4.9 压缩溢出文件算法（gp_workfile_compress_algorithm） 查看 12345[gpadmin@master ~]$ gpconfig -s gp_workfile_compress_algorithmValues on all segments are consistentGUC : gp_workfile_compress_algorithmMaster value: noneSegment value: none 设置 12可以设置2个值：none 默认值，表示不用压缩选项；zlib 使用zlib算法压缩gpconfig -c gp_workfile_compress_algorithm -v zlib 5 、资源队列 在GP4.x中加入了资源队列。主要是限制用户和单个sql对资源的消耗。 5.1 条件和建议 只有superuser才有资格创建资源队列 队列名捕不能是none ACTIVE_STATEMENTS 与 MAX_COST 必须二选一，不能都设置为-1。 官方建议使用MEMORY_LIMIT 和ACTIVE_STATEMENTS 来替代MAX_COST 如果队列中未设置MEMORY_LIMIT，则每个查询可用的内存值为系统参数statement_mem的值，最大可用内存为statement_mem /ACTIVE_STATEMENTS 并不是所有语句都受资源队列限制，默认情况下，只有SELECT, SELECT INTO, CREATE TABLE AS SELECT, 和DECLARE CURSOR受限，如果配置参数resource_select_only = off，则INSERT, UPDATE,DELETE语句也会受限 如果没有设置max_cost,那么每个语句使用的内存是MEMORY_LIMIT/ACTIVE_STATEMENTS,如果设置了max_cost,内存是MEMORY_LIMIT*(query_cost/max_cost),query_cost为实际SQL的cost 5.2 语法 12345678CREATE RESOURCE QUEUE name WITH (queue_attribute=value [, ... ]) # 后面参数说明 ACTIVE_STATEMENTS=integer [ MAX_COST=float [COST_OVERCOMMIT=&#123;TRUE|FALSE&#125;] ] -- [ MIN_COST=float ] -- [ PRIORITY=&#123;MIN|LOW|MEDIUM|HIGH|MAX&#125; ] -- [ MEMORY_LIMIT=&#x27;memory_units&#x27; ] -- ACTIVE_STATEMENTS：指运行同时运行的SQL数量，超过该数量的请求将会排队等待，默认为-1，表示不受限制。 MAX_COST：该队列允许运行的单个SQL最大COST值，默认为-1，表示不受限制。 COST_OVERCOMMIT：Greenplum苹果出的sql的cost大于MAX_COST采取策略（true默认：在空闲时候允许；限制运行false） MIN_COST：低于该COST值的SQL直接运行，不受队列资源的限制，默认为0。 PRIORITY：队列中任务分配CPU资源的优先级，默认为MEDIUM。 MEMORY_LIMIT：资源队列内存限制大小，单位为kB，MB，GB。默认为-1，表示不受限制。 5.3 实践 新建 12create resource queue prod_queue with (ACTIVE_STATEMENTS=100,MEMORY_LIMIT=&#x27;12800MB&#x27;,priority=high);create resource queue q_hank with (ACTIVE_STATEMENTS=10,MEMORY_LIMIT=&#x27;200MB&#x27;,PRIORITY=HIGH,COST_OVERCOMMIT=true,MIN_COST=100,MAX_COST=1000000); 修改 1234567#使用ALTER RESOURCE QUEUE命令来改变资源队列的限制 ALTER RESOURCE QUEUE q_hank WITH (ACTIVE_STATEMENTS=3); ALTER RESOURCE QUEUE q_hank WITH (MAX_COST=100000.0);#将活动语句数量或者内存限制重置为无限制，可以使用-1值。ALTER RESOURCE QUEUE q_hank WITH (MAX_COST=-1.0, MEMORY_LIMIT=‘2GB’); #改变查询优先级ALTER RESOURCE QUEUE q_hank WITH (PRIORITY=MIN); 删除 要删除一个资源队列，该队列不能与任何ROLE相关。 12# 格式：DROP RESOURCE QUEUE命令删除资源队列。DROP RESOURCE QUEUE q_hank; 添加用户到资源队列 123456789101112a) 赋予role资源管理队列alter role hank resource queue q_hank;b) 恢复角色到使用默认的资源队列ALTER ROLE hank RESOURCE QUEUE none; c)创建资源队列 连接数=3，最大内存使用：1024MB,优先级：低DROP RESOURCE QUEUE test_queuecreate resource queue test_queue with (active_statements=3,MEMORY_LIMIT=&#x27;1024MB&#x27;,PRIORITY=LOW)；d)创建用户 并指定默认资源队列CREATE ROLE tuser WITH LOGIN PASSWORD &#x27;tuser&#x27; resource queue test_queue; 查询语句 12345postgres=# SELECT rolname, rsqname FROM pg_roles, gp_toolkit.gp_resqueue_status WHERE pg_roles.rolresqueue=gp_toolkit.gp_resqueue_status.queueid; rolname | rsqname---------+------------ gpadmin | pg_default(1 row)","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"greenplum","slug":"greenplum","permalink":"http://yoursite.com/tags/greenplum/"}]},{"title":"分布式事务","slug":"分布式事务","date":"2021-12-01T10:30:14.000Z","updated":"2022-11-22T01:27:53.861Z","comments":true,"path":"2021/12/01/分布式事务/","link":"","permalink":"http://yoursite.com/2021/12/01/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"转载自 一、CAP 定理 CAP 首次在 ACM PODC 会议上作为猜想被提出，两年后被证明为定理，从此深深影响了分布式计算的发展。CAP 理论告诉我们，一个分布式系统不可能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三个基本需求，最多只能同时满足其中的两项。 一致性：数据在多个副本之间保持一致。当有一个节点的数据发生更新后，其它节点应该也能同步地更新数据。 可用性：对于用户的每一个操作请求，系统总能在有限的时间内返回结果。 分区容错性：分布式系统中的不同节点可能分布在不同的子网络中，这些子网络被称为网络分区。由于一些特殊原因导致子网络之间出现网络不连通的情况，系统仍需要能够保证对外提供一致性和可用性的服务。 CAP 定理告诉了我们同时满足这三项是不可能的，那么放弃其中的一项会是什么样的呢？ 放弃项 说明 放弃P 如果希望能够避免出现分区容错性问题，一种较为简单的做法是将所有数据放在一个节点上。这样肯定不会受网络分区影响。但此时分布式系统也失去了意义。因此在实际的架构设计中，P是一定要满足的。 放弃A 放弃可用性就是在系统遇到网络分区或其他故障时，受影响的服务可以暂时不对外提供，等到系统恢复后再对外提供服务。 放弃C 放弃一致性不代表完全放弃数据一致性，这样的话系统就没有意义了。而是放弃数据的强一致性，保留最终一致性。这样的系统无法保证数据保持实时的一致性，但能够承诺数据最终会达到一个一致的状态。 实际的实现中，我们往往会把精力花在如何根据业务特点在 C（一致性）和 A（可用性）之间寻求平衡。 二、BASE 理论 BASE 是 Basically Available（基本可用）、Soft state（软状态）和 Eventually consistent（最终一致性）三个短语的简写。BASE 是对 CAP 中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结。其核心思想是：即使无法做到强一致性，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性。 基本可用：基本可用是指在分布式系统出现不可预知的故障时，允许损失部分性能。比如：正常情况下 0.5 秒就能返回结果的服务，但在故障情况（网络分区或其他故障）下，需要 1~2 秒；正常情况下，电商网站的首页展示的是每个用户个性化的推荐内容，但在节日大促的情况下，展示的是统一的推荐内容。 软状态：软状态是指运行系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。比如秒杀系统中，用户余额的扣减和商家余额的增加可以存在延时，当用户余额减了之后即可返回支付成功，商家余额的增加可以等系统压力小的时候再做。 最终一致性：最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能达到一个一致的状态。这也是分布式系统的一个基本要求。 严格遵守 ACID 的分布式事务我们称为刚性事务，而遵循 BASE 理论的事务我们称为柔性事务。在分布式环境下，刚性事务会让系统的可用性变得难以忍受，因此实际生产中使用的分布式事务都是柔性事务，其中使用最多的就是 2PC、3PC 和 TCC。 三、2PC 协议 2PC 是二阶段提交（Two-phase Commit）的缩写，顾名思义，这个协议分两阶段完成。第一个阶段是准备阶段，第二个阶段是提交阶段，准备阶段和提交阶段都是由事务管理器（协调者）发起的，协调的对象是资源管理器（参与者）。二阶段提交协议的概念来自 X/Open 组织提出的分布式事务的规范 XA 协议，协议主要定义了（全局）事务管理器和（局部）资源管理器之间的接口。XA 接口是双向的系统接口，在事务管理器以及一个或多个资源管理器之间形成通信桥梁。Java 平台上的事务规范 JTA（Java Transaction API）提供了对 XA 事务的支持，它要求所有需要被分布式事务管理的资源（由不同厂商实现）都必须实现规定接口（XAResource 中的 prepare、commit 和 rollback 等）。 两阶段如下： 准备阶段：协调者向参与者发起指令，参与者评估自己的状态，如果参与者评估指令可以完成，参与者会写 redo 和 undo 日志，然后锁定资源，执行操作，但是并不提交。 提交阶段：如果每个参与者明确返回准备成功，也就是预留资源和执行操作成功，协调者向参与者发起提交指令，参与者提交资源变更的事务，释放锁定的资源；如果任何一个参与者明确返回准备失败，也就是预留资源或者执行操作失败，协调者向参与者发起中止指令，参与者取消已经变更的事务，执行 undo 日志，释放锁定的资源。 两阶段提交协议成功场景示意图如下： 我们看到两阶段提交协议在准备阶段锁定资源，是一个重量级的操作，并能保证强一致性，但是实现起来复杂、成本较高，不够灵活，更重要的是它有如下致命的问题： 阻塞：从上面的描述来看，对于任何一次指令必须收到明确的响应，才会继续做下一步，否则处于阻塞状态，占用的资源被一直锁定，不会被释放。 单点故障：如果协调者宕机，参与者没有了协调者指挥，会一直阻塞，尽管可以通过选举新的协调者替代原有协调者，但是如果之前协调者在发送一个提交指令后宕机，而提交指令仅仅被一个参与者接受，并且参与者接收后也宕机，新上任的协调者无法处理这种情况。 脑裂：协调者发送提交指令，有的参与者接收到执行了事务，有的参与者没有接收到事务，就没有执行事务，多个参与者之间是不一致的。 上面所有的这些问题，都是需要人工干预处理，没有自动化的解决方案，因此两阶段提交协议在正常情况下能保证系统的强一致性，但是在出现异常情况下，当前处理的操作处于错误状态，需要管理员人工干预解决，因此可用性不够好，这也符合 CAP 定理的一致性和可用性不能兼得的原理。 四、3PC 协议 三阶段提交协议（3PC 协议）是两阶段提交协议的改进版本。它通过超时机制解决了阻塞的问题，并且把两个阶段增加为三个阶段： 询问阶段：协调者询问参与者是否可以完成指令，协调者只需要回答是还是不是，而不需要做真正的操作，这个阶段参与者在等待超时后会自动中止。 准备阶段：如果在询问阶段所有的参与者都返回可以执行操作，协调者向参与者发送预执行请求，然后参与者写 redo 和 undo 日志，锁定资源，执行操作，但是不提交操作；如果在询问阶段任何参与者返回不能执行操作的结果，则协调者向参与者发送中止请求，这里的逻辑与两阶段提交协议的的准备阶段是相似的，这个阶段参与者在等待超时后会自动提交。 提交阶段：如果每个参与者在准备阶段返回准备成功，也就是预留资源和执行操作成功，协调者向参与者发起提交指令，参与者提交资源变更的事务，释放锁定的资源；如果任何一个参与者返回准备失败，也就是预留资源或者执行操作失败，协调者向参与者发起中止指令，参与者取消已经变更的事务，执行 undo 日志，释放锁定的资源，这里的逻辑与两阶段提交协议的提交阶段一致。 三阶段提交协议成功场景示意图如下： 这里与两阶段提交协议有两个主要的不同： 增加了一个询问阶段，询问阶段可以确保尽可能早的发现无法执行操作而需要中止的行为，但是它并不能发现所有的这种行为，只会减少这种情况的发生。 增加了等待超时的处理逻辑，如果在询问阶段等待超时，则自动中止；如果在准备阶段之后等待超时，则自动提交。这也是根据概率统计上的正确性最大。 三阶段提交协议相比二阶段提交协议，避免了资源被无限锁定的情况。但也增加了系统的复杂度，增加了参与者和协调者之间的通信次数。 五、TCC 协议 无论是 2PC 还是 3PC，都存在一个大粒度资源锁定的问题。为了解释这个问题，我们先来想象这样一种场景，用户在电商网站购买商品1000元，使用余额支付800元，使用红包支付200元。我们看一下在 2PC 中的流程： prepare 阶段： 下单系统插入一条订单记录，不提交 余额系统减 800 元，给记录加锁，写 redo 和 undo 日志，不提交 红包系统减 200 元，给记录加锁，写 redo 和 undo 日志，不提交 commit 阶段： 下单系统提交订单记录 余额系统提交，释放锁 红包系统提交，释放锁 为什么说这是一种大粒度的资源锁定呢？是因为在 prepare 阶段，当数据库给用户余额减 800 元之后，为了维持隔离性，会给该条记录加锁，在事务提交前，其它事务无法再访问该条记录。但实际上，我们只需要预留其中的 800 元，不需要锁定整个用户余额。这是 2PC 和 3PC 的局限，因为这两者是资源层的协议，无法提供更灵活的资源锁定操作。为了解决这个问题，TCC 应运而生。TCC 本质上也是一个二阶段提交协议，但和 JTA 中的二阶段协议不同的是，它是一个服务层的协议，因此开发者可以根据业务自由控制资源锁定的粒度。我们等会儿可以看到 TCC 在上面这个场景中的优势，但在那之前，我们先来看一下 TCC 协议的运行过程。 TCC 将事务的提交过程分为 try-confirm-cancel(实际上 TCC 就是 try、confirm、cancel 的简称) 三个阶段: try：完成业务检查、预留业务资源 confirm：使用预留的资源执行业务操作（需要保证幂等性） cancel：取消执行业务操作，释放预留的资源（需要保证幂等性） 和 JTA 二阶段事务的参与方都要实现 prepare、commit、rollback 一样，TCC 的事务参与方也必须实现 try、confirm、cancel 三个接口。流程如下： 事务发起方向事务协调器发起事务请求，事务协调器调用所有事务参与者的 try 方法完成资源的预留，这时候并没有真正执行业务，而是为后面具体要执行的业务预留资源，这里完成了一阶段。 如果事务协调器发现有参与者的 try 方法预留资源时候发现资源不够，则调用参与方的 cancel 方法回滚预留的资源，需要注意 cancel 方法需要实现业务幂等，因为有可能调用失败（比如网络原因参与者接受到了请求，但是由于网络原因事务协调器没有接受到回执）会重试。 如果事务协调器发现所有参与者的 try 方法返回都 OK，则事务协调器调用所有参与者的 confirm 方法，不做资源检查，直接进行具体的业务操作。 如果协调器发现所有参与者的 confirm 方法都 OK 了，则分布式事务结束。 如果协调器发现有些参与者的 confirm 方法失败了，或者由于网络原因没有收到回执，则协调器会进行重试。这里如果重试一定次数后还是失败，会怎么样？常见的是做事务补偿。 TCC 执行场景示意图如下： 现在我们再回到开始的那个支付场景中，看看 TCC 在该场景中的流程： Try操作 tryX 下单系统创建待支付订单 tryY 冻结账户红包 200 元 tryZ 冻结资金账户 800 元 Confirm操作 confirmX 订单更新为支付成功 confirmY 扣减账户红包 200 元 confirmZ 扣减资金账户 800 元 Cancel操作 cancelX 订单处理异常，资金红包退回，订单支付失败 cancelY 冻结红包失败，账户余额退回，订单支付失败 cancelZ 冻结余额失败，账户红包退回，订单支付失败 可以看到，我们使用了冻结代替了原先的账号锁定（实际操作中，冻结操作可以用数据库减操作+日志实现），这样在冻结操作之后，事务提交之前，其它事务也能使用账户余额，提高了并发性。 总结一下，相比于二阶段提交协议，TCC 主要有以下区别： 2PC 位于资源层而 TCC 位于服务层。 2PC 的接口由第三方厂商实现，TCC 的接口由开发人员实现。 TCC 可以更灵活地控制资源锁定的粒度。 TCC 对应用的侵入性强。业务逻辑的每个分支都需要实现 try、confirm、cancel 三个操作，应用侵入性较强，改造成本高。","categories":[{"name":"分布式事务","slug":"分布式事务","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"}],"tags":[{"name":"分布式事务","slug":"分布式事务","permalink":"http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"}]},{"title":"volatile","slug":"volatile","date":"2021-11-26T07:52:21.000Z","updated":"2022-11-22T01:27:53.846Z","comments":true,"path":"2021/11/26/volatile/","link":"","permalink":"http://yoursite.com/2021/11/26/volatile/","excerpt":"","text":"1、概述 volatile 经常比喻成轻量级synchronized，是JVM提供的轻量级同步机制，对比之下，它就少了个原子性。它有特性：可见性，不保证原子性，有序性（禁止指令重排） 2、可见性 当一个线程修改了声明为volatile变量的值，变量的新值对于其他要读该变量的线程来说是立即可见的。 为什么会有可见性问题？ 说到底是因为内存和 CPU 的速度相差较大，所以有必要在内存和 CPU 间引入缓存。JVM定义了一套内存模型，规范程序中各个变量的访问规则模型。 注意：这里的主内存、工作内存与 Java 内存区域中的 Java 堆、栈、方法区不是同一层次的内存划分，这两者基本上没有关系。 有volatile修饰的共享变量进行写操作的时候多出一条带lock前缀的指令，它的作用： Lock前缀的指令让线程工作内存中的值写回主内存中； 通过缓存一致性协议，其他线程如果工作内存中存了该共享变量的值，就会失效； 其他线程会重新从主内存中获取最新的值； 实现可见性。规则如下： lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占状态。 unlock（解锁）：作用于主内存变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。 read（读取）：作用于主内存变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用 load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 use（使用）：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。 assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。 store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作。 write（写入）：作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中。 规则很多，记住2个：read–&gt;load–&gt;use 、assign–&gt;store–&gt;write。连续执行不能跳跃。 3、不保证原子性 4、禁止指令重排","categories":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/categories/JVM/"}],"tags":[{"name":"volatile","slug":"volatile","permalink":"http://yoursite.com/tags/volatile/"}]},{"title":"对象头","slug":"对象头","date":"2021-11-26T01:28:48.000Z","updated":"2022-11-22T01:27:53.922Z","comments":true,"path":"2021/11/26/对象头/","link":"","permalink":"http://yoursite.com/2021/11/26/%E5%AF%B9%E8%B1%A1%E5%A4%B4/","excerpt":"","text":"1、堆对象存储介绍 我们编写一个Java类，编译后会生成.class文件，当类加载器将class文件加载到jvm时，会生成一个klass类型的对象(c++)，称为类描述元数据，存储在方法区中（jdk1.8之后的元数据区）。当使用new创建对象时，就是根据类描述元数据Klass创建的对象oop，存储在堆中。每个java对象都有相同的组成部分，称为对象头。Java对象在64位JVM上存储： 由上图可知，Java对象的对象头有3部分构成 标记字（Mark Word） 类指针（Klass Word） 数组长度（数组对象才有） 2、标记字（Mark Word） 主要用来表示对象的线程锁状态，另外还可以用来配合GC、以及存放该对象的hashCode。下图是Java对象处于5种不同锁状态时，Mark Word中在64位虚拟机上的表现形式，上面每一行代表对象处于某种状态时的样子。 锁状态 存储内容 标志位 未锁定 对象的哈希码，分代年龄 01 偏向性 持有偏向锁的线程Id，偏向时间戳，分代年龄 01 轻量级锁 指向栈中锁记录的指针 00 重量级锁 指向对象监视器Monitor的指针 10 GC 空，不需要记录信息 11 未锁定（26位没有使用）：25位没使用，31位的对象hash码，1位未使用，4位分代年龄，1位偏向锁标志为0，2位锁标志位01。 偏向锁（1位没有使用）：54位持有当前锁线程Id，2位获得锁时候的时间戳，4位分代年龄，1位偏向锁标志为1，2位锁标志位01。 轻量级锁：62位指针，指向线程栈帧中锁的记录，2位锁标志位00。 重量级锁：62位指针，指向对象监视器Monitor对象，2位锁标志位10。 GC：没有记录锁状态，GC垃圾清理过程中一些过程数据。 几个属性说明 偏向锁位（biased_lock）：对象是否启用偏向锁标记，只占1个二进制位。为1时表示对象启用偏向锁，为0时表示对象没有偏向锁。lock和biased_lock共同表示对象处于什么锁状态。 分代年龄（age）：4位的Java对象年龄。在GC中，如果对象在Survivor区复制一次，年龄增加1。当对象达到设定的阈值时，将会晋升到老年代。默认情况下，并行GC的年龄阈值为15，并发GC的年龄阈值为6。由于age只有4位，所以最大值为15，这就是-XX:MaxTenuringThreshold选项最大值为15的原因。 对象哈希code（identity_hashcode）：31位的对象标识hashCode，采用延迟加载技术。调用方法System.identityHashCode()计算，并会将结果写到该对象头中。当对象加锁后（偏向、轻量级、重量级），MarkWord的字节没有足够的空间保存hashCode，因此该值会移动到管程Monitor中。 偏向锁的线程ID（thread）：持有偏向锁的线程ID。 时间戳（epoch）：偏向锁的时间戳。 指针（ptr_to_lock_record）：轻量级锁状态下，指向栈中锁记录的指针。 指针（ptr_to_heavyweight_monitor）：重量级锁状态下，指向对象监视器Monitor的指针。 锁升级过程 ​ 1. 初期锁对象刚创建时，还没有任何线程来竞争，这时偏向锁标识位是0，锁状态01，说明该对象处于无锁状态（无线程竞争它）。 ​ 2. 当有一个线程来竞争锁时，先用偏向锁，表示锁对象偏爱这个线程，这个线程要执行这个锁关联的任何代码，不需要再做任何检查和切换，这种竞争不激烈的情况下，效率非常高。这时Mark Word会记录自己偏爱的线程的ID，把该线程当做自己的熟人。 ​ 3. 当有两个线程开始竞争这个锁对象，情况发生变化了，不再是偏向（独占）锁了，锁会升级为轻量级锁，两个线程公平竞争，哪个线程先占有锁对象并执行代码，锁对象的Mark Word就执行哪个线程的栈帧中的锁记录。 如果竞争的这个锁对象的线程更多，导致了更多的切换和等待，JVM会把该锁对象的锁升级为重量级锁，这个就叫做同步锁，这个锁对象Mark Word再次发生变化，会指向一个监视器对象（每个对象都有一个），这个监视器对象用集合的形式，来登记和管理排队的线程。 3、类指针（Klass Word） 这一部分用于存储对象的类型指针，该指针指向它的类元数据，JVM通过这个指针确定对象是哪个类的实例。该指针的位长度为JVM的一个字大小，即32位的JVM为32位，64位的JVM为64位。 JVM 有个功能是 CompressedOops ，目的是为了在 64bit 机器上使用 32bit 的原始对象指针（oop，ordinary object pointer，这里直接就当成指针概念理解就可以了，不用关心啥是 ordinary）来节约成本（减少内存/带宽使用），提高性能（提高 Cache 命中率）。 使用了这个压缩功能，每个对象中的 Klass* 字段就会被压缩成 32bit（不是所有的 oop 都会被压缩的），总所周知 Klass* 指向的 Klass 在永久代（Java7 及之前）。但是在 Java8 及之后，永久代没了，有了一个 Metaspace，于是之前压缩指针 Klass* 指向的这块 Klass 区域有了一个名字 —— Compressed Class Space。Compressed Class Space 是 Metaspace 的一部分，默认大小为 1G。所以其实 Compressed Class Space 这个名字取得很误导，压缩的并不是 Klass，而是 Klass*。 4、数组长度（数组对象才有） 只有数组对象保存了这部分数据。该数据在32位和64位JVM中长度都是32bit。","categories":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/categories/JVM/"}],"tags":[{"name":"对象头","slug":"对象头","permalink":"http://yoursite.com/tags/%E5%AF%B9%E8%B1%A1%E5%A4%B4/"}]},{"title":"锁膨胀","slug":"锁膨胀","date":"2021-11-25T11:03:07.000Z","updated":"2022-11-22T01:27:54.225Z","comments":true,"path":"2021/11/25/锁膨胀/","link":"","permalink":"http://yoursite.com/2021/11/25/%E9%94%81%E8%86%A8%E8%83%80/","excerpt":"","text":"1、什么是锁膨胀 synchronized的锁膨胀是jdk1.6对synchronized的优化，锁的状态总共有四种，**无锁，偏向锁，轻量级锁，重量级锁。**锁的升级是单向的，只能从低级的锁升级到高级的锁。锁的膨胀过程为无锁-&gt;偏向锁-&gt;轻量级锁-&gt;重量级锁。 无锁：顾名思义就是没有锁，偏向锁默认是在4s后开启，这期间对象锁是无锁状态 偏向锁：jdk15中被废弃，可以通过参数：XX:+UseBiasedLocking开启，开启后会收到废弃的警告。偏向锁并不是一把锁，其实是一个标志，适合一个线程反复获取锁的场景。 轻量级锁：当有第二个线程通过尝试获取锁的时候，就会升级为轻量级锁，该锁适合于多个没有竞争的线程下执行。 重量级锁：线程之间产生竞争之后会升级为重量级锁，需要由用户态转为内核态，未获取锁的线程不占用cpu，会进入等待队列中挂起，由线程调度器进行调度。 最开始只有一个线程时会开启偏向锁，当有第二个线程获取锁时，会发生锁膨胀升级为轻量级锁，此时偏向锁失效，会进行锁撤销，该操作开销花费还是比较大的，这也是为什么在jdk15中废弃了偏向锁。 2、偏向锁 jdk15之前创建一个对象的时候会默认开启偏向锁，偏向锁的开启是延时的（延时4s，JVM有一些默认线程，这些线程中包含了synchronized代码，而这些代码肯定是有竞争的，如果直接开启偏向锁，会引发锁升级等操作，影响性能），不会立刻生效，可以通过vm参数来调整。 锁对象第一次被线程A获取的时候，会将线程id设置到锁对象的mark word中，当线程A再次进入已经获取到锁的同步代码块时，根据mark word中的记录就可以知道对象锁是偏向自己的，因此无需再获取锁，这样就提高了锁重入的性能。由此可见偏向锁其实并非是锁，只不过是在mark word中做了一个标记。 3、轻量级锁 ​ 偏向锁偏向线程A之后当线程B需要获取锁的时候，会先查看拥有偏向锁的线程A是否有效（通过对象头中的epoch进行判断），如果无效，线程B会将锁偏向自己，如果有效，偏向锁会升级为轻量级锁，A仍然会持有锁。当偏向锁升级为轻量级锁之后，会在线程栈中存储Lock Record，每个线程的栈中都有一个Lock Record（锁记录），用来存储锁对象的mark word。t0线程会通过CAS将锁对象中的mark word修改为指向自己的Lock Record指针，如果成功则t0线程获取该对象锁，如果失败则t0尝试将锁膨胀为重量级锁。 ​ 批量重偏向： 我们创建了多个对象，线程t0将这些对象作为锁对象，当线程t1也要将这些对象作为锁对象的时候，会依次将偏向锁撤销然后升级为轻量级锁，当撤销的次数超过默认阈值20之后，不会再进行偏向撤销升级为轻量级锁，而是直接将锁对象偏向t1，这个过程就是批量重偏向。 4、重量级锁 ​ 当升级为重量级锁之后，会将锁对象mark word中的锁标志位改成10，此时会先尝试自旋的方式来获取锁，可以避免线程的阻塞，当cpu数量跟自旋线程数量差不多的时候，避免了线程的挂起和唤醒，从而提升了性能，但是自旋线程多余cpu数量的时候，性能开销就比较大了，大量的线程自旋会占用cpu，因此该自旋被设计为是自适应的，jvm会在符合条件的时候放弃自旋，然后将线程放入阻塞队列中阻塞。 ​ 每个对象都有一个monitor实例，monitor是由c++编写的，其底层是使用了操作系统的mutex互斥量，会涉及到用户态到内核态之间的切换，这就是重量级锁开销大的原因。 Waitset 线程获取过锁，执行wait等方法时会进入到这里。 EntryList 等待获取锁的线程集合，这里的线程是阻塞状态，上面提到的自旋就是避免了线程进入这个集合中阻塞。 Owner 标识当前拥有锁的线程。 5、锁降级 当没有锁竞争的时候，会在STW（stop the world）时将锁降级为轻量级锁或偏向锁。","categories":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/categories/JVM/"}],"tags":[{"name":"锁膨胀","slug":"锁膨胀","permalink":"http://yoursite.com/tags/%E9%94%81%E8%86%A8%E8%83%80/"}]},{"title":"synchronized","slug":"synchronized","date":"2021-11-25T10:24:03.000Z","updated":"2022-11-22T01:27:53.835Z","comments":true,"path":"2021/11/25/synchronized/","link":"","permalink":"http://yoursite.com/2021/11/25/synchronized/","excerpt":"","text":"Java中使用 Synchronized 是能够实现线程同步的，即加锁。并且实现的是悲观锁，在操作同步资源的时候直接先加锁。在 jdk6 之后便引入了“偏向锁”和“轻量级锁”，所以总共有4种锁状态，级别由低到高依次为：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态。这几个状态会随着竞争情况逐渐升级。 1、使用方式 实例方法 123public synchronized void add(int val)&#123; this.count += val;&#125; 实例方法内代码块 12345public void add(int val)&#123; synchronized(this)&#123; this.count += val; &#125;&#125; 静态方法 123public static synchronized void add(int val)&#123; count += val; // count 是静态变量 &#125; 静态方法内代码块 12345public static void add4(int val) &#123; synchronized (MyClass.class) &#123; count += val; &#125; &#125; 2、synchronized 特性 原子性 所谓原子性就是指一个操作或者多个操作，要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。 可见性 其中synchronized对一个类或对象加锁时，一个线程如果要访问该类或对象必须先获得它的锁，而这个锁的状态对于其他任何线程都是可见的，并且在释放锁之前会将对变量的修改刷新到主存当中，保证资源变量的可见性。 有序性 Java允许编译器和处理器对指令进行重排，但是指令重排并不会影响单线程的顺序，它影响的是多线程并发执行的顺序性。synchronized保证了每个时刻都只有一个线程访问同步代码块，也就确定了线程执行同步代码块是分先后顺序的，保证了有序性。 可重入性 通俗一点讲就是说一个线程拥有了锁仍然还可以重复申请锁。 3、底层原理 3.1 之前 在JDK1.6之前，都是通过JVM中monitorenter和monitorenter字节码依赖于底层的操作系统的Mutex ([m’juːteks] 互斥的) Lock来实现的，但是由于使用Mutex Lock需要将当前线程挂起并从用户态切换到内核态来执行，这种切换的代价是非常昂贵的。也就是我们现在说的重量级锁。 synchronization基于进入和退出Monitor对象实现，synchronized修饰的同步代码块是依据指令 monitorenter 和 monitorexit 指令。synchronized 修饰的同步方法不是由 monitorenter 和 monitorexit 指令来实现同步的，而是由方法调用指令读取运行时常量池中方法的 ACC_SYNCHRONIZED 标志来隐式实现的。 总言之，JDK1.6之前synchronization实现直接是依据对象监视器（Monitor）的monitorenter、monitorenter指令实重量级锁。 3.2 锁升级 JDK1.6 时候引入了偏向锁、轻量级锁、重量级锁概念，来减少锁竞争带来的上下文切换。所谓锁升级：简单理解为synchronized 同步锁初始为偏向锁，一步步升级为重量级锁。 对象头的Mark Word中会随着锁的不同状态，有不同的记录，如下表： 锁状态 存储内容 标志位 未锁定 对象的哈希码，分代年龄 01 偏向性 持有偏向锁的线程Id，偏向时间戳，分代年龄 01 轻量级锁 指向栈中锁记录的指针 00 重量级锁 指向对象监视器Monitor的指针 10 GC 空，不需要记录信息 11 偏向锁 ，当一个线程再次访问这个同步代码或方法时，该线程只需去对象头中去判断一下是否当前线程是否持有该偏向锁就可以了。 轻量级锁 当有另外一个线程竞争获取这个锁时，由于该锁已经是偏向锁，当发现对象头中的线程 ID 不是自己的线程 ID，就会进行 CAS 操作获取锁，如果获取成功，直接替换对象头中的线程 ID 为自己的 ID，该锁会保持偏向锁状态；如果获取锁失败，代表当前锁有一定的竞争，偏向锁将升级为轻量级锁。 重量级锁 轻量级锁支持自旋，因此其他线程再次争抢时，如果CAS失败，将不再会进入阻塞状态，而是不断自旋。如果自旋锁重试之后抢锁依然失败，那么同步锁就会升级至重量级锁。 3.3 JVM优化 偏向锁升级问题 偏向锁的撤销需要等待全局安全点(JVM的stop the world)，暂停持有该锁的线程，同时检查该线程是否还在执行该方法，如果是，则升级锁，反之则被其它线程抢占。 1234// 关闭偏向锁（默认打开）-XX:-UseBiasedLocking// 设置重量级锁-XX:+UseHeavyMonitors 轻量级锁自旋问题 1-XX:-UseSpinning 自适应的自旋锁 在JDK 1.6中引入了自适应自旋锁。这就意味着自旋的时间不再固定了，而是由前一次在同一个锁上的自旋 时间及锁的拥有者的状态来决定的。 锁消除 对一些同步的代码，经过逃逸分析，被检测到不可能存在共享数据竞争，进行锁消除。 锁粗化 如果虚拟机探测到有这样一串零碎的操作都对同一个对象加锁，将会把加锁同步的范围扩展(粗化)到整个操作序列的外部。","categories":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/categories/JVM/"}],"tags":[{"name":"锁","slug":"锁","permalink":"http://yoursite.com/tags/%E9%94%81/"}]},{"title":"JVM逃逸分析","slug":"JVM逃逸分析","date":"2021-11-25T10:23:27.000Z","updated":"2022-11-22T01:27:53.375Z","comments":true,"path":"2021/11/25/JVM逃逸分析/","link":"","permalink":"http://yoursite.com/2021/11/25/JVM%E9%80%83%E9%80%B8%E5%88%86%E6%9E%90/","excerpt":"","text":"-XX:+DoEscapeAnalysis ： 开启逃逸分析 -XX:-DoEscapeAnalysis ： 关闭逃逸分析 1、什么是逃逸分析 逃逸分析(Escape Analysis)是目前JVM中一项比较重要的优化技术。通过逃逸分析，HotSpot编译器能够分析出一个对象的使用范围从而考虑是否将其分配在堆内存中。逃逸分析的核心思想就是分析对象动态作用域：当一个对象在方法中被定义后，它可能被外部方法所引用，称为方法逃逸。甚至某些情况还需要被外部线程访问，称为线程逃逸。 1234567891011121314151617181920public class EscapeAnalysis &#123; /** * 没逃逸 * @return */ public void escape1()&#123; Stu ojb = new Stu(); &#125; /** * 逃逸 * @return */ public Stu escape2()&#123; return new Stu(); &#125;&#125; escape1()内部实现只构建了一个对象没有被外部引用，这种情况就属于没有逃逸出方法； escape2()同样也构建了一个对象但是会将引用返回给调用，此时构建的对象是可以被外部访问的，这种就称之为方法逃逸。 如果我们能够确定一个变量不会逃逸到方法或者线程外，则是有可能对其进行一些优化的：同步省略、标量替换、栈上分配。这里同步省略会在后面多线程分析文章中介绍，这里主要介绍另外两种。 2、JIT优化 2.1 同步省略 如果一个对象被发现只能从一个线程被访问到，那么对于这个对象的操作可以不考虑同步。 123456789101112public void fun() &#123; Stu stu = new Stu(); synchronized(stu) &#123; System.out.println(stu); &#125;&#125;//优化后public void fun() &#123; Stu stu = new Stu(); System.out.println(stu);&#125; 2.2 栈上分配 众所周知，在JVM中对象的创建都是在堆上分配的，因为堆内存上访问是线程共享的，所有线程只要有该对象的引用就能够访问到堆中存储的对象数据。而JIT经过逃逸分析后，如果确定某个对象不会逃逸到方法之外，那么还有必要让其在堆上分配吗？如果能够改变Java对象都在堆上分配的原则将其分配到栈上那会发生什么呢？Java对象都在堆上分配的原则能够将没有逃逸出方法外的对象分配在栈上，其所占的空间就会随着栈帧出栈(方法执行完成)而自动销毁，可以大幅度减少垃圾收集器的压力从而提高系统性能。在HotSpot JVM中，栈上分配其实并没有真正意义上实现，而是使用标量替换。 2.3 标量替换 ​ 这里的标量(Scalar)和我们在数学中所了解的标量有所区别，在这里标量指的是一个无法再分解成更小的数据。在Java中原始数据类型如int、long等都属于标量。而其他可以继续分解的数据都称为聚合量(Aggregate)，最典型的就是对象。 如果经过逃逸分析确定一个对象不会被外部访问从而触发JIT优化，就会尝试将该对象进行拆解为若干个其中包含的成员变量来代替，在执行时就不会再去直接创建这个对象了，这个过程就是标量替换。这里我们用实际示例来描述一下让大家更容易理解并且印象更深刻。 123456789101112131415161718192021public class EscapeAnalysis &#123; public static void allocation()&#123; Escape escape = new Escape(3,29); &#125; public static void main(String[] args) &#123; allocation(); &#125; static class Escape&#123; private int variable1; private int variable2; public Escape(int variable1, int variable2) &#123; this.variable1 = variable1; this.variable2 = variable2; &#125; &#125; 上面这段代码我们可以看出，allocation()中我们构建了一个Escape对象，并且该对象没有逃逸出方法外。那么经过JIT优化后并不会直接去创建这个对象，而是使用两个标量代替。 123456789101112public class EscapeAnalysis &#123; public static void allocation()&#123; int variable1 = 3; int variable2 = 29; &#125; public static void main(String[] args) &#123; allocation(); &#125;&#125;","categories":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/categories/JVM/"}],"tags":[{"name":"逃逸分析","slug":"逃逸分析","permalink":"http://yoursite.com/tags/%E9%80%83%E9%80%B8%E5%88%86%E6%9E%90/"}]},{"title":"JIT","slug":"JIT","date":"2021-11-25T06:49:21.000Z","updated":"2022-11-22T01:27:53.367Z","comments":true,"path":"2021/11/25/JIT/","link":"","permalink":"http://yoursite.com/2021/11/25/JIT/","excerpt":"","text":"1、即时编译JIT(Just in Time) 我们大家所了解的传统JVM解析器执行Java程序是先通过javac对其进行源码编译然后转为字节码文件，然后再通过解释字节码转为机器指令一条条读取翻译的。显而易见Java编译器经过编译再执行的话，执行速度必然比直接执行要慢很多，而HotSpot虚拟机针对这种场景进行了优化，引进了JIT即时编译技术。 2、热点代码 JIT技术的引入不会影响原本JVM编译执行，只是当发现某个方法或者代码块运行特别频繁时会将其标记为热点代码。然后会将其直接编译为本地机器相关的机器码并优化，最后将这部分代码缓存起来。 3、热点探测 基于计数器的热点探测（HotSpot采用）：虚拟机为每个方法或是代码块建立一个计数器，统计执行的次数，若此处超过规定阈值则标记为热点代码。 优点：统计结果精准严谨。 缺点：实现较为复杂，并且需要为每个方法或是代码块都建立并维护计数器，无法直接获取方法调用关系。 基于采样的热点探测：虚拟机周期性检查各个线程栈顶，若某个方法出现在栈顶频率较高，则标记为热点代码。 优点：实现简单高效，可展开堆栈获取方法调用关系 缺点：缺乏精准度，线程阻塞或其他因素可能会扰乱热点探测。 4、优化 当热点探测识别出热点代码后会触发JIT，除了会对字节码进行缓存外还会对代码进行各种优化。 例如：逃逸分析、锁膨胀、方法内联、空值检查消除、类型检查消除","categories":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/categories/JVM/"}],"tags":[{"name":"即时编译","slug":"即时编译","permalink":"http://yoursite.com/tags/%E5%8D%B3%E6%97%B6%E7%BC%96%E8%AF%91/"}]},{"title":"sparkSQL","slug":"sparkSQL","date":"2021-11-17T09:35:32.000Z","updated":"2022-11-22T01:27:53.753Z","comments":true,"path":"2021/11/17/sparkSQL/","link":"","permalink":"http://yoursite.com/2021/11/17/sparkSQL/","excerpt":"","text":"","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark存储","slug":"spark存储","date":"2021-11-17T08:42:28.000Z","updated":"2022-11-22T01:27:53.782Z","comments":true,"path":"2021/11/17/spark存储/","link":"","permalink":"http://yoursite.com/2021/11/17/spark%E5%AD%98%E5%82%A8/","excerpt":"","text":"Spark 存储系统：负责维护所有暂存在内存与磁盘中的数据，这些数据包括 Shuffle 中间文件、RDD Cache 以及广播变量。 1、BlockManagerMaster 2、BlockManager BlockManager 的核心职责，在于管理数据块的元数据（Meta data），这些元数据记录并维护数据块的地址、位置、尺寸以及状态。 2.1 MemoryStore: 负责内存中的数据存取 自己持有一个Map：LinkedHashMap&lt;BlockId, MemoryEntry&gt;，图如下： BlockId 用于标记 Block 的身份，需要注意的是，BlockId 不是一个仅仅记录 Id 的字符串，而是一种记录 Block 元信息的数据结构。BlockId 这个数据结构记录的信息非常丰富，包括 Block 名字、所属 RDD、Block 对应的 RDD 数据分区、是否为广播变量、是否为 Shuffle Block，等等。 MemoryEntry 是对象，它用于承载数据实体，数据实体可以是某个 RDD 的数据分区，也可以是广播变量。存储在 LinkedHashMap 当中的 MemoryEntry，相当于是通往数据实体的地址。 2.2 DiskStore: 负责磁盘中的数据存取 一个帮手：DiskBlockManager，来帮他维护元数据，图如下： 帮手 DiskBlockManager 是类对象，它的 getFile 方法以 BlockId 为参数，返回磁盘文件。换句话说，给定数据块，要想知道它存在了哪个磁盘文件，需要调用 getFile 方法得到答案。有了数据块与文件之间的映射关系，我们就可以轻松地完成磁盘中的数据访问。 以 Shuffle 为例，在 Shuffle Write 阶段，每个 Task 都会生成一份中间文件，每一份中间文件都包括带有 data 后缀的数据文件，以及带着 index 后缀的索引文件。那么对于每一份文件来说，我们都可以通过 DiskBlockManager 的 getFile 方法，来获取到对应的磁盘文件，如下图所示。 Spark 提供了 3 种配置项设置途径，分别是 spark-defaults.conf 配置文件、命令行参数和 SparkConf 对象。其中第一种方式用于全局设置，而后两者的适用范围是应用本身。对于这 3 种方式，Spark 会按照 “SparkConf 对象 -&gt; 命令行参数 -&gt; 配置文件”的顺序，依次读取配置项的参数值。对于重复设置的配置项，Spark 以前面的参数取值为准。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark共享变量","slug":"spark共享变量","date":"2021-11-17T07:49:49.000Z","updated":"2022-11-22T01:27:53.754Z","comments":true,"path":"2021/11/17/spark共享变量/","link":"","permalink":"http://yoursite.com/2021/11/17/spark%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F/","excerpt":"","text":"算子都是作用（Apply）在 RDD 之上的。RDD 的计算以数据分区为粒度，依照算子的逻辑，Executors 以相互独立的方式，完成不同数据分区的计算与转换。不难发现，对于 Executors 来说，分区中的数据都是局部数据。换句话说，在同一时刻，隶属于某个 Executor 的数据分区，对于其他 Executors 来说是不可见的。 不过，在做应用开发的时候，总会有一些计算逻辑需要访问“全局变量”，比如说全局计数器，而这些全局变量在任意时刻对所有的 Executors 都是可见的、共享的。那么问题来了，像这样的全局变量，或者说共享变量，Spark 又是如何支持的呢？ Spark 提供了两类共享变量，分别是广播变量（Broadcast variables）和累加器（Accumulators）。 广播变量（Broadcast variables） 给定普通变量 x，通过调用 SparkContext 下的 broadcast API 即可完成广播变量的创建。 12345val list: List[String] = List(&quot;Apache&quot;, &quot;Spark&quot;) // sc为SparkContext实例val bc = sc.broadcast(list)bc.value 累加器（Accumulators） 累加器，顾名思义，它的主要作用是全局计数（Global counter）。与单机系统不同，在分布式系统中，我们不能依赖简单的普通变量来完成全局计数，而是必须依赖像累加器这种特殊的数据结构才能达到目的。 12val ac = sc.longAccumulator(&quot;Empty string&quot;)// 当遇到空字符串时，累加器加1ac.add(1) SparkContext 还提供了 doubleAccumulator 和 collectionAccumulator 这两种不同类型的累加器。doubleAccumulator 用于对 Double 类型的数值做全局计数；而 collectionAccumulator 允许开发者定义集合类型的累加器。 总结 广播变量由 Driver 端定义并初始化，各个 Executors 以只读（Read only）的方式访问广播变量携带的数据内容。 累加器也是由 Driver 定义的，但 Driver 并不会向累加器中写入任何数据内容，累加器的内容更新，完全是由各个 Executors 以只写（Write only）的方式来完成，而 Driver 仅以只读的方式对更新后的内容进行访问。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark内存管理","slug":"spark内存管理","date":"2021-11-17T07:15:51.000Z","updated":"2022-11-22T01:27:53.755Z","comments":true,"path":"2021/11/17/spark内存管理/","link":"","permalink":"http://yoursite.com/2021/11/17/spark%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","excerpt":"","text":"对于任意一个 Executor 来说，Spark 会把内存分为 4 个区域，分别是 Reserved Memory、User Memory、Execution Memory 和 Storage Memory。 Reserved Memory 固定为 300MB，不受开发者控制，它是 Spark 预留的、用来存储各种 Spark 内部对象的内存区域。 User Memory 用于存储开发者自定义的数据结构，例如 RDD 算子中引用的数组、列表、映射等等。 Execution Memory 用来执行分布式任务。分布式任务的计算，主要包括数据的转换、过滤、映射、排序、聚合、归并等环节，而这些计算环节的内存消耗，统统来自于 Execution Memory。 Storage Memory 用于缓存分布式数据集，比如 RDD Cache、广播变量等等。 在 Spark 1.6 版本之前，Execution Memory 和 Storage Memory 的空间划分是静态的，一旦空间划分完毕，不同内存区域的用途与尺寸就固定了。在 1.6 版本之后，Spark 推出了统一内存管理模式，在这种模式下，Execution Memory 和 Storage Memory 之间可以相互转化。 Execution Memory 和 Storage Memory 之间的抢占规则，一共可以总结为 3 条： 如果对方的内存空间有空闲，双方可以互相抢占； 对于 Storage Memory 抢占的 Execution Memory 部分，当分布式任务有计算需要时，Storage Memory 必须立即归还抢占的内存，涉及的缓存数据要么落盘、要么清除； 对于 Execution Memory 抢占的 Storage Memory 部分，即便 Storage Memory 有收回内存的需要，也必须要等到分布式任务执行完毕才能释放。 Reserved Memory 固定为 300MB。 spark.memory.fraction 用于标记 Spark 处理分布式数据集的内存总大小，这部分内存包括 Execution Memory 和 Storage Memory 两部分，也就是图中绿色的矩形区域。 （M – 300）* （1 – mf）刚好就是 User Memory 的区域大小，也就是图中蓝色区域的部分。 spark.memory.storageFraction 则用来进一步区分 Execution Memory 和 Storage Memory 的初始大小。我们之前说过，Reserved Memory 固定为 300MB。（M – 300）* mf * sf 是 Storage Memory 的初始大小，相应地，（M – 300）* mf * （1 – sf）就是 Execution Memory 的初始大小。 设置总结： 对于 ETL（Extract、Transform、Load）类型的作业来说，数据往往都是按照既定的业务逻辑依序处理，其中绝大多数的数据形态只需访问一遍，很少有重复引用的情况。因此，在 ETL 作业中，RDD Cache 并不能起到提升执行性能的作用，那么自然我们也就没必要使用缓存了。在这种情况下，我们就应当把 sf 的值设置得低一些，压缩 Storage Memory 可用空间，从而尽量把内存空间留给 Execution Memory。 相反，如果你的应用场景是机器学习、或是图计算，这些计算任务往往需要反复消耗、迭代同一份数据，处理方式就不一样了。在这种情况下，咱们要充分利用 RDD Cache 提供的性能优势，自然就要把 sf 这个参数设置得稍大一些，从而让 Storage Memory 有足够的内存空间，来容纳需要频繁访问的分布式数据集。 RDD Cache 当同一个 RDD 被引用多次时，就可以考虑对其进行 Cache，从而提升作业的执行效率。 12345678// 按照单词做分组计数val wordCounts: RDD[(String, Int)] = kvRDD.reduceByKey((x, y) =&gt; x + y) wordCounts.cache// a.使用cache算子告知Spark对wordCounts加缓存wordCounts.count// b.触发wordCounts的计算，并将wordCounts缓存到内存 // 打印词频最高的5个词汇wordCounts.map&#123;case (k, v) =&gt; (v, k)&#125;.sortByKey(false).take(5) // 将分组计数结果落盘到文件val targetPath: String = _wordCounts.saveAsTextFile(targetPath) 由于 cache 函数并不会立即触发 RDD 在内存中的物化，因此我们还需要调用 count 算子来触发这一执行过程。 下面的两条语句与上面的a.b是完全等价的，二者的含义都是把 RDD 物化到内存 12wordCounts.cachewordCounts.persist(MEMORY_ONLY) 添加 Cache 来说，相比 cache 算子，persist 算子更具备普适性，结合多样的存储级别（如这里的 MEMORY_ONLY），persist 算子允许开发者灵活地选择 Cache 的存储介质、存储形式以及副本数量。 Spark 支持丰富的存储级别，每一种存储级别都包含 3 个最基本的要素。 存储介质：数据缓存到内存还是磁盘，或是两者都有 存储形式：数据内容是对象值还是字节数组，带 SER 字样的表示以序列化方式存储，不带 SER 则表示采用对象值 副本数量：存储级别名字最后的数字代表拷贝数量，没有数字默认为 1 份副本。 Spark 支持的存储级别总结到了下表，其中打钩的地方，表示某种存储级别支持的存储介质与存储形式:","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark基础","slug":"spark基础","date":"2021-11-15T01:46:14.000Z","updated":"2022-11-22T01:27:53.762Z","comments":true,"path":"2021/11/15/spark基础/","link":"","permalink":"http://yoursite.com/2021/11/15/spark%E5%9F%BA%E7%A1%80/","excerpt":"","text":"本文代码是在sprk-shell终端使用scala。 1、word count wikipedia spark介绍文档下载，根据文件统计word count。步骤如下： 读取内容：调用 Spark 文件读取 API，加载 wikiOfSpark.txt 文件内容； 分词：以行为单位，把句子打散为单词； 分组计数：按照单词做分组计数。 12345678910111213141516import org.apache.spark.rdd.RDD // 这里需要根据自己文件目录val file: String = &quot;/Users/hf/blog/wikiOfSpark.txt&quot;// 读取文件内容val lineRDD: RDD[String] = spark.sparkContext.textFile(file)// 以行为单位做分词val wordRDD: RDD[String] = lineRDD.flatMap(line =&gt; line.split(&quot; &quot;))val cleanWordRDD: RDD[String] = wordRDD.filter(word =&gt; !word.equals(&quot;&quot;))// 把RDD元素转换为（Key，Value）的形式val kvRDD: RDD[(String, Int)] = cleanWordRDD.map(word =&gt; (word, 1))// 按照单词做分组计数val wordCounts: RDD[(String, Int)] = kvRDD.reduceByKey((x, y) =&gt; x + y)// 打印词频最高的5个词汇wordCounts.map&#123;case (k, v) =&gt; (v, k)&#125;.sortByKey(false).take(5) spark 和 sparkContext 分别是两种不同的开发入口实例： spark是开发入口 SparkSession 实例，SparkSession 在 spark-shell 中会由系统自动创建； sparkContext 是开发入口 SparkContext 实例。 从 2.0 版本开始，SparkSession 取代了 SparkContext，成为统一的开发入口。换句话说，要开发 Spark 应用，你必须先创建 SparkSession 2、RDD RDD 是 Spark 对于分布式数据集的抽象，RDD 是 Spark 对于分布式数据集的抽象，它用于囊括所有内存中和磁盘中的分布式数据实体，每一个 RDD 都代表着一种分布式数据形态！RDD 4大属性： partitions：数据分片； partitioner：分片切割规则； dependencies：RDD 依赖； compute：转换函数 在 RDD 的编程模型中，一共有两种算子，Transformations 类算子和 Actions 类算子。在这样的编程模型下，Spark 在运行时的计算被划分为两个环节： 基于不同数据形态之间的转换，构建计算流图（DAG，Directed Acyclic Graph）； 通过 Actions 类算子，以回溯的方式去触发执行这个计算流图。 3、算子 开发者调用的各类 Transformations 算子，并不立即执行计算 当且仅当开发者调用 Actions 算子时，之前调用的转换算子才会付诸执行。 4、分布式计算 4.1 进程模型 在 Spark 的应用开发中，任何一个应用程序的入口，都是带有 SparkSession 的 main 函数。SparkSession 包罗万象，它在提供 Spark 运行时上下文的同时（如调度系统、存储系统、内存管理、RPC 通信），也可以为开发者提供创建、转换、计算分布式数据集（如 RDD）的开发 API。 不过，在 Spark 分布式计算环境中，有且仅有一个 JVM 进程运行这样的 main 函数，这个特殊的 JVM 进程，在 Spark 中有个专门的术语，叫作“Driver”。Driver 最核心的作用在于，解析用户代码、构建计算流图，然后将计算流图转化为分布式任务，并把任务分发给集群中的执行进程交付运行。换句话说，Driver 的角色是拆解任务、派活儿，而真正干活儿的“苦力”，是执行进程。这样的执行进程可以有一个或是多个，它们也有专门的术语，叫作“Executor”。 4.2 Driver 在 Spark 的 Driver 进程中，DAGScheduler、TaskScheduler 和 SchedulerBackend 这三个对象通力合作，依次完成分布式任务调度的 3 个核心步骤，也就是： 根据用户代码构建计算流图； 根据计算流图拆解出分布式任务； 将分布式任务分发到 Executors 中去。 集团：Spark；总公司：Driver；多个分公司：Executors。 Driver（总公司）：DAGScheduler （总公司总架构师），TaskScheduler（总公司施工总经理）和SchedulerBackend（总公司人力资源总监）为公司2位元老。 Executors（分公司）：ExecutorBackend（分公司人力资源主管）。 简而言之，DAGScheduler 手里有“活儿”，SchedulerBackend 手里有“人力”，TaskScheduler 的核心职能，就是把合适的“活儿”派发到合适的“人”的手里。TaskScheduler 承担的是承上启下、上通下达的关键角色。 DAGScheduler（手里有活）–&gt; TaskScheduler（分配活）–&gt;SchedulerBackend（手里有人力） 4.2.1 DAGScheduler 核心职责，是把计算图 DAG 拆分为执行阶段 Stages，Stages 指的是不同的运行阶段，同时还要负责把 Stages 转化为任务集合 TaskSets，也就是把“建筑图纸”转化成可执行、可操作的“建筑项目”。 根据用户代码构建 DAG； 以 Shuffle 为边界切割 Stages； 基于 Stages 创建 TaskSets，并将 TaskSets 提交给 TaskScheduler 请求调度。 对于提请执行的每一个 Stage，DAGScheduler 根据 Stage 内 RDD 的 partitions 属性创建分布式任务集合 TaskSet。TaskSet 包含一个又一个分布式任务 Task，RDD 有多少数据分区，TaskSet 就包含多少个 Task。换句话说，Task 与 RDD 的分区，是一一对应的。 4.2.2 SchedulerBackend SchedulerBackend 用一个叫做 ExecutorDataMap 的数据结构，来记录每一个计算节点中 Executors 的资源状态。 这里的 ExecutorDataMap 是一种 HashMap，它的 Key 是标记 Executor 的字符串，Value 是一种叫做 ExecutorData 的数据结构。ExecutorData 用于封装 Executor 的资源状态，如 RPC 地址、主机地址、可用 CPU 核数和满配 CPU 核数等等，它相当于是对 Executor 做的“资源画像”。SchedulerBackend 以 WorkerOffer 为粒度提供计算资源。其中，WorkerOffer 封装了 Executor ID、主机地址和 CPU 核数，它用来表示一份可用于调度任务的空闲资源。 SchedulerBackend 与集群内所有 Executors 中的 ExecutorBackend 保持周期性通信，双方通过 LaunchedExecutor、RemoveExecutor、StatusUpdate 等消息来互通有无、变更可用计算资源。 4.2.3 TaskScheduler TaskScheduler 是按照任务的本地倾向性，来遴选出 TaskSet 中适合调度的 Tasks！ Task 与 RDD 的 partitions 是一一对应的，在创建 Task 的过程中，DAGScheduler 会根据数据分区的物理地址，来为 Task 设置 locs 属性。locs 属性记录了数据分区所在的计算节点、甚至是 Executor 进程 ID。 Spark 调度系统的核心思想，是“数据不动、代码动”。也就是说，在任务调度的过程中，为了完成分布式计算，Spark 倾向于让数据待在原地、保持不动，而把计算任务（代码）调度、分发到数据所在的地方，从而消除数据分发引入的性能隐患。毕竟，相比分发数据，分发代码要轻量得多。 4.2.4 ExecutorBackend 作为分公司的人力资源主管，ExecutorBackend 拿到“活儿”之后，随即把活儿派发给分公司的建筑工人。这些工人，就是 Executors 线程池中一个又一个的 CPU 线程，每个线程负责处理一个 Task。 每当 Task 处理完毕，这些线程便会通过 ExecutorBackend，向 Driver 端的 SchedulerBackend 发送 StatusUpdate 事件，告知 Task 执行状态。接下来，TaskScheduler 与 SchedulerBackend 通过接力的方式，最终把状态汇报给 DAGScheduler。 总结spark任务调度步骤如下： DAGScheduler 以 Shuffle 为边界，将开发者设计的计算图 DAG 拆分为多个执行阶段 Stages，然后为每个 Stage 创建任务集 TaskSet。 SchedulerBackend 通过与 Executors 中的 ExecutorBackend 的交互来实时地获取集群中可用的计算资源，并将这些信息记录到 ExecutorDataMap 数据结构。 与此同时，SchedulerBackend 根据 ExecutorDataMap 中可用资源创建 WorkerOffer，以 WorkerOffer 为粒度提供计算资源。 对于给定 WorkerOffer，TaskScheduler 结合 TaskSet 中任务的本地性倾向，按照 PROCESS_LOCAL、NODE_LOCAL、RACK_LOCAL 和 ANY 的顺序，依次对 TaskSet 中的任务进行遍历，优先调度本地性倾向要求苛刻的 Task。 被选中的 Task 由 TaskScheduler 传递给 SchedulerBackend，再由 SchedulerBackend 分发到 Executors 中的 ExecutorBackend。Executors 接收到 Task 之后，即调用本地线程池来执行分布式任务。 5、shuffle 定义：集群范围内跨节点、跨进程的数据分发。 ap 阶段与 Reduce 阶段，通过生产与消费 Shuffle 中间文件的方式，来完成集群范围内的数据交换。换句话说，Map 阶段生产 Shuffle 中间文件，Reduce 阶段消费 Shuffle 中间文件，二者以中间文件为媒介，完成数据交换。 DAGScheduler 会为每一个 Stage 创建任务集合 TaskSet，而每一个 TaskSet 都包含多个分布式任务（Task）。在 Map 执行阶段，每个 Task（以下简称 Map Task）都会生成包含 data 文件与 index 文件的 Shuffle 中间文件，如图所示。也就是说，Shuffle 文件的生成，是以 Map Task 为粒度的，Map 阶段有多少个 Map Task，就会生成多少份 Shuffle 中间文件。 1个task一个：Shuffle 中间文件=data 文件+ index 文件 data文件：task中数据（Key，Value）键值对 index文件：键值对，在reduce阶段被哪些task消费","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"怎么画架构图","slug":"怎么画架构图","date":"2021-11-04T02:07:57.000Z","updated":"2022-11-22T01:27:53.966Z","comments":true,"path":"2021/11/04/怎么画架构图/","link":"","permalink":"http://yoursite.com/2021/11/04/%E6%80%8E%E4%B9%88%E7%94%BB%E6%9E%B6%E6%9E%84%E5%9B%BE/","excerpt":"","text":"架构域包括：业务架构、数据架构、产品架构、应用架构、技术架构、运行架构（时序）、部署架构 1、业务（逻辑）架构 在业务需求初期，将模糊的需求描述转变成清晰的问题域，梳理出清晰的业务流程。为产品架构提供输入。 一般是业务初期，需求是比较模糊，它一般是来自老板、运营或者客户，我们需要将这个产品的所有问题域罗列清楚，根据一套逻辑思路进行业务的拆分，总体原则是对业务进行业务边界的划分。 2、数据架构 对存储数据（资源）的架构方法，考虑到各个系统应用场景、不同时间段的应用场景对数据进行诸如数据异构、读写分离、数据库或NOSQL的策略、缓存的使用、分布式数据（数据库）策略等等。 3 、产品架构 4、应用架构 描述了IT系统功能和系统技术实现的内容。 企业级的应用架构 企业层面的应用架构起到了统一规划、承上启下的作用，向上承接了企业战略发展方向和业务模式，向下规划和指导企业各个IT系统的定位和功能。在企业架构中，应用架构是最重要和工作量最大的部分，他包括了企业的应用架构蓝图、架构标准/原则、系统的边界和定义、系统间的关联关系等方面的内容。 单个系统的应用架构 在开发或设计单一IT系统时，设计系统的主要模块和功能点，系统技术实现是从前端展示到业务处理逻辑，到后台数据是如何架构的。这方面的工作一般属于项目组，而不是企业架构的范畴，不过各个系统的架构设计需要遵循企业总体应用架构原则。 4.1 系统功能视角 4.2 系统技术实现视角 5、技术架构 就是每一层，我们都用什么组件、什么技术解决什么问题. 6、运行架构(时序图、状态图、活动图) 7、部署架构 部署架构也叫网络架构，就是底层服务器、网路的设计，提供网络安全、服务可靠性的设计。再简单一些理解，就是你这些应用、数据库都放在那台服务器上，这些服务器都在哪个ip端，怎么进行访问。","categories":[{"name":"架构图","slug":"架构图","permalink":"http://yoursite.com/categories/%E6%9E%B6%E6%9E%84%E5%9B%BE/"}],"tags":[{"name":"架构图","slug":"架构图","permalink":"http://yoursite.com/tags/%E6%9E%B6%E6%9E%84%E5%9B%BE/"}]},{"title":"springBoot插件","slug":"springBoot插件","date":"2021-10-30T09:37:03.000Z","updated":"2022-11-22T01:27:53.806Z","comments":true,"path":"2021/10/30/springBoot插件/","link":"","permalink":"http://yoursite.com/2021/10/30/springBoot%E6%8F%92%E4%BB%B6/","excerpt":"","text":"转载：https://juejin.cn/post/6844904099721248775 一.实战：编写spring boot插件 1.为什么要编写boot插件 因为我们在开发的时候需要提供一些共同的功能，所以我们编写个共同的jar包。开发人员在使用jar包的时候不用考虑jar包的内容，直接使用具体的功能即可，但是可能由于包路径的不同，你所编写的bean没有被初始化到spring容器中。不应该让开发人员去扫描你的包路径去初始化bean。所以我们要自己动手去把bean初始化到bean容器中，这也是spring扩展能力的由来（spriing.factories） 2.实战 编写插件代码,编写配置类（例如：DemoAutoConfig），在其中定义你需要的bean 在resources下创建META-INF/spring.factories 编写spring.factories 123org.springframework.boot.autoconfigure.EnableAutoConfiguration&#x3D;com.demo.DemoAutoConfig复制代码 二.spring.factories 常用配置接口 1. org.springframework.boot.SpringApplicationRunListener SpringApplicationRunListener来监听Spring Boot的启动流程，并且在各个流程中处理自己的逻辑。在应用启动时，在Spring容器初始化的各个阶段回调对应的方法。 2. org.springframework.context.ApplicationContextInitializer ApplicationContextInitializer是在springboot启动过程上下文 ConfigurableApplicationContext刷新方法前(refresh)调用，对ConfigurableApplicationContext的实例做进一步的设置或者处理。 3.org.springframework.boot.autoconfigure.EnableAutoConfiguration 定义系统自动装配的类。 4.org.springframework.boot.env.EnvironmentPostProcessor 配置环境的集中管理。比如扩展去做排除加载系统默认的哪些配置类，方便自定义扩展。 5.org.springframework.boot.autoconfigure.AutoConfigurationImportFilter 自动装配类排除 三.spring factories 原理 1.获取配置流程 在启动类注解@SpringBootApplication中可以看到引用了@EnableAutoConfiguration。 其中@Import(AutoConfigurationImportSelector.class) 1234567891011public String[] selectImports(AnnotationMetadata annotationMetadata) &#123; if (!isEnabled(annotationMetadata)) &#123; return NO_IMPORTS; &#125; AutoConfigurationMetadata autoConfigurationMetadata &#x3D; AutoConfigurationMetadataLoader .loadMetadata(this.beanClassLoader); AutoConfigurationEntry autoConfigurationEntry &#x3D; getAutoConfigurationEntry(autoConfigurationMetadata, annotationMetadata); return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations());&#125;复制代码 其中getAutoConfigurationEntry方法 12345678910111213141516protected AutoConfigurationEntry getAutoConfigurationEntry(AutoConfigurationMetadata autoConfigurationMetadata, AnnotationMetadata annotationMetadata) &#123; if (!isEnabled(annotationMetadata)) &#123; return EMPTY_ENTRY; &#125; AnnotationAttributes attributes &#x3D; getAttributes(annotationMetadata); List&lt;String&gt; configurations &#x3D; getCandidateConfigurations(annotationMetadata, attributes); configurations &#x3D; removeDuplicates(configurations); Set&lt;String&gt; exclusions &#x3D; getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations &#x3D; filter(configurations, autoConfigurationMetadata); fireAutoConfigurationImportEvents(configurations, exclusions); return new AutoConfigurationEntry(configurations, exclusions);&#125;复制代码 其中getCandidateConfigurations 12345678protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) &#123; List&lt;String&gt; configurations &#x3D; SpringFactoriesLoader.loadFactoryNames(getSpringFactoriesLoaderFactoryClass(), getBeanClassLoader()); Assert.notEmpty(configurations, &quot;No auto configuration classes found in META-INF&#x2F;spring.factories. If you &quot; + &quot;are using a custom packaging, make sure that file is correct.&quot;); return configurations;&#125;复制代码 调用了SpringFactoriesLoader.loadFactoryNames 12345public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryType, @Nullable ClassLoader classLoader) &#123; String factoryTypeName &#x3D; factoryType.getName(); return loadSpringFactories(classLoader).getOrDefault(factoryTypeName, Collections.emptyList());&#125;复制代码 loadSpringFactories方法 1234567891011121314151617181920212223242526272829303132333435public static final String FACTORIES_RESOURCE_LOCATION &#x3D; &quot;META-INF&#x2F;spring.factories&quot;;private static final Map&lt;ClassLoader, MultiValueMap&lt;String, String&gt;&gt; cache &#x3D; new ConcurrentReferenceHashMap&lt;&gt;();private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader) &#123; MultiValueMap&lt;String, String&gt; result &#x3D; cache.get(classLoader); if (result !&#x3D; null) &#123; return result; &#125; try &#123; Enumeration&lt;URL&gt; urls &#x3D; (classLoader !&#x3D; null ? classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION)); result &#x3D; new LinkedMultiValueMap&lt;&gt;(); while (urls.hasMoreElements()) &#123; URL url &#x3D; urls.nextElement(); UrlResource resource &#x3D; new UrlResource(url); Properties properties &#x3D; PropertiesLoaderUtils.loadProperties(resource); for (Map.Entry&lt;?, ?&gt; entry : properties.entrySet()) &#123; String factoryTypeName &#x3D; ((String) entry.getKey()).trim(); for (String factoryImplementationName : StringUtils.commaDelimitedListToStringArray((String) entry.getValue())) &#123; result.add(factoryTypeName, factoryImplementationName.trim()); &#125; &#125; &#125; cache.put(classLoader, result); return result; &#125; catch (IOException ex) &#123; throw new IllegalArgumentException(&quot;Unable to load factories from location [&quot; + FACTORIES_RESOURCE_LOCATION + &quot;]&quot;, ex); &#125;&#125;复制代码 2.加载配置流程 在main方法启动的时候我们会调用SpringApplication.run方法 run方法中调用了getSpringFactoriesInstances 调用createSpringFactoriesInstances 123456789101112131415161718private &lt;T&gt; List&lt;T&gt; createSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, ClassLoader classLoader, Object[] args, Set&lt;String&gt; names) &#123; List&lt;T&gt; instances &#x3D; new ArrayList&lt;&gt;(names.size()); for (String name : names) &#123; try &#123; Class&lt;?&gt; instanceClass &#x3D; ClassUtils.forName(name, classLoader); Assert.isAssignable(type, instanceClass); Constructor&lt;?&gt; constructor &#x3D; instanceClass.getDeclaredConstructor(parameterTypes); T instance &#x3D; (T) BeanUtils.instantiateClass(constructor, args); instances.add(instance); &#125; catch (Throwable ex) &#123; throw new IllegalArgumentException(&quot;Cannot instantiate &quot; + type + &quot; : &quot; + name, ex); &#125; &#125; return instances;&#125;复制代码 四.总结 这是一种类似插件的设计方式，只要引入对应的jar包，就会扫描到jar里的spring.factories，对应的实现类也就会被实例化。","categories":[{"name":"springBoot","slug":"springBoot","permalink":"http://yoursite.com/categories/springBoot/"}],"tags":[{"name":"插件","slug":"插件","permalink":"http://yoursite.com/tags/%E6%8F%92%E4%BB%B6/"}]},{"title":"常见的限流算法","slug":"常见的限流算法","date":"2021-10-17T07:34:56.000Z","updated":"2022-11-22T01:27:53.947Z","comments":true,"path":"2021/10/17/常见的限流算法/","link":"","permalink":"http://yoursite.com/2021/10/17/%E5%B8%B8%E8%A7%81%E7%9A%84%E9%99%90%E6%B5%81%E7%AE%97%E6%B3%95/","excerpt":"","text":"1、固定时间窗口（计数器） 定义 比如在1分钟内限制请求次数不超过100，定义一个变量counter，每来一个请求则counter加1，如果在一分钟之内counter累计超过100，新来的请求则超过限制。然后，超过1s之后，这个counter就重置。 缺陷（临界问题） 这种方法会有一个问题，那就是比如某个人在0：59秒的时候打了100个请求过来，然后，在1min的时候counter就置零了，他又在1：01的时候打了100个请求过来，所以说，在最近的1min之内其实是接收到200个请求，这明显超出了我们的预期100个请求，所以说这种方法不满足我们的需求。 产品 阿里的sentinel 2、滑动时间窗口 定义 将时间周期分为N个小周期，分别记录每个小周期内访问次数，并且根据时间滑动，统计窗口内若干计数器之和。 比如一个时间窗口也是一分钟。然后我们将时间窗口进行划分，如下图，滑动窗口划成了6格，所以每格代表的是10秒钟。每过10秒钟，我们的时间窗口就会往右滑动一格。每一个格子都有自己独立的计数器counter，比如当一个请求 在0:35秒的时候到达，那么0:30~0:39对应的counter就会加1。 解决临界 那么滑动窗口怎么解决刚才的临界问题的呢？在上图中，0:59到达的100个请求会落第六个格子中，而1:00到达的请求会落在第七个格子中。当时间到达1:00时，我们的窗口会往右移动一格，那么此时时间窗口内的总请求数量一共是200个，超过了限定的100个，所以此时能够检测出来触发了限流。 3、漏桶 定义 桶中的容量是固定的，如果流入请求的速率&gt;流出的请求速率，桶中的水会溢出，请求都是拒绝访问。 流入：以任意速率往桶中放入请求。 流出：以固定速率从桶中处理请求。 请求：是唯一不重复的标识。 4、令牌 定义 系统会按恒定 1/QPS 时间间隔（如果 QPS=100，则间隔是 10ms）往桶里加入 Token100个，如果桶已经满了就不再加了。新请求来临时，会各自拿走一个 Token，如果没有 Token 可拿了就阻塞或者拒绝服务。 流入：以固定速率往桶中放入令牌。 请求：需要先获取令牌，无令牌直接拒绝服务。 流出：以固定速率从桶中处理请求。","categories":[{"name":"高并发","slug":"高并发","permalink":"http://yoursite.com/categories/%E9%AB%98%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"限流","slug":"限流","permalink":"http://yoursite.com/tags/%E9%99%90%E6%B5%81/"}]},{"title":"docker原理","slug":"docker原理","date":"2021-10-15T05:51:29.000Z","updated":"2022-11-22T01:27:53.557Z","comments":true,"path":"2021/10/15/docker原理/","link":"","permalink":"http://yoursite.com/2021/10/15/docker%E5%8E%9F%E7%90%86/","excerpt":"","text":"虚拟机 有kernel，硬件输入输出的模拟（hypervisor）。 虚拟机是一种模拟系统，即在软件层面上通过模拟硬件的输入和输出，让虚拟机的操作系统得以运行在没有物理硬件的环境中（也就是宿主机的操作系统上）。其中，这个能够模拟出硬件输入输出，让虚拟机的操作系统可以启动起来的程序，被叫做hypervisor。 一般来说，虚拟机都会有自己的kernel，自己的硬件，这样虚拟机启动的时候需要先做开机自检，启动kernel，启动用户进程等一系列行为，虽然现在电脑运行速度挺快，但是这一系列检查做下来，也要几十秒，也就是虚拟机需要几十秒来启动。 docker容器 与宿主机kernel是一致的，也没有进行硬件虚拟，这种虚拟机被命名为操作系统层虚拟化，也被叫做容器。docker容器和宿主机共享linux kernel。为了让容器像虚拟机那样有独立的文件系统，进程系统，内存系统，等等一系列，linux宿主机系统采用的办法是：通过隔离容器不让它看到主机的文件系统，进程系统，内存系统，等等一系列。 Docker使用Go语言编写，并且使用了一系列Linux内核提供的特性来实现其功能。一个能执行Docker的系统分为两大部分： Linux核心元件 Cgroup – 用来分配硬件资源 Namespace – 用来隔离不同Container的执行空间 AUFS(chroot) – 用来建立不同Container的档案系统 SELinux – 用来确保Container的网路的安全 Netlink – 用来让不同Container之间的行程进行沟通 Netfilter – 建立Container埠为基础的网路防火墙封包过滤 AppArmor – 保护Container的网路及执行安全 Linux Bridge – 让不同Container或不同主机上的Container能沟通","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"datax原理","slug":"datax原理","date":"2021-10-15T05:32:15.000Z","updated":"2022-11-22T01:27:53.545Z","comments":true,"path":"2021/10/15/datax原理/","link":"","permalink":"http://yoursite.com/2021/10/15/datax%E5%8E%9F%E7%90%86/","excerpt":"","text":"总揽 基于插件，DataX 可支持任意数据源到数据源，只要实现了 Reader/Writer Plugin，官方已经实现了主流的数据源插件，比如 MySQL、Oracle、SQLServer 等，当然我们也可以开发一个 DataX 插件。 核心概念 DataX 核心主要由 **Job、Task Group、Task、Channel **等概念组成： 1、Job 在 DataX 中用来描述一个源端到一个目的端的同步作业（可以理解为对应我们编写的任务作业的Json文件），是 DataX 数据同步面向用户的最小业务单元。一个Job 对应 一个 JobContainer， JobContainer 负责 Job 的全局切分、调度、前置语句和后置语句等工作。 2、Task Group 一组 Task 的集合，根据 DataX 的公平分配策略，公平地分配 Task 到对应的 TaskGroup 中。一个 TaskGroup 对应一个 TaskGroupContainer，负责执行一组 Task。 3、Task Job 的最小执行单元，一个 Job 可根据 Reader 端切分策略，且分成若干个 Task，以便于并发执行。 4、Channel DataX 会单独启动一条线程运行运行一个 Task，而 Task 会持有一个 Channel，用作 Reader 与 Writer 的数据传输媒介，Channel 作为传输通道，既能充当缓冲层，同时还能对数据传输进行限流操作。DataX 的数据流向都是按照 Reader—&gt;Channel—&gt;Writer 的方向流转，用如下图表示：","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"datax","slug":"datax","permalink":"http://yoursite.com/tags/datax/"}]},{"title":"greenplum若干测试","slug":"greenplum若干测试","date":"2021-10-15T02:45:18.000Z","updated":"2022-11-22T01:27:53.635Z","comments":true,"path":"2021/10/15/greenplum若干测试/","link":"","permalink":"http://yoursite.com/2021/10/15/greenplum%E8%8B%A5%E5%B9%B2%E6%B5%8B%E8%AF%95/","excerpt":"","text":"greenplum 若干测试 1、配置 配置 备注 阿里云原生数据仓库 PostgreSQL版 地址 版本 6.0标准版 master 1个 segment 4个 磁盘总容量 ESSD云盘，500G 2、数据准备 数据文件csv大小20G，约3千万行。 表结构（用户表） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879CREATE TABLE obj.adm_cust_large ( active character varying(128), address character varying(500), ads_received_num bigint, ads_viewed_num bigint, anchor_point_type character varying(128), arrival_channel character varying(128), brand_cognition_channel character varying(128), brand_recognition_channel character varying(128), budget character varying(128), check_satisfaction character varying(128), check_time timestamp without time zone, children_education_stage character varying(128), children_num character varying(128), complaint_approval character varying(128), complaint_completed character varying(128), complaint_satisfaction character varying(128), complaint_settle_time bigint, complaint_total_num bigint, current_business_district character varying(128), current_city character varying(128), current_house_area character varying(128), current_province character varying(128), cust_id character varying(128), cust_name character varying(128), cust_stat character varying(128), cust_type character varying(128), deal_total_amount character varying(128), deal_total_num bigint, deal_type character varying(128), deliver_satisfaction character varying(128), deliver_time timestamp without time zone, entertainment character varying(128), factors character varying(128), family_decision_maker character varying(128), family_structure character varying(128), favorite_hotel_brands character varying(128), follow_up_time timestamp without time zone, hobby character varying(128), house_property character varying(128), industry character varying(128), intent_layout character varying(128), intent_level character varying(128), intent_square character varying(128), long_rent_cust character varying(128), marital_status character varying(128), media_channel character varying(128), media_prefer character varying(128), media_touch character varying(128), membership_age character varying(128), membership_level character varying(128), native_place character varying(128), occupation character varying(128), other_brands character varying(128), property_cust character varying(128), proprietor character varying(128), purpose character varying(128), recommender_type character varying(128), report_repair_satisfaction character varying(128), report_repair_time timestamp without time zone, report_repair_total_num character varying(128), sex character varying(128), share_num bigint, sign_amount character varying(128), sign_satisfaction character varying(128), sign_time timestamp without time zone, solicit_amount character varying(128), solicit_project character varying(128), source_biz character varying(128), sport character varying(128), staff character varying(128), subscription_project character varying(128), subscription_satisfaction character varying(128), visit_num character varying(128), visit_peers_num character varying(128), visit_time timestamp without time zone, weekend_business_district character varying(128), id bigint); 3、测试 3.1 单列索引 cust_id列新建索引。 没有索引 12&#x2F;&#x2F; 耗时：5-6sselect * from obj.adm_cust_large where cust_id&#x3D;&#39;1000005&#39;; 普通B-Tree索引 1234&#x2F;&#x2F; 耗时：131s，索引大小：2549 MBcreate index idx_cust_id on obj.adm_cust_large (cust_id);&#x2F;&#x2F; 耗时：12-25msselect * from obj.adm_cust_large where cust_id&#x3D;&#39;1000005&#39;; gin 索引 12345&#x2F;&#x2F; 耗时：131s，索引大小：8174 MBcreate index idx_gin_cust_id on obj.adm_cust_large_copy using gin (( array[&#39;cust_id:&#39;||cust_id]));&#x2F;&#x2F; 耗时：10-22msselect * from obj.adm_cust_large where array[&#39;cust_id:&#39;||cust_id] @&gt; array[&#39;cust_id:1000005&#39;]; 3.2 多列索引 cust_id,cust_name。 每个列一个B-Tree索引 12345678&#x2F;&#x2F; 耗时：131s，索引大小：2549 MBcreate index idx_cust_id on obj.adm_cust_large (cust_id);&#x2F;&#x2F; 耗时：133s，索引大小：2549 MBcreate index idx_cust_name on obj.adm_cust_large (cust_name);&#x2F;&#x2F; 耗时：11-28 msselect * from obj.adm_cust_large where cust_id&#x3D;&#39;1000006&#39; and cust_name&#x3D;&#39;方木&#39;; 多列B-Tree索引 1234567891011&#x2F;&#x2F; 耗时：204.3s，索引大小：2166 MBcreate index idx_cust_id_name on obj.adm_cust_large (cust_id,cust_name);&#x2F;&#x2F; 耗时：12-27 msselect * from obj.adm_cust_large where cust_id&#x3D;&#39;1000006&#39; and cust_name&#x3D;&#39;方木&#39;;&#x2F;&#x2F; 耗时： 11-23 msselect * from obj.adm_cust_large where cust_id&#x3D;&#39;1000006&#39;;&#x2F;&#x2F; 耗时： 9.9 s (不符合最左匹配)select * from obj.adm_cust_large where cust_name&#x3D;&#39;方木&#39;; gin多值索引 12345678910111213&#x2F;&#x2F; 耗时：1732.3.3s，索引大小：5295 MBcreate index idx_gin_cust_id_name on obj.adm_cust_large_copy using gin (( array[&#39;cust_id:&#39;||cust_id,&#39;cust_name:&#39;||cust_name]));&#x2F;&#x2F; 耗时：12-33 msselect * from obj.adm_cust_large where array[&#39;cust_id:&#39;||cust_id,&#39;cust_name:&#39;||cust_name] @&gt; array[&#39;cust_id:1000005&#39;,&#39;cust_name:方木&#39;];&#x2F;&#x2F; 耗时：11-28 ms(没有最左匹配问题)select * from obj.adm_cust_large where array[&#39;cust_id:&#39;||cust_id,&#39;cust_name:&#39;||cust_name] @&gt; array[&#39;cust_id:1000005&#39;];&#x2F;&#x2F; 耗时：11-29 ms(没有最左匹配问题)select * from obj.adm_cust_large where array[&#39;cust_id:&#39;||cust_id,&#39;cust_name:&#39;||cust_name] @&gt; array[&#39;cust_name:方木&#39;]; 3.3 索引的选择 B-Tree 索引 优点：通过实验来看，占磁盘空间较gin小。 缺点：对于字段查询顺序有要求，会出现不是最左匹配索引失效问题。对于多字段随意顺序组合查询，难选择一个方案来构建索引。 gin 优点：构建gin数组索引，与B-Tree索引对比，它没有最左匹配问题，任意列组合/单独 顺序对与性能没有影响，速度杠杠的。 缺点：较普通的B-Tree索引大，因为它可以理解为B-Tree的展开树；不支持模糊和比较操作符查询（这是由于gp中数组操作特性决定），也是说它只能用包含/等于来圈群。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"greenplum","slug":"greenplum","permalink":"http://yoursite.com/tags/greenplum/"}]},{"title":"编写一个starter","slug":"编写一个starter","date":"2021-10-09T01:14:31.000Z","updated":"2022-11-22T01:27:54.219Z","comments":true,"path":"2021/10/09/编写一个starter/","link":"","permalink":"http://yoursite.com/2021/10/09/%E7%BC%96%E5%86%99%E4%B8%80%E4%B8%AAstarter/","excerpt":"","text":"原理和规范 原理 首先，SpringBoot 在启动时会去依赖的starter包中寻找 resources/META-INF/spring.factories文件，然后根据文件中配置的Jar包去扫描项目所依赖的Jar包，这类似于 Java 的 SPI 机制。 第二步，根据 spring.factories配置加载AutoConfigure类。 最后，根据 @Conditional注解的条件，进行自动配置并将Bean注入Spring Context 上下文当中。 我们也可以使用@ImportAutoConfiguration(&#123;MyServiceAutoConfiguration.class&#125;) 指定自动配置哪些类。 规范 Spring官方的Starter一般采取spring-boot-starter-&#123;name&#125;的命名方式，如 spring-boot-starter-web。 非官方的Starter，官方建议 命名应遵循&#123;name&#125;-spring-boot-starter的格式。 Maven依赖 12345678910111213141516171819202122 &lt;dependencies&gt; &lt;!--使jar可以在spring里自动装配--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-autoconfigure&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 在properties里写配置时候，可以提示（它会在编译时在META-INF下生成spring-configuration-metadata.json ）--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;","categories":[{"name":"springBoot","slug":"springBoot","permalink":"http://yoursite.com/categories/springBoot/"}],"tags":[{"name":"springBoot","slug":"springBoot","permalink":"http://yoursite.com/tags/springBoot/"}]},{"title":"springBoot自动装配","slug":"springBoot自动装配","date":"2021-09-28T02:38:35.000Z","updated":"2022-11-22T01:27:53.807Z","comments":true,"path":"2021/09/28/springBoot自动装配/","link":"","permalink":"http://yoursite.com/2021/09/28/springBoot%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D/","excerpt":"","text":"在使用springBoot时候，会引入很多starter，而我们也没做任何的配置，就可以在项目中直接使用spring的注解拿到starter里的类。","categories":[{"name":"springBoot","slug":"springBoot","permalink":"http://yoursite.com/categories/springBoot/"}],"tags":[{"name":"自动装配","slug":"自动装配","permalink":"http://yoursite.com/tags/%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D/"}]},{"title":"单点登录","slug":"单点登录","date":"2021-09-26T03:35:57.000Z","updated":"2022-11-22T01:27:53.880Z","comments":true,"path":"2021/09/26/单点登录/","link":"","permalink":"http://yoursite.com/2021/09/26/%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95/","excerpt":"","text":"1、介绍 单点登录（Single Sign On），简称为 SSO，是比较流行的企业业务整合的解决方案之一。SSO的定义是在多个应用系统中，用户只需要登录一次就可以访问所有相互信任的应用系统。 要实现SSO，需要以下主要的功能： 所有应用系统共享一个身份认证系统。统一的认证系统是SSO的前提之一。认证系统的主要功能是将用户的登录信息和用户信息库相比较，对用户进行登录认证；认证成功后，认证系统应该生成统一的认证标志（ticket），返还给用户。另外，认证系统还应该对ticket进行效验，判断其有效性。 所有应用系统能够识别和提取ticket信息，要实现SSO的功能，让用户只登录一次，就必须让应用系统能够识别已经登录过的用户。应用系统应该能对ticket进行识别和提取，通过与认证系统的通讯，能自动判断当前用户是否登录过，从而完成单点登录的功能。 2、实现方式 2.2 共享cookie 共享Cookie，就是主域名Cookie在二级域名下的共享，举个例子：写在父域名top.com下的Cookie，在app1.top.com、app2.top.com等子域名都是可以共享访问的。 如果我们的多个系统可以做到：前端同域、后端同Redis，那么便可以使用 共享cookie做sso。 2.3 URL重定向 如果我们的多个系统：部署在不同的域名之下，但是后端可以连接同一个Redis，那么便可以使用URL重定向的方式做到单点登录。","categories":[{"name":"单点登录","slug":"单点登录","permalink":"http://yoursite.com/categories/%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95/"}],"tags":[{"name":"sso","slug":"sso","permalink":"http://yoursite.com/tags/sso/"}]},{"title":"Java面试整理","slug":"Java面试整理","date":"2021-09-20T09:38:19.000Z","updated":"2022-11-22T01:27:53.385Z","comments":true,"path":"2021/09/20/Java面试整理/","link":"","permalink":"http://yoursite.com/2021/09/20/Java%E9%9D%A2%E8%AF%95%E6%95%B4%E7%90%86/","excerpt":"","text":"1、Java 基础 1.1 集合 List 123456实现：ArrayList、LinkedList、Vector、StackArrayList线程不安全初始容量被设置为10每次扩容的大小为newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); --&gt; n+n/2 （jdk1.8）CopyOnWriteArrayList 是线程安全的 ArrayList，写操作（add、set、remove 等等）时，把原数组拷贝一份出来，然后在新数组进行写操作，操作完后，再将原数组引用指向到新数组。CopyOnWriteArrayList 可以替代 Collections.synchronizedList(List list)。 Set 12实现：HashSet、LinkedHashSet、TreeSet Map 123456789101112131415161718192021 数组+单链表+红黑数（ 增删查时间复杂度都是O(logn) ,一般为2分所以底数是2） HashMap 里面是一个数组，然后数组中每个元素是一个单向链表。链表的实例是嵌套类 Entry 的实例。 Entry 包含四个属性：key, value, hash 值和用于单向链表的 next。 默认大小capacity为16，负载因子loadFactor为 0.75 threshold：扩容的阈值&#x3D; capacity * loadFactor &#x3D;16*0.75&#x3D;12 下标：获取key.hashCode()，然后将hashCode高16位和低16位异或（^）操作，然后与当前数组长度-1结果进行与（&amp;）操作，最终结果就是数组的下标值 &#x2F;&#x2F;第一种：先得到key的值 然后通过key值得到value值Set&lt;Integer&gt; set&#x3D;map.keySet();Set&lt;Map.Entry&lt;String, String&gt;&gt; entries &#x3D; map.entrySet(); Iterator&lt;String&gt; iterator &#x3D; keySet.iterator(); while (iterator.hasNext())&#123; String next &#x3D; iterator.next(); &#125; for (Map.Entry&lt;String, String&gt; entry : entries) &#123; String key &#x3D; entry.getKey(); String value &#x3D; entry.getValue(); &#125; 1.2 多线程 新建方式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 1 继承Thread类，重写run方法public class MyThread extends Thread &#123; @Override public void run() &#123; for (int i = 0; i &lt; 100; i++) &#123; System.out.println(this.getName() + &quot;:&quot; + i); &#125; &#125;&#125;// 2 实现Runnable 接口public class MyRunnable implements Runnable &#123; @Override public void run() &#123; for(int i = 0;i &lt; 100;i++)&#123; //由于实现接口的方式就不能直接使用Thread类的方法了，但是可以间接使用。 System.out.println(Thread.currentThread().getName()+&quot;:&quot;+i);; &#125; &#125;&#125;//3 实现Callable接口 (有返回值) static class CallAbleImpl implements Callable&lt;Boolean&gt; &#123; @Override public Boolean call() throws Exception &#123; System.out.println(&quot;等待2s开始&quot;); Thread.sleep(2000); System.out.println(&quot;等待2s结束&quot;); return true; &#125; &#125; public static void main(String[] args) throws Exception &#123; CallAbleImpl callAble = new CallAbleImpl(); FutureTask&lt;Boolean&gt; futureTask = new FutureTask&lt;&gt;(callAble); Thread thread = new Thread(futureTask); thread.start(); if (futureTask.get()) &#123; System.out.println(futureTask.get()); &#125; &#125;//4 线程池 public static void main(String[] args) throws Exception &#123; ExecutorService executorService = Executors.newFixedThreadPool(1); executorService.submit(() -&gt; &#123; System.out.println(&quot;线程池创建一个线程&quot;); &#125;); executorService.shutdown(); &#125; 线程池底层使用的是ThreadPoolExecutor，它有7个参数。 1234567891011121314151617181920212223242526 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) corePoolSize：线程池中的核心线程数，核心线程会一直存活，及时没有任务需要执行maximumPoolSize：线程池最大线程数，它表示在线程池中最多能创建多少个线程； 当线程数&gt;=corePoolSize，且任务队列已满时。线程池会创建新线程来处理任务 当线程数=maxPoolSize，且任务队列已满时，线程池会拒绝处理任务而抛出异常keepAliveTime：线程池中非核心线程闲置超时时长（准确来说应该是没有任务执行时的回收时间，后面会分析）；TimeUnit：时间单位。可选的单位有分钟（MINUTES），秒（SECONDS），毫秒(MILLISECONDS) 等；workQueue：任务的阻塞队列，缓存将要执行的Runnable任务，由各线程轮询该任务队列获取任务执行。可以选择以下几个阻塞队列。ArrayBlockingQueue：是一个基于数组结构的有界阻塞队列，此队列按 FIFO（先进先出）原则对元素进行排序。LinkedBlockingQueue：一个基于链表结构的阻塞队列，此队列按FIFO （先进先出） 排序元素，吞吐量通常要高于ArrayBlockingQueue。静态工厂方法Executors.newFixedThreadPool()使用了这个队列。SynchronousQueue：一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于LinkedBlockingQueue，静态工厂方法Executors.newCachedThreadPool使用了这个队列。PriorityBlockingQueue：一个具有优先级的无限阻塞队列。ThreadFactory：线程创建的工厂。可以进行一些属性设置，比如线程名，优先级等等，有默认实现。RejectedExecutionHandler：任务拒绝策略，当运行线程数已达到maximumPoolSize，队列也已经装满时会调用该参数拒绝任务，默认情况下是AbortPolicy，表示无法处理新任务时抛出异常。以下是JDK1.5提供的四种策略。AbortPolicy：直接抛出异常。CallerRunsPolicy：只用调用者所在线程来运行任务。DiscardOldestPolicy：丢弃队列里最老的一个任务，并执行当前任务。DiscardPolicy：不处理，丢弃掉。 2.3 Java8 Java8函数编程 2、JVM 3、框架 3.1 spring 3.2 mybatis 3.3 dubbo 4、数据库 5、分布式 6、设计模式 7、项目 8、方法论","categories":[],"tags":[]},{"title":"用户中心设计","slug":"用户中心设计","date":"2021-09-15T01:22:14.000Z","updated":"2022-11-22T01:27:54.179Z","comments":true,"path":"2021/09/15/用户中心设计/","link":"","permalink":"http://yoursite.com/2021/09/15/%E7%94%A8%E6%88%B7%E4%B8%AD%E5%BF%83%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"1、表结构 1.1 rbac 典型的用户权限系统rbac，围绕以下3个实体，每个实体之间都是多对多关系，实际会有5张表。 资源权限：被安全管理的对象（菜单、按钮等）。 角色： 用户通过业务需求确定一个角色，并按照实际的业务场景，赋予角色对应的权限的过程，角色也可以理解是权限的集合，是众多权限颗粒组成。 用户： 系统实际的操作员。 1.2 多租户 在2B企业的多租户环境下，还需要考虑租户、部门组织，售卖产品这3个实体如何融洽在RABC权限系统中。 租户：一般租户可以是独立的小微企业，也可以是一个集团公司，针对集团公司，下属分公司可以单独作为一个租户，集团公司的租户之间实质是个多层级树结构。 组织部门：每个租户可以不同的组织，也可以有组织共享，因此租户和组织部门之间是多对多关系。 产品：企业出售产品给租户，显而易见租户和产品之间是个多对多的关系。 1.3 其他 企业产品售卖可能会涉及到如下问题 产品合约租期问题，这里暂不考虑。 集团公司父子租户，会有产品/权限是否继承问题，针对这个问题可以在具体数据存储上规避，即每个租户都是拥有独立的产品/权限。 1.4 表结构关系图 单箭头，1:N的关系，不使用中间表承载关系，荣冗余关系在多的一方（箭头指向一方） 双箭头，N:N的关系，使用中间表承载关系 用户表：用户的租户信息，可以通过租户-组织部门关联到，但是考虑到租户用户查询很常见，在用户表里冗余了租户id。 租户表 123456789101112131415161718192021222324CREATE TABLE `bas_tenant` ( `id` bigint(20) NOT NULL COMMENT &#x27;主键&#x27;, `ctime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `mtime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;修改时间&#x27;, `tenant_id` bigint(20) NOT NULL COMMENT &#x27;企业租户id&#x27;, `parent_id` bigint(20) NOT NULL DEFAULT &#x27;-1&#x27; COMMENT &#x27;上级id&#x27;, `tenant_code` varchar(50) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#x27;租户编码&#x27;, `tenant_name` varchar(500) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;租户名称&#x27;, `user_id` bigint(20) NOT NULL COMMENT &#x27;租户默认管理员用户id&#x27;, `company_code` varchar(50) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#x27;企业编码&#x27;, `company_name` varchar(500) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#x27;企业名称&#x27;, `company_email` varchar(100) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#x27;企业邮箱&#x27;, `company_phone` varchar(50) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#x27;企业联系电话&#x27;, `business_license` longtext COLLATE utf8mb4_unicode_ci COMMENT &#x27;营业执照(base64)&#x27;, `type` tinyint(2) NOT NULL DEFAULT &#x27;1&#x27; COMMENT &#x27;租户类型 1:个人租户2:企业租户&#x27;, `icon` longtext COLLATE utf8mb4_unicode_ci COMMENT &#x27;图标(base64)&#x27;, `logo` longtext COLLATE utf8mb4_unicode_ci COMMENT &#x27;logo(base64)&#x27;, `banner` longtext COLLATE utf8mb4_unicode_ci COMMENT &#x27;banner(base64)&#x27;, `auth_status` tinyint(2) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;认证状态 0：待认证 1：认证通过 2：认证不通过,3:认证中;4:已注销&#x27;, `audit_status` tinyint(2) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;审核状态 0：待审核 1：审核通过 2：审核不通过,3:未审核&#x27;, `invalid` char(1) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;N&#x27; COMMENT &#x27;Y(无效)N(有效)&#x27;, PRIMARY KEY (`id`) USING BTREE, UNIQUE KEY `uk_tenant_id` (`tenant_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#x27;租户表&#x27;; 产品表 12345678910111213CREATE TABLE `bas_product` ( `id` bigint(20) NOT NULL COMMENT &#x27;主键&#x27;, `ctime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `mtime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;修改时间&#x27;, `product_Id` bigint(20) NOT NULL COMMENT &#x27;产品id&#x27;, `product_name` varchar(64) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;产品名称&#x27;, `product_code` varchar(64) COLLATE utf8mb4_unicode_ci NOT NULL COMMENT &#x27;产品标识&#x27;, `description` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#x27;描述&#x27;, `invalid` char(1) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;N&#x27; COMMENT &#x27;Y(无效)N(有效)&#x27;, PRIMARY KEY (`id`) USING BTREE, UNIQUE KEY `uk_product_code` (`product_code`) USING BTREE, UNIQUE KEY `uk_product_id` (`product_Id`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci ROW_FORMAT=COMPACT COMMENT=&#x27;产品表&#x27;; 组织部门表 12345678910111213CREATE TABLE `bas_dept` ( `id` bigint(20) NOT NULL COMMENT &#x27;主键&#x27;, `ctime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `mtime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;修改时间&#x27;, `parent_id` bigint(20) NOT NULL DEFAULT &#x27;-1&#x27; COMMENT &#x27;上级id&#x27;, `dept_id` bigint(20) NOT NULL COMMENT &#x27;组织部门id&#x27;, `dept_code` varchar(64) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;组织部门标识&#x27;, `dept_name` varchar(64) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;组织部门名称&#x27;, `description` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#x27;描述&#x27;, `invalid` char(1) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;N&#x27; COMMENT &#x27;Y(无效)N(有效)&#x27;, PRIMARY KEY (`id`) USING BTREE, KEY `dept_id_idx` (`dept_id`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci ROW_FORMAT=COMPACT COMMENT=&#x27;组织部门表&#x27;; 用户表 123456789101112131415161718192021222324CREATE TABLE `bas_user` ( `id` bigint(20) NOT NULL COMMENT &#x27;主键&#x27;, `ctime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `mtime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;修改时间&#x27;, `tenant_id` bigint(20) NOT NULL DEFAULT &#x27;1234567890&#x27; COMMENT &#x27;租户id&#x27;, `user_id` bigint(20) NOT NULL COMMENT &#x27;用户id&#x27;, `user_account` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;用户账号&#x27;, `user_name` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;用户姓名&#x27;, `password` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;登录密码&#x27;, `salt` varchar(100) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;密码盐值&#x27;, `owner` tinyint(1) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;租户默认管理员 1 是 0 否&#x27;, `sex` char(1) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#x27;性别&#x27;, `e_mail` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#x27;邮箱地址&#x27;, `identity_code` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#x27;唯一身份标识（身份证号、工号等）&#x27;, `mobile_phone` bigint(11) DEFAULT NULL COMMENT &#x27;手机号码&#x27;, `telephone` varchar(50) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#x27;固定电话号码&#x27;, `address` varchar(500) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#x27;联系地址&#x27;, `icon` text COLLATE utf8mb4_unicode_ci COMMENT &#x27;图标（base64）&#x27;, `ext` text COLLATE utf8mb4_unicode_ci COMMENT &#x27;扩展信息&#x27;, `status` tinyint(1) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;用户状态 0正常 1冻结&#x27;, `invalid` char(1) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;N&#x27; COMMENT &#x27;Y(无效)N(有效)&#x27;, PRIMARY KEY (`id`) USING BTREE, KEY `tenant_user_id_idx` (`tenant_id`,`user_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#x27;用户表&#x27;; 角色表 123456789101112CREATE TABLE `bas_role` ( `id` bigint(20) NOT NULL COMMENT &#x27;主键&#x27;, `ctime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `mtime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;修改时间&#x27;, `tenant_id` bigint(20) NOT NULL DEFAULT &#x27;1234567890&#x27; COMMENT &#x27;租户id&#x27;, `role_id` bigint(20) NOT NULL COMMENT &#x27;角色id&#x27;, `role_name` varchar(64) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;角色名&#x27;, `description` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#x27;描述&#x27;, `invalid` char(1) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;N&#x27; COMMENT &#x27;Y(无效)N(有效)&#x27;, PRIMARY KEY (`id`) USING BTREE, UNIQUE KEY `uk_role_id` (`role_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci ROW_FORMAT=COMPACT COMMENT=&#x27;角色表&#x27;; 产品权限菜单表 1234567891011121314151617CREATE TABLE `bas_product_priv` ( `id` bigint(20) NOT NULL COMMENT &#x27;主键&#x27;, `ctime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `mtime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;修改时间&#x27;, `product_id` bigint(20) NOT NULL COMMENT &#x27;所属产品id&#x27;, `parent_id` bigint(20) NOT NULL DEFAULT &#x27;-1&#x27; COMMENT &#x27;上级id&#x27;, `priv_id` bigint(20) NOT NULL COMMENT &#x27;权限/菜单id&#x27;, `priv_code` varchar(100) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;权限/菜单标识&#x27;, `priv_name` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;权限/菜单名称&#x27;, `is_menu` tinyint(1) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;是否是菜单 1 是 0 否&#x27;, `display` tinyint(1) NOT NULL DEFAULT &#x27;1&#x27; COMMENT &#x27;展示状态 1:显示, 0:隐藏&#x27;, `sort` int(11) NOT NULL DEFAULT &#x27;999999999&#x27; COMMENT &#x27;排序字段，数越小越在前&#x27;, `icon` text COLLATE utf8mb4_unicode_ci COMMENT &#x27;icon（url/base54）&#x27;, `invalid` char(1) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;N&#x27; COMMENT &#x27;Y(无效)N(有效)&#x27;, PRIMARY KEY (`id`), UNIQUE KEY `pri_id_idx` (`priv_id`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci ROW_FORMAT=COMPACT COMMENT=&#x27;产品权限菜单表&#x27;; 用户组织部门关联表 123456789CREATE TABLE `rel_user_dept` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `ctime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `mtime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;修改时间&#x27;, `user_id` bigint(20) NOT NULL COMMENT &#x27;用户id&#x27;, `dept_id` bigint(20) unsigned NOT NULL COMMENT &#x27;部门id&#x27;, `invalid` char(1) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;N&#x27; COMMENT &#x27;Y(无效)N(有效)&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#x27;组织部门-用户关联表&#x27;; 用户角色关联表 12345678910CREATE TABLE `rel_user_role` ( `id` bigint(20) NOT NULL COMMENT &#x27;主键&#x27;, `ctime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `mtime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;修改时间&#x27;, `tenant_id` bigint(20) NOT NULL DEFAULT &#x27;1234567890&#x27; COMMENT &#x27;租户id&#x27;, `user_id` bigint(20) NOT NULL COMMENT &#x27;用户id&#x27;, `role_id` bigint(20) NOT NULL COMMENT &#x27;角色id&#x27;, `invalid` char(1) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;N&#x27; COMMENT &#x27;Y(无效)N(有效)&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci ROW_FORMAT=COMPACT COMMENT=&#x27;用户-角色关联表&#x27;; 角色权限关联表 12345678910CREATE TABLE `rel_role_priv` ( `id` bigint(20) NOT NULL COMMENT &#x27;主键&#x27;, `ctime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `mtime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;修改时间&#x27;, `tenant_id` bigint(20) NOT NULL DEFAULT &#x27;1234567890&#x27; COMMENT &#x27;租户id&#x27;, `role_id` bigint(20) NOT NULL COMMENT &#x27;角色id&#x27;, `priv_id` bigint(20) NOT NULL COMMENT &#x27;权限/菜单id&#x27;, `invalid` char(1) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;N&#x27; COMMENT &#x27;Y(无效)N(有效)&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci ROW_FORMAT=COMPACT COMMENT=&#x27;角色-权限菜单关联表&#x27;; 租户产品关联表 123456789CREATE TABLE `rel_tenant_product` ( `id` bigint(20) NOT NULL COMMENT &#x27;主键&#x27;, `ctime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `mtime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;修改时间&#x27;, `tenant_id` bigint(20) NOT NULL COMMENT &#x27;租户id&#x27;, `product_id` bigint(20) NOT NULL COMMENT &#x27;产品id&#x27;, `invalid` char(1) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;N&#x27; COMMENT &#x27;Y(无效)N(有效)&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci ROW_FORMAT=COMPACT COMMENT=&#x27;租户-产品关联表&#x27;; 租户部门表 123456789CREATE TABLE `rel_tenant_dept` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `ctime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, `mtime` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;修改时间&#x27;, `tenant_id` bigint(20) NOT NULL COMMENT &#x27;租户id&#x27;, `dept_id` bigint(20) unsigned NOT NULL COMMENT &#x27;部门id&#x27;, `invalid` char(1) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#x27;N&#x27; COMMENT &#x27;Y(无效)N(有效)&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#x27;租户-组织部门关联表&#x27;; 2、系统初始化流程 2.1 内置数据 将售卖的产品和产品的受限资源填充到产品表、菜单/权限表里面。 2.2 客户数据同步 企业客户有自己的组织和用户，我们可以需要同步到本系统来，使用组织表、用户-组织关联表、用户表。 2.3 租户注册 企业用户注册租户。使用租户表、角色表（添加租户默认超级管理员）、用户表（添加注册用户）、用户-角色表（默认此用户为租户默认超级管理员）。 2.4 租户关联信息配置 租户购买产品信息配置，租户拥有那些组织部门，租户设置产品使用角色控制产品使用者权限。 使用租户产品表、角色表、租户-组织表，用户表（回填租户id）。 2.5 权限配置 租户配置其产品使用者权限。使用角色-权限表、用户-角色表。 2.6 流程模块图 3、系统分析 本身不考虑多租户问题，简单的rbac那一套，5张表就能解决一个通用的用户中心表结构，这里为了后续业务发展设计了多租户，本身多租户业内大概有3种解决方案。 3.1 独立数据库 这是第一种方案，即一个租户一个数据库，这种方案的用户数据隔离级别最高，安全性最好，但成本较高。 优点：为不同的租户提供独立的数据库，有助于简化数据模型的扩展设计，满足不同租户的独特需求；如果出现故障，恢复数据比较简单。 缺点： 增多了数据库的安装数量，随之带来维护成本和购置成本的增加。 这种方案与传统的一个客户、一套数据、一套部署类似，差别只在于软件统一部署在运营商那里。如果面对的是银行、医院等需要非常高数据隔离级别的租户，可以选择这种模式，提高租用的定价。如果定价较低，产品走低价路线，这种方案一般对运营商来说是无法承受的。 3.2 共享数据库，独立 Schema 这是第二种方案，即多个或所有租户共享Database，但是每个租户一个Schema（也可叫做一个user）。底层库比如是：DB2、ORACLE等，一个数据库下可以有多个SCHEMA 优点： 为安全性要求较高的租户提供了一定程度的逻辑数据隔离，并不是完全隔离；每个数据库可支持更多的租户数量。 缺点： 如果出现故障，数据恢复比较困难，因为恢复数据库将牵涉到其他租户的数据； 如果需要跨租户统计数据，存在一定困难。 3.3 共享数据库，共享 Schema，共享数据表 这是第三种方案，即租户共享同一个Database、同一个Schema，但在表中增加TenantID多租户的数据字段。这是共享程度最高、隔离级别最低的模式。 即每插入一条数据时都需要有一个客户的标识。这样才能在同一张表中区分出不同客户的数据。 优点：三种方案比较，第三种方案的维护和购置成本最低，允许每个数据库支持的租户数量最多。 缺点： 隔离级别最低，安全性最低，需要在设计开发时加大对安全的开发量； 数据备份和恢复最困难，需要逐表逐条备份和还原。如果希望以最少的服务器为最多的租户提供服务，并且租户接受牺牲隔离级别换取降低成本，这种方案最适合。 4、本系统分析 4.1 实现方式 我们使用的是共享数据库，共享 Schema，共享数据表方式来实现多租户架构，这里还考虑了租户父子层级关系，每个表之间尽量遵循DB的三范式设计，使它结构较清晰后续扩展较好。 4.2 外部系统对接 4.2.1 主数据对接 一般客户方，都有建设主数据，这里我们只需要编写程序，同步过来即可，涉及到数据表有： 组织表 用户表 用户-组织关系表 4.2.2 单点登录对接 从上述流程可以看出，sso登录有几个关键点 sso登录地址 sso验证ST地址 令牌ST和用户绑定token 上述流程是SSO登录完整流程，有的企业SSO对接，它的流程更为简单，只有上图红色部分，其余过程SSO帮助我们拦截处理了，我们只需要提供一个可接受ST的url给SSO就可以。","categories":[{"name":"用户中心","slug":"用户中心","permalink":"http://yoursite.com/categories/%E7%94%A8%E6%88%B7%E4%B8%AD%E5%BF%83/"}],"tags":[{"name":"rbac","slug":"rbac","permalink":"http://yoursite.com/tags/rbac/"}]},{"title":"spark实践","slug":"spark实践","date":"2021-09-10T09:00:59.000Z","updated":"2022-11-22T01:27:53.800Z","comments":true,"path":"2021/09/10/spark实践/","link":"","permalink":"http://yoursite.com/2021/09/10/spark%E5%AE%9E%E8%B7%B5/","excerpt":"","text":"1、发展历程 2009年Spark诞生于伯克利AMPLab。 项目在2010年早些时候开源，很多早期关于Spark系统的思想在不同论文中发表。 项目开源之后，在GitHub上成立了Spark开发社区并在2013年成为Apache孵化项目。 该项目在2014年2月成为Apache顶级项目。 2014年5月30日Spark 1.0.0版正式上线。 2、特点 2.1 轻量快速 Spark通过减少磁盘IO来达到性能的提升，它们将中间处理数据全部放到了内存中。Spark使用了RDD（Resilient Distributed Datasets）数据抽象，这允许它可以在内存中存储数据，只在需要时才持久化到磁盘。这种做法大大的减少了数据处理过程中磁盘的读写，大幅度的降低了运行时间。 2.2 易于使用 Spark支持多语言。Spark允许Java、Scala、Python及R。 2.3 支持复杂查询 除了简单的map及reduce操作之外，Spark还支持filter、foreach、reduceByKey、aggregate以及SQL查询、流式查询等复杂查询。 2.4 实时的流处理 Spark Streaming主要用来对数据进行实时处理。 2.5 与已存Hadoop数据整合 Spark不仅可以独立的运行（使用standalone模式），还可以运行在当下的YARN管理集群中。它还可以读取已有的任何Hadoop数据，这是个非常大的优势，它可以运行在任何Hadoop数据源上，比如HBase、HDFS等。 3、核心模块 Spark主要包括Spark Core和在Spark Core基础之上建立的应用框：架交互式查询Spark SQL、实时流处理Spark Streaming、机器学习MLlib、图计算GraphX。 Core库中主要包括上下文（Spark Context）、抽象数据集（RDD）、调度器（Scheduler）、**洗牌（shuffle）和序列化器（Serializer）**等。Spark系统中的计算、IO、调度和shuffle等系统基本功能都在其中。 4、运行模式 Spark支持三种集群资源管理器，分别是Standalone、Yarn和Mesos，其中Standalone为Spark自带。 4.1 Standalone 运行 12# n 代表几个线程运行spark-shell --master local[n] 读取本地文件 12345scala&gt; var tf=sc.textFile(&quot;file:/media/psf/hf/名字.md&quot;)tf: org.apache.spark.rdd.RDD[String] = file:/media/psf/hf/名字.md MapPartitionsRDD[9] at textFile at &lt;console&gt;:24scala&gt; tf.countres10: Long = 1495 读取hdfs文件 12345scala&gt; var tt=sc.textFile(&quot;hdfs://c1:9000/hf/资料/user.csv&quot;)tt: org.apache.spark.rdd.RDD[String] = hdfs://c1:9000/hf/资料/user.csv MapPartitionsRDD[11] at textFile at &lt;console&gt;:24scala&gt; tt.countres11: Long = 4 4.2 yarn运行 12# 设置","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark安装","slug":"spark安装","date":"2021-09-10T03:59:18.000Z","updated":"2022-11-22T01:27:53.796Z","comments":true,"path":"2021/09/10/spark安装/","link":"","permalink":"http://yoursite.com/2021/09/10/spark%E5%AE%89%E8%A3%85/","excerpt":"","text":"Scala 安装 spark基于scala开发，所以我们先安装scala。 1234wget http://www.scala-lang.org/files/archive/scala-2.11.8.tgztar -zxvf scala-2.11.8.tgz -C /usr/localcd /usr/localmv scala-2.11.8 scala 环境变量 1234567891011vim /etc/profileexport SCALA_HOME=/usr/local/scalaexport PATH=$PATH:$SCALA_HOME/binsource /etc/profile# 测试scalaWelcome to Scala 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_301).Type in expressions for evaluation. Or try :help.scala&gt;:q spark安装 123456789101112131415161718192021222324252627282930mkdir -p /opt/sparkwget https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgzcd /opt/sparktar -zxvf spark-3.1.2-bin-hadoop3.2.tgz# 环境变量export SPARK_HOME=/opt/spark/spark-3.1.2-bin-hadoop3.2export PATH=$PATH:$SPARK_HOME/binsource /etc/profile# 测试[hf@c1 spark]$ spark-shell2021-09-10 12:13:48,004 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSetting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).Spark context Web UI available at http://c1:4040Spark context available as &#x27;sc&#x27; (master = local[*], app id = local-1631247232980).Spark session available as &#x27;spark&#x27;.Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ &#x27;_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 3.1.2 /_/Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_301)Type in expressions to have them evaluated.Type :help for more information.scala&gt;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"hadoop总结","slug":"hadoop总结","date":"2021-09-05T02:25:18.000Z","updated":"2022-11-22T01:27:53.647Z","comments":true,"path":"2021/09/05/hadoop总结/","link":"","permalink":"http://yoursite.com/2021/09/05/hadoop%E6%80%BB%E7%BB%93/","excerpt":"","text":"1. 版本历史 0.X：最早一个版本 1.X：主要修复0.X版本若干bug 2.X：架构产生大变化，引入yarn，目前生产使用最多的版本 3.X：在2.X基础上引入hdfs新特性，未来的趋势 2. 运行模式 本地模式：无任何守护进程，所有程序运行在同一个JVM，调试MR程序非常高校 伪分布式：守护进程运行在本地，模拟一个小集群，换句话就是可以配置一台机器的hadoop集群 完全分布式：守护进程运行在一个集群上，需要多台机器 3. 组成 3.1 hdfs namenode：主节点，主要负责集群的管理，元数据信息保存 datanode：从节点：存储数据 3.1.1 数据块 磁盘默认读写的数据块大小，一般文件系统为512字节，在hdfs中默认为128M，它的设计主要为了节省寻址开销，例如：一个寻址时间为10ms，传输速率为100M/s，为了使寻址时间占用传输时间的1%，我们设置块大小为100M。 3.1.2 namenode 维护文件系统树及整颗树所有的文件的目录。这些信息以2个文件形式永远保存在本地磁盘：命名空间镜像；编辑日志文件。元数据文件的写操作为同步&amp;原子性。 没有namenode，文件系统将无法使用，hadoop提供2种机制保障容错： 配置多个文件系统保存元数据文件，例：本地磁盘；网络文件系统（NFS）。 secondNamenode：在另外一台物理机器，定期合并命名空间镜像；编辑日志文件。 3.1.3 块缓存 对于访问频繁的文件，文件对应的块会被缓存在datanode的内存中，默认一个块只缓存在一个datanode中。 3.1.4 联邦hdfs namenode在内存中保存文件和数据块的引用关系，在一个大集群中内存将成为系统横向扩展的瓶颈。联邦环境下，每个namenode维护一个命名空间卷（命名空间元数据、数据块池），一个datanode需要注册到每个namenode中，需要存储多个数据块池的数据。 3.1.5 命令 1bin&#x2F;hadoop fs 具体命令 或者 bin&#x2F;hdfs dfs 具体命令 -help 1hdfs dfs -help ls -ls 123456# 显示当前目录结构hadoop fs -ls &lt;path&gt;# 递归显示当前目录结构hadoop fs -ls -R &lt;path&gt;# 显示根目录下内容hadoop fs -ls / -cat | text 123# 二选一执行即可hadoop fs -text &lt;path&gt; hadoop fs -cat &lt;path&gt; -put | -copyFromLocal 123# 二选一执行即可hadoop fs -put [localsrc] [dst] hadoop fs -copyFromLocal [localsrc] [dst] -get | -copyToLocal 123# 二选一执行即可hadoop fs -get [dst] [localsrc] hadoop fs -copyToLocal [dst] [localsrc] -rm 1234# 删除文件hadoop fs -rm &lt;path&gt;# 递归删除目录和文件hadoop fs -rm -R &lt;path&gt; -mkdir 1234# 创建目录hadoop fs -mkdir &lt;path&gt; # 递归创建目录hadoop fs -mkdir -p &lt;path&gt; -touch ｜ touchz getmerge job -kill -moveFromLocal -appendToFile -cp 1hadoop fs -cp [src] [dst] mv 1hadoop fs -mv [src] [dst] -tail 123hadoop fs -tail &lt;path&gt; # 和Linux下一样，会持续监听文件内容变化 并显示文件的最后一千字节hadoop fs -tail -f &lt;path&gt; -du 12# 统计文件系统的可用空间信息hadoop fs -df -h / -setrep 权限控制 1234567# 权限控制和Linux上使用方式一致# 变更文件或目录的所属群组。 用户必须是文件的所有者或超级用户。hadoop fs -chgrp [-R] GROUP URI [URI ...]# 修改文件或目录的访问权限 用户必须是文件的所有者或超级用户。hadoop fs -chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; URI [URI ...]# 修改文件的拥有者 用户必须是超级用户。hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ] 文件检测 123456789-d：如果路径是目录，返回 0。-e：如果路径存在，则返回 0。-f：如果路径是文件，则返回 0。-s：如果路径不为空，则返回 0。-r：如果路径存在且授予读权限，则返回 0。-w：如果路径存在且授予写入权限，则返回 0。-z：如果文件长度为零，则返回 0。 mapreduce input : 读取文本文件； splitting : 将文件按照行进行拆分，此时得到的 K1 行数，V1 表示对应行的文本内容； mapping : 并行将每一行按照空格进行拆分，拆分得到的 List(K2,V2)，其中 K2 代表每一个单词，由于是做词频统计，所以 V2 的值为 1，代表出现 1 次； shuffling：由于 Mapping 操作可能是在不同的机器上并行处理的，所以需要通过 shuffling 将相同 key 值的数据分发到同一个节点上去合并，这样才能统计出最终的结果，此时得到 K2 为每一个单词，List(V2) 为可迭代集合，V2 就是 Mapping 中的 V2； Reducing : 这里的案例是统计单词出现的总次数，所以 Reducing 对 List(V2) 进行归约求和操作，最终输出。 yarn ResourceManager：分配资源调度的分配 NodeManager：负责具体工作","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"DDD动动开发","slug":"DDD动动开发","date":"2021-09-03T06:49:52.000Z","updated":"2022-11-22T01:27:53.353Z","comments":true,"path":"2021/09/03/DDD动动开发/","link":"","permalink":"http://yoursite.com/2021/09/03/DDD%E5%8A%A8%E5%8A%A8%E5%BC%80%E5%8F%91/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Mysql安装","slug":"Mysql安装","date":"2021-09-02T12:13:57.000Z","updated":"2022-11-22T01:27:53.404Z","comments":true,"path":"2021/09/02/Mysql安装/","link":"","permalink":"http://yoursite.com/2021/09/02/Mysql%E5%AE%89%E8%A3%85/","excerpt":"","text":"系统 12cat /etc/redhat-releaseCentOS Linux release 7.9.2009 (Core) 1、centos下安装 下载并安装MySQL官方的 Yum Repository 1sudo rpm -ivh https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm 确认仓库是否成功 12345sudo yum repolist all | grep mysql | grep enabled#如果展示像下面,则表示成功添加仓库mysql-connectors-community/x86_64 MySQL Connectors Community enabled: 51mysql-tools-community/x86_64 MySQL Tools Community enabled: 63mysql57-community/x86_64 MySQL 5.7 Community Server enabled: 267 yum 安装 1yum -y install mysql57-community-release-el7-10.noarch.rpm 报错 1234567从 file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql 检索密钥源 &quot;MySQL 5.7 Community Server&quot; 的 GPG 密钥已安装，但是不适用于此软件包。请检查源的公钥 URL 是否配置正确。 失败的软件包是：mysql-community-server-5.7.38-1.el7.x86_64 GPG 密钥配置为：file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql# 运行rpm --import https://repo.mysql.com/RPM-GPG-KEY-mysql-2022 安装MySQL服务器 1yum -y install mysql-community-server 启动设置 启动 123systemctl start mysqld.servicesystemctl enable mysqld.servicesystemctl status mysqld.service 查看密码 123grep &quot;password&quot; /var/log/mysqld.log# 密码：baerel(-+4jN2021-09-02T12:05:40.956639Z 1 [Note] A temporary password is generated for root@localhost: baerel(-+4jN 登录&amp;设置 123456789mysql -u root -ppwd：输入上述密码# 修改密码限制 set global validate_password_policy=0; set global validate_password_length=0; ALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;root@1298&#x27;; grant all privileges on *.* to &#x27;root&#x27;@&#x27;%&#x27; identified by &#x27;root@1298&#x27; with grant option; 查看物理目录 1show global variables like &quot;%datadir%&quot;; ubuntu下安装 查看mysql包信息 sudo apt-cache show mysql-server,ubuntu18 适配是5.7，随后直接 sudo apt install mysql-server -y 进行下载了 安装 12sudo apt install mysql-server -yservice mysql status 跳过密码和远程登录 sudo vi /etc/mysql/mysql.conf.d/mysqld.cnf 在39行下添加上这一行 1，这样就不用管密码了，再顺便开启远程登录，注释掉 2 设置root用户认证方式为mysql 123mysqluse msyqlmysql&gt;UPDATE user SET plugin=&#x27;mysql_native_password&#x27; WHERE User=&#x27;root&#x27;; 重启 1sudo service mysql restart 登录修改密码 12345$ mysqlmysql&gt; update mysql.user set authentication_string = password(&#x27;newPwd&#x27;) where user = &#x27;root&#x27;;Query OK, 1 row affected, 1 warning (0.01 sec)Rows matched: 1 Changed: 1 Warnings: 1 注释掉之前免密（skip-grant-tables） 1sudo vi /etc/mysql/mysql.conf.d/mysqld.cnf msyql配置文件 查找 123 mysqld --help --verbose | grep &#x27;cnf&#x27;/etc/my.cnf /etc/mysql/my.cnf /usr/etc/my.cnf ~/.my.cnf my.cnf, $MYSQL_TCP_PORT, /etc/services, built-in default","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"CDH安装","slug":"CDH安装","date":"2021-09-01T09:04:59.000Z","updated":"2022-11-22T01:27:53.351Z","comments":true,"path":"2021/09/01/CDH安装/","link":"","permalink":"http://yoursite.com/2021/09/01/CDH%E5%AE%89%E8%A3%85/","excerpt":"","text":"ASDhf123$","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"http://yoursite.com/tags/CDH/"}]},{"title":"shell脚本","slug":"shell脚本","date":"2021-08-23T01:50:09.000Z","updated":"2022-11-22T01:27:53.743Z","comments":true,"path":"2021/08/23/shell脚本/","link":"","permalink":"http://yoursite.com/2021/08/23/shell%E8%84%9A%E6%9C%AC/","excerpt":"","text":"参考自阮一峰的：https://wangdoc.com/bash/ 命令格式 命令行环境中，主要通过使用 Shell 命令，进行各种操作。Shell 命令基本都是下面的格式。 1$ command [ arg1 ... [ argN ]] 分号 使得一行可以放置多个命令，上一个命令执行结束后，再执行第二个命令。 1clear;ls &amp;&amp; 上一个命令执行成功后，再执行第二个命令。 1clear &amp;&amp; ls | | 上一个命令执行失败后，再执行第二个命令。 123(base) ➜ ~ cat a.txt || echo &#x27;a.txt do not exit&#x27;cat: a.txt: No such file or directorya.txt do not exit 快捷键 ctrl + l：清除屏幕并将当前行移到页面顶部。 ctrl + c：中止当前正在执行的命令。 ctrl + d：关闭 Shell 会话。 ctrl + u：从光标位置删除到行首。 ctrl + k：从光标位置删除到行尾。 shift + PageUp：向上滚动。 shift + PageDown：向下滚动。 ↑,↓：浏览已执行命令的历史记录。 tab：补全命令。 type Bash 本身内置了很多命令，同时也可以执行外部程序。怎么知道一个命令是内置命令，还是外部程序呢？ type命令用来判断命令的来源。 1234(base) ➜ ~ type echoecho is a shell builtin(base) ➜ ~ type lsls is an alias for ls -G echo echo命令的作用是在屏幕输出一行文本，可以将该命令的参数原样输出。 12(base) ➜ ~ echo hellohello 如果想要输出的是多行文本，即包括换行符。这时就需要把多行文本放在引号里面 12345678(base) ➜ ~ echo &quot;&lt;HTML&gt; &lt;HEAD&gt; &lt;TITLE&gt;Page Title&lt;/TITLE&gt; &lt;/HEAD&gt;&quot;&lt;HTML&gt; &lt;HEAD&gt; &lt;TITLE&gt;Page Title&lt;/TITLE&gt; &lt;/HEAD&gt; -n 取消末尾的回车符，使得下一个提示符紧跟在输出内容的后面. 123456(base) ➜ ~ echo a ; echo bab(base) ➜ ~ echo -n a ; echo bab -e -e参数会解释引号（双引号和单引号）里面的特殊字符（比如换行符\\n）。如果不使用-e参数，即默认情况下，引号会让特殊字符变成普通字符，echo不解释它们，原样输出。 123456789101112$ echo &quot;Hello\\nWorld&quot;Hello\\nWorld# 双引号的情况$ echo -e &quot;Hello\\nWorld&quot;HelloWorld# 单引号的情况$ echo -e &#x27;Hello\\nWorld&#x27;HelloWorld 模式扩展 模式扩展与正则表达式的关系是，模式扩展早于正则表达式出现，可以看作是原始的正则表达式。它的功能没有正则那么强大灵活，但是优点是简单和方便。 Shell 接收到用户输入的命令以后，会根据空格将用户的输入，拆分成一个个词元（token）。然后，Shell 会扩展词元里面的特殊字符，扩展完成后才会调用相应的命令。Bash 是先进行扩展，再执行命令。因此，扩展的结果是由 Bash 负责的，与所要执行的命令无关。命令本身并不存在参数扩展，收到什么参数就原样执行。 Bash 一共提供八种扩展。 波浪线扩展 ? 字符扩展 * 字符扩展 方括号扩展 大括号扩展 变量扩展 子命令扩展 算术扩展 波浪线扩展 12345# 当前用户目录cd ~# 指定用户目录cd ~username ? 字符扩展 ?字符代表文件路径里面的任意单个字符，不包括空字符。例如：???，就是代表三个字符。 12(base) ➜ ~ ls ???.shapp.sh * 字符扩展 当前目录（不包含子目录）任意数量的任意字符 1234567(base) ➜ ~ ls -l *.zip-rw-r--r--@ 1 hf staff 83774040 6 24 10:44 tt.zip-rw-r--r-- 1 hf staff 2327 1 13 2021 归档.zip-rw-r--r--@ 1 hf staff 2007108 4 25 15:42 部署手册.zip(base) ➜ ~ ls t*.jstest.js 子目录任意数量的任意字符 1234567891011# 一层子目录 (*/*)(base) ➜ test ls */*.txtsubtest/subtest.txt# 2层子目录(*/*/*)(base) ➜ test ls */*/*.txtsubtest/subtest-sub/subtest-sub.txt# 当前目录和所有子目录（**/*）(base) ➜ test ls -l **/*.txt-rw-r--r-- 1 hf staff 8 8 23 11:42 subtest/subtest-sub/subtest-sub.txt-rw-r--r-- 1 hf staff 9 8 23 11:37 subtest/subtest.txt-rw-r--r-- 1 hf staff 4 8 23 11:37 test.txt 方括号扩展 方括号扩展的形式是[...]，只有文件确实存在的前提下才会扩展。如果文件不存在，就会原样输出。括号之中的任意一个字符。比如，[aeiou]可以匹配五个元音字母中的任意一个。 123456789101112131415161718(base) ➜ test ls -ltrtotal 32-rw-r--r-- 1 hf staff 7 8 23 11:45 a.txt-rw-r--r-- 1 hf staff 8 8 23 11:46 ab.txt-rw-r--r-- 1 hf staff 8 8 23 11:46 c.txt-rw-r--r-- 1 hf staff 8 8 23 11:47 b.txt# [ab].txt，匹配a或者b中一个(base) ➜ test ls [ab].txta.txt b.txt# [^ab]=[!ab]，排除a、b 之外的字符(base) ➜ test ls [^ab].txtc.txt# [start-end] 扩展(base) ➜ test ls [a-z].txta.txt b.txt c.txt 简写形式[start-end] [a-z]：所有小写字母。 [a-zA-Z]：所有小写字母与大写字母。 [a-zA-Z0-9]：所有小写字母、大写字母与数字。 [abc]*：所有以a、b、c字符之一开头的文件名。 program.[co]：文件program.c与文件program.o。 BACKUP.[0-9][0-9][0-9]：所有以BACKUP.开头，后面是三个数字的文件名。 大括号扩展 大括号扩展&#123;...&#125;表示分别扩展成大括号里面的所有值，各个值之间使用逗号分隔。比如，&#123;1,2,3&#125;扩展成1 2 3。大括号内部的逗号前后不能有空格。否则，大括号扩展会失效 12345(base) ➜ test echo &#123;1,2,3&#125;1 2 3(base) ➜ test echo a&#123;b,c,d,e&#125;ab ac ad ae 简写形式&#123;start..end&#125; 123456# 默认步长为1(base) ➜ test echo &#123;0..9&#125;0 1 2 3 4 5 6 7 8 9# 指定步长，&#123;start..end..step）(base) ➜ test echo &#123;1..10..2&#125;1 3 5 7 9 应用 新建：2020到2021这段时间：”年份-月份“目录， 123456789101112131415161718192021222324252627(base) ➜ test mkdir &#123;2020..2021&#125;-&#123;01..12&#125;(base) ➜ test ls -ltrtotal 0drwxr-xr-x 2 hf staff 64 8 23 12:12 2020-01drwxr-xr-x 2 hf staff 64 8 23 12:12 2020-02drwxr-xr-x 2 hf staff 64 8 23 12:12 2020-03drwxr-xr-x 2 hf staff 64 8 23 12:12 2020-04drwxr-xr-x 2 hf staff 64 8 23 12:12 2020-05drwxr-xr-x 2 hf staff 64 8 23 12:12 2020-06drwxr-xr-x 2 hf staff 64 8 23 12:12 2020-07drwxr-xr-x 2 hf staff 64 8 23 12:12 2020-08drwxr-xr-x 2 hf staff 64 8 23 12:12 2020-09drwxr-xr-x 2 hf staff 64 8 23 12:12 2020-10drwxr-xr-x 2 hf staff 64 8 23 12:12 2020-11drwxr-xr-x 2 hf staff 64 8 23 12:12 2020-12drwxr-xr-x 2 hf staff 64 8 23 12:12 2021-01drwxr-xr-x 2 hf staff 64 8 23 12:12 2021-02drwxr-xr-x 2 hf staff 64 8 23 12:12 2021-03drwxr-xr-x 2 hf staff 64 8 23 12:12 2021-04drwxr-xr-x 2 hf staff 64 8 23 12:12 2021-05drwxr-xr-x 2 hf staff 64 8 23 12:12 2021-06drwxr-xr-x 2 hf staff 64 8 23 12:12 2021-07drwxr-xr-x 2 hf staff 64 8 23 12:12 2021-08drwxr-xr-x 2 hf staff 64 8 23 12:12 2021-09drwxr-xr-x 2 hf staff 64 8 23 12:12 2021-10drwxr-xr-x 2 hf staff 64 8 23 12:12 2021-11drwxr-xr-x 2 hf staff 64 8 23 12:12 2021-12 变量扩展 $&#123;&#125; Bash 将美元符号$开头的词元视为变量，将其扩展成变量值。 123456# 直接跟在$后(base) ➜ test echo $JAVA_HOME/Library/Java/JavaVirtualMachines/jdk1.8.0_261.jdk/Contents/Home# 变量名除了放在美元符号后面，也可以放在$&#123;&#125;里面。(base) ➜ test echo $&#123;JAVA_HOME&#125;/Library/Java/JavaVirtualMachines/jdk1.8.0_261.jdk/Contents/Home 子命令扩展 $(command) 12(base) ➜ test echo $(date)2021年 8月23日 星期一 13时24分30秒 CST 算术扩展 $((...)) 12(base) ➜ test echo $((2+2))4 变量 语法：variable=value 12345678910111213141516171819# 当前shell创建变量，多个变量使用;分隔(base) ➜ test foo=1# 读取变量 $var=$&#123;var&#125;=$&quot;var&quot;(base) ➜ test echo $foo1# 删除变量(base) ➜ test unset foo(base) ➜ test echo &quot;$foo&quot;# export 子shell可操作的变量(base) ➜ test foo=10(base) ➜ test echo $foo10# 开启子shell(base) ➜ test basbash-3.2$ echo $foo10 特殊变量 这些变量的值由 Shell 提供，用户不能进行赋值！ 1234567`$?`为上一个命令的退出码，用来判断上一个命令是否执行成功。返回值是`0`，表示上一个命令执行成功；如果是非零，上一个命令执行失败。$$为当前 Shell 的进程 ID。$_为上一个命令的最后一个参数。$!为最近一个后台执行的异步命令的进程 ID。$0为当前 Shell 的名称（在命令行直接执行时）或者脚本名（在脚本中执行时）。$-为当前 Shell 的启动参数。$#表示脚本的参数数量，$@表示脚本的参数值 变量默认值 只返回默认值：$&#123;varname:-word&#125; 1234bash-3.2$ echo $&#123;count:-10&#125;10bash-3.2$ echo $count 返回默认值，且赋值：$&#123;varname:=word&#125; 1234bash-3.2$ echo $&#123;count:=10&#125;10bash-3.2$ echo $count10 测试变量吃否存在，存在=1：$&#123;varname:+word&#125; 1234bash-3.2$ echo $&#123;foo:+1&#125;1bash-3.2$ echo $&#123;fooo:+1&#125; 终断shell执行，并抛出异常信息 123# 1表示脚本的第一个参数。如果该参数不存在，就退出脚本并报错bash-3.2$ filename=$&#123;1:?&quot;filename missing.&quot;&#125;bash: 1: filename missing. declare 命令 语法：declare option variable=vale declare命令的主要参数（OPTION）如下。 -a：声明数组变量。 -f：输出所有函数定义。 -F：输出所有函数名。 -i：声明整数变量。 -l：声明变量为小写字母。 -p：查看变量信息。 -r：声明只读变量。 -u：声明变量为大写字母。 -x：该变量输出为环境变量。 -i参数 -i参数声明整数变量以后，可以直接进行数学运算。 1234bash-3.2$ declare -i foo=11 uoo=10 resbash-3.2$ res=foo*uoobash-3.2$ echo $res110 -x参数 -x参数等同于export命令，可以输出一个变量为子 Shell 的环境变量。 12bash-3.2$ declare -x foobash-3.2$ export foo -r参数 -r参数可以声明只读变量，无法改变变量值，也不能unset变量。等同于命令：readonly 1234# readonly foo=1bash-3.2$ declare -r o=10bash-3.2$ o=11bash: o: readonly variable -p参数 输出已定义变量的值，对于未定义的变量，会提示找不到 12bash-3.2$ declare -p foodeclare -ix foo=&quot;11&quot; let 命令 执行算术表达式 123456789# let命令可以直接计算 1 + 2。[deploy@localhost ~]$ let foo=1+1[deploy@localhost ~]$ echo $foo2# let命令的参数表达式如果包含空格，就需要使用引号。[deploy@localhost ~]$ let &quot;foo = 1 *4&quot;[deploy@localhost ~]$ echo $foo4 赋值多个变量 1234# 赋值表达式之间使用空格分隔[deploy@localhost ~]$ let &quot;v1=3&quot; &quot;v2=4&quot;[deploy@localhost ~]$ echo $v1,$v23,4 字符串 字符串长度：$&#123;#varname&#125; 12345[deploy@localhost ~]$ pwd/home/deploy[deploy@localhost ~]$ mypath=$PWD[deploy@localhost ~]$ echo $&#123;#mypath&#125;12 子字符串：&#123;varname:offset:length&#125; 123456789# 从位置`offset`开始（从`0`开始计算），长度为`length`[deploy@localhost ~]$ echo $&#123;mypath:0:5&#125;/home# 省略length，则从位置offset开始，一直返回到字符串的结尾[deploy@localhost ~]$ echo $&#123;mypath:5&#125;/deploy# offset为负值，表示从字符串的末尾开始算起,负数前面必须有一个空格， 以防止与$&#123;variable:-word&#125;的变量的设置默认值语法混淆[deploy@localhost ~]$ echo $&#123;mypath: -7:7&#125;/deploy 头匹配 1234567891011121314151617181920212223&#96;&#96;&#96;$&#123;variable##patern&#125;&#96;&#96;&#96;检查字符串开头匹配否（最长匹配，贪婪匹配），匹配删除开头，返回剩余部分&#96;&#96;&#96;$&#123;variable&#x2F;#pattern&#x2F;string&#125;&#96;&#96;&#96;将头部匹配的部分，替换成其他内容&#96;&#96;&#96;shell(base) ➜ test mypath&#x3D;$PWD&#x2F;test.txt(base) ➜ test pwd&#x2F;Users&#x2F;hf&#x2F;test# &#123;variable#pattern&#125;，删除最短匹配（非贪婪匹配）的部分，返回剩余部分(base) ➜ test echo $&#123;mypath#&#x2F;*&#x2F;&#125;hf&#x2F;test&#x2F;test.txt# &#123;variable##pattern&#125;，检查字符串开头匹配否（最长匹配，贪婪匹配），匹配删除开头，返回剩余部分(base) ➜ test echo $&#123;mypath##&#x2F;*&#x2F;&#125;test.txt# &#123;variable&#x2F;#pattern&#x2F;string&#125;匹配成功，替换(base) ➜ test foo&#x3D;JPG.JPG.JPG(base) ➜ test echo $&#123;foo&#x2F;#JPG&#x2F;jpg&#125;jpg.JPG.JPG# 如果匹配不成功，则返回原始字符串(base) ➜ test echo $&#123;mypath##123&#125;&#x2F;Users&#x2F;hf&#x2F;test&#x2F;test.txt 尾匹配 123456789101112131415161718192021&#96;&#96;&#96;$&#123;variable%%pattern&#125;&#96;&#96;&#96;删除最长匹配（贪婪匹配）的部分，返回剩余部分&#96;&#96;&#96;$&#123;variable&#x2F;%pattern&#x2F;string&#125;&#96;&#96;&#96;将尾部匹配的部分，替换成其他内容&#96;&#96;&#96;shell(base) ➜ test mypath&#x3D;$PWD&#x2F;test.txt.txt# $&#123;variable%pattern&#125; 删除最短匹配（非贪婪匹配）的部分，返回剩余部分(base) ➜ test echo $&#123;mypath%.*&#125;&#x2F;Users&#x2F;hf&#x2F;test&#x2F;test.txt# $&#123;variable%%pattern&#125;删除最长匹配（贪婪匹配）的部分，返回剩余部分(base) ➜ test echo $&#123;mypath%%.*&#125;&#x2F;Users&#x2F;hf&#x2F;test&#x2F;test# &#123;variable&#x2F;%pattern&#x2F;string&#125;将尾部匹配的部分，替换成其他内容(base) ➜ test foo&#x3D;JPG.JPG.JPG(base) ➜ test echo $&#123;foo&#x2F;%JPG&#x2F;jpg&#125;JPG.JPG.jpg# 如果匹配不成功，则返回原始字符串(base) ➜ test echo $&#123;mypath%123&#125;&#x2F;Users&#x2F;hf&#x2F;test&#x2F;test.txt.txt 任意位置匹配 string 替换，但仅替换第一个匹配1234567891011121314&#96;&#96;&#96;$&#123;variable&#x2F;&#x2F;pattern&#x2F;string&#125;&#96;&#96;&#96;最长匹配（贪婪匹配）的那部分被 string 替换，所有匹配都替换上面两种语法都是最长匹配（贪婪匹配）下的替换，区别是前一个语法仅仅替换第一个匹配，后一个语法替换所有匹配。&#96;&#96;&#96;shell(base) ➜ test path&#x3D;&#x2F;home&#x2F;cam&#x2F;foo&#x2F;foo.name# $&#123;variable&#x2F;pattern&#x2F;string&#125;最长匹配（贪婪匹配）的那部分被 string 替换，但仅替换第一个匹配(base) ➜ test echo $&#123;path&#x2F;foo&#x2F;bar&#125;&#x2F;home&#x2F;cam&#x2F;bar&#x2F;foo.name# $&#123;variable&#x2F;&#x2F;pattern&#x2F;string&#125;最长匹配（贪婪匹配）的那部分被 string 替换，所有匹配都替换(base) ➜ test echo $&#123;path&#x2F;&#x2F;foo&#x2F;bar&#125;&#x2F;home&#x2F;cam&#x2F;bar&#x2F;bar.name 改变大小写 12345[deploy@localhost ~]$ foo=/test[deploy@localhost ~]$ echo $&#123;foo,,&#125;/test[deploy@localhost ~]$ echo $&#123;foo^^&#125;/TEST 算术表达式 ((...))运算符 不支持非整数参数。 12345678910111213141516# 会自动忽略内部的空格((foo=1+3))=((foo=1+ 3))[deploy@localhost ~]$ ((foo=1+3))[deploy@localhost ~]$ echo $foo4# 读取运算后的值 $((a+b))[deploy@localhost ~]$ echo $((foo=2+3))5# 逗号前后两个表达式都会执行，然后返回后一个表达式的值[deploy@localhost ~]$ echo $((foo = 1 + 2, 3 * 4))12# 改变运算顺序[deploy@localhost ~]$ echo $(( (2 + 3) * 4 ))20 数值进制 12345678910111213141516# number：没有任何特殊表示法的数字是十进制数（以10为底）。[deploy@localhost ~]$ echo $((10))10# 0number：八进制数。[deploy@localhost ~]$ echo $((010))8# 0xnumber：十六进制数。[deploy@localhost ~]$ echo $((0x10))16# base#number：base进制的数。[deploy@localhost ~]$ echo $((2#10))2[deploy@localhost ~]$ echo $((8#10))8[deploy@localhost ~]$ echo $((16#10))16 运算符 123456789101112131415161718192021222324252627282930313233343536+：加法-：减法*：乘法/：除法（整除）%：余数**：指数++：自增运算（前缀或后缀）--：自减运算（前缀或后缀）&lt;&lt;：位左移运算，把一个数字的所有位向左移动指定的位。&gt;&gt;：位右移运算，把一个数字的所有位向右移动指定的位。&amp;：位的“与”运算，对两个数字的所有位执行一个AND操作。|：位的“或”运算，对两个数字的所有位执行一个OR操作。~：位的“否”运算，对一个数字的所有位取反。^：位的异或运算（exclusive or），对两个数字的所有位执行一个异或操作。&lt;：小于&gt;：大于&lt;=：小于或相等&gt;=：大于或相等==：相等!=：不相等&amp;&amp;：逻辑与||：逻辑或!：逻辑否expr1?expr2:expr3：三元条件运算符parameter = value：简单赋值。parameter += value：等价于parameter = parameter + value。parameter -= value：等价于parameter = parameter – value。parameter *= value：等价于parameter = parameter * value。parameter /= value：等价于parameter = parameter / value。parameter %= value：等价于parameter = parameter % value。parameter &lt;&lt;= value：等价于parameter = parameter &lt;&lt; value。parameter &gt;&gt;= value：等价于parameter = parameter &gt;&gt; value。parameter &amp;= value：等价于parameter = parameter &amp; value。parameter |= value：等价于parameter = parameter | value。parameter ^= value：等价于parameter = parameter ^ value。 expr 命令 不支持非整数参数。 1234567# 注意空格[deploy@localhost ~]$ expr 2 + 35[deploy@localhost ~]$ foo=2[deploy@localhost ~]$ expr $foo + 35 let命令 1234# 注意没空格[deploy@localhost ~]$ let x=3+4[deploy@localhost ~]$ echo $x7 脚本 开始行 脚本的第一行通常是指定解释器，即这个脚本必须通过什么解释器执行。这一行以#!字符开头，这个字符称为 Shebang，所以这一行就叫做 Shebang 行。 #!后面就是脚本解释器的位置，Bash 脚本的解释器一般是/bin/sh或/bin/bash。#!与脚本解释器之间有没有空格，都是可以的。 123#!/bin/sh# 或者#!/bin/bash 如果 Bash 解释器不放在目录/bin，脚本就无法执行了。为了保险，可以写成下面这样。上面命令使用env命令（这个命令总是在/usr/bin目录），返回 Bash 可执行文件的位置 1#!/usr/bin/env bash Shebang 行不是必需的，但是建议加上这行。如果缺少该行，就需要手动将脚本传给解释器。举例来说，脚本是script.sh，有 Shebang 行的时候，可以直接调用执行。 1$ ./script.sh 如果没有 Shebang 行，就只能手动将脚本传给解释器来执行。 123$ /bin/sh ./script.sh# 或者$ bash ./script.sh env env命令总是指向/usr/bin/env文件，或者说，这个二进制文件总是在目录/usr/bin。 #!/usr/bin/env NAME这个语法的意思是，让 Shell 查找$PATH环境变量里面第一个匹配的NAME。如果你不知道某个命令的具体路径，或者希望兼容其他用户的机器，这样的写法就很有用。 /usr/bin/env bash的意思就是，返回bash可执行文件的位置，前提是bash的路径是在$PATH里面。其他脚本文件也可以使用这个命令。比如 Node.js 脚本的 Shebang 行，可以写成下面这样。 1#!/usr/bin/env node 注释 Bash 脚本中，#表示注释，可以放在行首，也可以放在行尾。 1234# 本行是注释echo &#x27;Hello World!&#x27;echo &#x27;Hello World!&#x27; # 井号后面的部分也是注释 脚本参数 调用脚本的时候，脚本文件名后面可以带有参数。 1script.sh word1 word2 word3 上面例子中，script.sh是一个脚本文件，word1、word2和word3是三个参数。 脚本文件内部，可以使用特殊变量，引用这些参数。 $0：脚本文件名，即script.sh。 $1~$9：对应脚本的第一个参数到第九个参数。 $#：参数的总数。 $@：全部的参数，参数之间使用空格分隔。 $*：全部的参数，参数之间使用变量$IFS值的第一个字符分隔，默认为空格，但是可以自定义。 如果脚本的参数多于9个，那么第10个参数可以用$&#123;10&#125;的形式引用，以此类推。 例子： 123456789#!/bin/bash# script.shecho &quot;全部参数：&quot; $@echo &quot;命令行参数数量：&quot; $#echo &#x27;$0 = &#x27; $0echo &#x27;$1 = &#x27; $1echo &#x27;$2 = &#x27; $2echo &#x27;$3 = &#x27; $3 结果： 1234567$ ./script.sh a b c全部参数：a b c命令行参数数量：3$0 = script.sh$1 = a$2 = b$3 = c shift命令 shift命令可以改变脚本参数，每次执行都会移除脚本当前的第一个参数（$1），使得后面的参数向前一位，即$2变成$1、$3变成$2、$4变成$3，以此类推。 while循环结合shift命令，也可以读取每一个参数。 123456789#!/bin/bashecho &quot;一共输入了 $# 个参数&quot;while [ &quot;$1&quot; != &quot;&quot; ]; do echo &quot;剩下 $# 个参数&quot; echo &quot;参数：$1&quot; shiftdone shift命令可以接受一个整数作为参数，指定所要移除的参数个数，默认为1。 1shift 2 if 语法 1234567if commands; then commands[elif commands; then commands...][else commands]fi if关键字后面，跟的是一个命令。这个命令可以是test命令，也可以是其他命令。命令的返回值为0表示判断成立，否则表示不成立。因为这些命令主要是为了得到返回值，所以可以视为表达式。 test 123456789101112131415161718192021222324# 写法一test expression# 写法二[ expression ]# 写法三[[ expression ]]# 写法一if test -e /tmp/foo.txt ; then echo &quot;Found foo.txt&quot;fi# 写法二if [ -e /tmp/foo.txt ] ; then echo &quot;Found foo.txt&quot;fi# 写法三if [[ -e /tmp/foo.txt ]] ; then echo &quot;Found foo.txt&quot;fi 实际上，[这个字符是test命令的一种简写形式，可以看作是一个独立的命令，这解释了为什么它后面必须有空格。expression 可以有很多，下面列举常用的表达式。 文件判断expression 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[ -a file ]：如果 file 存在，则为true。[ -b file ]：如果 file 存在并且是一个块（设备）文件，则为true。[ -c file ]：如果 file 存在并且是一个字符（设备）文件，则为true。[ -d file ]：如果 file 存在并且是一个目录，则为true。[ -e file ]：如果 file 存在，则为true。[ -f file ]：如果 file 存在并且是一个普通文件，则为true。[ -g file ]：如果 file 存在并且设置了组 ID，则为true。[ -G file ]：如果 file 存在并且属于有效的组 ID，则为true。[ -h file ]：如果 file 存在并且是符号链接，则为true。[ -k file ]：如果 file 存在并且设置了它的“sticky bit”，则为true。[ -L file ]：如果 file 存在并且是一个符号链接，则为true。[ -N file ]：如果 file 存在并且自上次读取后已被修改，则为true。[ -O file ]：如果 file 存在并且属于有效的用户 ID，则为true。[ -p file ]：如果 file 存在并且是一个命名管道，则为true。[ -r file ]：如果 file 存在并且可读（当前用户有可读权限），则为true。[ -s file ]：如果 file 存在且其长度大于零，则为true。[ -S file ]：如果 file 存在且是一个网络 socket，则为true。[ -t fd ]：如果 fd 是一个文件描述符，并且重定向到终端，则为true。 这可以用来判断是否重定向了标准输入／输出／错误。[ -u file ]：如果 file 存在并且设置了 setuid 位，则为true。[ -w file ]：如果 file 存在并且可写（当前用户拥有可写权限），则为true。[ -x file ]：如果 file 存在并且可执行（有效用户有执行／搜索权限），则为true。[ file1 -nt file2 ]：如果 FILE1 比 FILE2 的更新时间最近，或者 FILE1 存在而 FILE2 不存在，则为true。[ file1 -ot file2 ]：如果 FILE1 比 FILE2 的更新时间更旧，或者 FILE2 存在而 FILE1 不存在，则为true。[ FILE1 -ef FILE2 ]：如果 FILE1 和 FILE2 引用相同的设备和 inode 编号，则为true。#!/bin/bashFILE=~/.bashrcif [ -e &quot;$FILE&quot; ]; then if [ -f &quot;$FILE&quot; ]; then echo &quot;$FILE is a regular file.&quot; fi if [ -d &quot;$FILE&quot; ]; then echo &quot;$FILE is a directory.&quot; fi if [ -r &quot;$FILE&quot; ]; then echo &quot;$FILE is readable.&quot; fi if [ -w &quot;$FILE&quot; ]; then echo &quot;$FILE is writable.&quot; fi if [ -x &quot;$FILE&quot; ]; then echo &quot;$FILE is executable/searchable.&quot; fielse echo &quot;$FILE does not exist&quot; exit 1fi 字符串判断expression 123456789101112131415161718192021222324252627[ string ]：如果string不为空（长度大于0），则判断为真。[ -n string ]：如果字符串string的长度大于零，则判断为真。[ -z string ]：如果字符串string的长度为零，则判断为真。[ string1 = string2 ]：如果string1和string2相同，则判断为真。[ string1 == string2 ] 等同于[ string1 = string2 ]。[ string1 != string2 ]：如果string1和string2不相同，则判断为真。[ string1 &#x27;&gt;&#x27; string2 ]：如果按照字典顺序string1排列在string2之后，则判断为真。[ string1 &#x27;&lt;&#x27; string2 ]：如果按照字典顺序string1排列在string2之前，则判断为真。#!/bin/bashANSWER=maybeif [ -z &quot;$ANSWER&quot; ]; then echo &quot;There is no answer.&quot; &gt;&amp;2 exit 1fiif [ &quot;$ANSWER&quot; = &quot;yes&quot; ]; then echo &quot;The answer is YES.&quot;elif [ &quot;$ANSWER&quot; = &quot;no&quot; ]; then echo &quot;The answer is NO.&quot;elif [ &quot;$ANSWER&quot; = &quot;maybe&quot; ]; then echo &quot;The answer is MAYBE.&quot;else echo &quot;The answer is UNKNOWN.&quot;fi 整数判断expression 1234567891011121314151617181920212223242526272829[ integer1 -eq integer2 ]：如果integer1等于integer2，则为true。[ integer1 -ne integer2 ]：如果integer1不等于integer2，则为true。[ integer1 -le integer2 ]：如果integer1小于或等于integer2，则为true。[ integer1 -lt integer2 ]：如果integer1小于integer2，则为true。[ integer1 -ge integer2 ]：如果integer1大于或等于integer2，则为true。[ integer1 -gt integer2 ]：如果integer1大于integer2，则为true。#!/bin/bashINT=-5if [ -z &quot;$INT&quot; ]; then echo &quot;INT is empty.&quot; &gt;&amp;2 exit 1fiif [ $INT -eq 0 ]; then echo &quot;INT is zero.&quot;else if [ $INT -lt 0 ]; then echo &quot;INT is negative.&quot; else echo &quot;INT is positive.&quot; fi if [ $((INT % 2)) -eq 0 ]; then echo &quot;INT is even.&quot; else echo &quot;INT is odd.&quot; fifi 正则判断expression 123456789101112131415# regex是一个正则表示式，=~是正则比较运算符[[ string1 =~ regex ]]# 判断变量INT的字符串形式，是否满足^-?[0-9]+$的正则模式，如果满足就表明它是一个整数。#!/bin/bashINT=-5if [[ &quot;$INT&quot; =~ ^-?[0-9]+$ ]]; then echo &quot;INT is an integer.&quot; exit 0else echo &quot;INT is not an integer.&quot; &gt;&amp;2 exit 1fi 算术条件 如果算术计算的结果是非零值，则表示判断成立。这一点跟命令的返回值正好相反，需要小心。 123456789101112# 判断if ((3 &gt; 2)); then echo &quot;true&quot;fi# 赋值，(( foo = 5 ))完成了两件事情。首先把5赋值给变量foo，然后根据返回值5，判断条件为真if (( foo = 5 ));then echo &quot;foo is $foo&quot;; fifoo is 5# 赋值语句返回等号右边的值，如果返回的是0，则判断为假if (( foo = 0 ));then echo &quot;It is true.&quot;;else echo &quot;It is false.&quot;; fiIt is false. expression 逻辑组合 AND运算：符号&amp;&amp;，也可使用参数-a。OR运算：符号||，也可使用参数-o。NOT运算：符号! AND的例子，判断整数是否在某个范围之内，&amp;&amp;用来连接两个判断条件：大于等于$MIN_VAL，并且小于等于$MAX_VAL。 123456789101112131415161718# #!/bin/bashMIN_VAL=1MAX_VAL=100INT=50if [[ &quot;$INT&quot; =~ ^-?[0-9]+$ ]]; then if [[ $INT -ge $MIN_VAL &amp;&amp; $INT -le $MAX_VAL ]]; then echo &quot;$INT is within $MIN_VAL to $MAX_VAL.&quot; else echo &quot;$INT is out of range.&quot; fielse echo &quot;INT is not an integer.&quot; &gt;&amp;2 exit 1fi 否定操作符!例子，最好用圆括号确定转义的范围。 12345if [ ! \\( $INT -ge $MIN_VAL -a $INT -le $MAX_VAL \\) ]; then echo &quot;$INT is outside $MIN_VAL to $MAX_VAL.&quot;else echo &quot;$INT is in range.&quot;fi case case结构用于多值判断，可以为每个值指定对应的命令，跟包含多个elif的if结构等价，但是语义更好。它的语法如下。 123456789101112131415161718192021case expression in pattern ) commands ;; pattern ) commands ;; ...esac#!/bin/bashOS=$(uname -s)case &quot;$OS&quot; in FreeBSD) echo &quot;This is FreeBSD&quot; ;; Darwin) echo &quot;This is Mac OSX&quot; ;; AIX) echo &quot;This is AIX&quot; ;; Minix) echo &quot;This is Minix&quot; ;; Linux) echo &quot;This is Linux&quot; ;; *) echo &quot;Failed to identify this OS&quot; ;;esac case的匹配模式可以使用各种通配符，下面是一些例子。 a)：匹配a。 a|b)：匹配a或b。 [[:alpha:]])：匹配单个字母。 ???)：匹配3个字符的单词。 *.txt)：匹配.txt结尾。 *)：匹配任意输入，通过作为case结构的最后一个模式。 1234567891011#!/bin/bashecho -n &quot;输入一个字母或数字 &gt; &quot;read charactercase $character in [[:lower:]] | [[:upper:]] ) echo &quot;输入了字母 $character&quot; ;; [0-9] ) echo &quot;输入了数字 $character&quot; ;; * ) echo &quot;输入不符合要求&quot;esac while 只要符合条件，就不断循环执行指定的语句 12345678910111213#关键字do可以跟while不在同一行，这时两者之间不需要使用分号分隔while condition; do commandsdone#!/bin/bashnumber=0while [ &quot;$number&quot; -lt 10 ]; do echo &quot;Number = $number&quot; number=$((number + 1))done util 与while循环恰好相反，只要不符合判断条件（判断条件失败），就不断循环执行指定的语句 123456789101112#关键字do可以跟util不在同一行，这时两者之间不需要使用分号分隔until condition; do commandsdone#!/bin/bashnumber=0until [ &quot;$number&quot; -ge 10 ]; do echo &quot;Number = $number&quot; number=$((number + 1))done for … in 123456789101112131415161718192021222324#关键字do可以跟for不在同一行，这时两者之间不需要使用分号分隔for variable in list; do commandsdonefor i in word1 word2 word3; do echo $idonefor i in *.png; do ls -l $idone# in list的部分可以省略，这时list默认等于脚本的所有参数$@。但是，为了可读性，最好还是不要省略，参考下面的例子。for filename; do echo &quot;$filename&quot;done# 等同于for filename in &quot;$@&quot; ; do echo &quot;$filename&quot;done for 1234567891011121314151617181920212223242526272829#关键字do可以跟for不在同一行，这时两者之间不需要使用分号分隔for (( expression1; expression2; expression3 )); do commandsdonefor (( i=0; i&lt;5; i=i+1 )); do echo $idone# break终止循环for number in 1 2 3 4 5 6do echo &quot;number is $number&quot; if [ &quot;$number&quot; = &quot;3&quot; ]; then break fidone# continue命令立即终止本轮循环，开始执行下一轮循环while read -p &quot;What file do you want to test?&quot; filenamedo if [ ! -e &quot;$filename&quot; ]; then echo &quot;The file does not exist.&quot; continue fi echo &quot;You entered a valid file..&quot;done select 结构 select结构主要用来生成简单的菜单。它的语法与for...in循环基本一致。 12345select name[in list]do commandsdone Bash 会对select依次进行下面的处理。 select生成一个菜单，内容是列表list的每一项，并且每一项前面还有一个数字编号。 Bash 提示用户选择一项，输入它的编号。 用户输入以后，Bash 会将该项的内容存在变量name，该项的编号存入环境变量REPLY。如果用户没有输入，就按回车键，Bash 会重新输出菜单，让用户选择。 执行命令体commands。 执行结束后，回到第一步，重复这个过程。 1234567#!/bin/bash# select.shselect brand in Samsung Sony iphone symphony Waltondo echo &quot;You have chosen $brand&quot;done 执行上面的脚本，Bash 会输出一个品牌的列表，让用户选择。 1234567$ ./select.sh1) Samsung2) Sony3) iphone4) symphony5) Walton#? 如果用户没有输入编号，直接按回车键。Bash 就会重新输出一遍这个菜单，直到用户按下Ctrl + c，退出执行。 select可以与case结合，针对不同项，执行不同的命令。 12345678910111213141516171819#!/bin/bashecho &quot;Which Operating System do you like?&quot;select os in Ubuntu LinuxMint Windows8 Windows10 WindowsXPdo case $os in &quot;Ubuntu&quot;|&quot;LinuxMint&quot;) echo &quot;I also use $os.&quot; ;; &quot;Windows8&quot; | &quot;Windows10&quot; | &quot;WindowsXP&quot;) echo &quot;Why don&#x27;t you try Linux?&quot; ;; *) echo &quot;Invalid entry.&quot; break ;; esacdone 函数 定义使用 123456789101112131415161718# 第一种fn() &#123; # codes&#125;# 第二种function fn() &#123; # codes&#125;# 定义hello函数hello() &#123; echo &quot;Hello $1&quot;&#125;# 调用：函数名 参数hello worldhello world 全局/局部变量 函数体内直接声明的变量，属于全局变量，整个脚本都可以读取。这一点需要特别小心。 123456789101112# 全局fn () &#123; foo=1 echo &quot;fn: foo = $foo&quot;&#125;# 局部fn () &#123; local foo foo=1 echo &quot;fn: foo = $foo&quot;&#125; 数组 创建 12345678910111213141516171819202122ARRAY=(value1 value2 ... valueN)ARRAY=( value1 value2 value3)array=(a b c)array=([2]=c [0]=a [1]=b)days=(Sun Mon Tue Wed Thu Fri Sat)days=([0]=Sun [1]=Mon [2]=Tue [3]=Wed [4]=Thu [5]=Fri [6]=Sat)# 当前目录的所有 MP3 文件，放进一个数组mp3s=( *.mp3 )# declare -a命令声明一个数组declare -a arrayName#将用户的命令行输入，存入一个数组read -a dice 读取 12345678910111213141516171819# 读取数组指定位置，大括号是必不可少echo $&#123;array[i]&#125; for i in &quot;$&#123;names[@]&#125;&quot;; do echo $idoneactivities=( swimming &quot;water skiing&quot; canoeing &quot;white-water rafting&quot; surfing )for act in &quot;$&#123;activities[@]&#125;&quot;; \\do \\echo &quot;Activity: $act&quot;; \\doneActivity: swimmingActivity: water skiingActivity: canoeingActivity: white-water raftingActivity: surfing 拷贝数组 1234# 数组activities被拷贝给了另一个数组hobbieshobbies=( &quot;$&#123;activities[@]&#125;&quot; )# 新数组hobbies在数组activities的所有成员之后，又添加了一个成员hobbies=( &quot;$&#123;activities[@]&#125;&quot; diving ) 数组长度 12345678910111213141516$&#123;#array[*]&#125;$&#123;#array[@]&#125;# 把字符串赋值给100位置的数组元素，这时的数组只有一个元素$ a[100]=foo$ echo $&#123;#a[*]&#125;1$ echo $&#123;#a[@]&#125;1# 数组成员，就会返回该成员的字符串长度$ a[100]=foo$ echo $&#123;#a[100]&#125;3 数组下标 $&#123;!array[@]&#125;或$&#123;!array[*]&#125;，可以返回数组的成员序号，即哪些位置是有值的。 123456789101112$ arr=([5]=a [9]=b [23]=c)$ echo $&#123;!arr[@]&#125;5 9 23$ echo $&#123;!arr[*]&#125;5 9 23arr=(a b c d)for i in $&#123;!arr[@]&#125;;do echo $&#123;arr[i]&#125;done 提取数组成员 $&#123;array[@]:position:length&#125;的语法可以提取数组成员。 12345678$ food=( apples bananas cucumbers dates eggs fajitas grapes )$ echo $&#123;food[@]:1:1&#125;bananas$ echo $&#123;food[@]:1:3&#125;bananas cucumbers dates# 如果省略长度参数length，则返回从指定位置开始的所有成员。$ echo $&#123;food[@]:4&#125;eggs fajitas grapes 追加数组成员 使用+=赋值运算符。它能够自动地把值追加到数组末尾。否则，就需要知道数组的最大序号，比较麻烦。 1234567$ foo=(a b c)$ echo $&#123;foo[@]&#125;a b c$ foo+=(d e f)$ echo $&#123;foo[@]&#125;a b c d e f 删除数组 删除一个数组成员，使用unset命令。 12345678910$ foo=(a b c d e f)$ echo $&#123;foo[@]&#125;a b c d e f$ unset foo[2]$ echo $&#123;foo[@]&#125;a b d e f# unset ArrayName可以清空整个数组。unset foo","categories":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/categories/linux/"}],"tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"greenplum系统统计","slug":"greenplum系统统计","date":"2021-08-19T07:37:32.000Z","updated":"2022-11-22T01:27:53.634Z","comments":true,"path":"2021/08/19/greenplum系统统计/","link":"","permalink":"http://yoursite.com/2021/08/19/greenplum%E7%B3%BB%E7%BB%9F%E7%BB%9F%E8%AE%A1/","excerpt":"","text":"1、数据库对象尺寸函数 函数名 返回类型 描述 pg_column_size(any) int 存储一个指定的数值需要的字节数（可能压缩过） pg_database_size(oid) bigint 指定OID的数据库使用的磁盘空间 pg_database_size(name) bigint 指定名称的数据库使用的磁盘空间 pg_indexes_size(regclass) bigint 关联指定表OID或表名的表索引的使用总磁盘空间 pg_relation_size(relation regclass, fork text) bigint 指定OID或名的表或索引，通过指定fork(‘main’, ‘fsm’ 或’vm’)所使用的磁盘空间 pg_relation_size(relation regclass) bigint pg_relation_size(..., 'main')的缩写 pg_size_pretty(bigint) text Converts a size in bytes expressed as a 64-bit integer into a human-readable format with size units pg_size_pretty(numeric) text 把以字节计算的数值转换成一个人类易读的尺寸单位 pg_table_size(regclass) bigint 指定表OID或表名的表使用的磁盘空间，除去索引（但是包含TOAST，自由空间映射和可视映射） pg_tablespace_size(oid) bigint 指定OID的表空间使用的磁盘空间 pg_tablespace_size(name) bigint 指定名称的表空间使用的磁盘空间 pg_total_relation_size(regclass) bigint 指定表OID或表名使用的总磁盘空间，包括所有索引和TOAST数据 2、数据库相关 所有数据库大小 1select pg_database.datname, pg_size_pretty(pg_database_size(pg_database.datname)) AS size from pg_database; 指定数据库大小 1select pg_size_pretty(pg_database_size(&#x27;db_name&#x27;)); 3、schema相关 查看所schema 1select * from pg_namespace; 指定schema，表大小排序 1select relname, pg_size_pretty(pg_relation_size(relid)) from pg_stat_user_tables where schemaname=&#x27;schema_name&#x27; order by pg_relation_size(relid) desc; 指定schema，索引大小排序 1select indexrelname, pg_size_pretty(pg_indexes_size(relid)) from pg_stat_user_indexes where schemaname=&#x27;schema_name&#x27; order by pg_relation_size(relid) desc; 指定schema，所有表的列 1select * from information_schema.columns where table_schema=&#x27;schema_name&#x27; and table_name&lt;&gt;&#x27;pg_stat_statements&#x27;; 指定schema，某个表的列 1select column_name from information_schema.columns where table_schema=&#x27;schema_name&#x27; and table_name=&#x27;table&#x27;; 4、表 查看一个表大小 1select pg_size_pretty(pg_relation_size(&#x27;table_name&#x27;)); 查出所有表，并按大小排序 12345678SELECT table_schema || &#x27;.&#x27; || table_name AS table_full_name, pg_size_pretty(pg_total_relation_size(&#x27;&quot;&#x27; ||table_schema || &#x27;&quot;.&quot;&#x27; || table_name || &#x27;&quot;&#x27;)) AS sizeFROM information_schema.tablesORDER BY pg_total_relation_size(&#x27;&quot;&#x27; || table_schema || &#x27;&quot;.&quot;&#x27; || table_name || &#x27;&quot;&#x27;)DESC limit 20 重命名表名 1alter table old.name rename to new_name; 5、索引相关 新建索引 12345// B-Tree索引create index idx_name on table_name (colume);// Gin索引 (原生的gin索引需要在多值上新建，tree_gin 扩展可以在单列上新建gin)create index idx_name on table_name using gin (array[1,2]); 删除索引 1drop index [schema.]idx_name; 删除重复记录 1delete from t where (gp_segment_id, ctid) not in (select gp_segment_id, min(ctid) from t group by xx, gp_segment_id);","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"greenplum","slug":"greenplum","permalink":"http://yoursite.com/tags/greenplum/"}]},{"title":"greenplum数据操作","slug":"greenplum数据操作","date":"2021-08-12T02:43:47.000Z","updated":"2022-11-22T01:27:53.622Z","comments":true,"path":"2021/08/12/greenplum数据操作/","link":"","permalink":"http://yoursite.com/2021/08/12/greenplum%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C/","excerpt":"","text":"copy 使用copy命令进行数据导入时，数据需要经过Master节点分发到Segment节点；同样使用copy命令进行数据卸载，数据也需要由Segment发送到Master节点，由Master节点汇总后再写入外部文件。这样就限制了数据加载与卸载的效率，但是数据量较小的情况下，copy命令就非常方便。 准备数据目录 1mkdir -p /path/on/gpinstall/machine/data 连接gp 1psql -h ip -p port -U username dbname copy to 把一个表的所有内容都拷贝到一个文件。 1234// 系统的copy命令，需要管理员用户执行 [header] 有就是带表头字段名copy [schema.]tablename to &#x27;/path/on/gp-machine&#x27; delimiter &#x27;,&#x27; csv [header]// pg自带的copy，不需要管理员就可以执行 [header] 有就是带表头字段名\\copy [schema.]tablename to &#x27;/path/on/gp-machine&#x27; delimiter &#x27;,&#x27; csv [header] copy from 从一个文件里拷贝数据到一个表里。 1234// 系统的copy命令，需要管理员用户执行copy [schema.]tablename from &#x27;/path/on/gp-machine&#x27; with csv// pg自带的copy，不需要管理员就可以执行\\copy [schema.]tablename to &#x27;/path/on/gp-machine&#x27; with csv shell 脚本例子 扫描/opt/greenplum 目录下所有的csv文件，使用copy命令导入db，前提db里面表结构需要存在。每个csv对应一张表。 12345678910111213141516171819#!/usr/bin/env baship=$1;port=$2;username=$3;pwd=$4;folder=$5ip=$&#123;ip:-&#x27;127.0.0.1&#x27;&#125;;port=$&#123;port:-5432&#125;username=$&#123;username:-&#x27;gpadmin&#x27;&#125;;pwd=$&#123;pwd-:&#x27;gpadmin&#x27;&#125;folder=$&#123;folder:-&#x27;/opt/greenplum&#x27;&#125;# 免密export PGPASSWORD=$pwdfor csv_file_path in $folder/*/*.csv ; do csv_file_name=$&#123;csv_file_path##/*/&#125; table_name=$&#123;csv_file_name%.*&#125; schema_with_csv=$&#123;csv_file_path#/opt/greenplum/&#125; schema=$&#123;schema_with_csv%%/*&#125; copy_sql=$&#123;copy_sql:=&quot;\\copy $schema.$table_name from&quot; &quot;&#x27;$csv_file_path&#x27;&quot;&#125; del_sql=$&#123;del_sql:=&quot;delete from $schema.$table_name&quot;&#125; psql -h $ip -p $port -U $username -d dbname -c &quot;$del_sql&quot; &amp;&amp; psql -h $ip -p $port -U $username -d dbname -c &quot;$copy_sql with csv&quot; unset copy_sql unset del_sqldone gpfdist 进入容器，创建外部文件目录 12docker exec -it gp6 /bin/bashmkdir extdata 开启gpfdist 12su - gpadminnohup gpfdist -d /data/gpdata -p 8085 -l /tmp/gpfdist.log &amp; -d 数据文件所放的目录，我这里的路径为/extdata -p 设置访问gpfdist端口，这个可以根据实际情况写。 -l 设置日志文件所放的目录，这个参数也可以不用填写。 copy文件到容器 12// 注意和 gpfdist -d /extdata/ 的目录要一样docker cp /file/path/on/local containerId:/extdata 新建外部表和内部表 1234567891011CREATE external TABLE ext_cust ( name varchar, age smallint, id bigint) location(&#x27;gpfdist://主节点的ip:8085/adm_cust.csv&#x27;) format &#x27;csv&#x27; (delimiter &#x27;,&#x27;);CREATE TABLE inner_cust ( name varchar, age smallint, id bigint); 加载数据 1insert into inner_cust select * from ext_cust; gpload","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"greenplum","slug":"greenplum","permalink":"http://yoursite.com/tags/greenplum/"}]},{"title":"greenplum执行计划","slug":"greenplum执行计划","date":"2021-08-11T03:48:57.000Z","updated":"2022-11-22T01:27:53.621Z","comments":true,"path":"2021/08/11/greenplum执行计划/","link":"","permalink":"http://yoursite.com/2021/08/11/greenplum%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92/","excerpt":"","text":"Greenplum的执行计划大部分和postgresql一样，但是作为MPP数据库，SQL执行上有很多MPP痕迹。 执行计划命令 1EXPLAIN [ ANALYZE ] [ VERBOSE ] statement ANALYZE：执行命令并显示实际运行时间。 VERBOSE：显示规划树完整的内部表现形式，而不仅是一个摘要。通常，这个选项只是在特殊的调试过程中有用，VERBOSE 输出是否打印工整的，具体取决于配置参数 explain_pretty_print 的值。 statement：查询执行计划的 SQL 语句，可以是任何 select、insert、update、delete、values、execute、declare 语句。 分布式执行计划 广播和重分布 Greenplum是分布式架构，关联数据在不同的节点上，需要数据计算就需要发生数据迁移。数据迁移有2种方式：广播和重分布。 广播（Broadcast）：即一个表的所有数据进行广播，使每个节点上都存在一份全量数据 重分布（Redistribute）：关联键和原本的分布键不一致，使用此刻的关联键来重新分布数据到每个节点 术语 数据扫描方式 顺序扫描 Seq Scan 将数据文件从头到尾读取一次。 索引扫描 Index Scan 通过索引定位到数据，定位到的数据较小，再返回数据。 位图堆扫描：Bitmap Heap Scan 当索引定位到的数据在整表中占比较大的时候，通过索引定位到的数据会使用位图的方式对索引字段进行位图堆表扫描，以确定结果数据的准确。对于数据仓库应用而言，很少用这种扫描方式。 1set enable_seqscan = off; 通过隐藏字段tid扫描：Tid Scan ctid是PostgreSQL中标记数据位置的字段，通过这个字段来查找数据，速度非常快，类似于Oracle的rowid。Greenplum是一个分布式数据库，每一个子节点都是一个PostgreSQL数据库，每 一个子节点都单独维护自己的一套ctid字段。 1Select * from test1 where ctid=&#x27;(1,1)&#x27;; 如果在Greenplum中通过ctid来找数据，还必须通过制定另外 一个隐藏字段(gp_segment_id)来确定取哪一个数据库的ctid。 1Select * from test1 where ctid=&#x27;(1,1)&#x27; and gp_segment_id=1; 子查询扫描（Subquery Scan'*SELECT*':） 只要SQL中有子查询，需要对子查询的结果做顺序扫描，就会进行子查询扫描。 函数扫描：Function Scan 数据库中有一些函数的返回值是一个结果集，当数据库从这个结果集中取出数据的时候，就会用到这个Function Scan。 分布式执行 Gather Motion(N:1) 聚合操作，在Master上将子节点所有的数据聚合起来。一般 的聚合规则是:哪一个子节点的数据先返回到Master上就将该节 点的数据先放在MASTER上。 Broadcast Motion(N:N) 广播，将每个Segment上某一个表的数据全部发送给所有 Segment。这样每一个Segment都相当于有一份全量数据，最好不要有！ Redistribute Motion(N:N) 当需要做跨库关联或者聚合的时候，当数据不能满足广播的 条件，或者广播的消耗过大时，Greenplum就会选择重分布数 据，即数据按照新的分布键(关联键)重新打散到每个Segment 上，重分布一般在以下三种情况下会发生。 1231. 关联:将每个Segment的数据根据关联键重新计算hash值， 并根据Greenplum的路由算法路由到目标子节点中，使关联时属 于同一个关联键的数据都在同一个Segment上。2. Group By:当表需要Group By，但是Group By的字段不是分 布键时，为了使Group By的字段在同一个库中，Greenplum会分 两个Group By操作来执行，首先，在单库上执行一个Group By操 作，从而减少需要重分布的数据量;然后将结果数据按照Group By字段重分布，之后再做聚合获得最终结果。3. 开窗函数:跟Group By类似，开窗函数(Window Function)的实现也需要将数据重分布到每个节点上进行计算， 不过其实现比Group By更复杂一些 切片(Slice)","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"greenplum","slug":"greenplum","permalink":"http://yoursite.com/tags/greenplum/"}]},{"title":"greenplum业务案例","slug":"greenplum业务案例","date":"2021-08-08T10:22:07.000Z","updated":"2022-11-22T01:27:53.616Z","comments":true,"path":"2021/08/08/greenplum业务案例/","link":"","permalink":"http://yoursite.com/2021/08/08/greenplum%E4%B8%9A%E5%8A%A1%E6%A1%88%E4%BE%8B/","excerpt":"","text":"千万标签圈人 数据2千万，有标签：名字，住址。 12345create table tmp_id as select generate_series(1,20000000) as id distributed by (id);create table obj.staff (id int primary key, name varchar(50),address varchar(50)) ;create table staff as (select id ,&#x27;tom-&#x27;||id as name, &#x27;梦想小镇-&#x27;||id||&#x27;号&#x27; as address from tmp_id) distributed by (id);","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"greenplum","slug":"greenplum","permalink":"http://yoursite.com/tags/greenplum/"}]},{"title":"gp业务案例","slug":"gp业务案例","date":"2021-08-08T10:21:31.000Z","updated":"2022-11-22T01:27:53.615Z","comments":true,"path":"2021/08/08/gp业务案例/","link":"","permalink":"http://yoursite.com/2021/08/08/gp%E4%B8%9A%E5%8A%A1%E6%A1%88%E4%BE%8B/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"操作系统和文件","slug":"操作系统和文件","date":"2021-08-06T08:46:37.000Z","updated":"2022-11-22T01:27:54.013Z","comments":true,"path":"2021/08/06/操作系统和文件/","link":"","permalink":"http://yoursite.com/2021/08/06/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%92%8C%E6%96%87%E4%BB%B6/","excerpt":"","text":"磁盘 由于单一盘片容量有限，一般硬盘都有两张以上的盘片，每个盘片有两面，都可记录信息，所以一张盘片对应着两个磁头。盘片被分为许多扇形的区域，每个区域叫一个扇区，硬盘中每个扇区的大小固定为512字节。盘片表面上以盘片中心为圆心，不同半径的同心圆称为磁道，不同盘片相同半径的磁道所组成的圆柱称为柱面。磁道与柱面都是表示不同半径的圆，在许多场合，磁道和柱面可以互换使用。磁盘大小=磁头数（一般2个，上下面） 磁道数 * 每个磁道的扇区个数 * 扇区大小* 查看磁盘信息 12345678 fdisk -l /dev/sda1 磁盘 /dev/sda1：1 MB, 1048576 字节，2048 个扇区Units = 扇区 of 1 * 512 = 512 bytes// 扇区大小扇区大小(逻辑/物理)：512 字节 / 512 字节// 数据块大小，每次IO操作系统读取的字节数I/O 大小(最小/最佳)：512 字节 / 512 字节 扇区：磁盘最小的物理存储单元 123456789101112131415161718$ fdisk -l磁盘 /dev/sda：216.9 GB, 216895848448 字节，423624704 个扇区Units = 扇区 of 1 * 512 = 512 bytes ---------&gt; 扇区大小 512字节，0.5k扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：512 字节 / 512 字节磁盘标签类型：dos磁盘标识符：0x00033c33 设备 Boot Start End Blocks Id System/dev/sda1 2048 4095 1024 83 Linux/dev/sda2 * 4096 1052671 524288 83 Linux/dev/sda3 1052672 423624703 211286016 8e Linux LVM磁盘 /dev/mapper/vg_root-lv_root：216.4 GB, 216354783232 字节，422567936 个扇区Units = 扇区 of 1 * 512 = 512 bytes扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：512 字节 / 512 字节 磁盘块：操作系统将相邻的扇区组合在一起，形成一个块，对块进行管理。每个磁盘块可以包括 2、4、8、16、32 或 64 个扇区，磁盘块是操作系统所使用的逻辑概念。为了更好地管理磁盘空间和更高效地从硬盘读取数据，操作系统规定一个磁盘块中只能放置一个文件，因此文件所占用的空间，只能是磁盘块的整数倍，那就意味着会出现文件的实际大小，会小于其所占用的磁盘空间的情况。 12345678$ stat /boot 文件：&quot;/boot&quot; 大小：4096 块：8 IO 块：4096 目录 --------&gt;8个扇区组成，4096bytes，4K设备：802h/2050d Inode：64 硬链接：5权限：(0555/dr-xr-xr-x) Uid：( 0/ root) Gid：( 0/ root)最近访问：2021-10-13 11:43:54.291848769 +0800最近更改：2021-01-22 16:15:30.350797414 +0800最近改动：2021-01-22 16:15:30.350797414 +0800 页：内存的最小存储单位。页的大小通常为磁盘块大小的 2^n 倍 12$ getconf PAGE_SIZE4096 --------&gt;4096字节，4k","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://yoursite.com/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[]},{"title":"B-Tree和B+Tree","slug":"B-Tree和B-Tree","date":"2021-08-06T07:04:36.000Z","updated":"2022-11-22T01:27:53.350Z","comments":true,"path":"2021/08/06/B-Tree和B-Tree/","link":"","permalink":"http://yoursite.com/2021/08/06/B-Tree%E5%92%8CB-Tree/","excerpt":"","text":"转载：https://blog.csdn.net/yin767833376/article/details/81511377 二叉查找树 二叉树具有以下性质：左子树的键值小于根的键值，右子树的键值大于根的键值。 如下图所示就是一棵二叉查找树， 对该二叉树的节点进行查找发现深度为1的节点的查找次数为1，深度为2的查找次数为2，深度为n的节点的查找次数为n，因此其平均查找次数为 (1+2+2+3+3+3) / 6 = 2.3次 平衡二叉树（AVL Tree） 平衡二叉树（AVL树）在符合二叉查找树的条件下，还满足任何一个节点的两个子树的高度最大差为1。下面的两张图片，左边是AVL树，它的任何节点的两个子树的高度差&lt;=1；右边的不是AVL树，其根节点的左子树高度为3，而右子树高度为1； 如果在AVL树中进行插入或删除节点，可能导致AVL树失去平衡，这种失去平衡的二叉树可以概括为四种姿态：LL（左左）、RR（右右）、LR（左右）、RL（右左）。它们的示意图如下： 这四种失去平衡的姿态都有各自的定义： LL：LeftLeft，也称“左左”。插入或删除一个节点后，根节点的左孩子（Left Child）的左孩子（Left Child）还有非空节点，导致根节点的左子树高度比右子树高度高2，AVL树失去平衡。 RR：RightRight，也称“右右”。插入或删除一个节点后，根节点的右孩子（Right Child）的右孩子（Right Child）还有非空节点，导致根节点的右子树高度比左子树高度高2，AVL树失去平衡。 LR：LeftRight，也称“左右”。插入或删除一个节点后，根节点的左孩子（Left Child）的右孩子（Right Child）还有非空节点，导致根节点的左子树高度比右子树高度高2，AVL树失去平衡。 RL：RightLeft，也称“右左”。插入或删除一个节点后，根节点的右孩子（Right Child）的左孩子（Left Child）还有非空节点，导致根节点的右子树高度比左子树高度高2，AVL树失去平衡。 LL的旋转。LL失去平衡的情况下，可以通过一次旋转让AVL树恢复平衡。步骤如下： 将根节点的左孩子作为新根节点。 将新根节点的右孩子作为原根节点的左孩子。 将原根节点作为新根节点的右孩子。 LR的旋转：LR失去平衡的情况下，需要进行两次旋转，步骤如下： 围绕根节点的左孩子进行RR旋转。 围绕根节点进行LL旋转。 RL的旋转：RL失去平衡的情况下也需要进行两次旋转，旋转方法与LR旋转对称，步骤如下： 围绕根节点的右孩子进行LL旋转。 围绕根节点进行RR旋转。 B-Tree 平衡多路查找树。 B-Tree是为磁盘等外存储设备设计的一种平衡查找树。因此在讲B-Tree之前先了解下磁盘的相关知识。 系统从磁盘读取数据到内存时是以磁盘块（block）为基本单位的，位于同一个磁盘块中的数据会被一次性读取出来，而不是需要什么取什么。 InnoDB存储引擎中有页（Page）的概念，页是其磁盘管理的最小单位。InnoDB存储引擎中默认每个页的大小为16KB，可通过参数innodb_page_size将页的大小设置为4K、8K、16K，可通过如下命令查看页的大小： 1show variables like &#x27;innodb_page_size&#x27;; 系统一个磁盘块的存储空间往往没有这么大，因此InnoDB每次申请磁盘空间时都会是若干地址连续磁盘块来达到页的大小16KB。InnoDB在把磁盘数据读入到磁盘时会以页为基本单位，在查询数据时如果一个页中的每条数据都能有助于定位数据记录的位置，这将会减少磁盘I/O次数，提高查询效率。 B-Tree结构的数据可以让系统高效的找到数据所在的磁盘块。为了描述B-Tree，首先定义一条记录为一个二元组[key, data] ，key为记录的键值，对应表中的主键值，data为一行记录中除主键外的数据。对于不同的记录，key值互不相同。 一棵m阶的B-Tree有如下特性： 每个节点最多有m个孩子。 除了根节点和叶子节点外，其它每个节点至少有Ceil(m/2)个孩子。 若根节点不是叶子节点，则至少有2个孩子 所有叶子节点都在同一层，且不包含其它关键字信息 每个非终端节点包含n个关键字信息（P0,P1,…Pn, k1,…kn） 关键字的个数n满足：ceil(m/2)-1 &lt;= n &lt;= m-1 ki(i=1,…n)为关键字，且关键字升序排序。 Pi(i=1,…n)为指向子树根节点的指针。P(i-1)指向的子树的所有节点关键字均小于ki，但都大于k(i-1) B-Tree中的每个节点根据实际情况可以包含大量的关键字信息和分支，如下图所示为一个3阶的B-Tree： 每个节点占用一个盘块的磁盘空间，一个节点上有两个升序排序的关键字和三个指向子树根节点的指针，指针存储的是子节点所在磁盘块的地址。两个关键词划分成的三个范围域对应三个指针指向的子树的数据的范围域。以根节点为例，关键字为17和35，P1指针指向的子树的数据范围为小于17，P2指针指向的子树的数据范围为17~35，P3指针指向的子树的数据范围为大于35。 模拟查找关键字29的过程： 根据根节点找到磁盘块1，读入内存。【磁盘I/O操作第1次】 比较关键字29在区间（17,35），找到磁盘块1的指针P2。 根据P2指针找到磁盘块3，读入内存。【磁盘I/O操作第2次】 比较关键字29在区间（26,30），找到磁盘块3的指针P2。 根据P2指针找到磁盘块8，读入内存。【磁盘I/O操作第3次】 在磁盘块8中的关键字列表中找到关键字29。 分析上面过程，发现需要3次磁盘I/O操作，和3次内存查找操作。由于内存中的关键字是一个有序表结构，可以利用二分法查找提高效率。而3次磁盘I/O操作是影响整个B-Tree查找效率的决定因素。B-Tree相对于AVLTree缩减了节点个数，使每次磁盘I/O取到内存的数据都发挥了作用，从而提高了查询效率。 B+Tree B+Tree是在B-Tree基础上的一种优化，使其更适合实现外存储索引结构，InnoDB存储引擎就是用B+Tree实现其索引结构。 从上一节中的B-Tree结构图中可以看到每个节点中不仅包含数据的key值，还有data值。而每一个页的存储空间是有限的，如果data数据较大时将会导致每个节点（即一个页）能存储的key的数量很小，当存储的数据量很大时同样会导致B-Tree的深度较大，增大查询时的磁盘I/O次数，进而影响查询效率。在B+Tree中，所有数据记录节点都是按照键值大小顺序存放在同一层的叶子节点上，而非叶子节点上只存储key值信息，这样可以大大加大每个节点存储的key值数量，降低B+Tree的高度。 B+Tree相对于B-Tree有几点不同： 非叶子节点只存储键值信息。 所有叶子节点之间都有一个链指针。 数据记录都存放在叶子节点中。 将上一节中的B-Tree优化，由于B+Tree的非叶子节点只存储键值信息，假设每个磁盘块能存储4个键值及指针信息，则变成B+Tree后其结构如下图所示： 通常在B+Tree上有两个头指针，一个指向根节点，另一个指向关键字最小的叶子节点，而且所有叶子节点（即数据节点）之间是一种链式环结构。因此可以对B+Tree进行两种查找运算：一种是对于主键的范围查找和分页查找，另一种是从根节点开始，进行随机查找。 可能上面例子中只有22条数据记录，看不出B+Tree的优点，下面做一个推算： InnoDB存储引擎中页的大小为16KB，一般表的主键类型为INT（占用4个字节）或BIGINT（占用8个字节），指针类型也一般为4或8个字节，也就是说一个页（B+Tree中的一个节点）中大概存储16KB/(8B+8B)=1K个键值（因为是估值，为方便计算，这里的K取值为〖10〗3）。也就是说一个深度为3的B+Tree索引可以维护103 * 10^3 * 10^3 = 10亿 条记录。 实际情况中每个节点可能不能填充满，因此在数据库中，B+Tree的高度一般都在24层。mysql的InnoDB存储引擎在设计时是将根节点常驻内存的，也就是说查找某一键值的行记录时最多只需要13次磁盘I/O操作。 数据库中的B+Tree索引可以分为聚集索引（clustered index）和辅助索引（secondary index）。上面的B+Tree示例图在数据库中的实现即为聚集索引，聚集索引的B+Tree中的叶子节点存放的是整张表的行记录数据。辅助索引与聚集索引的区别在于辅助索引的叶子节点并不包含行记录的全部数据，而是存储相应行数据的聚集索引键，即主键。当通过辅助索引来查询数据时，InnoDB存储引擎会遍历辅助索引找到主键，然后再通过主键在聚集索引中找到完整的行记录数据。","categories":[{"name":"索引","slug":"索引","permalink":"http://yoursite.com/categories/%E7%B4%A2%E5%BC%95/"}],"tags":[{"name":"B-Tree","slug":"B-Tree","permalink":"http://yoursite.com/tags/B-Tree/"}]},{"title":"greenplum使用问题记录","slug":"greenplum使用问题记录","date":"2021-08-06T02:36:26.000Z","updated":"2022-11-22T01:27:53.617Z","comments":true,"path":"2021/08/06/greenplum使用问题记录/","link":"","permalink":"http://yoursite.com/2021/08/06/greenplum%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/","excerpt":"","text":"删除database异常 123456789// 有其他client在连接gpadmin=# drop database hf;ERROR: database &quot;hf&quot; is being accessed by other usersDETAIL: There is 1 other session using the database.// 执行SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname=&#x27;hf&#x27; AND pid&lt;&gt;pg_backend_pid();//再删除gpadmin=# drop database hf;DROP DATABASE","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"greenplum","slug":"greenplum","permalink":"http://yoursite.com/tags/greenplum/"}]},{"title":"greenplum最佳实践","slug":"greenplum最佳实践","date":"2021-08-03T23:00:35.000Z","updated":"2022-11-22T01:27:53.622Z","comments":true,"path":"2021/08/04/greenplum最佳实践/","link":"","permalink":"http://yoursite.com/2021/08/04/greenplum%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","excerpt":"","text":"版本 Gp6.x，docker镜像：datagrip/greenplum。用户名: gpadmin 密码: pivotal 用户名: root 密码: pivotal。 123docker run -it --name greenplum -p 5432:5432 -d datagrip/greenplum```docker exec -it greenplum /bin/bash``` su - gpadmin 123select version();PostgreSQL 9.4.24 (Greenplum Database 6.13.0 build commit:4f1adf8e247a9685c19ea02bcaddfdc200937ecd Open Source) on x86_64-unknown-linux-gnu, compiled by gcc (GCC) 6.4.0, 64-bit compiled on Dec 18 2020 22:31:16 常用数据类型 数值类型 类型 存储空间 描述 范围 bit 变长 位存储 smallint 2byte 小范围整数 -32768～+32767 integer 4byte 常用整数 -2147483648～±2147483647 bigint 8byte 大范围整数 -9223372036854775808～±9223372036854775807 decimal 变长 用户声明精度和标度、精确。numeric(precision,scale) 无限制 numeric 变长 用户声明精度和标度、精确 无限制 real 4byte 变精度、不精确 6位10进制数字精度 double precision 8byte 变精度、不精确 15位10进制数字精度 serial 4byte 自增整数 1～2147483647 bigserial 8byte 大范围自增整数 1～9223372036854775807 字符类型 类型 描述 character varying(n), varchar(n) 变长，有长度限制 character(n), char(n) 定长 text 变长，无长度限制 时间类型 类型 存储 描述 最低值 最高值 时间精度 timestamp[(p)][without time zone] 8byte 日期和时间 4713BC 5874897AD 1毫秒 timestamp[(p)][with time zone] 8byte 日期和时间，零时区 4713BC 5874897AD 1毫秒 interval[(p)] 12byte 时间间隔 -178000000年 178000000年 1毫秒 date 4byte 只用于表示时间 4713BC 5874897AD 1天 time[(p) [without time zone] 8byte 只用于表示一日内时间 0:00:00 24:00:00 1毫秒 time[(p) [with time zone] 12byte 只用于表示一日内时间，带时区 00:00:00+1459 00:00:00-1459 1毫秒 常用函数 当前时间：2021-08-04 时间类型 函数 返回类型 描述 例子 结果 age(timestamp,timestamp) interval 减去参数后的“符号化”结果 select age(timestamp'2020-08-05',timestamp'2000-07-03') 20 years 1 mon 2 days age(timestamp) interval 从current_date减去参数后的结果 select age(timestamp'2011-05-11') 10 years 2 mons 24 days current_date date 当前的日期 select current_date 2021-08-04 current_time time with time zone 当日时间 select current_time 06:47:42.075513+00 current_timestamp timestamp with time zone 当前事务开始时的时间戳 select current_timestamp 2021-08-04 07:02:56.231219+00 date_part(text,timestamp) double precision 获取子域（等效于extract） select date_part('second',timestamp '2021-08-04 10:30:21') 21 date_trunc(text,timestamp) timestamp 截断成指定的精度 select date_trunc('year', timestamp'2021-09-07 11:12:03') 2021-01-01 00:00:00 extract(field from timestamp) double precision 获取子域 select extract(second from timestamp '2020-09-09 11:00:32') 32 now() timestamp with time zone 当前时速开始的时间戳 select now() 2021-08-04 07:05:06.794196+00 123456789-- interval 时间间隔 ，时间加减select &#x27;2000-01-01&#x27;::timestamp + interval &#x27;10 year 2 day 10 minute 20 second&#x27; as cust_time;// 等同 (复数)select &#x27;2000-01-01&#x27;::timestamp + interval &#x27;10 years 2 days 10 minutes 20 seconds&#x27; as cust_time;-- 获取 当年/当月第一天select date_trunc(&#x27;year&#x27;,now())::date; --2021-01-01select date_trunc(&#x27;month&#x27;,now())::date; --2021-08-01 字符串类型 这里的位置/下标都是从1开始。 函数 返回类型 例子 描述 结果 string || string text select ‘Post’||‘gresql’ 字符串连接 Postgresql length(string) int select length('Postgresql') string中字符串长度 10 position(substring in string) int select position('Po' in 'Postgresql') 指定的子字符串的开始位置 1 substring(substr) (string [from int] [for int]) text select substring('Postgresql' from 5 for 10),前后都是闭区间 抽取子字符串 gresql trim( [leading | trailing | both] [characters] from string) text select trim(leading 'x' from 'xTomxx') 从字符串string的开发/结尾/两边，删除只包含characters中字符串（默认是一个空白）的最长字符串 Tom lower(string) text select lower('faN') 把字符串转换为小写 fan upper(string) text select upper('fan') 把字符串转换为大写 FAN overlay(string placing string from int [for int]) text select overlay('T$$' placing 'om' from 2 for 3) 替换子字符串 TOM replace(string text, from text, to text) text select replace('T$$','$$','om') 把字段string中出现所有子字符串(from)替换成子字符串(to) Tom spli_part(string text,delimiter text, field text) text select split_part(‘abc|tom|hij’,’|’,2) 根据delimiter分割string 返回生成的第field个子字符串 tom 数值计算函数 dp = double precision 函数 返回类型 描述 例子 结果 abs(x) 绝对值 绝对值 select abs(-1) 1 floor(dp 或 numeric) 与输入相同 不大于参数的最大整数，向下 select floor(-23.3) -24 ceil(dp 或 numeric) 与输入相同 不小于参数的最小整数，向上 select ceil(-23.6) -23 ceiling(dp 或 numeric) 与输入相同 不小于参数的最小整数（同上） select ceiling(-23.6) -23 exp(dp 或 numeric) 与输入相同 自然指数 select exp(1.0) 2.718281828459045 ln(dp 或 numeric) 与输入相同 自然对数 select ln(1.0) 0 log(dp 或 numeric) 与输入相同 以10为底数的对数 select log(100.0) 2 log(b numeric,x numeric) numeric 以b为底数的对数 select log(10.0,100.0) 2 mod(y,x) 与参数类型相同 y/x余数 select mod(10.1,5) 0.1 pi() dp “ π”值 select pi() 3.141592653589793 power(a numeric,b numeric) numeric a的b次幂 select power(2,3) 8 radians(dp) dp 角度转换成弧度(1弧度=180/π) select radians(180) 3.141592653589793 ，即“ π”值 random() dp 0.0-1.0 之间的随机数 select random() 0.9652733290567994 round(v numeric,s int) numeric 将v保留s位小数，且四舍五入 select round(-23.55,1) -23.6 sign(dp 或 numeric) 与输入相同 参数的符号（-1，0，1） select sign(0) 0 sqrt(dp 或 numeric) 与输入相同 平方根 select sqrt(4.0) 2 cbrt(dp) dp 立方根 select sqrt(27.0) 3 trunc(v numeric, s int) numeric 将v保留s位小数，不四舍五入 select trunc(27.456,2) 27.45 其他常用函数 123456789101112131415161718192021222324252627-- 随机生成区间内数值（闭区间），默认间隔为1 // 生成 5 6 7 8 9 10select * from generate_series(5,10);// 生成 5 7 9 （指定步长2）select * from generate_series(5,10,2);// 示例create table gen as select generate_series(5,10) as id,&#x27;hi&#x27;::varchar as name distributed by (id);-- 列转行，将字符串连接string_agg(str,&#x27;连接符&#x27;)// hi--hi--hi--hi--hi--hiselect string_agg(name,&#x27;--&#x27;) from gen;-- 行转列，将行字符串拆分成列insert into gen values(12,&#x27;i--am--an--engineer&#x27;);select id,regexp_split_to_table(name,&#x27;--&#x27;) from gen where position(&#x27;--&#x27; in name)&gt;1;11 i11 am11 an11 engineer-- hash函数// md5 精度128位，返回字符串 5d41402abc4b2a76b9719d911017c592select md5(&#x27;hello&#x27;);// hashbpchar 精度32位，返回时个integerselect hashbpchar(&#x27;hello&#x27;); 数组 操作符 操作符 描述 例子 结果（f否 t是） == 等于 select array[1,2.6,3.3]::int[]=array[1,2,3]; f &lt;&gt; 不等于 select array[1,2,4]&lt;&gt;array[1,2,3]; t &lt; 小于 select array[1,4,4] &lt; array[1,2,4]; t &gt; 大于 select array[1,4,4] &gt; array[1,2,4]; t &lt;= 小于等于 select array[1,1,4] &lt;= array[1,2,4]; t &gt;= 大于等于 select array[1,3,4] &gt;= array[1,2,4]; t @&gt; 包含 select array[1,3,4] @&gt; array[1,4]; t &lt;@ 包含于 select ARRAY[1,3,4] &lt;@ ARRAY[1,3,4,5]; t @@ 重叠（是否有相同元素） select ARRAY[1,3,4] &amp;&amp; ARRAY[0,1]; t || 1维数组与1维数组连接 select array[1,3,4] || 1维数组与2维数组连接 select array[1,3,4] || 元素与1维数组连接 select 3 || 1维数组与元素连接 select array[1,3,4] 函数 函数 返回类型 描述 例子 结果 array_prepend(anyarray,element) anyarray 数组前添加 select array_prepend(0,array[1,2,3]); {0,1,2,3} array_append(anyarray,element) anyarray 数组后添加 select array_append(array[1,2,3],3); {1,2,3,3} array_cat(anyarray,anyarray) anyarray 数据相连 select array_cat(array[1,2,3],array[4,5]); {1,2,3,4,5} array_ndims(anyarray) int 返回数组的维度 select array_ndims(array[[0,0],[1,1]]); 2 array_dims(anyarray) text 返回数组维数的文本表示，例如：[1:2][1:3] 表示2维数组，每个数组内有3个元素。 select array_dims(array[[1,2,3], [4,5,6]]); [1:2][1:3] array_fill(element,int[],int[]) anyarray 构建数组，element为数组元素，第一个int[]是数组的长度，第二个int[]是数组下界（左下标），下界默认是1 select array_fill(7, array[3], array[2]); [2:4]={7,7,7}，构建一个数组，数组元素从2-4位置填充为元素7 array_length(anyarray,int) int 返回数组指定维度的长度，1维里每个数组的长度 select array_length(array[[1,2,3],[4,5,6]], 1); 2 array_length(anyarray,int) int 返回数组指定维度的长度，2维里每个数组的长度 select array_length(array[[1,2,3],[4,5,6]], 2); 3 array_lower(anyarray,int) int 返回数组指定维度（第二元素指定值）的下界，本身gp里数组下界从1开始，这里特意从0开始 select array_lower('[0:2]=&#123;1,2,3&#125;'::int[], 1); 0 array_upper(anyarray,int) int 返回数组指定维度（第二元素指定值）的上界 select array_upper(array[1,2,3], 1); 3 array_remove(array,element) array 从一维数组中删除所有的指定元素 select array_remove(array[1,2,3,2], 2); {1,3} string_to_array(text,text,[,text]) 将文本使用分隔符分隔后转换为数组，如果指定第三个参数，则第三个参数在数组中被转换为NULL select string_to_array('xx~yy~zz', '~', 'yy'); array_to_string(anyarray,text,[,text]) text 将数组元素使用分隔符连接为文本，NULL可以使用指定元素（ 最后一个text，可省略）替换 select array_to_string(array[1, 2, 3, NULL, 4], ',', '*'); {1,2,3,*,4} cardinality(anyarray) int 返回数组所有维度的长度总和，如果是空数组则返回0 select cardinality(array[[1,2],[3,4],[5,6]]); 6 unset(anyarray) 单列元素的行 将数组元素打平转换为行 select unnest(array[[1,2,3],[4,5,6]]); 1…6 unset(anyarray,anyarray,...) 多列元素的行 第一个数组显示为第一列，第二个数组显示为第二列，以此类推 select * from unnest(ARRAY[1,2],ARRAY['one','two','three'],ARRAY[2,3,9]); 分析函数 开窗函数 含义：区别于聚合函数，它返回一条记录，开窗函数在不改变记录数同时，应用function到集合内每条记录（over筛选出记录集合），聚合开窗函数只能使用partition by子句，order by与聚合开窗函数一同使用。 格式：function over([partition by field ] [order by field]) function：sum()、avg()、max()、min()、count()、rank() dense_rank()、row_number() 计算每个人工资排名 1234567891011121314151617181920212223242526272829303132333435363738394041424344// 部门表create table emp_salary(depno smallint ,empno int, empname varchar,empsalary int) distributed by (empno);INSERT INTO &quot;public&quot;.&quot;emp_salary&quot;(&quot;depno&quot;,&quot;empno&quot;,&quot;empname&quot;,&quot;empsalary&quot;)VALUES(1,1,E&#x27;tom&#x27;,2000),(1,2,E&#x27;jim&#x27;,3000),(1,3,E&#x27;kit&#x27;,2600),(2,4,E&#x27;andy&#x27;,4190),(2,5,E&#x27;lucy&#x27;,2156),(2,6,E&#x27;lord&#x27;,2000),(2,7,E&#x27;david&#x27;,2000),(2,8,E&#x27;kober&#x27;,1456);// 部门内工资排名（工资相同的排名相同，后续非连续增加，rank()）select * ,rank() over(partition by depno order by empsalary desc) from emp_salary;2 4 andy 4190 12 5 lucy 2156 22 7 david 2000 32 6 lord 2000 32 8 kober 1456 51 2 jim 3000 11 3 kit 2600 21 1 tom 2000 3// 部门内工资排名（工资相同的排名相同，后续连续增加，dense_rank()）1 2 jim 3000 11 3 kit 2600 21 1 tom 2000 32 4 andy 4190 12 5 lucy 2156 22 7 david 2000 32 6 lord 2000 32 8 kober 1456 4// 部门内工资排名（工资相同的排名，也是连续增加，row_number()）select * ,row_number() over(partition by depno order by empsalary desc) from emp_salary;1 2 jim 3000 11 3 kit 2600 21 1 tom 2000 32 4 andy 4190 12 5 lucy 2156 22 7 david 2000 32 6 lord 2000 42 8 kober 1456 5 grouping sets 指定统计纬度，将统计后的数据放在一起展示。可用多个group by + union all 达到效果。 展示品牌和其细分领域都销售量。 1234567891011121314151617181920212223242526CREATE TABLE sales ( brand VARCHAR NOT NULL, segment VARCHAR NOT NULL, quantity INT NOT NULL, PRIMARY KEY (brand, segment));INSERT INTO sales (brand, segment, quantity)VALUES (&#x27;ABC&#x27;, &#x27;Premium&#x27;, 100), (&#x27;ABC&#x27;, &#x27;Basic&#x27;, 200), (&#x27;XYZ&#x27;, &#x27;Premium&#x27;, 100), (&#x27;XYZ&#x27;, &#x27;Basic&#x27;, 300); // 传统方式实现select brand,segment,sum(quantity) from sales group by brand,segmentunion allselect brand,null,sum(quantity) from sales group by brandunion allselect null,segment,sum(quantity) from sales group by segmentunion allselect null,null,sum(quantity) from sales ;// grouping set 方式select brand,segment,sum(quantity) from salesgroup by grouping sets((brand,segment),(brand),(segment),()); rollup 123&#96;&#96;&#96;sqlselect brand,segment,sum(quantity) from sales group by rollup(brand,segment); cube by a union all group by b union all group by a,b union all 无```的效果，即产生了8个分组集合（2的n次方）。123&#96;&#96;&#96;sqlselect brand,segment,sum(quantity) from sales group by cube(brand,segment) order by brand,segment; 复制表 表结构 123456789//复制出来的表，没有拷贝(约束、注释和序列)，只是简单的字段拷贝CREATE TABLE tabNew AS (select * from tabOld limit 0);//复制的表，里面没有数据，但是你可以指定复制标的约束，索引，注释，序列CREATE TABLE tabNew (LIKE tabOld [INCLUDING constraints] [INCLUDING indexes] [INCLUDING comments] [INCLUDING defaults]); including constraints ：复制约束including indexes ：复制索引including comments：复制注释including defaults：复制序列 表结构+数据 12create table tabNew as (select * from tabOld)select * into tabNew from tabOld; 数据 1insert into exit_table select * from exit_table_other; 分区表 gp里的分区表是逻辑概念，最多能有32,767个分区。在客户查看，会看到很多分区表。它分区类型可有如下：范围分区、列表分区 例子 范围分区 12345678910111213141516171819202122232425262728293031323334353637// 1 日期范围表分区CREATE TABLE sales (id int, date date, amt decimal(10,2))DISTRIBUTED BY (id)PARTITION BY RANGE (date)( START (date &#x27;2016-01-01&#x27;) INCLUSIVE END (date &#x27;2017-01-01&#x27;) EXCLUSIVE EVERY (INTERVAL &#x27;1 day&#x27;), DEFAULT PARTITION default_p); //用户也可以逐个声明并且命名每一个分区 CREATE TABLE sales (id int, date date, amt decimal(10,2))DISTRIBUTED BY (id)PARTITION BY RANGE (date)( PARTITION Jan16 START (date &#x27;2016-01-01&#x27;) INCLUSIVE , PARTITION Feb16 START (date &#x27;2016-02-01&#x27;) INCLUSIVE , PARTITION Mar16 START (date &#x27;2016-03-01&#x27;) INCLUSIVE , PARTITION Apr16 START (date &#x27;2016-04-01&#x27;) INCLUSIVE , PARTITION May16 START (date &#x27;2016-05-01&#x27;) INCLUSIVE , PARTITION Jun16 START (date &#x27;2016-06-01&#x27;) INCLUSIVE , PARTITION Jul16 START (date &#x27;2016-07-01&#x27;) INCLUSIVE , PARTITION Aug16 START (date &#x27;2016-08-01&#x27;) INCLUSIVE , PARTITION Sep16 START (date &#x27;2016-09-01&#x27;) INCLUSIVE , PARTITION Oct16 START (date &#x27;2016-10-01&#x27;) INCLUSIVE , PARTITION Nov16 START (date &#x27;2016-11-01&#x27;) INCLUSIVE , PARTITION Dec16 START (date &#x27;2016-12-01&#x27;) INCLUSIVE END (date &#x27;2017-01-01&#x27;) EXCLUSIVE ); //2 数字范围表分区CREATE TABLE rank (id int, rank int, year int, gender char(1), count int)DISTRIBUTED BY (id)PARTITION BY RANGE (year)( START (2006) END (2016) EVERY (1), DEFAULT PARTITION extra ); 列表分区 根据值value的分组，相同的数据归类到一组，也就一个分区中 123456789101112131415161718192021CREATE TABLE rank (id int, rank int, year int, gender char(1), count int ) DISTRIBUTED BY (id)PARTITION BY LIST (gender)( PARTITION girls VALUES (&#x27;F&#x27;), PARTITION boys VALUES (&#x27;M&#x27;), DEFAULT PARTITION other ); CREATE TABLE test_partition_list ( id int, name varchar(64), fdate varchar(10)) distributed by (id) PARTITION BY LIST (fdate) ( PARTITION p1 VALUES (&#x27;2017-01-01&#x27;, &#x27;2017-01-02&#x27;), PARTITION p2 VALUES (&#x27;2017-01-03&#x27;), DEFAULT PARTITION pd); 现有表分区 gp中表只能在创建时被分区。如果用户有一个表想要分区，用户必须创建一个新的分区表，把原始表的数据载入到新表，再删除原始表并且把分过区的表重命名为原始表的名称。用户还必须重新授权表上的权限。 1234567891011121314151617181920// 以老表为结构新建分区表CREATE TABLE sales2 (LIKE sales) PARTITION BY RANGE (date)( START (date 2016-01-01&#x27;) INCLUSIVE END (date &#x27;2017-01-01&#x27;) EXCLUSIVE EVERY (INTERVAL &#x27;1 month&#x27;) ); // 复制老表数据到新的分区表 INSERT INTO sales2 SELECT * FROM sales;// 删除老表DROP TABLE sales;// 重命名新表名为老表ALTER TABLE sales2 RENAME TO sales; // 授权 GRANT ALL PRIVILEGES ON sales TO admin; GRANT SELECT ON sales TO guest; 表类型和存储 表类型 堆（heap）表 Greenplum中默认和pg中一样都是使用堆存储表，堆表最适合于较小的表，适合更新、插入较频繁的表，在OLTP类型负载下表现最好。另外堆表的所有变更都会产生REDO，可以实现时间点恢复。但是堆表不能实现逻辑增量备份（因为表的任意一个数据块都有可能变更，不方便通过堆存储来记录位点）。 AO表（append only ） AO表顾名思义指只追加的表。其原理是，删除更新数据时，通过另一个BITMAP文件来标记被删除的行，通过bit以及偏移对齐来判定AO表上的某一行是否被删除。AO表非常适合OLAP场景，批量的数据写入，高压缩比，逻辑备份支持增量备份，因此每次记录备份到的偏移量即可。加上每次备份全量的BITMAP删除标记（很小）。 表存储 行存（heap ｜ AO） 以行的方式存储数据。因此读取数据的时候需要一行一行来获取，所以访问第一列和访问最后一列的成本实际上是不一样的。 列存（AO） 将数据以列为形式组织。读取任一列的成本是一样的，但是如果要读取多列，需要访问多个文件，访问的列越多，开销越大。适合于在少量列上计算数据聚集的数据仓库负载，或者是用于需要对单列定期更新但不修改其他列数据的情况。 创建 行堆表 1create table t (id int ) distributed by (id); 行AO表 1create table t (id int ) with (appendonly=true) distributed by (id); 列AO表 123create table t (id int ) with (appendonly=true,orientation=column) distributed by (id);// 表压缩，其中compresslevel是压缩率，取值为1~9,一般选择5就足够create table t (id int ) with (appendonly=true, orientation=column，compresstype=zlib, compresslevel=5) distributed by (id);","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"greenplum","slug":"greenplum","permalink":"http://yoursite.com/tags/greenplum/"}]},{"title":"greenplum基本概念","slug":"greenplum基本概念","date":"2021-07-14T08:59:35.000Z","updated":"2022-11-22T01:27:53.618Z","comments":true,"path":"2021/07/14/greenplum基本概念/","link":"","permalink":"http://yoursite.com/2021/07/14/greenplum%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","excerpt":"","text":"**Greenplum ** **PostgreSQL ** 4.x 8.2 5.x 8.3 6.x 9.4 7.x 12","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"greenplum","slug":"greenplum","permalink":"http://yoursite.com/tags/greenplum/"}]},{"title":"greenplum集群安装","slug":"greenplum集群安装","date":"2021-07-14T08:59:25.000Z","updated":"2022-11-22T01:27:53.641Z","comments":true,"path":"2021/07/14/greenplum集群安装/","link":"","permalink":"http://yoursite.com/2021/07/14/greenplum%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/","excerpt":"","text":"集群安装 集群安装参考：https://www.jianshu.com/p/8f71515dfd98 我是用是VirtualBox安装的3台centos7集群，gp版本为： greenplum-db-5.28.12-rhel7-x86_64.zip 。每个机器的IP如下： 123456# master192.168.56.101# seg1192.168.56.102# seg2192.168.56.103 gp访问信息 12345地址：192.168.56.101端口：5432初始数据库：postgres用户：gpadmin密码：gpadmin 1、安装依赖和配置（每台都执行） root 用户操作。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061yum install -y sedyum install -y taryum install -y perl# 设置hostvim /etc/hosts127.0.0.1 当前机器的hostname(c1 | c2 | c3)192.168.56.101 c1192.168.56.102 c2192.168.56.103 c3vim /etc/hostname当前机器的hostname(c1 | c2 | c3)# 设置hostnamevim /etc/sysconfig/networkNETWORKING=yesHOSTNAME=当前机器的hostname(c1 | c2 | c3)# 修改内核参数（不同版本可能会不一样，注意，不然导致初始化会报错）vim /etc/sysctl.confkernel.shmmax = 500000000kernel.shmmni = 4096kernel.shmall = 4000000000kernel.sem = 250 512000 100 2048kernel.sysrq = 1kernel.core_uses_pid = 1kernel.msgmnb = 65536kernel.msgmax = 65536kernel.msgmni = 2048net.ipv4.tcp_syncookies = 1net.ipv4.ip_forward = 0net.ipv4.conf.default.accept_source_route = 0net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_max_syn_backlog = 4096net.ipv4.conf.all.arp_filter = 1net.ipv4.ip_local_port_range = 1025 65535net.core.netdev_max_backlog = 10000net.core.rmem_max = 2097152net.core.wmem_max = 2097152vm.overcommit_memory = 2# 配置生效sysctl -p# 打开文件限制vim /etc/security/limits.conf* soft nofile 65536* hard nofile 65536* soft nproc 131072* hard nproc 131072# 关闭防火墙 systemctl status firewalld.service #查看防火墙状态 systemctl stop firewalld.service #关闭防火墙 systemctl disable firewalld.service #永久关闭防火墙 # 关闭SELINUX vim /etc/selinux/config SELINUX=disabled 2、新增用户和创建安装目录（每台都执行） 123456789# 新增用户gpadmin，密码也是gpadmingroupadd -g 530 gpadminuseradd -g 530 -u 530 -m -d /home/deploy -s /bin/bash deploychown -R deploy:deploy /home/deploypasswd gpadmin# 安装目录mkdir -p /opt/greenplumchown -R gpadmin:gpadmin /opt/greenplum 3、安装gp和添加配置文件（master节点，即c1） 上传包到master节点（c1），路径为：/opt/greenplum 12345678910111213141516171819202122232425262728# 安装（root 用户操作）unzip greenplum-db-5.28.12-rhel7-x86_64.zipchmod +x greenplum-db-5.28.12-rhel7-x86_64.bin./greenplum-db-5.24.2-rhel7-x86_64.binchown -R gpadmin:gpadmin /opt/greenplum# 添加配置文件（gpadmin用户操作）su - gpadminmkdir confvim hostlistc1c2c3vim seg_hostsc2c3# 打通各个节点source /opt/greenplum/greenplum-db/greenplum_path.shgpssh-exkeys -f /home/gpadmin/conf/hostlist# 验证各个节点联通性gpssh -f /home/gpadmin/conf/hostlist=&gt; pwd[c1] /home/gpadmin[c2] /home/gpadmin[c3] /home/gpadmin=&gt; exit 4、分发安装包到seg节点（master节点，即c1） gpadmin 用户操作。 1234567891011121314151617# 分发cd /opt/greenplum/tar -cf gp-5.28.tar ./greenplum-db-5.28.12/gpscp -f /home/gpadmin/conf/seg_hosts gp-5.28.tar =:/opt/greenplum/# 查看cd ~/conf/gpssh -f seg_hosts=&gt; cd /opt/greenplum[c2][c3]=&gt; tar -xf gp-5.28.tar[c2][c3]#建立软链接=&gt; ln -s greenplum-db-5.28.12 greenplum-db[c2][c3] 5、创建gp数据目录（master节点，即c1） 12345678910111213gpssh -f hostlist=&gt; mkdir gpdata[c1][c2][c3]=&gt; cd gpdata[c1][c2][c3]=&gt; mkdir gpmaster gpdatap1 gpdatap2 gpdatam1 gpdatam2[c1][c2][c3] 6、 配置.bash_profile环境变量（每台都执行） 123456cd ~vim .bash_profilesource /opt/greenplum/greenplum-db/greenplum_path.shexport MASTER_DATA_DIRECTORY=/data/gp/data/master/gpseg-1export PGPORT=5432export PGDATABASE=postgres 7、创建初始化配置文件（master节点，即c1） cp /opt/greenplum/docs/cli_help/gpconfigs/gpinitsystem_config /home/gpadmin/conf 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# FILE NAME: gpinitsystem_config# Configuration file needed by the gpinitsystem#################################################### REQUIRED PARAMETERS#################################################### Name of this Greenplum system enclosed in quotes.ARRAY_NAME=&quot;Greenplum&quot;#### Naming convention for utility-generated data directories.SEG_PREFIX=gpseg#### Base number by which primary segment port numbers #### are calculated.PORT_BASE=40000#### File system location(s) where primary segment data directories #### will be created. The number of locations in the list dictate#### the number of primary segments that will get created per#### physical host (if multiple addresses for a host are listed in #### the hostfile, the number of segments will be spread evenly across#### the specified interface addresses).# 每个主机几个seg节点就几个目录declare -a DATA_DIRECTORY=(/home/gpadmin/gpdata/gpdatap1 /home/gpadmin/gpdata/gpdatap2)#### OS-configured hostname or IP address of the master host.MASTER_HOSTNAME=hadoop001#### File system location where the master data directory #### will be created.MASTER_DIRECTORY=/home/gpadmin/gpdata/gpmaster#### Port number for the master instance. MASTER_PORT=5432#### Shell utility used to connect to remote hosts.TRUSTED_SHELL=ssh#### 机器资源不足不要太多，Maximum log file segments between automatic WAL checkpoints.CHECK_POINT_SEGMENTS=2#### Default server-side character set encoding.ENCODING=UNICODE#################################################### OPTIONAL MIRROR PARAMETERS#################################################### Base number by which mirror segment port numbers #### are calculated.MIRROR_PORT_BASE=43000#### Base number by which primary file replication port #### numbers are calculated.REPLICATION_PORT_BASE=41000#### Base number by which mirror file replication port #### numbers are calculated. MIRROR_REPLICATION_PORT_BASE=51000#### File system location(s) where mirror segment data directories #### will be created. The number of mirror locations must equal the#### number of primary locations as specified in the #### DATA_DIRECTORY parameter.declare -a MIRROR_DATA_DIRECTORY=(/home/gpadmin/gpdata/gpdatam1 /home/gpadmin/gpdata/gpdatam2)#################################################### OTHER OPTIONAL PARAMETERS#################################################### Create a database of this name after initialization.#DATABASE_NAME=postgres（这一行要注释掉，不然初始化时会失败）#### Specify the location of the host address file here instead of#### with the the -h option of gpinitsystem.MACHINE_LIST_FILE=/home/gpadmin/conf/seg_hosts 8、初始化数据库（master节点，即c1） 1gpinitsystem -c /home/gpadmin/conf/gpinitsystem_config -h /home/gpadmin/conf/seg_hosts [-s 主机名] 其中，-s 主机名可以省略，做master standy 节点用的 9、启动和停止gp 123456gpstart # 不用输yesgpstart -agpstop# 不用输yesgpstop -a 10 、访问数据库&amp;修改密码 123psql -d postgres// 修改用户gpadmin密码为gpadmin\\password gpadmin 11、其他命令 可以参考：https://gp-docs-cn.github.io/docs/utility_guide/admin_utilities/gpstop.html 12345gpstate -e #查看mirror的状态gpstate -f #查看standby master的状态gpstate -s #查看整个GP群集的状态gpstate -i #查看GP的版本gpstate --help #帮助文档，可以查看gpstate更多用法 12、允许远程登录 123到pg_hba.conf，一般是在master的数据目录里，再gpstop -u最后一行添加host all all 0.0.0.0/0 md5","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"greenplum","slug":"greenplum","permalink":"http://yoursite.com/tags/greenplum/"}]},{"title":"docker容器迁移","slug":"docker容器迁移","date":"2021-07-08T05:59:49.000Z","updated":"2022-11-22T01:27:53.560Z","comments":true,"path":"2021/07/08/docker容器迁移/","link":"","permalink":"http://yoursite.com/2021/07/08/docker%E5%AE%B9%E5%99%A8%E8%BF%81%E7%A7%BB/","excerpt":"","text":"1、备份 1234#创建数据卷容器docker run it -v &#x2F;dbdata --name dbdata ubuntu# 使用数据卷容器备份docker run -volumes-from dbdata -v $ (pwd) :&#x2F;backup - -name worker ubuntu tar cvf &#x2F;backup&#x2F;backup.tar &#x2F;dbdata **说明：**首先利用ubuntu镜像创建了一个容器worker。 使用–volumes-from dbdata参数来让worker容器挂载dbdata容器的数据卷(即dbdata数据卷)；使用-v $(pwd) :/backup 参数来挂载本地的当前目录到 worker容器的/backup 目录。worker容器启动后，使用 tar cvf /backup/backup.tar /dbdata命令将/dbdata 下内容备份为容器内的/ backup/backup. tar，即宿主主机当前目录下的 backup . tar。 2、恢复 如果要恢复数据到一个容器，可以按照下面的操作 。 首先创建一个带有数据卷的容器 dbdata2: 1docker run -v /dbdata --name dbdata2 ubuntu /bin/bash 然后创建另一个新的容器，挂载 dbdata2 的容器，并使用 untar 解压备份文件到所挂 载的容器卷中: 1docker run --volumes-from dbdata2 -v $(pwd) :/backup busybox tar xvf /backup/backup.tar","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"mysql若干理解","slug":"mysql若干理解","date":"2021-07-06T07:08:51.000Z","updated":"2022-11-22T01:27:53.703Z","comments":true,"path":"2021/07/06/mysql若干理解/","link":"","permalink":"http://yoursite.com/2021/07/06/mysql%E8%8B%A5%E5%B9%B2%E7%90%86%E8%A7%A3/","excerpt":"","text":"配置文件读取顺序： 1/etc/my.cnf-&gt;/etc/mysql/my.cnf-&gt;/usr/local/mysql/etc/my.cnf-&gt;~/.my.cnf","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"命令行操作各类常用工具","slug":"命令行操作各类常用工具","date":"2021-07-06T05:31:13.000Z","updated":"2022-11-22T01:27:53.892Z","comments":true,"path":"2021/07/06/命令行操作各类常用工具/","link":"","permalink":"http://yoursite.com/2021/07/06/%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C%E5%90%84%E7%B1%BB%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7/","excerpt":"","text":"1、mysql 登陆/登出 123mysql -h 127.0.01 -uroot -puse mydb;quit; 查看表结构 12desc mytable;show create table tablename; 导入导出 12345678910111213# 导出结构和数据1. 导出db不带建库语句mysqldump -h 127.0.0.1 -u root -p dbname &gt; savePath&#x2F;customeDef.sql2. 导出db带建库语句(--databases)mysqldump -h 127.0.0.1 -u root -p --databases dbname &gt; savePath&#x2F;customeDef.sql3. 只导出表数据结构(-d)mysqldump -h 127.0.0.1 -u root -p dbname -d tablename &gt; savePath&#x2F;customeDef.sql4. 只导出insert 语句(-t)mysqldump -h 127.0.0.1 -u root -p dbname -t tablename &gt; savePath&#x2F;customeDef.sql# 导入1. 进入mysql：mysql -h 127.0.01 -uroot -p 2. source pathofSql 查看版本 121. 进入mysql能看到2. 进入后：status ； select version(); 查看数据目录 1SHOW VARIABLES LIKE &#39;datadir&#39;;","categories":[{"name":"命令行","slug":"命令行","permalink":"http://yoursite.com/categories/%E5%91%BD%E4%BB%A4%E8%A1%8C/"}],"tags":[]},{"title":"netty快速开始","slug":"netty快速开始","date":"2021-06-25T06:59:34.000Z","updated":"2022-11-22T01:27:53.712Z","comments":true,"path":"2021/06/25/netty快速开始/","link":"","permalink":"http://yoursite.com/2021/06/25/netty%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/","excerpt":"","text":"netty在Java中高级开发中使用很广泛，可扩展性也很强，但是它的api风格乍一眼又是让人难以磨的找头脑，但是也很固化，例如server和client的启动连接都是模版性的代码，我们在日常使用中，主要的精力关注在业务代码处理就好。下面使用netty仿造一个聊天室程序。 1、服务端 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public class ChatServer &#123; public static void main(String[] args) throws InterruptedException &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); ServerBootstrap serverBootstrap = new ServerBootstrap(); try &#123; serverBootstrap. group(bossGroup, workerGroup). channel(NioServerSocketChannel.class). childHandler(new ChatServerInitializer()); ChannelFuture channelFuture = serverBootstrap.bind(8899).sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); &#125; &#125;&#125;public class ChatServerInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new DelimiterBasedFrameDecoder(4096, Delimiters.lineDelimiter())); pipeline.addLast(new StringDecoder(CharsetUtil.UTF_8)); pipeline.addLast(new StringEncoder(CharsetUtil.UTF_8)); pipeline.addLast(new ChatServerHandler()); &#125;&#125;public class ChatServerHandler extends SimpleChannelInboundHandler&lt;String&gt; &#123; private static final ChannelGroup channelGroup = new DefaultChannelGroup(GlobalEventExecutor.INSTANCE); @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; Channel channel = ctx.channel(); // 使用channelGroup简化广播 channelGroup.writeAndFlush(&quot;[服务器] - &quot; + channel.remoteAddress() + &quot; 加入\\n&quot;); // 先广播再添加，保证新上线的不接收广播的消息：已上线的A和B收到，新上线的C不收到 channelGroup.add(channel); &#125; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; Channel channel = ctx.channel(); // 给服务端自己看 System.out.println(channel.remoteAddress() + &quot; 上线&quot;); &#125; @Override protected void channelRead0(ChannelHandlerContext ctx, String msg) throws Exception &#123; Channel channel = ctx.channel(); channelGroup.forEach(ch -&gt; &#123; if (channel == ch) &#123; ch.writeAndFlush(&quot;[自己]：&quot; + msg + &quot;\\n&quot;); &#125; else &#123; ch.writeAndFlush(&quot;[&quot; + channel.remoteAddress() + &quot;]：&quot; + msg + &quot;\\n&quot;); &#125; &#125;); &#125; @Override public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; Channel channel = ctx.channel(); // 给服务端自己看 System.out.println(channel.remoteAddress() + &quot; 下线&quot;); &#125; @Override public void handlerRemoved(ChannelHandlerContext ctx) throws Exception &#123; Channel channel = ctx.channel(); // 会自动删除断掉的channel，断掉的channel也不会收到广播 channelGroup.writeAndFlush(&quot;[服务器] - &quot; + channel.remoteAddress() + &quot; 离开\\n&quot;); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 2、客户端 1234567891011121314151617181920212223242526272829303132333435363738394041public class ChatClient &#123; public static void main(String[] args) throws InterruptedException, IOException &#123; EventLoopGroup eventLoopGroup = new NioEventLoopGroup(); try &#123; Bootstrap bootstrap = new Bootstrap(); bootstrap. group(eventLoopGroup). channel(NioSocketChannel.class). handler(new ChatClientInitializer()); // 在这里发送，在MyChatClientHandler中接收服务端的返回 Channel channel = bootstrap.connect(&quot;localhost&quot;, 8899).sync().channel(); Scanner scan = new Scanner(System.in); while (scan.hasNext()) &#123; channel.writeAndFlush(scan.next() + &quot;\\r\\n&quot;); &#125; &#125; finally &#123; eventLoopGroup.shutdownGracefully(); &#125; &#125;&#125;public class ChatClientInitializer extends ChannelInitializer&lt;SocketChannel&gt; &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ChannelPipeline pipeline = ch.pipeline(); pipeline.addLast(new DelimiterBasedFrameDecoder(4096, Delimiters.lineDelimiter())); pipeline.addLast(new StringDecoder(CharsetUtil.UTF_8)); pipeline.addLast(new StringEncoder(CharsetUtil.UTF_8)); pipeline.addLast(new ChatClientHandler()); &#125;&#125;public class ChatClientHandler extends SimpleChannelInboundHandler&lt;String&gt;&#123; @Override protected void channelRead0(ChannelHandlerContext ctx, String msg) throws Exception &#123; // 客户端简单接收服务端传来的消息，输入消息在MyChatClient中 System.out.println(msg); &#125;&#125; 3、说明 聊天室是个c-s结构，所以我们需要编写server和client。在netty中服务端的类，一般都是Server*格式。netty使用的是Reactor编程模型，即可以简单理解为：thread-1处理客户端连接事件（accpet），thread-(2-n) 处理client的读写事件。所以在编写ChatServer时候需要new 出来2个事件处理EventLoopGroup，它的本质是个线程池，如下图： ServerBootStrap 是服务端启动器，它需要初始化一些属性，如上文代码看到的，group（干活的线程组）、channel（使用通道）、handler（事件响应器）。然后需要bind到一个port，设置成sync，返回时一个Future对象，这里可以看出来netty的异步特性了，这个异步对象可以获取到channel。 同理对于client端BootStrap也是类似过程，不再赘述。","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"netty","slug":"netty","permalink":"http://yoursite.com/tags/netty/"}]},{"title":"文件传输","slug":"文件传输","date":"2021-06-24T03:18:00.000Z","updated":"2022-11-22T01:27:54.023Z","comments":true,"path":"2021/06/24/文件传输/","link":"","permalink":"http://yoursite.com/2021/06/24/%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93/","excerpt":"","text":"文件上传和下载是日常开发中经常遇到问题，在spring中我们可以很方便使用，代码如下： Spring 1 文件上传 12345678910111213141516171819202122232425262728@PostMapping(&quot;/upload&quot;) public String upload(MultipartFile uploadFile, HttpServletRequest request) &#123; File dir = new File(&quot;/usr/local/file&quot;); //文件目录不存在，就创建一个 if (!dir.isDirectory()) &#123; boolean mkdirs = dir.mkdirs(); if (!mkdirs) &#123; log.error(dir.getAbsolutePath()+&quot;文件目录创建失败！&quot;); return &quot;创建文件目录失败！&quot;; &#125; &#125; try &#123; String filename = uploadFile.getOriginalFilename(); //服务端保存的文件对象 File fileServer = new File(dir, filename); // 实现上传 uploadFile.transferTo(fileServer); String filePath = request.getScheme() + &quot;://&quot; + request.getServerName() + &quot;:&quot; + request.getServerPort() +dir.getAbsolutePath() + filename; // 返回可供访问的网络路径 return filePath; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return &quot;上传失败&quot;; &#125; 2 文件下载 1234567891011121314151617181920@GetMapping(&quot;/download&quot;)public boolean download(String fileName, HttpServletRequest request, HttpServletResponse response) &#123; File tempFile = new File(&quot;/Users/hf/MyProjects/untitled/file/&quot; + fileName); try (InputStream inputStream = new FileInputStream(tempFile)) &#123; response.setContentType(&quot;application/force-download&quot;); response.setHeader(&quot;Content-Disposition&quot;, &quot;attachment;fileName=&quot; + fileName); OutputStream os = response.getOutputStream(); byte[] b = new byte[2048]; int length; while ((length = inputStream.read(b)) &gt; 0) &#123; os.write(b, 0, length); &#125; os.flush(); os.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); return false; &#125; return true;&#125;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"spring netty","slug":"spring-netty","permalink":"http://yoursite.com/tags/spring-netty/"}]},{"title":"flink初探","slug":"flink初探","date":"2021-06-23T03:52:39.000Z","updated":"2022-11-22T01:27:53.572Z","comments":true,"path":"2021/06/23/flink初探/","link":"","permalink":"http://yoursite.com/2021/06/23/flink%E5%88%9D%E6%8E%A2/","excerpt":"","text":"1 安装&amp;启动 mac 上使用homebrew演示。 1234567891011## 安装brew install apache-flink## 检查版本号flink --versionVersion: 1.13.1, Commit ID: a7f3192## 启动cd /usr/local/Cellar/apache-flink/1.13.1/libexec/bin./start-cluster.shStarting cluster.## web页面http://localhost:8081/ 2 idea创建工程 新增maven的快速构建脚手架 新增工程","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"flink","slug":"flink","permalink":"http://yoursite.com/tags/flink/"}]},{"title":"设计模式","slug":"设计模式","date":"2021-06-22T06:33:14.000Z","updated":"2022-11-22T01:27:54.223Z","comments":true,"path":"2021/06/22/设计模式/","link":"","permalink":"http://yoursite.com/2021/06/22/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"策略模式 在策略模式（Strategy Pattern）中，一个类的行为或其算法可以在运行时更改。这种类型的设计模式属于行为型模式。 在策略模式中，我们创建表示各种策略的对象和一个行为随着策略对象改变而改变的 context 对象。策略对象改变 context 对象的执行算法。 介绍 意图：定义一系列的算法,把它们一个个封装起来, 并且使它们可相互替换。 主要解决：在有多种算法相似的情况下，使用 if…else 所带来的复杂和难以维护。 何时使用：一个系统有许多许多类，而区分它们的只是他们直接的行为。 如何解决：将这些算法封装成一个一个的类，任意地替换。 关键代码：实现同一个接口。 应用实例： 1、诸葛亮的锦囊妙计，每一个锦囊就是一个策略。 2、旅行的出游方式，选择骑自行车、坐汽车，每一种旅行方式都是一个策略。 3、JAVA AWT 中的 LayoutManager。 优点： 1、算法可以自由切换。 2、避免使用多重条件判断。 3、扩展性良好。 缺点： 1、策略类会增多。 2、所有策略类都需要对外暴露。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public interface GoToBeijing &#123; /** * 去北京 */ void toBeijing();&#125;public class Bus implements GoToBeijing &#123; /** * 坐汽车 */ @Override public void toBeijing() &#123; System.out.println(&quot;go to beijing byBus&quot;); &#125;&#125;public class Fly implements GoToBeijing &#123; /** * 坐飞机 */ @Override public void toBeijing() &#123; System.out.println(&quot;go to beijing take a plane&quot;); &#125;&#125;public class StrategyContext &#123; private GoToBeijing toBeijing; public StrategyContext(GoToBeijing toBeijing) &#123; this.toBeijing = toBeijing; &#125; public void toBeijing() &#123; toBeijing.toBeijing(); &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; StrategyContext context = new StrategyContext(new Bus()); context.toBeijing(); StrategyContext context2 = new StrategyContext(new Fly()); context2.toBeijing(); &#125;&#125; 核心思路 事件的抽象 实现方法分类 调度实现 与命令模式很相像，我觉得他们之间的区别在于解决问题的角度不一样，命令模式侧重于请求和执行2者解耦，策略强调的是事务多种解决方案或者途径。 工厂模式 工厂模式（Factory Pattern）是 Java 中最常用的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。 在工厂模式中，我们在创建对象时不会对客户端暴露创建逻辑，并且是通过使用一个共同的接口来指向新创建的对象。 介绍 意图：定义一个创建对象的接口，让其子类自己决定实例化哪一个工厂类，工厂模式使其创建过程延迟到子类进行。 主要解决：主要解决接口选择的问题。 何时使用：我们明确地计划不同条件下创建不同实例时。 如何解决：让其子类实现工厂接口，返回的也是一个抽象的产品。 关键代码：创建过程在其子类执行。 应用实例： 1、您需要一辆汽车，可以直接从工厂里面提货，而不用去管这辆汽车是怎么做出来的，以及这个汽车里面的具体实现。 2、Hibernate 换数据库只需换方言和驱动就可以。 优点： 1、一个调用者想创建一个对象，只要知道其名称就可以了。 2、扩展性高，如果想增加一个产品，只要扩展一个工厂类就可以。 3、屏蔽产品的具体实现，调用者只关心产品的接口。 缺点：每次增加一个产品时，都需要增加一个具体类和对象实现工厂，使得系统中类的个数成倍增加，在一定程度上增加了系统的复杂度，同时也增加了系统具体类的依赖。这并不是什么好事。 例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public interface Shape &#123; void draw();&#125;public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;构造圆形&quot;); &#125;&#125;public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;构造直角形&quot;); &#125;&#125;public class Square implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;构造方形&quot;); &#125;&#125;public class ShapeFactory &#123; public static Shape getShape(String shapeType) &#123; if (shapeType.equalsIgnoreCase(&quot;CIRCLE&quot;)) &#123; return new Circle(); &#125; if (shapeType.equalsIgnoreCase(&quot;RECTANGLE&quot;)) &#123; return new Rectangle(); &#125; if (shapeType.equalsIgnoreCase(&quot;SQUARE&quot;)) &#123; return new Square(); &#125; return null; &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; Shape circle = ShapeFactory.getShape(&quot;CIRCLE&quot;); circle.draw(); Shape rectangle = ShapeFactory.getShape(&quot;RECTANGLE&quot;); rectangle.draw(); Shape square = ShapeFactory.getShape(&quot;SQUARE&quot;); square.draw(); &#125;&#125; 核心思路 需要若干实例 构建工厂 抽象工厂模式 抽象工厂模式（Abstract Factory Pattern）是围绕一个超级工厂创建其他工厂。该超级工厂又称为其他工厂的工厂。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。 在抽象工厂模式中，接口是负责创建一个相关对象的工厂，不需要显式指定它们的类。每个生成的工厂都能按照工厂模式提供对象。 介绍 意图：提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。 主要解决：主要解决接口选择的问题。 何时使用：系统的产品有多于一个的产品族，而系统只消费其中某一族的产品。 如何解决：在一个产品族里面，定义多个产品。 关键代码：在一个工厂里聚合多个同类产品。 应用实例：工作了，为了参加一些聚会，肯定有两套或多套衣服吧，比如说有商务装（成套，一系列具体产品）、时尚装（成套，一系列具体产品），甚至对于一个家庭来说，可能有商务女装、商务男装、时尚女装、时尚男装，这些也都是成套的，即一系列具体产品。 优点：当一个产品族中的多个对象被设计成一起工作时，它能保证客户端始终只使用同一个产品族中的对象。 缺点：产品族扩展非常困难，要增加一个系列的某一产品，既要在抽象的 Creator 里加代码，又要在具体的里面加代码。 例子 实例化若干一个创建不同颜色的形状的图形，颜色：一个工厂来创建；形状：一个工厂来创建。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public abstract class AbstractFactory &#123; public abstract Color getColor(String color); public abstract Shape getShape(String shape) ;&#125;public class ColorFactory extends AbstractFactory &#123; @Override public Shape getShape(String shapeType) &#123; return null; &#125; @Override public Color getColor(String color) &#123; if (color == null) &#123; return null; &#125; if (color.equalsIgnoreCase(&quot;RED&quot;)) &#123; return new Red(); &#125; else if (color.equalsIgnoreCase(&quot;GREEN&quot;)) &#123; return new Green(); &#125; else if (color.equalsIgnoreCase(&quot;BLUE&quot;)) &#123; return new Blue(); &#125; return null; &#125;&#125;public class ShapeFactory extends AbstractFactory&#123; @Override public Color getColor(String color) &#123; return null; &#125; @Override public Shape getShape(String shapeType) &#123; if(shapeType == null)&#123; return null; &#125; if(shapeType.equalsIgnoreCase(&quot;CIRCLE&quot;))&#123; return new Circle(); &#125; else if(shapeType.equalsIgnoreCase(&quot;RECTANGLE&quot;))&#123; return new Rectangle(); &#125; else if(shapeType.equalsIgnoreCase(&quot;SQUARE&quot;))&#123; return new Square(); &#125; return null; &#125;&#125;public class FactoryProducer &#123; public static AbstractFactory getFactory(String choice) &#123; if (choice.equalsIgnoreCase(&quot;SHAPE&quot;)) &#123; return new ShapeFactory(); &#125; else if (choice.equalsIgnoreCase(&quot;COLOR&quot;)) &#123; return new ColorFactory(); &#125; return null; &#125;&#125; 核心思路 一个综合产品是由多个产品组成 每个产品由一个工厂来生产 抽象一个大工厂作为每个产品工厂的工厂 建造者模式 建造者模式（Builder Pattern）使用多个简单的对象一步一步构建成一个复杂的对象。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。 一个 Builder 类会一步一步构造最终的对象。该 Builder 类是独立于其他对象的。 介绍 意图：将一个复杂的构建与其表示相分离，使得同样的构建过程可以创建不同的表示。 主要解决：主要解决在软件系统中，有时候面临着”一个复杂对象”的创建工作，其通常由各个部分的子对象用一定的算法构成；由于需求的变化，这个复杂对象的各个部分经常面临着剧烈的变化，但是将它们组合在一起的算法却相对稳定。 何时使用：一些基本部件不会变，而其组合经常变化的时候。 如何解决：将变与不变分离开。 关键代码：建造者：创建和提供实例，导演：管理建造出来的实例的依赖关系。 应用实例： 1、去肯德基，汉堡、可乐、薯条、炸鸡翅等是不变的，而其组合是经常变化的，生成出所谓的”套餐”。 2、JAVA 中的 StringBuilder。 优点： 1、建造者独立，易扩展。 2、便于控制细节风险。 缺点： 1、产品必须有共同点，范围有限制。 2、如内部变化复杂，会有很多的建造类。 例子 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public abstract class AbstractBuilder &#123; abstract void buildPart1(); abstract void buildPart2(); abstract void buildPart3();&#125;public class BuilderImpl extends AbstractBuilder &#123; @Override void buildPart1() &#123; System.out.println(&quot;构建对象第一步&quot;); &#125; @Override void buildPart2() &#123; System.out.println(&quot;构建对象第二步&quot;); &#125; @Override void buildPart3() &#123; System.out.println(&quot;构建对象第三步&quot;); &#125;&#125;public class Director &#123; private AbstractBuilder builder; private Integer condition; public Director(AbstractBuilder builder, Integer condition) &#123; this.builder = builder; this.condition = condition; &#125; public void build(AbstractBuilder builder) &#123; switch (condition) &#123; case 1: builder.buildPart1(); break; case 2: builder.buildPart2(); break; case 3: builder.buildPart3(); break; default: builder.buildPart1(); &#125; &#125;&#125; 核定思路 对象构建需要若干步骤 构建抽象类，抽象每一步 一个协调者负责协调构建的每一步 命令模式 命令模式（Command Pattern）是一种数据驱动的设计模式，它属于行为型模式。请求以命令的形式包裹在对象中，并传给调用对象。调用对象寻找可以处理该命令的合适的对象，并把该命令传给相应的对象，该对象执行命令。 介绍 意图：将一个请求封装成一个对象，从而使您可以用不同的请求对客户进行参数化。 主要解决：在软件系统中，行为请求者与行为实现者通常是一种紧耦合的关系，但某些场合，比如需要对行为进行记录、撤销或重做、事务等处理时，这种无法抵御变化的紧耦合的设计就不太合适。 关键代码：定义三个角色：1、Receiver 真正的命令执行对象 2、Command 命令3、Invoker 命令调用协调者 流程：调用者→接受者→命令。 优点： 1、降低了系统耦合度。 2、新的命令可以很容易添加到系统中去。 缺点：使用命令模式可能会导致某些系统有过多的具体命令类。 例子 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374//抽象命令public abstract class Command &#123; public abstract void execute(); &#125;//具体命令1public class ConcreteCommand1 extends Command &#123; private Receiver receiver; public ConcreteCommand1(Receiver receiver) &#123; this.receiver = receiver; &#125; @Override public void execute() &#123; receiver.doSomething(); &#125;&#125;// 具体命令2public class ConcreteCommand2 extends Command &#123; private Receiver receiver; public ConcreteCommand2(Receiver receiver) &#123; this.receiver = receiver; &#125; @Override public void execute() &#123; receiver.doSomething(); &#125;&#125;// 具体命令执行者抽象类public abstract class Receiver &#123; public abstract void doSomething();&#125;//具体命令执行者1public class Receiver1 extends Receiver &#123; @Override public void doSomething() &#123; &#125;&#125;//具体命令执行者2public class Receiver2 extends Receiver &#123; @Override public void doSomething() &#123; &#125;&#125;//命令调用的协调者public class Invoker &#123; private Command command; public void setCommand(Command command) &#123; this.command = command; &#125; public void action() &#123; command.execute(); &#125;&#125;//test方法public class Main &#123; public static void main(String[] args)&#123; Receiver receiver = new Receiver1(); Command command = new ConcreteCommand1(receiver); Invoker invoker = new Invoker(); invoker.setCommand(command); invoker.action(); &#125;&#125; 核心思路 执行者抽象 命令抽象（里面包含命令的执行者） 事务协调者（持有命令对象） 观察者模式 当对象间存在一对多关系时，则使用观察者模式（Observer Pattern）。比如，当一个对象被修改时，则会自动通知它的依赖对象。观察者模式属于行为型模式 介绍 意图：定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。 主要解决：一个对象状态改变给其他对象通知的问题，而且要考虑到易用和低耦合，保证高度的协作。 何时使用：一个对象（目标对象）的状态发生改变，所有的依赖对象（观察者对象）都将得到通知，进行广播通知。 如何解决：使用面向对象技术，可以将这种依赖关系弱化。 关键代码：在抽象类里有一个 ArrayList 存放观察者们。 应用实例： 1、拍卖的时候，拍卖师观察最高标价，然后通知给其他竞价者竞价。 2、西游记里面悟空请求菩萨降服红孩儿，菩萨洒了一地水招来一个老乌龟，这个乌龟就是观察者，他观察菩萨洒水这个动作。 优点： 1、观察者和被观察者是抽象耦合的。 2、建立一套触发机制。 缺点： 1、如果一个被观察者对象有很多的直接和间接的观察者的话，将所有的观察者都通知到会花费很多时间。 2、如果在观察者和观察目标之间有循环依赖的话，观察目标会触发它们之间进行循环调用，可能导致系统崩溃。 3、观察者模式没有相应的机制让观察者知道所观察的目标对象是怎么发生变化的，而仅仅只是知道观察目标发生了变化。 例子 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273// 抽象观察者public abstract class Observer &#123; public abstract void update();&#125;// 具体被观察者public class User implements Observer &#123; private String name; private String message; public User(String name) &#123; this.name = name; &#125; @Override public void update(String message) &#123; this.message = message; read(); &#125; public void read() &#123; System.out.println(name + &quot; 收到推送消息： &quot; + message); &#125; &#125;// 抽象被观察者public interface Observerable &#123; public void registerObserver(Observer o); public void removeObserver(Observer o); public void notifyObserver(); &#125;// 具体被观察者public class WechatServer implements Observerable &#123; //注意到这个List集合的泛型参数为Observer接口，设计原则：面向接口编程而不是面向实现编程 private List&lt;Observer&gt; list; private String message; public WechatServer() &#123; list = new ArrayList&lt;Observer&gt;(); &#125; @Override public void registerObserver(Observer o) &#123; list.add(o); &#125; @Override public void removeObserver(Observer o) &#123; if(!list.isEmpty()) list.remove(o); &#125; //遍历 @Override public void notifyObserver() &#123; for(int i = 0; i &lt; list.size(); i++) &#123; Observer oserver = list.get(i); oserver.update(message); &#125; &#125; public void setInfomation(String s) &#123; this.message = s; System.out.println(&quot;微信服务更新消息： &quot; + s); //消息更新，通知所有观察者 notifyObserver(); &#125;&#125; 核心思路 观察者 被观察者（里面包含观察者引用）","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"springBoot打包","slug":"springBoot打包","date":"2021-06-22T06:29:41.000Z","updated":"2022-11-22T01:27:53.806Z","comments":true,"path":"2021/06/22/springBoot打包/","link":"","permalink":"http://yoursite.com/2021/06/22/springBoot%E6%89%93%E5%8C%85/","excerpt":"","text":"一、什么是fat、thin jar springBoot项目可以运行java -jar xxx.jar 来启动，默认情况下它会把所有的依赖都打到一个jar里面，称作fat jar。但是一般情况下很多外部依赖的jar大多数不可变，若是每次的jar都是很大，势必在网络不好的情况下上传到服务器很慢。基于此，我们可以利用maven的2个插件也可以打出thin jar。 备注：java -jar xx.jar 是java自带的命令，和sprignBoot没半毛钱关系，因此我们只需要按照这个jar文件的规范，也可以将任何项目打包成可执行的jar。 二、MANIFEST.MF 文件 每一个jar包下面都有它，可见其重要性，它列出了jar包一些信息/清单。 正常的springBoot可执行jar 1234567891011121314Manifest-Version: 1.0Implementation-Title: af-appImplementation-Version: 0.0.1-SNAPSHOT// springBoot启动类Start-Class: com.af.app.afapp.AfAppApplication//类文件Spring-Boot-Classes: BOOT-INF/classes/ //依赖路径Spring-Boot-Lib: BOOT-INF/lib/ Build-Jdk-Spec: 1.8Spring-Boot-Version: 2.1.5.RELEASECreated-By: Maven Archiver 3.4.0//通过它来启动 Start-ClassMain-Class: org.springframework.boot.loader.JarLauncher mvn-jar-plugin 插件打出来的可执行jar 123456789Manifest-Version: 1.0Archiver-Version: Plexus ArchiverBuilt-By: yons// 类path，依赖的所有jarClass-Path: ../lib/data-asset-tag-api-1.0.0.jar ...Created-By: Apache Maven 3.6.0Build-Jdk: 1.8.0_201// 启动类Main-Class: com.dtwave.asset.tag.provider.DataAssetTagStarter 三、springBoot 可执行jar的maven插件 123456789101112 &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;!-- 可选--&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 这里我们直接给出springboot提供好的插件，就可以打出可执行的jar。注意若我们的工程没有继承spring-boot-starter-parent（这里面有了repackage 这个阶段），则还需要在上述的插件里面加个repackage，它就是将jar包可执化的关键。 四、maven-jar-plugin、maven-dependency-plugin 插件 12345678910111213141516171819202122232425262728293031323334353637&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;!--设置manifest.mf文件--&gt; &lt;manifest&gt; &lt;!-- 添加依赖jar路径 Class-Path --&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;!-- 设置入口程序 --&gt; &lt;mainClass&gt;com.af.app.afapp.AfAppApplication&lt;/mainClass&gt; &lt;classpathPrefix&gt;../libs&lt;/classpathPrefix&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!--拷贝项目依赖--&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-dependencies&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;!--自动在target目录下新建libs目录放依赖--&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/libs&lt;/outputDirectory&gt; &lt;overWriteReleases&gt;false&lt;/overWriteReleases&gt; &lt;overWriteSnapshots&gt;false&lt;/overWriteSnapshots&gt; &lt;overWriteIfNewer&gt;true&lt;/overWriteIfNewer&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; maven-jar-plugin 插件可以指定主程序入口，和可搜索的类路径。 dependency-plugin 主要用于依赖的copy，瘦身jar。 有兴趣的朋友可以研究下springBoot 启动的原理，它是怎么使用org.springframework.boot.loader.JarLauncher 来找到我们的启动类，完成程序的启动的。","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"maven","slug":"maven","permalink":"http://yoursite.com/tags/maven/"}]},{"title":"LockSupport","slug":"LockSupport","date":"2021-06-22T06:11:45.000Z","updated":"2022-11-22T01:27:53.398Z","comments":true,"path":"2021/06/22/LockSupport/","link":"","permalink":"http://yoursite.com/2021/06/22/LockSupport/","excerpt":"","text":"线程阻塞 LockSupport 相比较suspend会造成永久被挂起，它是安全的，即使unpark()方法再在前面执行也无所谓，因为它的底层使用的信号量，为每一个线程准备了许可，如果许可可用，则park惠消费它，如果不可以不可用则阻塞等待，而unpark是将一个许可变成可用，但是和信号量有些许区别，它是不可以累加的，每次最多有一个许可。 12345678910111213141516171819202122public class LockSupportDemo &#123; static Runnable r = () -&gt; &#123; try &#123; Thread.sleep(10); System.out.println(Thread.currentThread().getName() + &quot;执行&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; LockSupport.park(); &#125;; public static void main(String[] args) throws Exception &#123; Thread t1 = new Thread(r, &quot;1&quot;); Thread t2 = new Thread(r, &quot;2&quot;); t1.start(); t2.start(); LockSupport.unpark(t1); LockSupport.unpark(t2); t1.join(); t2.join(); &#125;&#125;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"多线程/并发","slug":"多线程-并发","permalink":"http://yoursite.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%B9%B6%E5%8F%91/"}]},{"title":"信号量","slug":"信号量","date":"2021-06-22T06:11:39.000Z","updated":"2022-11-22T01:27:53.859Z","comments":true,"path":"2021/06/22/信号量/","link":"","permalink":"http://yoursite.com/2021/06/22/%E4%BF%A1%E5%8F%B7%E9%87%8F/","excerpt":"","text":"Semaphore 和操作系统的信号量很类似，但是这里它也可以作为锁使用。即可以作为锁与协调线程作用 锁 1234567891011121314151617181920212223public class SemaphoreDemo &#123; static Semaphore semaphore = new Semaphore(1,true);//一个信号量且是公平的，默认非公平 static Runnable r = () -&gt; &#123; try &#123; System.out.println(Thread.currentThread().getName() + &quot;等待&quot;); semaphore.acquire(); System.out.println(Thread.currentThread().getName()+&quot;进入&quot;); semaphore.release(); System.out.println(Thread.currentThread().getName() + &quot;释放&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;; public static void main(String[] args) &#123; Thread t1 = new Thread(r,&quot;1&quot;); Thread t2 = new Thread(r,&quot;2&quot;); Thread t3 = new Thread(r,&quot;3&quot;); t1.start(); t2.start(); t3.start(); &#125;&#125; 打印结果 2等待 1等待 3等待 2进入 2释放 1进入 1释放 3进入 3释放 可以看到首先3个线程同时到来临界区边界，但是同时只有一个线程能进入临界区。 协调线程 1234567891011121314151617181920212223242526272829public class SemaohoreDemo2 &#123; public static void main(String[] args) &#123; ExecutorService pool = Executors.newCachedThreadPool(); final Semaphore semaphore = new Semaphore(3, true); for (int i = 0; i &lt; 6; i++) &#123; Runnable runnable = () -&gt; &#123; try &#123; semaphore.acquire();//获取信号灯许可 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; try &#123; Thread.sleep(2000);//为了打印，进入临界区总的线程数量 System.out.println(Thread.currentThread().getName()+&quot;进入临界区&quot;); Thread.sleep(4000);//模拟业务逻辑处理 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()+&quot;释放信号&quot;); semaphore.release();//释放信号灯 &#125;; pool.execute(runnable); &#125; pool.shutdown(); &#125;&#125; 打印结果 pool-1-thread-3进入临界区 pool-1-thread-2进入临界区 pool-1-thread-1进入临界区 pool-1-thread-3释放信号 pool-1-thread-1释放信号 pool-1-thread-2释放信号 pool-1-thread-4进入临界区 pool-1-thread-5进入临界区 pool-1-thread-6进入临界区 pool-1-thread-4释放信号 pool-1-thread-5释放信号 pool-1-thread-6释放信号","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"多线程/并发","slug":"多线程-并发","permalink":"http://yoursite.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%B9%B6%E5%8F%91/"}]},{"title":"循环栅栏","slug":"循环栅栏","date":"2021-06-22T06:11:32.000Z","updated":"2022-11-22T01:27:53.965Z","comments":true,"path":"2021/06/22/循环栅栏/","link":"","permalink":"http://yoursite.com/2021/06/22/%E5%BE%AA%E7%8E%AF%E6%A0%85%E6%A0%8F/","excerpt":"","text":"CyclicBarrier 和CountDownLatch整体的作用差不多，都是协调线程在某个点触发以后的任务，但是CountDownLatch 没有循环的功能。比如：部队里面士兵需要区演练，首先是集合，再做演练。看看下面的例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class CyclicBarrierDemo &#123; public static class Soldier implements Runnable &#123; private String soldier; private final CyclicBarrier cyclic; public Soldier(String soldier, CyclicBarrier cyclic) &#123; this.soldier = soldier; this.cyclic = cyclic; &#125; @Override public void run() &#123; try &#123; cyclic.await(); doWork(); cyclic.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125; void doWork() &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(soldier + &quot;任务完成&quot;); &#125; &#125; public static class BarrierRun implements Runnable &#123; boolean flag; int N; public BarrierRun(boolean flag, int n) &#123; this.flag = flag; N = n; &#125; @Override public void run() &#123; if (flag) &#123; System.out.println(&quot;司令：[士兵)&quot; + N + &quot;个，任务完成！]&quot;); &#125; else &#123; System.out.println(&quot;司令：[士兵)&quot; + N + &quot;个，集合完毕！]&quot;); flag = true; &#125; &#125; &#125; public static void main(String[] args) &#123; final int N = 10; Thread[] allSoldier = new Thread[N]; boolean flag = false; CyclicBarrier cyclicBarrier = new CyclicBarrier(N, new BarrierRun(flag, N)); System.out.println(&quot;集合队伍！&quot;); for (int i = 0; i &lt; N; i++) &#123; System.out.println(&quot;士兵&quot; + i + &quot;报道！&quot;); allSoldier[i] = new Thread(new Soldier(&quot;士兵&quot; + i, cyclicBarrier)); allSoldier[i].start(); &#125; &#125;&#125; 打印结果 集合队伍！ 士兵0报道！ 士兵1报道！ 士兵2报道！ 士兵3报道！ 士兵4报道！ 士兵5报道！ 士兵6报道！ 士兵7报道！ 士兵8报道！ 士兵9报道！ 司令：[士兵)10个，集合完毕！] 士兵9任务完成 士兵7任务完成 士兵6任务完成 士兵0任务完成 士兵8任务完成 士兵2任务完成 士兵4任务完成 士兵3任务完成 士兵1任务完成 士兵5任务完成 司令：[士兵)10个，任务完成！]","categories":[{"name":"多线程/并发","slug":"多线程-并发","permalink":"http://yoursite.com/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"多线程/并发","slug":"多线程-并发","permalink":"http://yoursite.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%B9%B6%E5%8F%91/"}]},{"title":"可重入锁","slug":"可重入锁","date":"2021-06-22T06:11:25.000Z","updated":"2022-11-22T01:27:53.891Z","comments":true,"path":"2021/06/22/可重入锁/","link":"","permalink":"http://yoursite.com/2021/06/22/%E5%8F%AF%E9%87%8D%E5%85%A5%E9%94%81/","excerpt":"","text":"可重入锁 ReentrantLock 线程基础里面我介绍了线程保持同步，使用的是synchronized 关键字配合wait和notify来使用，JDK5 以后提供的并发包里ReentrantLock 完全可以替代他们。 演示个累加例子 12345678910111213141516171819202122232425public class ReentryLock &#123; static int sum = 0; public static void main(String[] args) &#123; ReentrantLock reentrantLock = new ReentrantLock(); Runnable r = () -&gt; &#123; for (int i = 0; i &lt; 100; i++) &#123; reentrantLock.lock(); sum++; reentrantLock.unlock(); &#125; &#125;; Thread t1 = new Thread(r); Thread t2 = new Thread(r); t1.start(); t2.start(); try &#123; t1.join(); t2.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(sum); &#125; &#125; //运行结果：200 ReentrantLock关键方法/功能 可以中断响应 锁申请时等待限时 公平锁 **可以中断：**synchronized在等待锁时候，要么等待，要么获得锁，而ReentryLock可以优雅的中断等待。小明周末打电话给好朋友小刚取打篮球，小刚说自己有点事需要等下，好了给小明电话，于是小明等待小刚的电话，等待过程中小明突然接到女朋友电话，她买好了电影票，电影还有半个小时开场了，这下肯定去陪女朋友，不然单身一辈子了，于是立马电话和小刚说自己有急事不去打篮球了。相反小刚手头事一时半会忙不完，也可以电话给小明，今天不能去打篮球了。这就是中断的好处。下面看看这个简答的例子： 123456789101112131415161718192021222324252627282930313233343536 public class ReentryLockInter &#123; static ReentrantLock lock = new ReentrantLock();//小明 static ReentrantLock lock2 = new ReentrantLock();//小刚 static boolean isInterrupt = false; static String friend ; private static void setInterrupt(boolean isInterrupt,String friend) &#123; ReentryLockInter.isInterrupt = isInterrupt; ReentryLockInter.friend = friend; &#125; public static void main(String[] args) &#123; Runnable r = () -&gt; &#123; if (isInterrupt) &#123; try &#123; lock.lockInterruptibly(); System.out.println(Thread.currentThread().getName() + &quot;突然有急事，如是打电话给&quot;+friend+&quot;，今天有事，不能去打篮球了&quot;); lock2.lockInterruptibly(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; if(lock.isHeldByCurrentThread())lock.unlock(); if(lock2.isHeldByCurrentThread())lock2.unlock(); &#125; &#125; &#125;; Thread t1 = new Thread(r); t1.setName(&quot;小明&quot;); setInterrupt(true,&quot;小刚&quot;); Thread t2 = new Thread(r); t2.setName(&quot;小刚&quot;); setInterrupt(true,&quot;小明&quot;); t1.start(); t2.start(); &#125;&#125; 打印结果 小刚突然有急事，如是打电话给小明，今天有事，不能去打篮球了 小明突然有急事，如是打电话给小明，今天有事，不能去打篮球了 锁申请时等待限时：这个功能可以很好的·防止死锁的产生。还是拿小明周末约小刚打篮球的事情，这次小明女朋友想要他下午5点去陪她看电影，而目前是早上8点，还有好几个小时，于是小明可以等小刚一段时间忙好，相约去打篮球。 123456789101112131415161718192021222324252627282930313233343536public class ReentryLockTest3 &#123; static ReentrantLock lock = new ReentrantLock();//小明 static ReentrantLock lock2 = new ReentrantLock();//小刚 static int time = 0; static String friend; private static void setInterrupt(int time, String friend) &#123; ReentryLockTest3.time = time; ReentryLockTest3.friend = friend; &#125; public static void main(String[] args) &#123; Runnable r = () -&gt; &#123; try &#123; if (lock.tryLock(time, TimeUnit.HOURS)) &#123; System.out.println(Thread.currentThread().getName() + &quot;等&quot; + friend + &quot;一起去打篮球&quot;); &#125; else &#123; lock.lockInterruptibly(); lock2.lockInterruptibly(); System.out.println(Thread.currentThread().getName() + &quot;打电话给&quot; + friend + &quot;说，等你太久了，今天不去打篮球了&quot;); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; if (lock.isHeldByCurrentThread()) lock.unlock(); if (lock2.isHeldByCurrentThread()) lock2.unlock(); &#125; &#125;; Thread t1 = new Thread(r, &quot;小明&quot;); setInterrupt(5, &quot;小刚&quot;); t1.start(); Thread t2 = new Thread(r, &quot;小明&quot;); setInterrupt(0, &quot;小刚&quot;); t2.start(); &#125;&#125; 打印结果 小明等小刚一起去打篮球 小明打电话给小刚说，等你太久了，今天不去打篮球了 **公平锁：**小明因为女朋友，取消了和小刚约一起去打篮球，来到电影院，今天电影院人真多，原来是一个当红电影明星来宣传电影，需要排队进场，如是他和女朋友去一起排队，大家都是先来先到顺序进场。想想如果此时有个人搞特权，在队伍的最后面被安排先进场了，大家肯定不爽。这就是公平和非公平锁。公平锁实现成本较高，因为它的实现是维护一个有序队列 12345678910111213141516171819202122232425public class ReentryLockTest4 &#123; static ReentrantLock fairLock = new ReentrantLock(true);//默认不写为false static Runnable enteringCinema = () -&gt; &#123; try &#123; fairLock.lock(); System.out.println(Thread.currentThread().getName() + &quot;进场&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if (fairLock.isHeldByCurrentThread()) fairLock.unlock(); &#125; &#125;; public static void main(String[] args) &#123; Thread t1 = new Thread(enteringCinema, &quot;路人甲&quot;); Thread t2 = new Thread(enteringCinema, &quot;小明&quot;); Thread t3 = new Thread(enteringCinema, &quot;小明女朋友&quot;); Thread t4 = new Thread(enteringCinema, &quot;路人乙&quot;); t1.start(); t2.start(); t3.start(); t4.start(); &#125;&#125; 打印结果 路人甲进场 小明进场 小明女朋友进场 路人乙进场 Condition，ReentrantLock好帮手 1234567891011121314151617181920212223242526272829303132public class ConditionDemo &#123; static ReentrantLock lock = new ReentrantLock(); static Condition condition = lock.newCondition(); static Runnable r = () -&gt; &#123; lock.lock(); try &#123; System.out.println(&quot;执行condition.await()方法，暂停方法&quot;); condition.await(); System.out.println(&quot;恢复执行&quot;);//end &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;; public static void main(String[] args) &#123; Thread t1 = new Thread(r); t1.start(); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; lock.lock(); condition.signal(); lock.unlock();//若这个被注释，则end执行不到，得到的和suspend效果一样，线程被长久挂起，很危险 System.out.println(&quot;执行condition.signal()方法&quot;); &#125;&#125;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"多线程/并发","slug":"多线程-并发","permalink":"http://yoursite.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%B9%B6%E5%8F%91/"}]},{"title":"CountDownLatch","slug":"CountDownLatch","date":"2021-06-22T06:11:18.000Z","updated":"2022-11-22T01:27:53.352Z","comments":true,"path":"2021/06/22/CountDownLatch/","link":"","permalink":"http://yoursite.com/2021/06/22/CountDownLatch/","excerpt":"","text":"CountDownLatch 为一个倒计时计数器，可以将线程固定在某几个程序运行结束后再运行，比如我们航天工程里面火箭发射，前置很多检查需要都都准备就绪才能发射。 1234567891011121314151617181920212223242526public class CountDownLatchDemo &#123; static CountDownLatch countDownLatch = new CountDownLatch(5); static ReentrantLock lock = new ReentrantLock(); static AtomicInteger count = new AtomicInteger(1); public static void main(String[] args) throws Exception &#123; ExecutorService service = Executors.newCachedThreadPool(); Thread t = new Thread(() -&gt; &#123; try &#123; lock.lock(); Thread.sleep(100); System.out.println(&quot;火箭发射前设备状态检查&quot; + count.getAndIncrement()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; countDownLatch.countDown(); &#125;); for (int i = 0; i &lt; 5; i++) &#123; service.execute(t); &#125; countDownLatch.await(); System.out.println(&quot;设备状态检查执行完了，开始发射！&quot;); &#125;&#125; 打印结果 火箭发射前设备状态检查1 火箭发射前设备状态检查2 火箭发射前设备状态检查3 火箭发射前设备状态检查4 火箭发射前设备状态检查5 设备状态检查执行完了，开始发射！","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"多线程/并发","slug":"多线程-并发","permalink":"http://yoursite.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%B9%B6%E5%8F%91/"}]},{"title":"读写锁","slug":"读写锁","date":"2021-06-22T06:11:11.000Z","updated":"2022-11-22T01:27:54.224Z","comments":true,"path":"2021/06/22/读写锁/","link":"","permalink":"http://yoursite.com/2021/06/22/%E8%AF%BB%E5%86%99%E9%94%81/","excerpt":"","text":"ReadWriteLock 读写锁，提供了很好的读写分离，读读之间时并行，读写/写写之间串行，先看个例子 123456789101112131415161718192021222324252627282930313233343536373839404142public class ReadWriteLockDemo &#123; final static ReadWriteLock readWriteLock = new ReentrantReadWriteLock(); final static Lock readLock = readWriteLock.readLock(); final static Lock writeLock = readWriteLock.writeLock(); public void read(Lock lock) throws Exception &#123; Thread read = new Thread(() -&gt; &#123; lock.lock(); try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;); read.start(); &#125; public void write(Lock lock) throws Exception &#123; Thread write = new Thread(() -&gt; &#123; lock.lock(); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;); write.start(); &#125; public static void main(String[] args) throws Exception &#123; ReadWriteLockDemo demo = new ReadWriteLockDemo(); for (int i = 0; i &lt; 10; i++) &#123; demo.read(readLock); demo.write(writeLock); &#125; &#125;&#125;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"多线程/并发","slug":"多线程-并发","permalink":"http://yoursite.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%B9%B6%E5%8F%91/"}]},{"title":"Presto","slug":"Presto","date":"2021-06-22T06:04:04.000Z","updated":"2022-11-22T01:27:53.426Z","comments":true,"path":"2021/06/22/Presto/","link":"","permalink":"http://yoursite.com/2021/06/22/Presto/","excerpt":"","text":"1、简介 Hadoop提供了大数据存储和计算一套解决方案，完美地解决了大数据的存储和计算问题。但是Hadoop提供的Map-Reduce计算框架，适用于大数据量的离线和批量计算，它关注的吞吐量不是计算效率，在大数据量快速实时Ad-Hoc查询计算上表现很不友好。继Hive后，facebook公司在2012年开始开发Presto，与2013年正式开源，给Ad-Hoc查询带来了一股清凉的春风。Presto基于Java语言开发，在多数据源支持上，易用性，可扩展性上搜事大数据实时查询计算产品中的佼佼者。 2、特性 特点 说明 多数据源 目前Presto支持Mysql、PostgreSQL、Cassandra、Hive、Kafa等 SQL 完全兼容ANSI SQL，并提供sql shell 给用户 扩展性 基于SPI机制，易开发特定数据源连接器Connector 混合计算 多数据的混合计算 高性能 Presto官方测试是Hive性能的10倍以上 PipeLine 终端用户不用等待结果集处理完才看到结果，从一开始计算就可以产出一部分结果给终端 3、基本概念 3.1 服务进程 Coordinator 一般Coordinator部署在集群中一个单独节点，是整个Presto集群的管理节点。它主要用户接受客户端提交的查询，解析查询并生成查询计划，对任务进行调度，对worker管理。 Worker 工作节点，一般多个，主要进行数据的处理和Task的执行，它会周期性的向Coordinator进行Restful “沟通”，即心跳。 3.2 Presto模型 名称 说明 Connector Presto是通过Connector来访问不同的数据源的，可以将Connector当作访问不同数据源的驱动程序，每种Connector都实现了Presto里面的SPI接口。当需要使用某种Connnector时候需要在${PRESTO_HOME}/etc/catalog中配置。 Catalog 类似我们常规关系数据库Mysql里面实列。 Schema 类似我们常规关系数据库Mysql里面的database。 Table 类似我们常规关系数据库Mysql里面的table。 3.3 Presto查询模型 3.3.1 模型术语 Statement 客户端输入的SQL。 Query 客户端输入的SQL在Presto内部的表述（实例）。由Stage、Task、Driver、Split、Operator、DataSource组成。 Stage Query的组成部分，多个有层级关系的Stage组合成Query。 Exchage Stage之间是通过Exchage来相互连接，完成数据交换。 Task 正真运行在Worker上的，是Stage的逻辑上的拆分。 Driver 组成Task的单位，操作的集合。一个Driver处理一个Split，并生成输入输出。 Operator 具体作用在Split上的操作，如：过滤、加权、转换等。 Split 分片，大的数据集中一个小的数据集。 3.3.1 查询过程 整个过程大致分为7步 客户端通过Http请求发送查询语句给Coordinator。 Coordinator解析查询语句，生成查询计划。根据查询计划依次会生成：SqlQueryExecution、SqlStageExecution、HttpRemoteTask。 分发任务到各个Worker，全程是通过HttpRemoteTask里面的HttpClient将创建或者更新Task的请求发送到数据所在的节点。节点上的TaskResource接收到请求后在worker上启动/更新SqlTaskExecution对象，然后处理Split。 上游（距离数据源近的）Stage里面的Task，通过各种Connector从数据源里面读取数据。 下游Stage里面的Task读取上游Stage里输出的结果，拿到数据后在内存里面做计算和处理。 Coordinator从分发Task后就可以从最顶层的Stage（Single Stage）里面获取Task计算结果，缓存到buffer，直到所有计算结束。 Client从提交语句之后，会不停从Coordinator中获取本次查询的结果，不是等到查询结果都产生完毕才获取显示。 4、查询流程解析 presto 构建Restful服务使用的airlift框框。 4.1 客户端发起请求 123456789101112public final class Presto &#123; private Presto() &#123; &#125; public static void main(String[] args) &#123; Console console = singleCommand(Console.class).parse(args); if (console.helpOption.showHelpIfRequested() || console.versionOption.showVersionIfRequested()) &#123; return; &#125; System.exit(console.run() ? 0 : 1); &#125;&#125; 4.2 执行客户端命令 类：Console 1234567 public boolean run() &#123; ... if (hasQuery) &#123; return executeCommand(queryRunner, query, clientOptions.outputFormat, clientOptions.ignoreErrors); &#125; ...&#125; 4.3 发送Http请求 类：StatementClientV1 123456789101112private Request buildQueryRequest(ClientSession session, String query) &#123; HttpUrl url = HttpUrl.get(session.getServer()); if (url == null) &#123; throw new ClientException(&quot;Invalid server URL: &quot; + session.getServer()); &#125; // 注意路径 url = url.newBuilder().encodedPath(&quot;/v1/statement&quot;).build(); Request.Builder builder = prepareRequest(url).post(RequestBody.create(MEDIA_TYPE_TEXT, query)); ... return builder.build(); &#125; 4.4 Coordinator处理请求 类：StatementResource 123456789101112131415161718192021222324252627@Path(&quot;/v1/statement&quot;)public class StatementResource &#123;@POST @Produces(MediaType.APPLICATION_JSON) public Response createQuery( String statement, @HeaderParam(X_FORWARDED_PROTO) String proto, @Context HttpServletRequest servletRequest, @Context UriInfo uriInfo) &#123; ... // 核心 Query query = Query.create( sessionContext, statement, queryManager, sessionPropertyManager, exchangeClient, responseExecutor, timeoutExecutor, blockEncodingSerde); queries.put(query.getQueryId(), query); // 获取结果 QueryResults queryResults = query.getNextResult(OptionalLong.empty(), uriInfo, proto, DEFAULT_TARGET_RESULT_SIZE); ... return toResponse(query, queryResults); &#125;&#125; 后面逻辑较多，具体怎么个处理，有兴趣的可以参考：一条sql如何倍presto执行的。 5、Hive Connector Presto查询过程和Hive不同，它是从Hive-Metastore里面获取表的元数据信息和与之关联的数据文件信息，然后利用HdfsEnvironment操作HDFS文件。 persto配置的hive.properties文件 123456connector.name=hive-hadoop2hive.metastore.uri=thrift://p1:9083hive.metastore-cache-ttl=0shive.metastore-refresh-interval=0s// hdfs信息相关配置文件位置hive.config.resources=/etc/hive/conf/core-site.xml,/etc/hive/conf/hdfs-site.xml 获取Hive Metastore元数据 类：HiveMetastoreClientFactory 123public HiveMetastoreClient create(HostAndPort address)throws TTransportException &#123; return new ThriftHiveMetastoreClient(Transport.create(address, sslContext, socksProxy, timeoutMillis, metastoreAuthentication)); &#125; HiveMetastoreClient 里面方法，curd操作 加载元数据 类：CachingHiveMetastore 12345678910@ThreadSafepublic class CachingHiveMetastore implements ExtendedHiveMetastore&#123; //这里面有2个关键配置项，其他配置可以参看MetastoreClientConfig里面配置 // hive.metastore-refresh-interval，默认0 // hive.metastore-cache-ttl，默认0 public CachingHiveMetastore（...）&#123; // 主要的逻辑都是在构造函数里面完成的，太多了，省略了 ... &#125;&#125; 获取FileSystem 123456789public FileSystem getFileSystem(String user, Path path, Configuration configuration) throws IOException &#123; return hdfsAuthentication.doAs(user, () -&gt; &#123; // 从我们配置的hive的catalog文件里面读取hdfs相关信息 FileSystem fileSystem = path.getFileSystem(configuration); fileSystem.setVerifyChecksum(verifyChecksum); return fileSystem; &#125;); &#125; 6、插件开发 6.1 Connector开发 Presto目前支持很多的数据源，数据源都是以plugin形式添加的。下面是我安装的presto里面的plugin 12345678910111213141516171819202122232425262728drwxr-xr-x 2 root root 4096 Oct 29 13:34 tpchdrwxr-xr-x 2 root root 4096 Oct 29 13:34 mongodbdrwxr-xr-x 2 root root 4096 Oct 29 13:34 session-property-managersdrwxr-xr-x 2 root root 4096 Oct 29 13:34 postgresqldrwxr-xr-x 2 root root 4096 Oct 29 13:34 kududrwxr-xr-x 2 root root 4096 Oct 29 13:34 geospatialdrwxr-xr-x 2 root root 4096 Oct 29 13:34 example-httpdrwxr-xr-x 2 root root 4096 Oct 29 13:34 localfiledrwxr-xr-x 2 root root 4096 Oct 29 13:34 mysqldrwxr-xr-x 2 root root 4096 Oct 29 13:34 atopdrwxr-xr-x 2 root root 4096 Oct 29 13:34 accumulodrwxr-xr-x 2 root root 4096 Oct 29 13:34 sqlserverdrwxr-xr-x 2 root root 4096 Oct 29 13:34 redisdrwxr-xr-x 2 root root 4096 Oct 29 13:34 presto-elasticsearchdrwxr-xr-x 2 root root 4096 Oct 29 13:34 password-authenticatorsdrwxr-xr-x 2 root root 4096 Oct 29 13:34 blackholedrwxr-xr-x 2 root root 4096 Oct 29 13:34 redshiftdrwxr-xr-x 2 root root 4096 Oct 29 13:34 kafkadrwxr-xr-x 2 root root 4096 Oct 29 13:34 jmxdrwxr-xr-x 2 root root 4096 Oct 29 13:34 teradata-functionsdrwxr-xr-x 2 root root 4096 Oct 29 13:34 resource-group-managersdrwxr-xr-x 2 root root 4096 Oct 29 13:34 presto-thriftdrwxr-xr-x 2 root root 4096 Oct 29 13:34 memorydrwxr-xr-x 2 root root 4096 Oct 29 13:34 tpcdsdrwxr-xr-x 2 root root 4096 Oct 29 13:34 raptordrwxr-xr-x 2 root root 4096 Oct 29 13:34 mldrwxr-xr-x 2 root root 4096 Oct 29 13:34 cassandradrwxr-xr-x 2 root root 4096 Nov 25 21:39 hive-hadoop2 源代码里面有presto-example-http 的自定义connector代码示例。 添加插件 1234567891011121314151617presto-main&#x2F;etc&#x2F;config.properties下plugin.bundles&#x3D;\\ ..&#x2F;presto-blackhole&#x2F;pom.xml,\\ ..&#x2F;presto-memory&#x2F;pom.xml,\\ ..&#x2F;presto-jmx&#x2F;pom.xml,\\ ..&#x2F;presto-raptor&#x2F;pom.xml,\\ ..&#x2F;presto-hive-hadoop2&#x2F;pom.xml,\\ &#x2F;&#x2F;*******官方示例 example-http ********* ..&#x2F;presto-example-http&#x2F;pom.xml,\\ ..&#x2F;presto-kafka&#x2F;pom.xml, \\ ..&#x2F;presto-tpch&#x2F;pom.xml, \\ ..&#x2F;presto-local-file&#x2F;pom.xml, \\ ..&#x2F;presto-mysql&#x2F;pom.xml,\\ ..&#x2F;presto-sqlserver&#x2F;pom.xml, \\ ..&#x2F;presto-postgresql&#x2F;pom.xml, \\ ..&#x2F;presto-tpcds&#x2F;pom.xml 需要实现的接口 12345678910111213&#x2F;&#x2F; presto 会加载所有实现了Plugin的实现类Plugin&#x2F;&#x2F; 实例化Connector工厂ConnectorFactory&#x2F;&#x2F; connector连接器，里面有metadata、splitManager、recordSetProvider等实例Connector&#x2F;&#x2F; 元数据管理（schemas、tables等）ConnectorMetadata&#x2F;&#x2F; 数据分片ConnectorSplitManager&#x2F;&#x2F; 数据读取类ConnectorRecordSetProvider... 示例代码 1234567891011121314151617181920212223242526272829// 插件public class ExamplePlugin implements Plugin &#123; @Override public Iterable&lt;ConnectorFactory&gt; getConnectorFactories() &#123; return ImmutableList.of(new ExampleConnectorFactory()); &#125;&#125;// 连接器工厂public class ExampleConnectorFactory implements ConnectorFactory &#123; ... @Override public Connector create(String catalogName, Map&lt;String, String&gt; requiredConfig, ConnectorContext context) &#123; ... // 获取Connector 实例 return injector.getInstance(ExampleConnector.class); &#125;&#125;// 连接器，里面包含元数据管理，数据分片，数据读取类 等实例变量public class ExampleConnector implements Connector&#123; private static final Logger log = Logger.get(ExampleConnector.class); private final LifeCycleManager lifeCycleManager; private final ExampleMetadata metadata; private final ExampleSplitManager splitManager; private final ExampleRecordSetProvider recordSetProvider; // get/set 方法 ... &#125; 6.2 函数开发 Presto自定义了很多函数，在 函数分类： 标量函数（scalar） 聚合函数（aggregation） 开窗函数（window） 代码示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162// 标量函数@Description(“return array containing elements that match the given predicate”)//函数描述@ScalarFunction(value = “filter”, deterministic = false)public final class ArrayFilterFunction&#123;@TypeParameter(&quot;T&quot;)@TypeParameterSpecialization(name = &quot;T&quot;, nativeContainerType = long.class) // 参数类型@SqlType(&quot;array(T)&quot;)// 返回函数类型public static Block filterLong( @TypeParameter(&quot;T&quot;) Type elementType, @SqlType(&quot;array(T)&quot;) Block arrayBlock, @SqlType(&quot;function(T, boolean)&quot;) FilterLongLambda function)&#123; ... 实现逻辑 ... BlockBuilder resultBuilder = elementType.createBlockBuilder(null, positionCount); return resultBuilder.build(); &#125;&#125;// 开窗函数@WindowFunctionSignature(name = “rank”, returnType = “bigint”)public class RankFunction extends RankingWindowFunction&#123;private long rank;private long count;@Overridepublic void reset()&#123; rank = 0; count = 1;&#125;@Overridepublic void processRow(BlockBuilder output, boolean newPeerGroup, int peerGroupCount, int currentPosition)&#123; if (newPeerGroup) &#123; rank += count; count = 1; &#125; else &#123; count++; &#125; BIGINT.writeLong(output, rank); &#125;&#125;// 聚合函数@AggregationFunction(“count”)public final class CountAggregation &#123;private CountAggregation() &#123;&#125;// 数据在每个worker上分片输入@InputFunctionpublic static void input(@AggregationState LongState state) &#123;state.setLong(state.getLong() + 1);&#125;// worker间数据聚合@CombineFunctionpublic static void combine(@AggregationState LongState state, @AggregationState LongState otherState) &#123;state.setLong(state.getLong() + otherState.getLong());&#125;// 规约输出结果@OutputFunction(StandardTypes.BIGINT)public static void output(@AggregationState LongState state, BlockBuilder out) &#123;BIGINT.writeLong(out, state.getLong());&#125;&#125; 7、性能优化 Order by + limit order by 会扫描worker上表数据，耗内存，结合limit 使用减少计算消耗。 Group By 需要分组的字段，distinct后数量倒序放置 1select count(1),uid,age from user_sales group by uid,age; 使用模糊聚合函数 用精准误差换性能。 1234//不推荐select count(distinct(age)) from user_sales;//推荐select count(approx_distinct(age)) from user_sales; 合并like 1234//不推荐select *from user_sales where name like ‘%a%’ or name like ‘%b%’ or name like ‘%c%’;//推荐select* from user_sales where name regexp_like(name, a|b|c); 大表 join 小表 Presto 使用的是Distributed Hash Join，优先将右表进行hash分区的数据配发到集群中所有woker上（请确保此时的woker内存&gt;右表大小），然后才将左表hash的分区依次传输到相应的集群节点上。即，右表数据的分区会先全部到分布到所有计算节点，左表hash分区是流式传输到相应的计算节点。 关闭 Distributed Hash Join 如果数据存在倾斜，hash join 性能急剧下降，此时可以通过 ```set session distributes_join=’false’关闭hash join。那么2表就不会进行hash重分布，右表会被广播到左表Source Stage 的每个节点做join。 使用ORC存储和snappy压缩 针对hive创建表，建议使用orc格式存储，preto在代码层面对orc格式做了很多优化。 数据压缩可以减少节点间数据传输对IO带宽压力，对于即席查询需要快速解压，snappy比较合适。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"Presto","slug":"Presto","permalink":"http://yoursite.com/tags/Presto/"}]},{"title":"SPI","slug":"SPI","date":"2021-06-22T06:00:41.000Z","updated":"2022-11-22T01:27:53.448Z","comments":true,"path":"2021/06/22/SPI/","link":"","permalink":"http://yoursite.com/2021/06/22/SPI/","excerpt":"","text":"","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"SPI","slug":"SPI","permalink":"http://yoursite.com/tags/SPI/"}]},{"title":"多线程","slug":"多线程","date":"2021-06-22T05:50:55.000Z","updated":"2022-11-22T01:27:53.921Z","comments":true,"path":"2021/06/22/多线程/","link":"","permalink":"http://yoursite.com/2021/06/22/%E5%A4%9A%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"前言:目前大部分操作系统都是以线程为CPU调度和分派基本单位。多线程在日常程序中运用的还是比较多的，例如web容器帮我们在http层面可以同时处理多个请求，这些可能多我们是无感的。平常的开发中，我们需要结合业务场景来合理运用多线程，例如大文件的IO操作，大量消息发送，都可以运用多线程来处理，充分利用多核CPU性能。 一、创建线程的方式 继承Thread类创建线程、实现Runnable接口创建线程、使用Callable和Future创建线程、使用线程池 继承Thread类，重写run() 12345678910public class Test extends Thread &#123;@Overridepublic void run() &#123;System.out.println(“hello,word!”); &#125;public static void main(String[] args) &#123;Test test = new Test();test.start(); &#125;&#125; 实现Runnable接口创建线程 123456789101112131415161718public class Test implements Runnable &#123;@Overridepublic void run() &#123;System.out.println(“hello,word!”);&#125;public static void main(String[] args) &#123;Thread thread = new Thread(new Test());thread.start();&#125;&#125;//Java 8 lambda写法public class Test &#123;public static void main(String[] args) &#123;Thread thread = new Thread( () -&gt; System.out.println(“hello,word!”));thread.start();&#125;&#125; 使用Callable和Future创建线程 12345678910111213141516171819202122232425262728293031323334public class Test implements Callable &#123;@Overridepublic String call() throws Exception &#123;return “hello,word!”;&#125;public static void main(String[] args) &#123; Callable&lt;String&gt; test = new Test(); try &#123; FutureTask&lt;String&gt; futureTask = new FutureTask&lt;&gt;(test); Thread thread = new Thread(futureTask); thread.start(); System.out.println(futureTask.get()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125;&#125;//java8 lambda匿名类写法public class Test &#123;public static void main(String[] args) &#123;FutureTask futureTask=new FutureTask&lt;&gt;(()-&gt;”hello,word!”);Thread thread = new Thread(futureTask);thread.start();try &#123;System.out.println(futureTask.get());&#125; catch (InterruptedException e) &#123;e.printStackTrace();&#125; catch (ExecutionException e) &#123;e.printStackTrace();&#125;&#125;&#125; 使用线程池 1234567891011121314public class Test &#123;public static void main(String[] args) &#123;FutureTask futureTask=new FutureTask&lt;&gt;(()-&gt;”hello,word!”);ExecutorService service=Executors.newFixedThreadPool(2);service.submit(futureTask);try &#123;System.out.println( futureTask.get());&#125; catch (InterruptedException e) &#123;e.printStackTrace();&#125; catch (ExecutionException e) &#123;e.printStackTrace();&#125;&#125;&#125; 二、Callable Callable和Runnbale一样代表着任务，区别在于Callable有返回值并且可以抛出异常！Callable是一种可以返回结果的任务，这是它与Runnable的区别，但是通过适配器模式可以使Runnable与Callable类似。Future代表了一个异步的计算，可以从中得到计算结果、查看计算状态，其实现FutureTask可以被提交给Executor执行，多个线程可以从中得到计算结果。Callable和Future是配合使用的，当从Future中get结果时，如果结果还没被计算出来，那么线程将会被挂起，FutureTak内部使用一个单链表维持等待的线程；当计算结果出来后，将会对等待线程解除挂起，等待线程就都可以得到计算结果了。 三、Future接口、FutureTask类 Future是一个接口，代表了一个异步计算的结果。接口中的方法用来检查计算是否完成、等待完成和得到计算的结果。当计算完成后，只能通过get()方法得到结果，get方法会阻塞直到结果准备好了。如果想取消，那么调用cancel()方法。其他方法用于确定任务是正常完成还是取消了。一旦计算完成了，那么这个计算就不能被取消。 FutureTask类实现了RunnableFuture接口，而RunnnableFuture接口继承了Runnable和Future接口，所以说FutureTask是一个提供异步计算的结果的任务。 FutureTask可以用来包装Callable或者Runnbale对象。因为FutureTask实现了Runnable接口，所以FutureTask也可以被提交给Executor。 12345678910111213//FutureTask 2构造方法public FutureTask(Callable&lt;V&gt; callable) &#123; if (callable == null) throw new NullPointerException(); this.callable = callable; this.state = NEW; // ensure visibility of callable &#125; public FutureTask(Runnable runnable, V result) &#123; this.callable = Executors.callable(runnable, result); this.state = NEW; // ensure visibility of callable &#125; 四、线程池工厂 Executors面对开发人员的创建线程池的工厂类，它和线程执行器ExecutorService关系很紧密。 可以看到，提供了很多静态的方法，我们主要关注以下方法： newFixedThreadPool(int nThreads) 固定线程数量的线程池 newScheduledThreadPool(int corePoolSize) 拥有至少corePoolSize数量的线程池，可以延迟或者固定频率执行 newCachedThreadPool() 线程不固定线程池，常用于多且小的异步任务使用 newSingleThreadExecutor() 串行按照提交顺序执行任务，的单线程化线程池 newWorkStealingPool() 任务窃取线程池，JDK1.8以后加的，充分利用现有的cpu资源创建线程池,一个线程维护一个任务队列，常用在任务执行时间差别较大的业务 123456789101112131415161718192021222324public class ExecutorsDemo &#123; private static ExecutorService service = Executors.newCachedThreadPool(); private static ExecutorService service2 = Executors.newFixedThreadPool(3); private static ExecutorService service3 = Executors.newScheduledThreadPool(3); private static ExecutorService service4 = Executors.newSingleThreadExecutor(); private static ExecutorService service5 = Executors.newWorkStealingPool(); public static void main(String[] args) throws Exception &#123; addTask(&quot;newCachedThreadPool&quot;, service); addTask(&quot;newFixedThreadPool&quot;, service2); addTask(&quot;newScheduledThreadPool&quot;, service3); addTask(&quot;newSingleThreadExecutor&quot;, service4); addTask(&quot;newWorkStealingPool&quot;, service5); &#125; private static void addTask(String type, ExecutorService service) throws Exception &#123; for (int i = 0; i &lt; 10; i++) &#123; service.execute(() -&gt; System.out.println(type + &quot;,&quot; + Thread.currentThread().getName())); &#125; service.shutdown(); service.awaitTermination(5, TimeUnit.MILLISECONDS);//等待任务完成再关闭主main线程 &#125;&#125; **注意：**虽然service.shutdown()是关闭线程，此时已经提交的线程会被执行完毕，但是打印需要时间，所以再最后加了一句 service.awaitTermination(5, TimeUnit.MILLISECONDS)，目的是为了看到打印消息。 ScheduledExecutorService ScheduledExecutorService接口里面的4个方法 12345678public interface ScheduledExecutorService extends ExecutorService &#123;public &lt;V&gt; ScheduledFuture&lt;V&gt; schedule(Callable&lt;V&gt; callable,long delay, TimeUnit unit); public ScheduledFuture&lt;?&gt; scheduleAtFixedRate(Runnable command,long initialDelay,long period,TimeUnit unit);public ScheduledFuture&lt;?&gt; scheduleWithFixedDelay(Runnable command, long initialDelay,long delay,TimeUnit unit);&#125; 五、线程基础 stop方法（Deprecated） stop()会放弃持有的线程的锁，假设在为多个变量赋值的时候，刚好赋值到中间，执行了stop方法，很容易导致数据的不一致，此方法已经被废弃，尽量不要用，下面演示个demo 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class ThreadTest &#123; static Integer i, j; static Runnable r1 = () -&gt; &#123; while (true) &#123; i = new Random().nextInt(); j = i; &#125; &#125;; static Runnable r2 = () -&gt; &#123; while (true) &#123; if (i != j) &#123; System.out.println(&quot;i:&quot; + i + &quot;,j:&quot; + j); &#125; else &#123; System.out.println(&quot;00000&quot;); &#125; &#125; &#125;; public static void main(String[] args) &#123; Thread read = new Thread(r2); read.start(); while (true) &#123; Thread write = new Thread(r1); write.start(); try &#123; Thread.sleep(25); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; write.stop();//Deprecated &#125; &#125;&#125;打印结果0000000000i:2071039910,j:1508352969i:-1148719051,j:-7436740230000000000 线程中断来替代stop 上面代码读取的不一致，核心原因是一以为线程突然停止，那么有没有一种优雅的停止呢，JDK为我们提供了更优雅的方式，中断。我们将上述代码稍做改变，数据的一致性立马可以得到保证。 12345678910111213141516171819202122232425262728293031323334353637383940414243 public class ThreadTest &#123; static Integer i, j; static Runnable r1 = () -&gt; &#123; while (true) &#123; if(Thread.currentThread().isInterrupted()) &#123;//判断是否有中断标志 i = new Random().nextInt(); j = i; break; &#125; &#125; &#125;; static Runnable r2 = () -&gt; &#123; while (true) &#123; if (i != j) &#123; System.out.println(&quot;i:&quot; + i + &quot;,j:&quot; + j); &#125; else &#123; System.out.println(&quot;00000&quot;); &#125; &#125; &#125;; public static void main(String[] args) &#123; Thread read = new Thread(r2); read.start(); while (true) &#123; Thread write = new Thread(r1); write.start(); try &#123; Thread.sleep(25); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; write.interrupt();// 设置中断标志 &#125; &#125;&#125;结果0000000000··· 注意点：若是被设置了中断的线程使用了sleep方法，则运行时可能会抛出中断异常且中断标志会被清空，这里贴出异常，代码就不写出来了。 12345java.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at com.andyfan.thread.ThreadTest.lambda$static$0(ThreadTest.java:20) at com.andyfan.thread.ThreadTest$$Lambda$1&#x2F;558638686.run(Unknown Source) at java.lang.Thread.run(Thread.java:745) 等待、唤醒 wait/notify wait和notify时Object里面定义的方法。 工作原理：obj.wait()，线程进入等待队列，obj.notify() 则从等待队列里面随机选择一个线程继续执行等待前代码，意味着唤醒是非公平的。 wait和notify的调用必须在synchronized 代码块里，这也意味着必须要获得对象的监视器（锁）。 wait方法被调用后会主动释放监视器（锁），这个是sleep方法重要区分点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class ThreadTest2 &#123; final static Object object = new Object(); static Runnable r1 = () -&gt; &#123; synchronized (object) &#123; long start = 0l; try &#123; System.out.println(&quot;wait方法被执行了&quot;); start = System.currentTimeMillis(); object.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(); System.out.println(&quot;wait方法释放后被执行了,等待了notify方法释放锁&quot; + (System.currentTimeMillis() - start)/1000 + &quot;秒&quot;); &#125; &#125;; static Runnable r2 = () -&gt; &#123; synchronized (object) &#123; object.notify(); System.out.println(&quot;notify方法被执行了&quot;); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;notify休眠1秒后&quot;); &#125; &#125;; public static void main(String[] args) &#123; Thread t1 = new Thread(r1); Thread t2 = new Thread(r2); t1.start(); try &#123; Thread.sleep(20); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; t2.start(); &#125;&#125;打印结果wait方法被执行了notify方法被执行了notify休眠1秒后wait方法释放后被执行了,等待了notify方法释放锁1秒 suspend 和resume (Deprecated） suspend线程会被挂起，它是不释放监视器（锁）的，必须等到resume 才会释放锁。这2个方法也被废弃了，因为若resume在suspend前执行，会导致线程永远别挂起，自己和其他线程由于获取不到监视器根本没办法正常工作，下面演示个被一直被挂起例子 1234567891011121314151617181920212223242526272829303132public class ThreadTest3 &#123; final static Object object = new Object(); static Runnable r = () -&gt; &#123; synchronized (object) &#123; String name = Thread.currentThread().getName(); System.out.println(name); Thread.currentThread().suspend();//Deprecated System.out.println(name + &quot;被刮挂起后执行。。。&quot;); &#125; &#125;; public static void main(String[] args) &#123; Thread t1 = new Thread(r); Thread t2 = new Thread(r); t1.start(); t2.start(); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; t1.resume();//Deprecated t2.resume();//在t2的suspend前执行，导致t2一直被挂起！ try &#123; t1.join(); t2.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 用jstack -l pid 命令 查看堆栈信息,可以看到线程t2一直处于Runnable状态 12345678&quot;Thread-1&quot; #11 prio=5 os_prio=31 tid=0x00007fdbd703f800 nid=0x5903 runnable [0x000070000aa79000] java.lang.Thread.State: RUNNABLE at java.lang.Thread.suspend0(Native Method) at java.lang.Thread.suspend(Thread.java:1029) at com.andyfan.thread.ThreadTest3.lambda$static$0(ThreadTest3.java:14) - locked &lt;0x000000076abf9f58&gt; (a java.lang.Object) at com.andyfan.thread.ThreadTest3$$Lambda$1/558638686.run(Unknown Source) at java.lang.Thread.run(Thread.java:745) join和yield ​ join 方法其实内部调用的是wait方法，JDK实现代码块： 1234if (millis == 0) &#123; while (isAlive()) &#123; wait(0); &#125; &#125; yield（谦让） 个人觉得这个方法使用需要勇气,有点鸡肋，因为它执行了，表明是让出当前CPU时间片，让和它同等优先级的线程优先执行，但是它持有的锁又不释放，共有资源还会再接下来时间竞争，且时间还不固定。 1234567891011java public class ThreadTest4 &#123; static int j = 0; public static void main(String[] args) throws Exception &#123; Thread thread = new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; j = i; &#125; &#125;); thread.start(); thread.join();//main的输出，等待线程执行完再执行,不然输出可能是0或者很小的值 System.out.println(j); &#125;&#125;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"多线程/并发","slug":"多线程-并发","permalink":"http://yoursite.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%B9%B6%E5%8F%91/"}]},{"title":"Mybatis的映射器","slug":"MyBatis的映射器","date":"2021-06-22T05:45:30.000Z","updated":"2022-11-22T01:27:53.403Z","comments":true,"path":"2021/06/22/MyBatis的映射器/","link":"","permalink":"http://yoursite.com/2021/06/22/MyBatis%E7%9A%84%E6%98%A0%E5%B0%84%E5%99%A8/","excerpt":"","text":"映射器 映射器可以理解为我们平常说的mapper接口，一般映射器可以由接口+xml/注解方式组成。生命周期：一个会话内。 一、使用 接口+xml 1234567891011121314151617181920212223 @Mapper public interface userMapper&#123; User selectById(id); &#125; &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.vxiaoke.cp.base.mapper.UserAccountMapper&quot;&gt; &lt;resultMap id=&quot;BaseResultMap&quot; type=&quot;com.vxiaoke.cp.base.models.UserAccount&quot;&gt; &lt;id column=&quot;id&quot; property=&quot;id&quot; /&gt; &lt;result column=&quot;username&quot; property=&quot;ctime&quot; /&gt; &lt;result column=&quot;age&quot; property=&quot;mtime&quot; /&gt; &lt;/resultMap&gt; &lt;!-- 通用查询结果列 --&gt; &lt;sql id=&quot;Base_Column_List&quot;&gt; id, ctime, mtime, username, age &lt;/sql&gt; &lt;select id=&quot;selectById&quot; resultMap=&quot;BaseResultMap&quot;&gt; select &lt;include refid=&quot;Base_Column_List&quot;/&gt; from user where id = #&#123;id&#125; &lt;/select&gt; &lt;/mapper&gt; 接口+注解 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283 @Select(value=&quot;select *id,username,age from user where id=#&#123;id&#125;&quot;) public interface userMapper&#123; User selectById(id); &#125;``` ### 二、组成* MappedStatement * SqlSource* BoundSql### 三、说明* MappedStatement 这个类涉及的东西是比较多的，可以看看下面的表 |属性 | 类型|说明||---|---|---|----||resource | String| 类似mybatis-config.xml 文件名||Configuration | Configuration|配置类||String | id| 查找到哪个mapper标识，例如getById||KeyGenerator | keyGenerator| key生成，例如在insert后返回id||boolean | useCache| 是否用耳机缓存||SqlSource | sqlSource| 根据参数组装sql||ParameterMap | parameterMap|参数||... | ...| ...| * SqlSource&gt; 根据参数/其他规则组装sql，![](http://47.95.12.0:3389/ftp/SqlSource.png) ```java public interface SqlSource &#123; BoundSql getBoundSql(Object parameterObject);&#125;``` * BoundSql```java public class BoundSql &#123; private final String sql;//我们写的原生sql private final List&lt;ParameterMapping&gt; parameterMappings; private final Object parameterObject;//参数本身，例如POJO，Map等传入的参数，@Parm注解会解析成Map private final Map&lt;String, Object&gt; additionalParameters;//每个元素都是map，如：属性名，类型javaType，typeHandler等 private final MetaObject metaParameters; public BoundSql(Configuration configuration, String sql, List&lt;ParameterMapping&gt; parameterMappings, Object parameterObject) &#123; this.sql = sql; this.parameterMappings = parameterMappings; this.parameterObject = parameterObject; this.additionalParameters = new HashMap&lt;&gt;(); this.metaParameters = configuration.newMetaObject(additionalParameters); &#125; public String getSql() &#123; return sql; &#125; public List&lt;ParameterMapping&gt; getParameterMappings() &#123; return parameterMappings; &#125; public Object getParameterObject() &#123; return parameterObject; &#125; public boolean hasAdditionalParameter(String name) &#123; String paramName = new PropertyTokenizer(name).getName(); return additionalParameters.containsKey(paramName); &#125; public void setAdditionalParameter(String name, Object value) &#123; metaParameters.setValue(name, value); &#125; public Object getAdditionalParameter(String name) &#123; return metaParameters.getValue(name); &#125;&#125; Mapper本质 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130// MapperProxy工厂public class MapperProxyFactory&lt;T&gt; &#123; private final Class&lt;T&gt; mapperInterface; private final Map&lt;Method, MapperMethod&gt; methodCache = new ConcurrentHashMap&lt;Method, MapperMethod&gt;(); public MapperProxyFactory(Class&lt;T&gt; mapperInterface) &#123; this.mapperInterface = mapperInterface; &#125; public Class&lt;T&gt; getMapperInterface() &#123; return mapperInterface; &#125; public Map&lt;Method, MapperMethod&gt; getMethodCache() &#123; return methodCache; &#125; @SuppressWarnings(&quot;unchecked&quot;) protected T newInstance(MapperProxy&lt;T&gt; mapperProxy) &#123; return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[] &#123; mapperInterface &#125;, mapperProxy); &#125; public T newInstance(SqlSession sqlSession) &#123; final MapperProxy&lt;T&gt; mapperProxy = new MapperProxy&lt;T&gt;(sqlSession, mapperInterface, methodCache); return newInstance(mapperProxy); &#125;&#125;//MapperProxy JDK的动态代理，实现InvocationHandler 接口public class MapperProxy&lt;T&gt; implements InvocationHandler, Serializable &#123; private static final long serialVersionUID = -6424540398559729838L; private final SqlSession sqlSession; private final Class&lt;T&gt; mapperInterface; private final Map&lt;Method, MapperMethod&gt; methodCache; public MapperProxy(SqlSession sqlSession, Class&lt;T&gt; mapperInterface, Map&lt;Method, MapperMethod&gt; methodCache) &#123; this.sqlSession = sqlSession; this.mapperInterface = mapperInterface; this.methodCache = methodCache; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; try &#123; // 是否是个类，显然不是 if (Object.class.equals(method.getDeclaringClass())) &#123; return method.invoke(this, args); &#125; else if (isDefaultMethod(method)) &#123; return invokeDefaultMethod(proxy, method, args); &#125; &#125; catch (Throwable t) &#123; throw ExceptionUtil.unwrapThrowable(t); &#125; // 构建MapperMethod 对象，跳到下面*处 final MapperMethod mapperMethod = cachedMapperMethod(method); // 核心方法（注意！） return mapperMethod.execute(sqlSession, args); &#125;// * 是否在缓存里，否则初始化mapperMethod private MapperMethod cachedMapperMethod(Method method) &#123; MapperMethod mapperMethod = methodCache.get(method); if (mapperMethod == null) &#123; mapperMethod = new MapperMethod(mapperInterface, method, sqlSession.getConfiguration()); methodCache.put(method, mapperMethod); &#125; return mapperMethod; &#125;...//看看核心方法public class MapperMethod &#123; private final SqlCommand command; private final MethodSignature method; public MapperMethod(Class&lt;?&gt; mapperInterface, Method method, Configuration config) &#123; this.command = new SqlCommand(config, mapperInterface, method); this.method = new MethodSignature(config, mapperInterface, method); &#125;// 命令模式,对底层还是sqlSession来执行。 public Object execute(SqlSession sqlSession, Object[] args) &#123; Object result; switch (command.getType()) &#123; case INSERT: &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.insert(command.getName(), param)); break; &#125; case UPDATE: &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.update(command.getName(), param)); break; &#125; case DELETE: &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.delete(command.getName(), param)); break; &#125; case SELECT: if (method.returnsVoid() &amp;&amp; method.hasResultHandler()) &#123; executeWithResultHandler(sqlSession, args); result = null; &#125; else if (method.returnsMany()) &#123; result = executeForMany(sqlSession, args); &#125; else if (method.returnsMap()) &#123; result = executeForMap(sqlSession, args); &#125; else if (method.returnsCursor()) &#123; result = executeForCursor(sqlSession, args); &#125; else &#123; Object param = method.convertArgsToSqlCommandParam(args); result = sqlSession.selectOne(command.getName(), param);//sqlSession &#125; break; case FLUSH: result = sqlSession.flushStatements(); break; default: throw new BindingException(&quot;Unknown execution method for: &quot; + command.getName()); &#125; if (result == null &amp;&amp; method.getReturnType().isPrimitive() &amp;&amp; !method.returnsVoid()) &#123; throw new BindingException(&quot;Mapper method &#x27;&quot; + command.getName() + &quot; attempted to return null from a method with a primitive return type (&quot; + method.getReturnType() + &quot;).&quot;); &#125; return result; &#125;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"myBatis","slug":"myBatis","permalink":"http://yoursite.com/tags/myBatis/"}]},{"title":"MyBatis-StatementHandler","slug":"MyBatis-StatementHandler","date":"2021-06-22T05:45:09.000Z","updated":"2022-11-22T01:27:53.402Z","comments":true,"path":"2021/06/22/MyBatis-StatementHandler/","link":"","permalink":"http://yoursite.com/2021/06/22/MyBatis-StatementHandler/","excerpt":"","text":"四大金刚-StatementHandler 四大金刚中的核心，使用数据库的PreparedStatement执行数据库操作。 StatementHandler 数据库会话器 12345678910111213141516171819202122232425public interface StatementHandler &#123; Statement prepare(Connection connection, Integer transactionTimeout) throws SQLException; void parameterize(Statement statement) throws SQLException;//ParameterHandler 参数设置类 void batch(Statement statement) throws SQLException; int update(Statement statement) throws SQLException; &lt;E&gt; List&lt;E&gt; query(Statement statement, ResultHandler resultHandler) throws SQLException;//ResultSetHandler 结果集处理类 &lt;E&gt; Cursor&lt;E&gt; queryCursor(Statement statement) throws SQLException; BoundSql getBoundSql(); ParameterHandler getParameterHandler();&#125;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"myBatis","slug":"myBatis","permalink":"http://yoursite.com/tags/myBatis/"}]},{"title":"MyBatis-ParameterHandler","slug":"MyBatis-ParameterHandler","date":"2021-06-22T05:44:56.000Z","updated":"2022-11-22T01:27:53.401Z","comments":true,"path":"2021/06/22/MyBatis-ParameterHandler/","link":"","permalink":"http://yoursite.com/2021/06/22/MyBatis-ParameterHandler/","excerpt":"","text":"四大金刚-ParameterHandler 对预编译语句进行参数设置 12345678public interface ParameterHandler &#123; Object getParameterObject(); void setParameters(PreparedStatement ps) throws SQLException;&#125; 默认参数处理器 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class DefaultParameterHandler implements ParameterHandler &#123; private final TypeHandlerRegistry typeHandlerRegistry; private final MappedStatement mappedStatement; private final Object parameterObject; private BoundSql boundSql; private Configuration configuration; public DefaultParameterHandler(MappedStatement mappedStatement, Object parameterObject, BoundSql boundSql) &#123; this.mappedStatement = mappedStatement; this.configuration = mappedStatement.getConfiguration(); this.typeHandlerRegistry = mappedStatement.getConfiguration().getTypeHandlerRegistry(); this.parameterObject = parameterObject; this.boundSql = boundSql; &#125; @Override public Object getParameterObject() &#123; return parameterObject; &#125; @Override public void setParameters(PreparedStatement ps) &#123; ErrorContext.instance().activity(&quot;setting parameters&quot;).object(mappedStatement.getParameterMap().getId()); List&lt;ParameterMapping&gt; parameterMappings = boundSql.getParameterMappings(); if (parameterMappings != null) &#123; for (int i = 0; i &lt; parameterMappings.size(); i++) &#123; ParameterMapping parameterMapping = parameterMappings.get(i); if (parameterMapping.getMode() != ParameterMode.OUT) &#123; Object value; String propertyName = parameterMapping.getProperty(); if (boundSql.hasAdditionalParameter(propertyName)) &#123; // issue #448 ask first for additional params value = boundSql.getAdditionalParameter(propertyName); &#125; else if (parameterObject == null) &#123; value = null; &#125; else if (typeHandlerRegistry.hasTypeHandler(parameterObject.getClass())) &#123; value = parameterObject; &#125; else &#123; MetaObject metaObject = configuration.newMetaObject(parameterObject); value = metaObject.getValue(propertyName); &#125; //一系列的typeHandler，后面会讲 TypeHandler typeHandler = parameterMapping.getTypeHandler(); JdbcType jdbcType = parameterMapping.getJdbcType(); if (value == null &amp;&amp; jdbcType == null) &#123; jdbcType = configuration.getJdbcTypeForNull(); &#125; try &#123; typeHandler.setParameter(ps, i + 1, value, jdbcType); &#125; catch (TypeException e) &#123; throw new TypeException(&quot;Could not set parameters for mapping: &quot; + parameterMapping + &quot;. Cause: &quot; + e, e); &#125; catch (SQLException e) &#123; throw new TypeException(&quot;Could not set parameters for mapping: &quot; + parameterMapping + &quot;. Cause: &quot; + e, e); &#125; &#125; &#125; &#125; &#125;&#125;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"myBatis","slug":"myBatis","permalink":"http://yoursite.com/tags/myBatis/"}]},{"title":"MyBatis-Executor","slug":"MyBatis-Executor","date":"2021-06-22T05:42:22.000Z","updated":"2022-11-22T01:27:53.400Z","comments":true,"path":"2021/06/22/MyBatis-Executor/","link":"","permalink":"http://yoursite.com/2021/06/22/MyBatis-Executor/","excerpt":"","text":"四大金刚-Executor SqlSession 执行CURD，DefaultSqlSession为默认实现类。它里面有几个重要的方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192// select 方法 @Override public &lt;E&gt; List&lt;E&gt; selectList(String statement, Object parameter, RowBounds rowBounds) &#123; try &#123; MappedStatement ms = configuration.getMappedStatement(statement); return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER);//executor &#125; catch (Exception e) &#123; throw ExceptionFactory.wrapException(&quot;Error querying database. Cause: &quot; + e, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125;i &#125; //insert update delete 方法 @Override public int update(String statement, Object parameter) &#123; try &#123; dirty = true; MappedStatement ms = configuration.getMappedStatement(statement); return executor.update(ms, wrapCollection(parameter)); &#125; catch (Exception e) &#123; throw ExceptionFactory.wrapException(&quot;Error updating database. Cause: &quot; + e, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125; &#125; //提交 @Override public void commit(boolean force) &#123; try &#123; executor.commit(isCommitOrRollbackRequired(force)); dirty = false; &#125; catch (Exception e) &#123; throw ExceptionFactory.wrapException(&quot;Error committing transaction. Cause: &quot; + e, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125; &#125; // 回滚 @Override public void rollback(boolean force) &#123; try &#123; executor.rollback(isCommitOrRollbackRequired(force)); dirty = false; &#125; catch (Exception e) &#123; throw ExceptionFactory.wrapException(&quot;Error rolling back transaction. Cause: &quot; + e, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125; &#125; ``` * 幕后英雄Executor```javapublic interface Executor &#123; ResultHandler NO_RESULT_HANDLER = null; int update(MappedStatement ms, Object parameter) throws SQLException; &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey cacheKey, BoundSql boundSql) throws SQLException; &lt;E&gt; List&lt;E&gt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException; &lt;E&gt; Cursor&lt;E&gt; queryCursor(MappedStatement ms, Object parameter, RowBounds rowBounds) throws SQLException; List&lt;BatchResult&gt; flushStatements() throws SQLException; void commit(boolean required) throws SQLException; void rollback(boolean required) throws SQLException; CacheKey createCacheKey(MappedStatement ms, Object parameterObject, RowBounds rowBounds, BoundSql boundSql); boolean isCached(MappedStatement ms, CacheKey key); void clearLocalCache(); void deferLoad(MappedStatement ms, MetaObject resultObject, String property, CacheKey key, Class&lt;?&gt; targetType); Transaction getTransaction(); void close(boolean forceRollback); boolean isClosed(); void setExecutorWrapper(Executor executor);&#125; 简单执行器 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class SimpleExecutor extends BaseExecutor &#123; public SimpleExecutor(Configuration configuration, Transaction transaction) &#123; super(configuration, transaction); &#125; @Override public int doUpdate(MappedStatement ms, Object parameter) throws SQLException &#123; Statement stmt = null; try &#123; Configuration configuration = ms.getConfiguration(); StatementHandler handler = configuration.newStatementHandler(this, ms, parameter, RowBounds.DEFAULT, null, null); stmt = prepareStatement(handler, ms.getStatementLog()); return handler.update(stmt); &#125; finally &#123; closeStatement(stmt); &#125; &#125; @Override public &lt;E&gt; List&lt;E&gt; doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException &#123; Statement stmt = null; try &#123; Configuration configuration = ms.getConfiguration(); StatementHandler handler = configuration.newStatementHandler(wrapper, ms, parameter, rowBounds, resultHandler, boundSql); stmt = prepareStatement(handler, ms.getStatementLog()); return handler.&lt;E&gt;query(stmt, resultHandler); &#125; finally &#123; closeStatement(stmt); &#125; &#125; @Override protected &lt;E&gt; Cursor&lt;E&gt; doQueryCursor(MappedStatement ms, Object parameter, RowBounds rowBounds, BoundSql boundSql) throws SQLException &#123; Configuration configuration = ms.getConfiguration(); StatementHandler handler = configuration.newStatementHandler(wrapper, ms, parameter, rowBounds, null, boundSql); Statement stmt = prepareStatement(handler, ms.getStatementLog()); return handler.&lt;E&gt;queryCursor(stmt); &#125; @Override public List&lt;BatchResult&gt; doFlushStatements(boolean isRollback) throws SQLException &#123; return Collections.emptyList(); &#125;//对Sql编译和参数初始化 private Statement prepareStatement(StatementHandler handler, Log statementLog) throws SQLException &#123; Statement stmt; Connection connection = getConnection(statementLog); stmt = handler.prepare(connection, transaction.getTimeout()); //参数预编译 handler.parameterize(stmt);//设置参数 return stmt; &#125;&#125;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"myBatis","slug":"myBatis","permalink":"http://yoursite.com/tags/myBatis/"}]},{"title":"brew","slug":"brew","date":"2021-06-15T07:25:15.000Z","updated":"2022-11-22T01:27:53.452Z","comments":true,"path":"2021/06/15/brew/","link":"","permalink":"http://yoursite.com/2021/06/15/brew/","excerpt":"","text":"安装brew 1234//安装依赖工具xcode-select --install// 安装homebrew/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; 常用命令 123456789101112131415161718brew help 查看帮助信息brew -v 查看版本brew update 更新Homebrew自己brew --cache 查看install的包位置brew outdated 查询可更新的包brew upgrade 更新所有brew upgrade [包名] 更新指定包brew cleanup 清理所有包的旧版本brew cleanup [包名] 清理指定包的旧版本brew cleanup -n 查看可清理的旧版本包，不执行实际操作brew pin $FORMULA 锁定不想更新的包brew unpin $FORMULA 取消锁不想更新的包brew uninstall [包名] 卸载安装brew info [包名] 查看包信息brew list 查看安装列表brew search [包名] 查询可用包brew services start [包名] 启动服务brew services stop [包名] 停止服务 卸载brew 1234567cd `brew --prefix`rm -rf Cellarbrew prunerm `git ls-files`rm -r Library/Homebrew Library/Aliases Library/Formula Library/Contributionsrm -rf .gitrm -rf ~/Library/Caches/Homebrew","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"brew","slug":"brew","permalink":"http://yoursite.com/tags/brew/"}]},{"title":"fastjson","slug":"fastjson","date":"2021-06-11T08:37:45.000Z","updated":"2022-11-22T01:27:53.569Z","comments":true,"path":"2021/06/11/fastjson/","link":"","permalink":"http://yoursite.com/2021/06/11/fastjson/","excerpt":"","text":"抽象类序列化 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586@Datapublic class ConditionContentContext implements Serializable &#123; private ConditionTypeEnum type; private AbstractConditionContent content;&#125;@JSONType(seeAlso=&#123;EventConditionContent.class,FixConditionContent.class&#125;)public abstract class AbstractConditionContent implements Serializable &#123; @JSONField(format=&quot;yyyy-MM-dd HH:mm:ss&quot;) private Date startTime; @JSONField(format=&quot;yyyy-MM-dd HH:mm:ss&quot;) private Date endTime;&#125;@EqualsAndHashCode(callSuper = true)@Data@JSONType(typeName = &quot;eventConditionContent&quot;)public class EventConditionContent extends AbstractConditionContent implements Serializable &#123; private Integer doneLogic; private List&lt;MetaEvent&gt; doneEvents; private double timeGap; private TimeUnit timeUnit; private Integer notDoneLogic; private List&lt;MetaEvent&gt; notDoneEvents;&#125;@EqualsAndHashCode(callSuper = true)@Data@JSONType(typeName =&quot;fixConditionContent&quot;)public class FixConditionContent extends AbstractConditionContent implements Serializable &#123; private String frequency; private String cron;&#125;// 序列化ConditionContentContext contentContext=new ContentContext();String contextJSON=JSON.toJSONString(contentContext, SerializerFeature.WriteClassName)； &#123; // 有@type 这个包名字/类别名 &quot;@type&quot;:&quot;com.dtwave.asset.marketing.api.dto.plan.strategy.condition.ConditionContentContext&quot;, &quot;content&quot;:&#123; &quot;@type&quot;:&quot;eventConditionContent&quot;, &quot;doneEvents&quot;:[ &#123; &quot;accountCode&quot;:&quot;wxe2b3f176ba1a4f33&quot;, &quot;accountId&quot;:8207207676424, &quot;channelCode&quot;:&quot;WECHAT_OFFICIAL_ACCOUNTS&quot;, &quot;channelId&quot;:1, &quot;eventCode&quot;:&quot;MSG_TEXT&quot;, &quot;eventId&quot;:1 &#125; ], &quot;doneLogic&quot;:0, &quot;endTime&quot;:&quot;2021-06-12 20:52:16&quot;, &quot;notDoneEvents&quot;:[ &#123; &quot;accountCode&quot;:&quot;wxe2b3f176ba1a4f33&quot;, &quot;accountId&quot;:8207207676424, &quot;channelCode&quot;:&quot;WECHAT_OFFICIAL_ACCOUNTS&quot;, &quot;channelId&quot;:1, &quot;eventCode&quot;:&quot;UNSUBSCRIBE&quot;, &quot;eventId&quot;:10 &#125; ], &quot;notDoneLogic&quot;:0, &quot;startTime&quot;:&quot;2021-06-08 20:52:16&quot;, &quot;timeGap&quot;:2.2, &quot;timeUnit&quot;:&quot;DAYS&quot; &#125;, &quot;type&quot;:&quot;EVENT&quot;&#125; // 反序列化JSON.parseObject(contextJSON, ConditionContentContext.class);","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"fastjson","slug":"fastjson","permalink":"http://yoursite.com/tags/fastjson/"}]},{"title":"hive使用","slug":"hive使用","date":"2021-05-12T07:58:33.000Z","updated":"2022-11-22T01:27:53.657Z","comments":true,"path":"2021/05/12/hive使用/","link":"","permalink":"http://yoursite.com/2021/05/12/hive%E4%BD%BF%E7%94%A8/","excerpt":"","text":"当前所在库 1select current_databases(); 查询数据库 1show databases like &quot;*bank*&quot;; 导出表结构到本地 1hive -e &quot;use db; show create table adm_cust;&quot; &gt; /home/deploy/adm_cust_ddl.txt 导出表数据到本地 1hive -e &quot;select * from taghub.adm_cust&quot; &gt; /home/deploy/adm_cust.txt 导入数据 1231、hive2、use taghub;3、load data local inpath ‘xxxx/xxx/adm_cust.txt’ into table adm_cust; 新建表 1234// 创建表同时insert 数据1、create table xx as select xxx,xxx from xxx;// 只创建表2、create table xx like xxx; 查看表大小 1hadoop fs -du /user/hive/warehouse/taghub.db/table_name|awk &#x27;&#123; SUM += $1 &#125; END &#123; print SUM/(1024*1024*1024)&#125;&#x27;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"","slug":"git使用技巧","date":"2021-04-27T05:59:27.000Z","updated":"2024-02-22T10:28:06.071Z","comments":true,"path":"2021/04/27/git使用技巧/","link":"","permalink":"http://yoursite.com/2021/04/27/git%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/","excerpt":"","text":"1、合并不同url的项目 加入待合并的url仓库 1git remote add 自定义仓库名字 远程仓库url 拉待合并的ur仓库代码 1git fetch 自定义仓库名字 合并 12// 两个分支是两个不同的版本，具有不同的提交历史，需要允许git merge --allow-unrelated-histories 自定义仓库名字/分支名 取消合并 1git merge --abort 查看指定commit修改详情 1git show commitid 2、查看日志 git log 按时间先后顺序列出所有的提交，最近的更新排在最上面 git log --oneline 每个提交在一行内显示 git log -p -2 选项是 -p 或 --patch ，它会显示每次提交所引入的差异（按 补丁 的格式输出）。 你也可以限制显示的日志条目数量，例如使用 -2 选项来只显示最近的两次提交 git log --author=作者 查看某人提交记录 3、提交 编辑上一次提交信息 123git commit --amend -m &quot;提交信息&quot;# 编辑窗口编辑git commit --amend 4、回退版本 12# 直接回退git reset [--soft | --mixed | --hard] [HEAD] 模式 工作区 暂存区 本地仓库 soft 不变 不变 变 mixed（默认） 不变 变 变 hard 变 变 变 12# 产生一个新的提交回退git revert &lt;commit&gt; git revert HEAD~3会回退到最近的第4个提交的状态，而不是第3个！ 5、merge 命令 123# 将master合并到featuregit checkout featuregit merge maseter=git merge feature master 特点 在feature分支上有一个新的提交节点。 5、rebase 命令 123# 将master合并到featuregit checkout featuregit rebase master=git rebase feature master 特点 首先，git 会把 feature 分支里面的每个 commit 取消掉； 其次，把上面的操作临时保存成 patch 文件，存在 .git/rebase 目录下； 然后，把 feature 分支更新到目前最新的 master 分支； 最后，把上面保存的 patch 文件应用到 feature 分支上； 注意 在 rebase 的过程中，也许会出现冲突 conflict。在这种情况，git 会停止 rebase 并会让你去解决冲突。在解决完冲突后，用 命令去更新这些内容 1git add 中途出现异常退出了 vi窗口，执行下面命令可以返回编辑窗口 1git rebase --edit-todo git 继续应用余下的 patch 补丁文件 1git rebase --continue 在任何时候，我们都可以用 --abort 参数来终止 rebase 的行动，并且分支会回到 rebase 开始前的状态。 1git rebase —abort 合并自己提交 1234567891011121314151617181920212223git rebase -i commitId(需要合并提交的前一个，例如有0—3提交，合并 1 2 3 ，则commitId为0时候id)# 提交信息排列：老&gt;新pick 040a2f5 1pick 866c7e8 2pick 7a6abf0 3# Rebase 6ec3557..7a6abf0 onto 6ec3557 (3 commands)## Commands:# p, pick &lt;commit&gt; = use commit# r, reword &lt;commit&gt; = use commit, but edit the commit message# e, edit &lt;commit&gt; = use commit, but stop for amending# s, squash &lt;commit&gt; = use commit, but meld into previous commit# f, fixup &lt;commit&gt; = like &quot;squash&quot;, but discard this commit&#x27;s log message# x, exec &lt;command&gt; = run command (the rest of the line) using shell# b, break = stop here (continue rebase later with &#x27;git rebase --continue&#x27;)# d, drop &lt;commit&gt; = remove commit# l, label &lt;label&gt; = label current HEAD with a name# t, reset &lt;label&gt; = reset HEAD to a label# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]# . create a merge commit using the original merge commit&#x27;s... pick：保留该commit； reword：保留该commit但是修改commit信息； edit：保留该commit但是要修改commit内容； squash：将该commit和前一个commit合并； fixup：将该commit和前一个commit合并，并不保留该commit的commit信息； exec：执行shell命令； drop：删除该commit。 6、查看所有分支最新提交 12345678# 查看本地分支所有最新提交git for-each-ref --sort=-committerdate refs/heads/ --format=&#x27;%(color:yellow)%(refname:short)%(color:reset) - %(color:bold green)%(committerdate:relative)%(color:reset) - %(contents:subject) - %(authorname)&#x27;# 查看远程所有分支最新提交git for-each-ref --sort=-committerdate refs/remotes/origin/ --format=&#x27;%(color:blue)%(refname:short)%(color:reset) - %(color:bold green)%(committerdate:relative)%(color:reset) - %(contents:subject) - %(authorname)&#x27;# 查看本地和远程分支最新提交git for-each-ref --sort=-committerdate --format=&#x27;%(color:blue)%(refname:short)%(color:reset) - %(color:bold green)%(committerdate:relative)%(color:reset) - %(contents:subject) - %(authorname)&#x27; refs/heads/ refs/remotes/origin/ 7、查看分支变更内容 12# 当前分支branch_name可不写git log -p -1 [branch_name]","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"}]},{"title":"mysql","slug":"nginx","date":"2021-04-26T10:28:00.000Z","updated":"2022-11-22T01:27:53.720Z","comments":true,"path":"2021/04/26/nginx/","link":"","permalink":"http://yoursite.com/2021/04/26/nginx/","excerpt":"","text":"简介 Nginx (engine x) 是一个高性能的HTTP和反向代理web服务器，同时也提供了IMAP/POP3/SMTP服务。Nginx是由伊戈尔·赛索耶夫为俄罗斯访问量第二的Rambler.ru站点（俄文：Рамблер）开发的，第一个公开版本0.1.0发布于2004年10月4日 Nginx 安装 yum安装 1234567sudo yum install -y epel-releasesudo yum -y updatesudo yum install -y nginx安装成功后，默认的网站目录为： /usr/share/nginx/html默认的配置文件为：/etc/nginx/nginx.conf自定义配置文件目录为: /etc/nginx/conf.d/ docker-compose安装 12345678910111213141516➜ ~ mkdir docker_nginx &amp;&amp; cd docker_nginx➜ ~ vim docker-compose.yml// docker-compose.yml的内容version: &#x27;3.1&#x27;services: nginx: restart: always image: daocloud.io/library/nginx:latest container_name: nginx ports: - 8087:80 // 保存退出，运行如下命令➜ docker_nginx docker-compose up -dRecreating nginx ... done 浏览器访问：http://localhost:8087 ubuntu上安装 12345sudo apt install nginx# 之后可执行文件 /usr/bin /usr/sbin配置文件 /etclib文件 /usr/lib Nginx配置文件 1234567891011➜ docker_nginx docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESdbe84a274189 daocloud.io/library/nginx:latest &quot;/docker-entrypoint.…&quot; 5 minutes ago Up 5 minutes 0.0.0.0:8087-&gt;80/tcp nginx➜ docker_nginx docker exec -it dbe bashroot@dbe84a274189:/# lsbin boot dev docker-entrypoint.d docker-entrypoint.sh etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr varroot@dbe84a274189:/# cd etc/nginx/root@dbe84a274189:/etc/nginx# lsconf.d fastcgi_params koi-utf koi-win mime.types modules nginx.conf scgi_params uwsgi_params win-utfroot@dbe84a274189:/etc/nginx# cat nginx.conf nginx.conf 内容 1234567891011121314151617181920212223242526// worker_processes和worker_connections 决定了nginx的并发能力user nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; // 注意，/etc/nginx/conf.d下.conf文件都会加载 include /etc/nginx/conf.d/*.conf;&#125; default.conf 的内容 配置文件中除了注释的内容，最重要的就是server 模块 12345678910111213141516171819root@dbe84a274189:/etc/nginx# cd conf.d/root@dbe84a274189:/etc/nginx/conf.d# lsdefault.confroot@dbe84a274189:/etc/nginx/conf.d# cat default.confserver &#123; listen 80; listen [::]:80; server_name localhost; location / &#123; root /usr/share/nginx/html; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125;&#125; 修改docker-compose.yml 关联本地目录conf.d 到nginx配置文件目录 12345678910version: &#x27;3.1&#x27;services: nginx: restart: always image: daocloud.io/library/nginx:latest container_name: nginx ports: - 8087:80 volumes: - ./conf.d:/etc/nginx/conf.d 添加default.conf 需要添加default.conf，否则会访问不到nginx的首页 1234➜ docker_nginx lsconf.d docker-compose.yml➜ docker_nginx cd conf.d➜ conf.d vim default.conf 123456789101112131415server &#123; listen 80; listen [::]:80; server_name localhost; location / &#123; root /usr/share/nginx/html; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125;&#125; 重启nginx 123➜ conf.d docker-compose down➜ conf.d docker-compose build➜ conf.d docker-compose up -d Nginx功能 反向代理 正向代理 客户端自己设置 客户端了解背后的代理和目标服务器 对目标服务器访问隐藏了客户端ip 反向代理 配置是在目标服务器端 客户端不知道访问到的是哪一台目标服务器 对客户端隐藏了目标服务器的ip 配置location 优先级关系： (location = ) &gt; (location /xxx/yyy/zzz) &gt; (location ^~) &gt; (location ,*) &gt; (location /起始路径) &gt; (location /) 注释掉之前的location 模块，添加如下模块，让其跳转到百度首页 123location / &#123; proxy_pass https://www.baidu.com; &#125; 其他配置 12345678910111213141516171819# 1. = 匹配location = / &#123; # 精准匹配，主机名后面不能带任何的字符串&#125;# 2. 通用匹配location /xxx&#123; # 匹配所有以/xxx开头的路径 &#125;# 3. 正则匹配location ~/xxx &#123; # 匹配所有以/xxx开头的路径&#125;# 4. 匹配开头路径location ^~/images/ &#123; # 匹配所有以 /images 开头的路径&#125;# 5. ~* \\.(gif|jpg|png)$ &#123; # 匹配以gif或者jpg或者png为结尾的路径&#125; 负载均衡 轮询（默认） 权重（weight=2，默认为1） ip_hash（按照客户端IP地址的分配方式，可以确保相同客户端的请求一直发送到相同的服务器。配置了，其他配置都没效果了） 123456789101112131415 //配置要映射的服务器 upstream my_upstream &#123; ip_hash; server 192.168.90.01:8081 weight=2;; server 192.168.90.02:8082 weight=3; &#125;server &#123; listen 8080; server_name localhost; location / &#123; proxy_pass http://my_upstream; &#125; &#125; Nginx 常见命令 启动 /usr/sbin/nginx nginx 停止 nginx -s stop 查看nginx进程 ps -ef | grep nginx 重新加载配置文件 nginx -s reload 检配置文件是否正确 nginx -t 查看nginx 版本 nginx -v","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"}]},{"title":"Redis问题","slug":"Redis问题","date":"2021-04-21T09:22:44.000Z","updated":"2022-11-22T01:27:53.447Z","comments":true,"path":"2021/04/21/Redis问题/","link":"","permalink":"http://yoursite.com/2021/04/21/Redis%E9%97%AE%E9%A2%98/","excerpt":"","text":"数据服务使用Redis 做元数据信息的存储库，Redis 所在Linux服务器宕机重启后 Redis 中所有数据丢失，造成数据服务完全不能使用问题。 原因: 部署文档给出的 Redis 采用的持久化方式即默认的持久化方式 - RDB快照方式(某个时间点的一次全量数据备份), 并且采用的异步回写策略即BGSAVE命令，主进程fork后复制自身并通过这个新的进程回写磁盘，回写结束后新进程自行关闭。fork一个新的进程之后等于复制了当时的一个内存镜像，需要向机器申请内存资源，如果当时内存镜像比较大服务器会直接报内存无法分配无法持久化，需要修改内存申请策略。 ​ Linux内核会根据参数vm.overcommit_memory参数的设置决定是否放行。 ​ vm.overcommit_memory = 1，直接放行 ​ vm.overcommit_memory = 0：则比较此次请求分配的虚拟内存大小和系统当前空闲的物理内存加上swap，决定是否放行，默认策略。 ​ vm.overcommit_memory= 2：则会比较进程所有已分配的虚拟内存加上此次请求分配的虚拟内存和系统当前的空闲物理内存加上swap，决定是否放行。 解决方案: 12345678910111213141516171819202122232425262728293031323334 在Linux系统设置一个参数（vm.overcommit_memory）可解决。步骤如下： 1、编辑 sysctl.conf 配置文件 sudo vi /etc/sysctl.conf 2、另起一行增加参数 vm.overcommit_memory 配置，如下 vm.overcommit_memory = 1 3、使配置文件生效 sudo sysctl -p 此外，建议线上环境同时配置Redis的AOF持久化方式，AOF默认是关闭的，通过redis.conf配置文件进行开启，配置参考如下:## 此选项为aof功能的开关，默认为“no”，可以通过“yes”来开启aof功能 ## 只有在“yes”下，aof重写/文件同步等特性才会生效 appendonly yes ## 指定aof文件名称 appendfilename appendonly.aof ## 指定aof操作中文件同步策略，有三个合法值：always everysec no,默认为everysec appendfsync everysec ## 在aof-rewrite期间，appendfsync是否暂缓文件同步，&quot;no&quot;表示“不暂缓”，“yes”表示“暂缓”，默认为“no” no-appendfsync-on-rewrite no ## aof文件rewrite触发的最小文件尺寸(mb,gb),只有大于此aof文件大于此尺寸是才会触发rewrite，默认“64mb”，建议“512mb” auto-aof-rewrite-min-size 64mb ## 相对于“上一次”rewrite，本次rewrite触发时aof文件应该增长的百分比 ## 每一次rewrite之后，redis都会记录下此时“新aof”文件的大小(例如A)## aof文件增长到A*(1 + p)之后，触发下一次rewrite，每一次aof记录的添加，都会检测当前aof文件的尺寸。 auto-aof-rewrite-percentage 100Tips: 目前已经有项目和内研环境中招，接手过来还没有发现便捷、可用的修复方案，麻烦所有PM 和运维沟通尽快检查redis配置，避免出现问题造成数据服务无法使用。参考文档: https://www.cnblogs.com/tengjian/p/8991242.html https://zhuanlan.zhihu.com/p/77646963 https://blog.csdn.net/zhanglu1236789/article/details/56485213 安装 yum -y install redis systemctl start redis 远程连接 123456789[deploy@localhost src]$ whereis redisredis: /etc/redis.confsudo vim /etc/redis.conf 将bind 127.0.0.1注释掉 增加一行bind 0.0.0.0 //实现远程访问 默认为保护模式，把 protected-mode yes 改为 protected-mode no 默认为不守护进程模式，把daemonize no 改为daemonize yes 将 requirepass foobared前的“#”去掉，密码改为你想要设置的密码 如requirepass 123456，这样配置文件就修改好了","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"docker容器互联","slug":"docker容器互联","date":"2021-04-19T07:48:31.000Z","updated":"2022-11-22T01:27:53.559Z","comments":true,"path":"2021/04/19/docker容器互联/","link":"","permalink":"http://yoursite.com/2021/04/19/docker%E5%AE%B9%E5%99%A8%E4%BA%92%E8%81%94/","excerpt":"","text":"123docker run -d --name db training/postgres# --link name:alias，其中 name是要链接的容器的名称，alias 是别名 。docker run -d -P --name web --link db:db training/webapp python app.py 随着 Docker 网络的完善，强烈建议大家将容器加入自定义的 Docker 网络来连接多个容器，而不是使用 --link 参数。 1、新建网络 1docker network create -d bridge my-net -d 参数指定 Docker 网络类型，有 bridge overlay。其中 overlay 网络类型用于Swarm 2、连接容器 123456789101112131415# 每个都是在新终端docker run -it --rm --name busybox1 --network my-net busybox shdocker run -it --rm --name busybox2 --network my-net busybox shdocker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf92b313801a9 busybox &quot;sh&quot; 16 seconds ago Up 15 seconds busybox2ba68acad51e5 busybox &quot;sh&quot; 36 seconds ago Up 36 seconds busybox1/ # ping busybox1PING busybox1 (172.22.0.2): 56 data bytes64 bytes from 172.22.0.2: seq=0 ttl=64 time=0.137 ms64 bytes from 172.22.0.2: seq=1 ttl=64 time=0.179 ms64 bytes from 172.22.0.2: seq=2 ttl=64 time=0.093 ms","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"docker网络","slug":"docker网络","date":"2021-04-19T07:48:16.000Z","updated":"2022-11-22T01:27:53.568Z","comments":true,"path":"2021/04/19/docker网络/","link":"","permalink":"http://yoursite.com/2021/04/19/docker%E7%BD%91%E7%BB%9C/","excerpt":"","text":"","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"docker-compose","slug":"docker-compose","date":"2021-04-19T07:47:45.000Z","updated":"2022-11-22T01:27:53.554Z","comments":true,"path":"2021/04/19/docker-compose/","link":"","permalink":"http://yoursite.com/2021/04/19/docker-compose/","excerpt":"","text":"我们知道使用一个 Dockerfile 模板文件，可以让用户很方便的定义一个单独的应用容器。然而，在日常工作中，经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 Web 项目，除了 Web 服务容器本身，往往还需要再加上后端的数据库服务容器，甚至还包括负载均衡容器等。Compose 定位是 「定义和运行多个 Docker 容器的应用（Defining and running multi-container Docker applications）」，其前身是开源项目 Fig。项目由 Python 编写！目前 Docker 官方用 GO 语言 重写 了 Docker Compose，并将其作为了 docker cli 的子命令，称为 Compose V2。你可以参照官方文档安装，然后将熟悉的 docker-compose 命令替换为 docker compose，即可使用 Docker Compose。 1、安装 官方地址：https://docs.docker.com/compose/install/ 从github下载最新版docker-compose 1sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.23.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose 给docker-compose赋予可执行权限 1sudo chmod +x /usr/local/bin/docker-compose 验证 1docker-compose --version 1docker run -d -p 3306:3306 --privileged&#x3D;true -v &#x2F;han&#x2F;mysql&#x2F;log:&#x2F;var&#x2F;log&#x2F;mysql -v &#x2F;han&#x2F;mysql&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql -v &#x2F;han&#x2F;mysql&#x2F;conf:&#x2F;etc&#x2F;mysql&#x2F;conf.d -e MYSQL_ROOT_PASSWORD&#x3D;123456 --name mysql mysql:5.7 2、配置文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091version: &quot;3&quot;services: redis: image: redis:alpine ports: - &quot;6379&quot; networks: - frontend deploy: replicas: 2 update_config: parallelism: 2 delay: 10s restart_policy: condition: on-failure db: image: postgres:9.4 volumes: - db-data:/var/lib/postgresql/data networks: - backend deploy: placement: constraints: [node.role == manager] vote: image: dockersamples/examplevotingapp_vote:before ports: - &quot;5000:80&quot; networks: - frontend depends_on: - redis deploy: replicas: 2 update_config: parallelism: 2 restart_policy: condition: on-failure result: image: dockersamples/examplevotingapp_result:before ports: - &quot;5001:80&quot; networks: - backend depends_on: - db deploy: replicas: 1 update_config: parallelism: 2 delay: 10s restart_policy: condition: on-failure worker: image: dockersamples/examplevotingapp_worker networks: - frontend - backend deploy: mode: replicated replicas: 1 labels: [APP=VOTING] restart_policy: condition: on-failure delay: 10s max_attempts: 3 window: 120s placement: constraints: [node.role == manager] visualizer: image: dockersamples/visualizer:stable ports: - &quot;8080:8080&quot; stop_grace_period: 1m30s volumes: - &quot;/var/run/docker.sock:/var/run/docker.sock&quot; deploy: placement: constraints: [node.role == manager]networks: frontend: backend:volumes: db-data: 整个文档分为4部分，version, services, networks, volumes, 其中services中内容最长。 version compose版本与docker版本对照表: Compose Docker 3.8 19.03.0+ 3.7 18.06.0+ 3.6 18.02.0+ 3.5 17.12.0+ 3.4 17.09.0+ 3.3 17.06.0+ 3.2 17.04.0+ 3.1 1.13.1+ 3.0 1.13.0+ 2.4 17.12.0+ 2.3 17.06.0+ 2.2 1.13.0+ 2.1 1.12.0+ 2.0 1.10.0+ 常见命令 12docker-compose up -d · # 不写服务名，默认启动docker-compose.yml所有服务docker-compose up -d 服务名 # 启动docker-compose.yml的对应服务 重启策略 如：--restart=always no，默认策略，在容器退出时不重启容器 on-failure，在容器非正常退出时（退出状态非0），才会重启容器 on-failure:3，在容器非正常退出时重启容器，最多重启3次 always，在容器退出时总是重启容器 unless-stopped，在容器退出时总是重启容器，但是不考虑在Docker守护进程启动时就已经停止了的容器（docker进程重启后，不重启services）。","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"docker数据卷","slug":"docker数据卷","date":"2021-04-19T07:45:51.000Z","updated":"2022-11-22T01:27:53.561Z","comments":true,"path":"2021/04/19/docker数据卷/","link":"","permalink":"http://yoursite.com/2021/04/19/docker%E6%95%B0%E6%8D%AE%E5%8D%B7/","excerpt":"","text":"数据卷 Data Volumes 容器内数据直接映射到本地主机环境 数据卷容器（Data Volume Containers 使用特定容器维护数据卷 最原始的方法，说白了就是复制，宿主机和容器内容不交互的 1、docker cp 1docker cp /ht/docker/ localPath:containerPath 2、数据卷 数据卷（data volume，注：位置在/var/lib/docker/volumes）是容器可以访问，每个容器都有固定的运行目录在/var/lib/docker目录下；而每个容器卷的数据则默认单独存储在/var/lib/docker/volumes/目录底下。 docker run命令的-v选项能够实现容器间数据卷中数据的互相拷贝。 ​ 数据卷是被设计用来持久化数据的，它的生命周期独立于容器，Docker不会在容器被删除后自动删除数据卷，并且也不存在垃圾回收这样的机制来处理没有任何容器引用的数据卷，无主的数据卷可能会占据很多空间， 所以要及时删除。 新建 12# 数据卷默认会放到/var/lib/docker/volumes路径下，会发现所新建的数据卷位置，查看命令: ls -1 /var/lib/docker/volumesdocker volume create my-vol 查看 1docker volume ls 查看指定数据卷 1docker volume inspect my-vol 删除一个volume 1docker volume rm my-vol 启动一个容器（挂载数据卷） 123456789101112131415161718# source=my-vol(可以不存在),target=/webapp。都可以不存在，自动创建docker run -d \\ -it \\ --name webapp1 \\ --mount [type=volume] source=my-vol,target=/webapp \\ centos:7# 绑定数据卷，映射到主机指定路径下 docker run -d \\ -it \\ --name webapp2 \\ --mount type=bind,source=localPath,target=containerpath \\ centos:7# 临时数据卷，只存在于内存中 docker run -d \\ -it \\ --name webapp3 \\ --mount type=tmpfs,arget=containerpath \\ centos:7 3、数据卷容器 场景：多个容器之间共享一些持续更新的数据。 新建 1docker run -it --name db_data -v &#x2F;db_data centos:7 使用 12docker run -it --name db1 --volumes-from db_data centos:7docker run -it --name db2 --volumes-from db_data centos:7","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"docker命令","slug":"docker命令","date":"2021-04-19T07:03:22.000Z","updated":"2022-11-22T01:27:53.557Z","comments":true,"path":"2021/04/19/docker命令/","link":"","permalink":"http://yoursite.com/2021/04/19/docker%E5%91%BD%E4%BB%A4/","excerpt":"","text":"1、启动停止 启动守护进程 1systemctl daemon-reload 启动docker 1systemctl start docker 查看docker是否启动 1ps -ef |grep docker 停止docker 1systemctl stop docker 重启docker 1systemctl restart docker 2、帮助命令 1docker 命令 --help 3、镜像命令 查看镜像 123docker images-a, --all #列出所有镜像-q, --quiet #只显示镜像的id 搜索下载镜像 12docker search 镜像docker pull 镜像名[:tag] 删除镜像 123docker rmi -f 容器id #删除指定的容器docker rmi -f 容器id 容器id 容器id #删除多个容器docker rmi -f $(docker images -aq) #删除全部的容器 4、容器命令 创建并启动容器 123456789101112docker run [可选参数] image#参数说明--name=&quot;Name&quot; 容器名字-d 后台方式运行-rm 用完即删-it 使用交互模式运行，进入容器查看内容-v 挂载数据卷，-v 容器外目录:容器内目录-P 随机指定端口（大写）-p 指定容器的端口 -p 8080:8080 -p ip:主机端口：容器端口 -p 主机端口：容器端口 （常用） -p 容器端口 从容器中返回主机 12exitCtrl+p+q 列举容器 1234docker ps #列出正在运行的容器docker ps -a #列出正在运行的容器+历史运行过的容器docker ps -n=？ #显示最近创建的容器 例如：ocker ps -n=1docker ps -q #只显示容器编号 删除容器 1234docker rm 容器id #删除指定的容器，不能删除正在运行的容器docker rm -f 容器id #删除指定的容器，能删除正在运行的容器docker rm -f $(docker ps -aq) #删除所有容器for i in `docker ps -a|grep -i exit|awk &#x27;&#123;print $1&#125;&#x27;`;do docker rm -f $i;done #过滤出全部已经退出的容器并删掉 启动停止容器 1234docker start 容器id #启动容器docker restart 容器id #重启容器docker stop 容器id #停止容器docker kill 容器id #强制停止容器 重新命名容器 1docker rename oldname newname 运行临时容器 1docker run -it --rm image-id 运行容器 1docker run [-it] [-p hostport:containerPort | P] [-d] image-id [/bin/bash | sh ...] 提交容器为镜像 123456789101112# 步骤概括docker login --username=[用户名] -p=[密码] 远程ip:端口docker tag [镜像id] [远程ip:端口/自定义路径/*]:[版本号]docker push [远程ip:端口/自定义路径/*]:[版本号]# 实践docker commit -m=&quot;提交的描述信息&quot; -a=&quot;作者&quot; 容器id 镜像名:镜像版本docker save 镜像名:镜像版本 -o xxx.tardocker login# dockerhub的用户名xxxx=huangfan322docker tag 镜像名:镜像版本 huangfan322/镜像名:镜像版本docker push huangfan322/镜像名:镜像版本 导出/导入镜像 123456789docker export 容器id -o xx.tarcat xx.tar | docker import - image_name:tagdocker save 镜像id -o xx.tardocker load -i ./xx.tar# 注意载export后镜像运行需要载结尾加上原镜像的command，具体查看（完整信息）：docker ps --no-trunc# docker save保存的是镜像（image）,docker export保存的是容器（container)；# docker load用来载入镜像包，docker import用来载入容器包，但两者都会恢复为镜像；# docker load不能对载入的镜像重命名，而docker import可以为镜像指定新名称。# docker export的包会比save的包要小，原因是save的是一个分层的文件系统，export导出的只是一个linux系统的文件目录。 Docker-compose导出所用镜像 1docker-compose images| awk &#x27;FNR &gt; 2 &#123;print $2&quot;:&quot;$3&#125;&#x27;| sort -u|xargs docker save -o xxxx.tar 5、其他命令 日志 123docker logs [选项] 容器id -tf #显示日志，带时间戳--tail num #显示日志数量 查看容器中进程 1docker top 容器id 查看元数据 1docker inspect 容器id 进入容器 12docker exec -it 容器id /bin/bash #进入容器开启一个新的终端，可以在里面操作（常用）docker attach 容器id #进入容器正在执行的终端，不会启动新的进程 文件拷贝 12docker cp 容器id:容器内路径 主机路径 # 容器--&gt;主机docker cp 主机路径 容器id:容器内路径 # 主机--&gt;容器 加入固定network，使用固定ip 1docker run -it --net camera_backend --ip 172.18.0.18 --name camera_server -d bd95d1afac84 查看容器操作系统 1cat /etc/issue 容器网络 1234567891011121314151617181920212223#将容器连接到网络docker network connect#创建一个网络docker network create#断开容器的网络docker network disconnect#列出网络docker network ls#删除所有未使用的网络docker network prune#删除一个或多个网络docker network rm#显示一个或多个网络的详细信息docker network inspect#查看当前network下有哪些容器docker network inspect bridge#创建networkdocker network create --driver bridge --subnet 172.18.0.0/16 --gateway 172.18.0.1 mynet--driver bridge 网络模式为 桥接模式--subnet 172.18.0.0/16 设置子网--gateway 172.18.0.1 设置网关--mynet 自定义的network名","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"docker安装","slug":"docker安装","date":"2021-04-19T06:56:35.000Z","updated":"2023-05-31T02:29:13.357Z","comments":true,"path":"2021/04/19/docker安装/","link":"","permalink":"http://yoursite.com/2021/04/19/docker%E5%AE%89%E8%A3%85/","excerpt":"","text":"1、centos 123456789101112131415161718$ sudo yum install yum-utils device-mapper-persistent-data lvm2$ sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo# 查看版本yum list docker-ce --showduplicates | sort -rdocker-ce.x86_64 3:20.10.9-3.el7 docker-ce-stabledocker-ce.x86_64 3:20.10.9-3.el7 docker-cedocker-ce.x86_64 3:20.10.8-3.el7 docker-ce-stabledocker-ce.x86_64 3:20.10.8-3.el7 docker-cedocker-ce.x86_64 3:20.10.7-3.el7 docker-ce-stabledocker-ce.x86_64 3:20.10.7-3.el7 docker-cedocker-ce.x86_64 3:20.10.6-3.el7 docker-ce-stable...# 例如安装20.10.9-3.el7 （第二列:过后版本号）$ sudo yum install docker-ce-20.10.9-3.el7 docker-ce-cli-20.10.9-3.el7 containerd.io或者$ sudo yum install docker-ce docker-ce-cli containerd.io$ sudo systemctl enable docker$ sudo systemctl start docker 2、Debian 1234567$ sudo apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common$ curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -$ sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable&quot;$ sudo apt-get update$ sudo apt-get install docker-ce$ sudo systemctl enable docker$ sudo systemctl start docker 3、Fedora 12345$ sudo dnf -y install dnf-plugins-core$ sudo dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo$ sudo dnf install docker-ce$ sudo systemctl enable docker$ sudo systemctl start docker 4、Ubuntu 1234567$ sudo apt-get install apt-transport-https ca-certificates curl software-properties-common$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -$ sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot;$ sudo apt-get update$ sudo apt-get install docker-ce$ sudo systemctl enable docker$ sudo systemctl start docker 5、配置镜像地址 在 Linux 环境下，我们可以通过修改 /etc/docker/daemon.json ( 如果文件不存在，你可以直接创建它 ) 这个 Docker 服务的配置文件达到效果。 123456&#123; &quot;registry-mirrors&quot;: [ &quot;https://registry.docker-cn.com&quot; ]&#125; 重启生效 1$ sudo systemctl restart docker 验证是否生效 123456$ sudo docker info$ sudo docker info## ......Registry Mirrors: https://registry.docker-cn.com/## ......","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]},{"title":"zk","slug":"zk","date":"2021-04-16T07:12:00.000Z","updated":"2022-11-22T01:27:53.856Z","comments":true,"path":"2021/04/16/zk/","link":"","permalink":"http://yoursite.com/2021/04/16/zk/","excerpt":"","text":"1docker run -d --name zookeeper -v /opt/zookeeper/data:/opt/zookeeper/data -p 2181:2181 --restart always zookeeper:3.5.8 123wget https:&#x2F;&#x2F;dlcdn.apache.org&#x2F;zookeeper&#x2F;zookeeper-3.5.9&#x2F;apache-zookeeper-3.5.9-bin.tar.gztar -zxvf apache-zookeeper-3.5.9-bin.tar.gz 1、Zk命令 一、zk服务命令 启动ZK服务: bin/zkServer.sh start 查看ZK服务状态: bin/zkServer.sh status 停止ZK服务: bin/zkServer.sh stop 重启ZK服务: bin/zkServer.sh restart 连接服务器: bin/zkCli.sh -server 127.0.0.1:2181 连接到服务器后查看版本：version 二、zk客户端命令 帮助：help 退出客户端： quit 创建顺序节点：create -s /zk-test 123 创建临时节点：create -e /zk-temp 123 创建永久节点： create /zk-permanent 123 其他节点命令 1234567891011121314// 节点下的所有子节点ls [-w] path // 详细点较lsls2 path ---&gt; ls -s [-w] path (新版本)// 节点数据get [-w] path // 节点状态stat [-w] path// 设置/更新节点值set /zk-permanent 456// 删除节点delete path// 级联删除节点rmr path ---&gt; deleteall path (新版本) znode节点的状态信息 12345678910czxid. 节点创建时的zxid.mzxid. 节点最新一次更新发生时的zxid.ctime. 节点创建时的时间戳.mtime. 节点最新一次更新发生时的时间戳.dataVersion. 节点数据的更新次数.cversion. 其子节点的更新次数.aclVersion. 节点ACL(授权信息)的更新次数.ephemeralOwner. 如果该节点为ephemeral节点, ephemeralOwner值表示与该节点绑定的session id. 如果该节点不是ephemeral节点, ephemeralOwner值为0（持久节点）dataLength. 节点数据的字节数.numChildren. 子节点个数. 2、watch 机制 我是用的zk版本是3.7.x；【子】节点新建和重新设置watch不显示写出来。 2.1 zk命令行watch stat -w path 节点数据变更、节点删除2个watch事件。 1234567# client-1stat -w /nodeWatchedEvent state:SyncConnected type:NodeDataChanged path:/nodeWatchedEvent state:SyncConnected type:NodeDeleted path:/node# client-2set /node 123deleteall /node ls -w path 节点子目录变更、节点删除2个watch事件。 123456789# client-1ls -w /nodeWatchedEvent state:SyncConnected type:NodeChildrenChanged path:/nodeWatchedEvent state:SyncConnected type:NodeChildrenChanged path:/nodeWatchedEvent state:SyncConnected type:NodeDeleted path:/node# client-2create /node/childdeleteall /nodedelete /node get -w path 节点数据变更1个watch事件。 12345# client-1get -w /nodeWatchedEvent state:SyncConnected type:NodeDataChanged path:/node# client-2set /node 123 addWatch -m mode path 这个命令很厉害！本事watch试一次性的，它可以持续watch。mode 可选择有：PERSISTENT, PERSISTENT_RECURSIVE 1234567891011121314151617181920212223242526272829303132333435# 1、PERSISTENT，当前监听的节点有变化# client-1addWatch -m PERSISTENT /nodeWatchedEvent state:SyncConnected type:NodeCreated path:/nodeWatchedEvent state:SyncConnected type:NodeDataChanged path:/nodeWatchedEvent state:SyncConnected type:NodeChildrenChanged path:/nodeWatchedEvent state:SyncConnected type:NodeChildrenChanged path:/nodeWatchedEvent state:SyncConnected type:NodeDeleted path:/node# client-2create /nodeset /node 123create /node/childdelete /node/childdelete /node# 2、PERSISTENT_RECURSIVE，当前节点和子节点有变化# client-1addWatch -m PERSISTENT_RECURSIVE /nodeWatchedEvent state:SyncConnected type:NodeCreated path:/nodeWatchedEvent state:SyncConnected type:NodeCreated path:/node/child1WatchedEvent state:SyncConnected type:NodeCreated path:/node/child1/child2WatchedEvent state:SyncConnected type:NodeDataChanged path:/node/child1WatchedEvent state:SyncConnected type:NodeDataChanged path:/node/child1/child2WatchedEvent state:SyncConnected type:NodeDeleted path:/node/child1/child2WatchedEvent state:SyncConnected type:NodeDeleted path:/node/child1WatchedEvent state:SyncConnected type:NodeDeleted path:/node# client-2create /nodecreate /node/child1create /node/child1/child2set /node/child1 123set /node/child1/child2 123delete /node/child1/child2delete /node/child1delete /node **总结：**zk的watch机制在3.6.x之前是一次性的。之后有了永久性的递归监视（addWatch -m mode path）。在watch中，ls命令专注于节点目录本身的变更，get命令专注节点本身的数据变更。stat可以看着2这个集成，但是它又没有ls的监听目录下有新增目录情况。 watch事件类型有： 1234节点被新建：NodeCreated节点被删除：NodeDeleted节点数据新增/变更：NodeDataChanged节点子节点变更NodeChildrenChanged 2.2 Java客户端watch 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.13&lt;/version&gt; &lt;/dependency&gt; 1234567891011121314151617181920212223242526272829@Data@Slf4j@Configurationpublic class ZookeeperConfig &#123; private static final int TIMEOUT = 50000; @Value(&quot;$&#123;spring.cloud.zookeeper.connect-string&#125;&quot;) private String zkAddress; @Bean(name = &quot;zkClient&quot;) public ZooKeeper zkClient() &#123; ZooKeeper zooKeeper = null; try &#123; final CountDownLatch countDownLatch = new CountDownLatch(1); zooKeeper = new ZooKeeper(zkAddress, TIMEOUT, event -&gt; &#123; // 如果收到了服务端的响应事件，说明连接成功 if (Watcher.Event.KeeperState.SyncConnected == event.getState()) &#123; countDownLatch.countDown(); &#125; &#125;); countDownLatch.await(); log.info(&quot; 初始化ZooKeeper连接成功: &#123;&#125;&quot;, zooKeeper.getState()); &#125; catch (Exception e) &#123; log.error(&quot; 初始化Zookeeper连接状态异常: &#123;&#125;&quot;, e.getMessage()); &#125; return zooKeeper; &#125;&#125; 12345678910111213141516171819202122@Test public void watch() throws Exception &#123; // sets data on the node, or deletes the node zkClient.getData(&quot;/iuac-gateway&quot;, event -&gt; &#123; System.out.println(&quot;1:&quot; + event.getType()); // 需要的话再watch &#125;, new Stat()); // deletes the node of the given path or creates/delete a child under the node. zkClient.getChildren(&quot;/iuac-gateway&quot;, event -&gt; &#123; System.out.println(&quot;2:&quot; + event.getType()); &#125;); //creates/delete the node or sets the data on the node. zkClient.exists(&quot;/iuac-gateway&quot;, event -&gt; &#123; System.out.println(&quot;3:&quot; + event.getType()); &#125;); // 让方法循环等待 while (true) &#123; &#125; &#125;","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"maven默认内置属性","slug":"maven默认内置属性","date":"2021-03-25T09:42:06.000Z","updated":"2022-11-22T01:27:53.697Z","comments":true,"path":"2021/03/25/maven默认内置属性/","link":"","permalink":"http://yoursite.com/2021/03/25/maven%E9%BB%98%E8%AE%A4%E5%86%85%E7%BD%AE%E5%B1%9E%E6%80%A7/","excerpt":"","text":"Maven 服务的内置属性有六个类别，分别是内置属性、POM 属性、自定义属性、Settings 属性、Java 系统属性和环境属性。 一、内置属性 Maven 服务的内置属性，Maven预定义,用户可以直接使用。 变量 含义 ${basedir} 表示项目根目录，即包含pom.xml文件的目录 ${version} 表示项目版本 ${project.basedir} 同${basedir} ${project.baseUri} 表示项目文件地址 ${maven.build.timestamp} 表示项目构件开始时间 ${maven.build.timestamp.format} 表示属性${maven.build.timestamp}的展示格式,默认值为yyyyMMdd-HHmm 二、POM 属性 使用pom属性可以引用到pom.xml文件对应元素的值。例如${project.artifactId}对应了元素的值。 常用的POM属性包括： 变量 含义 ${project.build.sourceDirectory} 项目的主源码目录，默认为src/main/java/ ${project.build.testSourceDirectory} 项目的测试源码目录，默认为/src/test/java/ ${project.build.directory} 项目构建输出目录，默认为target/ ${project.build.outputDirectory} 项目主代码编译输出目录，默认为target/classes/ ${project.build.testOutputDirectory} 项目测试代码编译输出目录，默认为target/testclasses/ ${project.groupId} 项目的groupId.${project.artifactId}:项目的artifactId ${project.version} 项目的version,于${version}等价 ${project.build.finalName} 项目打包输出文件的名称，默认为project.artifactId{project.artifactId}project.artifactId{project.version} ${project.packaging} 打包类型，缺省为jar 三、自定义属性 在pom.xml文件的标签下定义的Maven属性。 12345&lt;project&gt; &lt;properties&gt; &lt;mydiy.prop&gt;hello&lt;/mydiy.prop&gt; &lt;/properties&gt;&lt;/projec 四、Settings属性 与pom属性同理,用户使用以settings.开头的属性引用settings.xml文件中的XML元素值。 12// 表示本地仓库的地址$&#123;settings.localRepository&#125; 五、Java系统属性 所有Java系统属性都可以使用Maven属性引用，可以通过命令行mvn help:system查看所有的Java系统属性。 例如${user.home}指向了用户目录。 六、环境变量属性 所有环境变量都可以使用以env.开头的Maven属性引用。可以通过命令行mvn help:system查看所有环境变量。 例如${env.JAVA_HOME}指代了JAVA_HOME环境变量的值。","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"maven","slug":"maven","permalink":"http://yoursite.com/tags/maven/"}]},{"title":"maven","slug":"maven","date":"2021-03-25T02:14:55.000Z","updated":"2022-11-30T10:59:14.778Z","comments":true,"path":"2021/03/25/maven/","link":"","permalink":"http://yoursite.com/2021/03/25/maven/","excerpt":"","text":"1. 概述 Maven 里对于构建过程抽象3个生命周期，每个周期包含很多阶段，但是这只是一种抽象，具体的实现是由插件完成，更准备的说是插件的目标完成。所以他们之间关系 生命周期:阶段—&gt; 插件:目标 在Maven里使用的命令格式：mvn 插件 : 目标，在maven中若干常用的阶段都会被绑定到插件的目标上，因此我们可以通过 mvn 阶段 运行命令。 2. 三套生命周期 Maven总共拥有三套相互独立的生命周期，分别为clean、default、site。每个生命周期包含一些阶段（phase），这些阶段是有顺序的，并且后面的阶段依赖于前面的阶段。 2.1 clean 生命周期，分为3个阶段 阶段 描述 pre-clean 执行一些需要在clean之前完成的工作 clean 移除所有上一次构建生成的文件 post-clean 执行一些需要在clean之后立刻完成的工作 2.2 default生命周期，分为23个阶段 阶段 描述 validate 验证项目结构是否正常，必要的配置文件是否存在 initialize 做构建前的初始化操作，比如初始化参数、创建必要的目录等 generate-sources 产生在编译过程中需要的源代码 process-sources 处理源代码，比如过滤值 generate-resources 产生主代码中的资源在 classpath 中的包 process-resources 将资源文件复制到 classpath 的对应包中 compile 编译项目中的源代码 process-classes 产生编译过程中生成的文件 generate-test-sources 产生编译过程中测试相关的代码 process-test-sources 处理测试代码 generate-test-resources 产生测试中资源在 classpath 中的包 process-test-resources 将测试资源复制到 classpath 中 test-compile 编译测试源码放入测试目标文件夹中 process-test-classes 后处理测试编译生成的文件，例如：对class文件进行字节码增强，对Maven 2.0.5及以上有效 test 运行测试案例 prepare-package 执行必要的操作，在真正打包前准备一个包。这通常会产生一个未打包、处理过版本。（Maven 2.1及以上） package 将编译后的 class 和资源打包成压缩文件，比如 rar pre-integration-test 做好集成测试前的准备工作，比如集成环境的参数设置 integration-test 集成测试 post-integration-test 完成集成测试后的收尾工作，比如清理集成环境的值 verify 检测测试后的包是否完好 install 将打包的组件以构件的形式，安装到本地依赖仓库中，以便共享给本地的其他项目 deploy 运行集成和发布环境，将测试后的最终包以构件的方式发布到远程仓库中，方便所有程序员共享 2.3 site 生命周期，分为4个阶段 阶段 描述 pre-site 执行一些需要在生成站点文档之前完成的工作 site 生成项目的站点文档 post-site 执行一些需要在生成站点文档之后完成的工作，并且为部署做准备 site-deploy 将生成的站点文档部署到特定的服务器上 3. 插件和目标 maven 的构建是基于插件的，每个插件包含若干个目标，介绍几个常用插件，其他参考：插件 3.1 maven-resources-plugin 该插件的作用 主要是处理将主程序与测试程序所需的源文件复制到输出编译文件夹中。 默认 会将resources文件夹下的文件全部copy到target的classes里。 指定特定（默认失效了） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// 使用resources标签处理 &lt;build&gt; &lt;!-- 资源目录 --&gt; &lt;resources&gt; &lt;resource&gt; &lt;!-- 设定主资源目录 --&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;excludes&gt; &lt;exclude&gt;**/*.yaml&lt;/exclude&gt; &lt;/excludes&gt; &lt;!-- 指定处理后的资源文件输出目录，默认是$&#123;build.outputDirectory&#125;指定的目录--&gt; &lt;!--&lt;targetPath&gt;$&#123;build.outputDirectory&#125;&lt;/targetPath&gt; --&gt; &lt;!-- 处理主资源目下的资源文件时，是否对主资源目录开启资源过滤 --&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; // 使用maven-resources-plugin 插件处理 &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-resources&lt;/id&gt; &lt;!-- 在default生命周期的 validate阶段就执行resources插件的copy-resources目标 --&gt; &lt;phase&gt;validate&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-resources&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;!-- 指定resources插件处理资源文件到哪个目录下 --&gt; &lt;outputDirectory&gt;$&#123;project.build.outputDirectory&#125;&lt;/outputDirectory&gt; &lt;!-- 也可以用下面这样的方式（指定相对url的方式指定outputDirectory） &lt;outputDirectory&gt;target/classes&lt;/outputDirectory&gt; --&gt; &lt;!-- 待处理的资源定义 --&gt; &lt;resources&gt; &lt;resource&gt; &lt;!-- 指定resources插件处理哪个目录下的资源文件 --&gt; &lt;directory&gt;src/main/$&#123;deploy.env&#125;/applicationContext.xml&lt;/directory&gt; &lt;!-- 指定不需要处理的资源 &lt;excludes&gt;&lt;exclude&gt;WEB-INF/*.*&lt;/exclude&gt;&lt;/excludes&gt; --&gt; &lt;!-- 是否对待处理的资源开启过滤模式 (resources插件的copy-resources目标也有资源过滤的功能，这里配置的 这个功能的效果跟&lt;build&gt;&lt;resources&gt;&lt;resource&gt;下配置的资源过滤是一样的，只不过可能执行的阶段不一样， 这里执行的阶段是插件指定的validate阶段，&lt;build&gt;&lt;resources&gt;&lt;resource&gt;下的配置将是在resources插件的resources目标执行时起作用（在process-resources阶段）) --&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/configuration&gt; &lt;inherited&gt;&lt;/inherited&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; 上传jar到Nexus 前提是setting里面有上传的server账户 12345 &lt;server&gt; &lt;id&gt;release&lt;/id&gt; &lt;username&gt;xxx&lt;/username&gt; &lt;password&gt;yyyyy&lt;/password&gt;&lt;/server&gt; 12345mvn deploy:deploy-file -DgroupId=com.dtwave.asset -DartifactId=gp-jdbc -Dversion=5.1.4 -Dpackaging=jar -Dfile=/Users/hf/Desktop/greeplum-jdbc-5.1.4.jar -Durl=http://repo2.dtwave-inc.com/repository/maven-releases/ -DrepositoryId=releasesmvn deploy:deploy-file -DgroupId=com.dtdream.insight -DartifactId=neo4j-jdbc-driver -Dversion=4.0.5 -Dpackaging=jar -Dfile=./neo4j-jdbc-driver-4.0.5.jar -Durl=http://maven.dtdream.com/content/repositories/releases/ -DrepositoryId=releasesneo4j-jdbc-driver-4.0.5 自动切换mirror 1234567891011121314151617181920212223242526&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;dtwave&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;dtwavemaven&lt;/name&gt; &lt;url&gt;http://repo2.dtwave-inc.com/repository/public/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;aliyun&lt;/id&gt; &lt;mirrorOf&gt;$&#123;aliyun&#125;&lt;/mirrorOf&gt; &lt;name&gt;aliyunmaven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;aliyunApache&lt;/id&gt; &lt;mirrorOf&gt;$&#123;aliyunApache&#125;&lt;/mirrorOf&gt; &lt;name&gt;aliyunApacheMaven&lt;/name&gt; &lt;url&gt;https://maven.aliyun.com/repository/apache-snapshots&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;netease&lt;/id&gt; &lt;url&gt;http://mirrors.163.com/maven/repository/maven-public/&lt;/url&gt; &lt;name&gt;neteaseMaven&lt;/name&gt; &lt;mirrorOf&gt;$&#123;netease&#125;&lt;/mirrorOf&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 使用阿里云作为默认（需要在某个项目下执行，即有pom） 1mvn help:effective-settings -Daliyun=central","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"maven","slug":"maven","permalink":"http://yoursite.com/tags/maven/"}]},{"title":"堡垒机传输文件","slug":"堡垒机传输文件","date":"2021-03-11T04:03:30.000Z","updated":"2022-11-22T01:27:53.894Z","comments":true,"path":"2021/03/11/堡垒机传输文件/","link":"","permalink":"http://yoursite.com/2021/03/11/%E5%A0%A1%E5%9E%92%E6%9C%BA%E4%BC%A0%E8%BE%93%E6%96%87%E4%BB%B6/","excerpt":"","text":"堡垒机传输文件 我使用的macoX系统，所以本次演示也是在mac上，使用homebrew 安装软件。 下载 123456// iterm2brew cask install iterm2// rz/szbrew install lrzsz// linuxyum install lrzsz -y iterm2-recv-zmodem.sh 123456789101112131415#!/bin/bashFILE=`osascript -e &#x27;tell application &quot;iTerm&quot; to activate&#x27; -e &#x27;tell application &quot;iTerm&quot; to set thefile to choose folder with prompt &quot;Choose a folder to place received files in&quot;&#x27; -e &quot;do shell script (\\&quot;echo \\&quot;&amp;(quoted form of POSIX path of thefile as Unicode text)&amp;\\&quot;\\&quot;)&quot;`if [[ $FILE = &quot;&quot; ]]; then echo Cancelled. # Send ZModem cancel echo -e \\\\x18\\\\x18\\\\x18\\\\x18\\\\x18 echo \\# Cancelled transfer echoelse echo $FILE cd &quot;$FILE&quot; /usr/local/bin/rz echo \\# Received $FILE echofi item2-send-zmodem.sh 123456789101112131415161718192021#!/bin/bashosascript -e &#x27;tell application &quot;iTerm2&quot; to version&#x27; &gt; /dev/null 2&gt;&amp;1 &amp;&amp; NAME=iTerm2 || NAME=iTermif [[ $NAME = &quot;iTerm&quot; ]]; then FILE=`osascript -e &#x27;tell application &quot;iTerm&quot; to activate&#x27; -e &#x27;tell application &quot;iTerm&quot; to set thefile to choose file with prompt &quot;Choose a file to send&quot;&#x27; -e &quot;do shell script (\\&quot;echo \\&quot;&amp;(quoted form of POSIX path of thefile as Unicode text)&amp;\\&quot;\\&quot;)&quot;`else FILE=`osascript -e &#x27;tell application &quot;iTerm2&quot; to activate&#x27; -e &#x27;tell application &quot;iTerm2&quot; to set thefile to choose file with prompt &quot;Choose a file to send&quot;&#x27; -e &quot;do shell script (\\&quot;echo \\&quot;&amp;(quoted form of POSIX path of thefile as Unicode text)&amp;\\&quot;\\&quot;)&quot;`fiif [[ $FILE = &quot;&quot; ]]; then echo Cancelled. # Send ZModem cancel echo -e \\\\x18\\\\x18\\\\x18\\\\x18\\\\x18 sleep 1 echo echo \\# Cancelled transferelse /usr/local/bin/sz &quot;$FILE&quot; -e -b sleep 1 echo echo \\# Received $FILEfi 文件 新建好上述2个文件：iterm2-recv-zmodem.sh，item2-send-zmodem.sh ，然后放到/usr/local/bin 目录下，并修改mod为777。 配置 iterm2：Profiles-&gt;Edit Profiles -&gt; Advanced-&gt;Triggers-&gt;Edit 选择你需要配置的Profiles，例如我这里事先配置好了跳板机的Profile 添加2行配置，顺序不能错 1234567Regular expression: \\*\\*B0100Action: Run Silent CoprocessParameters: /usr/local/bin/iterm2-send-zmodem.shRegular expression: \\*\\*B00000000000000Action: Run Silent CoprocessParameters: /usr/local/bin/iterm2-recv-zmodem.sh 连接堡垒机 Iterm2 使用command+o 快捷键调出配置好的Profiles界面，选择你刚配置的堡垒机Profile，效果如下 然后输入堡垒机密码，到选择内网服务器界面，选择你要进去一个。 本地上传 123cd /你要要存放文件的目录// root 用户可以不用sudo, rz 具体命令可以 rz --h 查看sudo rz 后续就会有弹框出来，选择你需要上传的文件，传输过程可能比较慢，耐心等待 下载到本地 12// root 用户可以不用sudo, sz 具体命令可以 sz --h 查看sudo sz /你要要下载文件路径","categories":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/categories/linux/"}],"tags":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"clickhouse实践案例","slug":"clickhouse实践案例","date":"2021-03-09T01:45:16.000Z","updated":"2022-11-22T01:27:53.527Z","comments":true,"path":"2021/03/09/clickhouse实践案例/","link":"","permalink":"http://yoursite.com/2021/03/09/clickhouse%E5%AE%9E%E8%B7%B5%E6%A1%88%E4%BE%8B/","excerpt":"","text":"用户留存 用户登陆表 1234CREATE TABLE IF NOT EXISTS login_log( user_id INT , log_time TIMESTAMP ) engine=MergeTree() order by user_id; 数据 1234567891011121314151617181920212223242526272829303132INSERT INTO login_log VALUES(1101,&#x27;2021-01-21 22:00:00&#x27;),(1101,&#x27;2021-01-20 22:00:00&#x27;),(1101,&#x27;2021-01-19 22:00:00&#x27;),(1101,&#x27;2021-01-17 22:00:00&#x27;),(1101,&#x27;2021-01-16 22:00:00&#x27;),(1101,&#x27;2021-01-21 22:00:00&#x27;),(1101,&#x27;2021-01-21 23:00:00&#x27;),(1101,&#x27;2021-01-20 23:00:00&#x27;),(1101,&#x27;2021-01-19 23:00:00&#x27;),(1101,&#x27;2021-01-17 23:00:00&#x27;),(1101,&#x27;2021-01-16 23:00:00&#x27;),(1101,&#x27;2021-01-21 23:00:00&#x27;),(4101,&#x27;2021-01-20 22:00:00&#x27;),(4101,&#x27;2021-01-19 22:00:00&#x27;),(4101,&#x27;2021-01-17 22:00:00&#x27;),(4101,&#x27;2021-01-16 22:00:00&#x27;),(2201,&#x27;2021-01-16 14:00:00&#x27;),(2201,&#x27;2021-01-15 23:04:00&#x27;),(2201,&#x27;2021-01-21 18:00:00&#x27;),(2201,&#x27;2021-01-20 21:00:00&#x27;),(2201,&#x27;2021-01-21 23:00:00&#x27;),(2201,&#x27;2021-01-20 23:00:00&#x27;),(3301,&#x27;2021-01-21 22:00:00&#x27;),(3301,&#x27;2021-01-19 22:00:00&#x27;),(3301,&#x27;2021-01-18 23:00:00&#x27;),(3301,&#x27;2021-01-17 23:00:00&#x27;),(3301,&#x27;2021-01-16 23:00:00&#x27;),(3301,&#x27;2021-01-15 23:00:00&#x27;); 如需查询用户在2021-01-17至2021-01-21登录流失情况，则SQL语句如下： 12345678910111213141516171819SELECT user_id, retention( date(log_time) = &#x27;2021-01-21&#x27;, date(log_time) = &#x27;2021-01-20&#x27;, date(log_time) = &#x27;2021-01-19&#x27;, date(log_time) = &#x27;2021-01-18&#x27;, date(log_time) = &#x27;2021-01-17&#x27; ) AS rFROM login_log GROUP BY user_idORDER BY user_id ASC;user_id|r |-------|-----------| 1101|[1,1,1,0,1]| 2201|[1,1,0,0,0]| 3301|[1,0,1,1,1]| 4101|[0,0,0,0,0]| 查询留存 12345678910111213141516171819202122232425262728293031323334353637383940SELECT DATE(TIMESTAMP &#x27;2021-01-15 00:00:00&#x27;) AS first_date, SUM(r[1]) AS &quot;第一天活跃用户&quot;, SUM(r[2])/ SUM(r[1]) AS &quot;次日留存&quot;, SUM(r[3])/ SUM(r[1]) AS &quot;3日留存&quot;, SUM(r[4])/ SUM(r[1]) AS &quot;7日留存&quot; FROM -- 计算2021-01-15活跃用户在第2、3、7日的登录情况，1/0 =&gt; 登录/未登录 ( WITH first_day_table AS ( SELECT TIMESTAMP &#x27;2021-01-15 00:00:00&#x27; AS first_day ) SELECT user_id, retention( DATE(log_time) = (SELECT DATE(first_day) FROM first_day_table), DATE(log_time) = (SELECT DATE(first_day + INTERVAL &#x27;1 day&#x27;) FROM first_day_table), DATE(log_time) = (SELECT DATE(first_day + INTERVAL &#x27;2 day&#x27;) FROM first_day_table), DATE(log_time) = (SELECT DATE(first_day + INTERVAL &#x27;6 day&#x27;) FROM first_day_table) ) AS r -- 过滤2021-01-15活跃用户在后续 1～7 日登录数据 FROM login_log WHERE ( log_time &gt;= TIMESTAMP &#x27;2021-01-15 00:00:00&#x27; ) AND ( log_time &lt;= TIMESTAMP &#x27;2021-01-15 00:00:00&#x27; + INTERVAL &#x27;6 day&#x27; ) GROUP BY user_id ) AS basic_table GROUP BY first_date; first_date|第一天活跃用户|次日留存|3日留存|7日留存|----------|-------|----|----|----|2021-01-15| 2| 1.0| 0.5| 0.0| 函数分析 123456语法：retention(cond1, cond2, ..., cond32);// 示例r1: date=2020-01-01（ cond1 条件）。r2: 2020-01-01=&lt;date&lt;=2020-01-02 (cond1 和 cond2 条件）。r3: 2020-01-01=&lt;date&lt;=2020-01-03 (cond1 和 cond3 条件）。retention(date = &#x27;2020-01-01&#x27;, date = &#x27;2020-01-02&#x27;, date = &#x27;2020-01-03&#x27;) as r 行为漏斗转化 用户行为表 12345678CREATE TABLE IF NOT EXISTS user_action( `uid` Int32, `event_type` String, `time` datetime)ENGINE = MergeTree()ORDER BY uid; 数据 123456789101112131415161718192021222324insert into user_action values(1,&#x27;浏览&#x27;,&#x27;2021-01-02 11:00:00&#x27;),(1,&#x27;点击&#x27;,&#x27;2021-01-02 11:10:00&#x27;),(1,&#x27;下单&#x27;,&#x27;2021-01-02 11:20:00&#x27;),(1,&#x27;支付&#x27;,&#x27;2021-01-02 11:30:00&#x27;),(2,&#x27;下单&#x27;,&#x27;2021-01-02 11:00:00&#x27;),(2,&#x27;支付&#x27;,&#x27;2021-01-02 11:10:00&#x27;),(1,&#x27;浏览&#x27;,&#x27;2021-01-02 11:00:00&#x27;),(3,&#x27;浏览&#x27;,&#x27;2021-01-02 11:20:00&#x27;),(3,&#x27;点击&#x27;,&#x27;2021-01-02 12:00:00&#x27;),(4,&#x27;浏览&#x27;,&#x27;2021-01-02 11:50:00&#x27;),(4,&#x27;点击&#x27;,&#x27;2021-01-02 12:00:00&#x27;),(5,&#x27;浏览&#x27;,&#x27;2021-01-02 11:50:00&#x27;),(5,&#x27;点击&#x27;,&#x27;2021-01-02 12:00:00&#x27;),(5,&#x27;下单&#x27;,&#x27;2021-01-02 11:10:00&#x27;),(6,&#x27;浏览&#x27;,&#x27;2021-01-02 11:50:00&#x27;),(6,&#x27;点击&#x27;,&#x27;2021-01-02 12:00:00&#x27;),(6,&#x27;下单&#x27;,&#x27;2021-01-02 12:10:00&#x27;); 30分钟作为一个时间窗口，查看窗口数据 123456789101112131415161718192021SELECT user_id, windowFunnel(1800)(time, event_type = &#x27;浏览&#x27;, event_type = &#x27;点击&#x27;, event_type = &#x27;下单&#x27;, event_type = &#x27;支付&#x27;) AS levelFROM ( SELECT time, event_type, uid AS user_id FROM action)GROUP BY user_iduser_id|level|-------|-----| 4| 2| 3| 1| 2| 0| 5| 2| 1| 4| 6| 3| 分析&quot;2021-01-02&quot;这天 路径为“浏览-&gt;点击-&gt;下单-&gt;支付”的转化情况 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849SELECT level_index,count(1) FROM( SELECT user_id, arrayWithConstant(level, 1) levels, arrayJoin(arrayEnumerate( levels )) level_index FROM ( SELECT user_id, windowFunnel(1800)( time, event_type = &#x27;浏览&#x27;, event_type = &#x27;点击&#x27; , event_type = &#x27;下单&#x27;, event_type = &#x27;支付&#x27; ) AS level FROM ( SELECT time, event_type , uid as user_id FROM user_action WHERE toDate(time) = &#x27;2020-01-02&#x27; ) GROUP BY user_id ))group by level_indexORDER BY level_index// 内部数据user_id|levels |level_index|-------|---------|-----------| 4|[1,1] | 1| 4|[1,1] | 2| 3|1 | 1| 5|[1,1] | 1| 5|[1,1] | 2| 1|[1,1,1,1]| 1| 1|[1,1,1,1]| 2| 1|[1,1,1,1]| 3| 1|[1,1,1,1]| 4| 6|[1,1,1] | 1| 6|[1,1,1] | 2| 6|[1,1,1] | 3|// 结果 level_index|count(1)|-----------|--------| 1| 5| 2| 4| 3| 2| 4| 1| 路径分析 关键路径分析：已经明确了要分析的路径，需要看下这些访问路径上的用户数据 智能路径分析：不确定有哪些路径，但是清楚目标路径是什么，需要知道用户在指定时间范围内都是通过哪些途径触达目标路径的 下单超过10分钟才支付 12345678910111213141516171819--明确了要分析的路径SELECT count(1) AS &quot;userCount&quot;, sum(cn) AS &quot;actionCount&quot;FROM ( SELECT uid, sequenceCount(&#x27;(?1)(?t&gt;=600)(?2)&#x27;)(toDateTime(time), event_type = &#x27;下单&#x27;, event_type = &#x27;支付&#x27;) AS cn FROM user_action GROUP BY uid)WHERE cn &gt;= 1--智能路径分析","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"clickHouse","slug":"clickHouse","permalink":"http://yoursite.com/tags/clickHouse/"}]},{"title":"clickhouse集群安装","slug":"clickhouse集群安装","date":"2021-03-04T07:21:17.000Z","updated":"2022-11-22T01:27:53.535Z","comments":true,"path":"2021/03/04/clickhouse集群安装/","link":"","permalink":"http://yoursite.com/2021/03/04/clickhouse%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/","excerpt":"","text":"1 准备 1.1 机器 192.168.90.41 16G 4C 192.168.90.42 16G 4C 1.2 检查cpu指令集 官方预构建的二进制文件通常针对x86_64进行编译，并利用SSE 4.2指令集。 1grep -q sse4_2 /proc/cpuinfo &amp;&amp; echo &quot;SSE 4.2 supported&quot; || echo &quot;SSE 4.2 not supported&quot; 要在不支持SSE 4.2或AArch64，PowerPC64LE架构的处理器上运行ClickHouse，您应该通过适当的配置调整从源代码构建ClickHouse 1.3 关闭防火墙 123sudo systemctl status firewalld.service #查看防火墙状态sudo systemctl stop firewalld.service #关闭防火墙sudo systemctl disable firewalld.service #永久关闭防火墙 1.4 设置时钟同步 123456## 安装sudo yum install -y ntp## 启动定时任务crontab -e## 输入界面键入*/1 * * * * /usr/sbin/ntpdate ntp4.aliyun.com; 1.5 安装JDK 12345下载 jdk-8u281-linux-x64.tar.gzvim ~/.bash_profileexport JAVA_HOME=/opt/jdk_1.8/jdk1.8.0_281export PATH=:$JAVA_HOME/bin:$PATHsource ~/.bash_profile 1.6 安装zookeeper 不说了。。。 2 rpm包安装 2.1 添加依赖和储库 1234567// 依赖sudo yum install -y unixODBC libicudatasudo yum install -y libxml2-devel expat-devel libicu-devel// 储库sudo yum install yum-utilssudo rpm --import https://repo.clickhouse.tech/CLICKHOUSE-KEY.GPGsudo yum-config-manager --add-repo https://repo.clickhouse.tech/rpm/stable/x86_64 2.2 安装 安装后默认会创建一个clickhoue用户 1234sudo yum install clickhouse-server clickhouse-clientcd /etc/clickhouse-serverllconfig.d config.xml users.d users.xml 2.3 修改配置文件 zk配置 在服务器的/etc/clickhouse-server/config.d目录下创建一个名为metrika.xml的配置文件 1234567891011&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;!—ZooKeeper配置，名称自定义，一般就用这个名字就好 --&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;!—节点配置，可以配置多个地址--&gt; &lt;host&gt;127.0.0.1&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt;&lt;/yandex&gt; 接着，在全局配置config.xml中使用&lt;include_from&gt;标签导入刚才定义的配置: 1&lt;include_from&gt;/etc/clickhouse-server/config.d/metrika.xml&lt;/include_from&gt; 并引用ZooKeeper配置的定义 ，incl与metrika.xml配置文件内的节点名称要彼此对应 1&lt;zookeeper incl=&quot;zookeeper-servers&quot; optional=&quot;false&quot; /&gt; 配置日志和数据路径到较大的空间的目录 1234sudo mkdir -p /clickhouse/data sudo mkdir -p /clickhouse/logsudo chown -R clickhouse:clickhouse /clickhousesudo vim /etc/clickhouse-server/config.xml 1234&lt;path&gt;/clickhouse/data&lt;/path&gt;&lt;tmp_path&gt;/clickhouse/data/tmp/&lt;/tmp_path&gt;&lt;log&gt;/clickhouse/log/clickhouse-server.log&lt;/log&gt;&lt;errorlog&gt;/clickhouse/log/clickhouse-server.err.log&lt;/errorlog&gt; 配置集群 123456789101112131415161718192021// 1个分片、1个副本 &lt;yandex&gt; &lt;!-- 之前的zk配置 --&gt; &lt;!-- 自定义集群名称 --&gt; &lt;clickhouse_remote_servers&gt; &lt;cluster1&gt; &lt;!-- 分片 --&gt; &lt;shard&gt; &lt;replica&gt; &lt;!-- 副本 --&gt; &lt;host&gt;ck1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;host&gt;ck2&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/cluster1&gt; &lt;/clickhouse_remote_servers&gt;&lt;/yandex&gt; 接着，在全局配置config.xml中使用&lt;include_from&gt;标签导入刚才定义的配置: 1&lt;remote_servers incl=&quot;clickhouse_remote_servers&quot; &gt;&lt;/remote_servers&gt; 启动 12345678910111213141516// 服务端sudo systemctl start clickhouse-serversudo systemctl status clickhouse-serversudo systemctl stop clickhouse-server// 若失败查看日志sudo journalctl -f// 客户端CREATE TABLE test_1 ON CLUSTER cluster1( id String,price Float64,create_time DateTime) ENGINE = ReplicatedMergeTree(&#x27;/clickhouse/tables/01/test_1&#x27;,&#x27;ck1&#x27;) PARTITION BY toYYYYMM(create_time)ORDER BY id","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"clickHouse","slug":"clickHouse","permalink":"http://yoursite.com/tags/clickHouse/"}]},{"title":"vim","slug":"vim","date":"2021-03-04T03:02:00.000Z","updated":"2022-11-22T01:27:53.845Z","comments":true,"path":"2021/03/04/vim/","link":"","permalink":"http://yoursite.com/2021/03/04/vim/","excerpt":"","text":"Vim VI = Visual editor（可视化编辑器），VIM=VI+improved。 1、模式 正常模式：进入vim默认的模式 插入模式：正常模式下，按i、a、o等 可视模式：正常模式下，按v 可视块模式：正常模式下，按ctr+v 替换模式：正常模式下，按R 2、全部复制 跳到第一行头： gg 可视模式：v 到最后一行：G 复制：y 粘贴：p 补充：VIM与系统剪贴板的复制粘贴，需要： 查看是否支持 -clipboard 不支持，+clipboard 支持 123$ vim --version | grep clipboard-clipboard +iconv +path_extra -toolbar+eval +mouse_dec +startuptime -xterm_clipboard 3、删除以#号开头的行 12:g&#x2F;^#&#x2F;d:%s&#x2F;^#.*\\n 4、删除以空格开头的行 123:g/^\\s/d “\\s代表空格”:%s/^\\s.*\\n 5、删除以多个#开头的注释行 1:g/^\\ .*#/d “(.*)代表若干” 6、删除空白行 1234567:g/^$/d:g/^\\s*$/d “删除由空格组成的行”:g/^[\\s|\\t]*$/d “删除由空格或tab键开头到结尾的行”:g/^[ |\\t]*$/d “删除由空格或tab键开头到结尾的行” 7、删除行首空格 1:%s/^\\s\\+ “\\+代表一个或多个\\s” 8、删除行末空格 1:%s/\\s\\+$ 9、加#注释所有行： 1:%s/^/#/g 10、查找注释掉的CONFIG： 1&#x2F;^#.*CONFIG.*$","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"vim","slug":"vim","permalink":"http://yoursite.com/tags/vim/"}]},{"title":"tmux","slug":"tmux","date":"2021-03-03T08:25:47.000Z","updated":"2022-11-22T01:27:53.836Z","comments":true,"path":"2021/03/03/tmux/","link":"","permalink":"http://yoursite.com/2021/03/03/tmux/","excerpt":"","text":"Tmux 1、简介 都说程序员不关电脑，但是电脑不关，session也会失效。对于后端程序员来说，连接服务器操作太多了，基本上我们会遇到几个问题： session失效，连接中断了，又是一顿操作ssh xx@xxx.xxx.xxx。 需要分屏在不同窗口操作！ 多个连接混乱，怎么“一目了然”？ tmux是在终端中运行的程序，并允许在其内部运行多个其他终端程序。tmux内部的每个程序都有自己的终端，该终端由tmux管理，可以从运行tmux的单个终端访问该终端-这称为多路复用，而tmux是终端多路复用器。实质Tmux 就是会话与窗口的&quot;解绑&quot;工具，将它们彻底分离。 安装 1brew install tmux 2、概念 会话：可以理解成我们服务器的&quot;session&quot;，每次开启tmux都是打开一个会话 1tmux new -s hf 窗口：进入会话里看到的终端界面 窗格：在窗口里分屏出来的窗口 **Tmux 窗口有大量的快捷键。所有快捷键都要通过前缀键唤起。默认的前缀键是Ctrl+b，即先按下Ctrl+b，快捷键才会生效 ** 3、使用 3.1 会话 会话是tmux里面很重要的概念，它是个联系我们窗口的钥匙。下次我们再操作可以拿着它开启之前窗口。 新建 1tmux new -s &lt;session-name&gt; 断开会话 想会话和窗口分离，可以有： 1Ctrl+b d 或者 tmux detach 重连会话 1tmux attach -t &lt;session-name&gt; 枪毙会话 1tmux kill-session &lt;session-name&gt; 切换会话 1tmux switch-session -t &lt;session-name&gt; 重命名会话 1tmux rename-session -t &lt;session-name&gt; &lt;new-session-name&gt; 查看 1tmux ls 或者 tmux list-session 快捷键 123Ctrl+b d：分离当前会话。Ctrl+b s：列出所有会话。Ctrl+b $：重命名当前会话。 3.2 窗口&amp; 窗格 在session里面默认只有一个窗口，我们也可以新建其他（用的比较少），通常是通过创建多个窗格。因为它们命令英文单词都比较长，所以就看看怎么使用快捷键。 12345678910111213141516171819202122// 窗口Ctrl+b c：创建一个新窗口，状态栏会显示多个窗口的信息。Ctrl+b p：切换到上一个窗口（按照状态栏上的顺序）。Ctrl+b n：切换到下一个窗口。Ctrl+b &lt;number&gt;：切换到指定编号的窗口，其中的&lt;number&gt;是状态栏上的窗口编号。Ctrl+b w：从列表中选择窗口。Ctrl+b ,：窗口重命名。// 窗格Ctrl+b %：划分左右两个窗格。Ctrl+b &quot;：划分上下两个窗格。Ctrl+b &lt;arrow key&gt;：光标切换到其他窗格。&lt;arrow key&gt;是指向要切换到的窗格的方向键，比如切换到下方窗格，就按方向键↓。Ctrl+b ;：光标切换到上一个窗格。Ctrl+b o：光标切换到下一个窗格。Ctrl+b &#123;：当前窗格与上一个窗格交换位置。Ctrl+b &#125;：当前窗格与下一个窗格交换位置。Ctrl+b Ctrl+o：所有窗格向前移动一个位置，第一个窗格变成最后一个窗格。Ctrl+b Alt+o：所有窗格向后移动一个位置，最后一个窗格变成第一个窗格。Ctrl+b x：关闭当前窗格。Ctrl+b !：将当前窗格拆分为一个独立窗口。Ctrl+b z：当前窗格全屏显示，再使用一次会变回原来大小。Ctrl+b Ctrl+&lt;arrow key&gt;：按箭头方向调整窗格大小。Ctrl+b q：显示窗格编号。 4、配置 4.1 快捷键配置 上述写的各种快捷键盘，例如：前缀快捷键、分屏快捷键、窗口移动快捷键等，都是可以通过配置文件设置的，文件的位置一般在 1～/.tmux.conf 具体怎么设置就不赘述，自行解决。 4.2 插件 tmux插件很多，这里就演示一个会话保存插件（防止意外关机死机，会话没保存），其他安装方式类似。 插件管理器 1git clone https:&#x2F;&#x2F;github.com&#x2F;tmux-plugins&#x2F;tpm ~&#x2F;.tmux&#x2F;plugins&#x2F;tpm 新增插件 123vim ～&#x2F;.tmux.conf&#x2F;&#x2F; 最后一行新增set -g @plugin &#39;tmux-plugins&#x2F;tmux-resurrect&#39; 安装 1prefix + I 使用 1234// 保存Ctrl+b + Ctrl-s // 恢复Ctrl+b + Ctrl- 窗口同步执行相同命令 123456# 开启Ctr+b 输入“:set synchronize-panes”# 关闭Ctr+b 输入“:set synchronize-panes”","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"tmux","slug":"tmux","permalink":"http://yoursite.com/tags/tmux/"}]},{"title":"clickhouse常用配置参数和错误","slug":"clickhouse常用配置参数和错误","date":"2021-02-23T08:32:22.000Z","updated":"2022-11-22T01:27:53.528Z","comments":true,"path":"2021/02/23/clickhouse常用配置参数和错误/","link":"","permalink":"http://yoursite.com/2021/02/23/clickhouse%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0%E5%92%8C%E9%94%99%E8%AF%AF/","excerpt":"","text":"参考：趣头条基于ClickHouse玩转每天1000亿数据量分享文档。 常用配置参数推荐 max_concurrent_queries 最大并发处理的请求数(包含select,insert等)，默认值100，推荐150(不够再加)，在我们的集群中出现过”max concurrent queries”的问题。 max_bytes_before_external_sort 当order by已使用max_bytes_before_external_sort内存就进行溢写磁盘(基于磁盘排序)，如果不设置该值，那么当内存不够时直接抛错，设置了该值order by可以正常完成，但是速度相对存内存来说肯定要慢点(实测慢的非常多，无法接受)。 background_pool_size 后台线程池的大小，merge线程就是在该线程池中执行，当然该线程池不仅仅是给merge线程用的，默认值16，推荐32提升merge的速度(CPU允许的前提下)。 max_memory_usage 单个SQL在单台机器最大内存使用量，该值可以设置的比较大，这样可以提升集群查询的上限。 max_memory_usage_for_all_queries 单机最大的内存使用量可以设置略小于机器的物理内存(留一点内操作系统)。 max_bytes_before_external_group_by 在进行group by的时候，内存使用量已经达到了max_bytes_before_external_group_by的时候就进行写磁盘(基于磁盘的group by相对于基于磁盘的order by性能损耗要好很多的)，一般max_bytes_before_external_group_by设置为max_memory_usage / 2，原因是在clickhouse中聚合分两个阶段： 查询并且建立中间数据； 合并中间数据 写磁盘在第一个阶段，如果无须写磁盘，clickhouse在第一个和第二个阶段需要使用相同的内存。 常见错误 Too many parts(304). Merges are processing significantly slower than inserts 相信很多同学在刚开始使用clickhouse的时候都有遇到过该异常，出现异常的原因是因为MergeTree的merge的速度跟不上目录生成的速度, 数据目录越来越多就会抛出这个异常, 所以一般情况下遇到这个异常，降低一下插入频次就ok了，单纯调整background_pool_size的大小是治标不治本的。 相信很多同学在刚开始使用clickhouse的时候都有遇到过该异常，出现异常的原因是因为MergeTree的merge的速度跟不上目录生成的速度, 数据目录越来越多就会抛出这个异常, 所以一般情况下遇到这个异常，降低一下插入频次就ok了，单纯调整background_pool_size的大小是治标不治本的。 我们的场景： 我们的插入速度是严格按照官方文档上面的推荐”每秒不超过1次的insert request”，但是有个插入程序在运行一段时间以后抛出了该异常，很奇怪。 问题排查： 排查发现失败的这个表的数据有一个特性，它虽然是实时数据但是数据的eventTime是最近一周内的任何时间点，我们的表又是按照day + hour组合分区的那么在极限情况下，我们的一个插入请求会涉及7*24分区的数据，也就是我们一次插入会在磁盘上生成168个数据目录(文件夹)，文件夹的生成速度太快，merge速度跟不上了，所以官方文档的上每秒不超过1个插入请求，更准确的说是每秒不超过1个数据目录。 case study： 分区字段的设置要慎重考虑，如果每次插入涉及的分区太多，那么不仅容易出现上面的异常，同时在插入的时候也比较耗时，原因是每个数据目录都需要和zookeeper进行交互。 DB::NetException: Connection reset by peer, while reading from socket xxx 查询过程中clickhouse-server进程挂掉。 问题排查： 排查发现在这个异常抛出的时间点有出现clickhouse-server的重启，通过监控系统看到机器的内存使用在该时间点出现高峰，在初期集群&quot;裸奔&quot;的时期，很多内存参数都没有进行限制，导致clickhouse-server内存使用量太高被OS KILL掉。 case study： 上面推荐的内存参数强烈推荐全部加上，max_memory_usage_for_all_queries该参数没有正确设置是导致该case触发的主要原因。 Memory limit (for query) exceeded:would use 9.37 GiB (attempt to allocate chunk of 301989888 bytes), maximum: 9.31 GiB 该异常很直接，就是我们限制了SQL的查询内存(max_memory_usage)使用的上线，当内存使用量大于该值的时候，查询被强制KILL。 对于常规的如下简单的SQL, 查询的空间复杂度为O(1) 。 12select count(1) from table where condition1 and condition2 select c1, c2 from table where condition1 and condition2 对于group by, order by , count distinct，join这样的复杂的SQL，查询的空间复杂度就不是O(1)了，需要使用大量的内存。 如果是group by内存不够，推荐配置上max_bytes_before_external_group_by参数，当使用内存到达该阈值，进行磁盘group by 如果是order by内存不够，推荐配置上max_bytes_before_external_sort参数，当使用内存到达该阈值，进行磁盘order by 如果是count distinct内存不够，推荐使用一些预估函数(如果业务场景允许)，这样不仅可以减少内存的使用同时还会提示查询速度 对于JOIN场景，我们需要注意的是clickhouse在进行JOIN的时候都是将&quot;右表&quot;进行多节点的传输的(右表广播)，如果你已经遵循了该原则还是无法跑出来，那么好像也没有什么好办法了 zookeeper的snapshot文件太大，follower从leader同步文件时超时 上面有说过clickhouse对zookeeper的依赖非常的重，表的元数据信息，每个数据块的信息，每次插入的时候，数据同步的时候，都需要和zookeeper进行交互，上面存储的数据非常的多。 就拿我们自己的集群举例，我们集群有60台机器30张左右的表，数据一般只存储2天，我们zookeeper集群的压力 已经非常的大了，zookeeper的节点数据已经到达500w左右，一个snapshot文件已经有2G+左右的大小了，zookeeper节点之间的数据同步已经经常性的出现超时。 问题解决： zookeeper的snapshot文件存储盘不低于1T，注意清理策略，不然磁盘报警报到你怀疑人生，如果磁盘爆了那集群就处于“残废”状态； zookeeper集群的znode最好能在400w以下； 建表的时候添加use_minimalistic_part_header_in_zookeeper参数，对元数据进行压缩存储，对于高版本的clickhouse可以直接在原表上面修改该setting信息，注意修改完了以后无法再回滚的。 zookeeper压力太大，clickhouse表处于”read only mode”，插入失败 zookeeper机器的snapshot文件和log文件最好分盘存储(推荐SSD)提高ZK的响应； 做好zookeeper集群和clickhouse集群的规划，可以多套zookeeper集群服务一套clickhouse集群。 最佳实践 实时写入使用本地表，不要使用分布式表 分布式表引擎会帮我们将数据自动路由到健康的数据表进行数据的存储，所以使用分布式表相对来说比较简单，对于Producer不需要有太多的考虑，但是分布式表有些致命的缺点。 数据的一致性问题，先在分布式表所在的机器进行落盘，然后异步的发送到本地表所在机器进行存储，中间没有一致性的校验，而且在分布式表所在机器时如果机器出现down机，会存在数据丢失风险； 据说对zookeeper的压力比较大(待验证)。 推荐使用(*)MergeTree引擎，该引擎是clickhouse最核心的组件，也是社区优化的重点 数据有保障，查询有保障，升级无感知。 谨慎使用on cluster的SQL 使用该类型SQL hang住的案例不少，我们也有遇到，可以直接写个脚本直接操作集群的每台进行处理。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"clickHouse","slug":"clickHouse","permalink":"http://yoursite.com/tags/clickHouse/"}]},{"title":"clickhouse分享","slug":"clickhouse分享","date":"2021-02-23T03:02:41.000Z","updated":"2022-11-22T01:27:53.467Z","comments":true,"path":"2021/02/23/clickhouse分享/","link":"","permalink":"http://yoursite.com/2021/02/23/clickhouse%E5%88%86%E4%BA%AB/","excerpt":"","text":"1. 简介 ClickHouse是一个用于联机分析(OLAP)的列式数据库管理系统(DBMS)，由俄罗斯“百度”之称的Yandex公司于2016年6月15日开源。它的诞生是在一款叫着metrica流量分析工具，基于前方探针采集回来的行为数据，进行一系列的数据分析，类似数仓(data warehouse)的OLAP分析。 在Metrca系统中探针采集的数据模型中，一次页面点击(click),会产生一个event事件，基于页面的点击事件流，面向数据仓库进行OLAP分析。 历程 olap架构 Yandex.Metrica形态 Mysql时期 ROLAP 固定报告 自主研发Metrage MLOAP 固定报告 OLAPServer HOLAP（Metrage+OLAPServer） 自助报告 ClickHouse ROLAP 自助报告 2. 为什么是它 市面上OLAP分析引擎很多，为什么偏偏ClickHouse能够被大众趋之若鹜？ 完备的DBMS：不仅是个数据库，也是个数据库系统 关系模型：有数据库、表、视图和函数。更好清晰的描述实体间的关系 SQL：极高的群众基础 列存储和数据压缩：典型的olap数据库特性。列式存储有效减少查询时所需扫描的数据量，且一般一列具有相同的数据结构，可以更好支持数据压缩。 向量化并行：利用CPU的SIMD（Single Instruction MUltiple Data），单条指令操作多条数据 多线程并行：向量化并行利用硬件采取数据并行（缺陷：不适应较多分支的判断），多线程级并行提高并发 多样化表引擎：合并树、内存、文件、接口等类型的20多种表引擎 算法在前，抽象再后：“选择大于努力”，按需选择，例如对于常量，使用Volnitsky算法；对于非常量，使用SIMDS暴力优化；正则匹配，使用re2和hyperscan算法。所有的选择都是性能排第一！ 勇于尝鲜，不行就换：时刻关注高性能的算法，验证测试。 特定场景，特殊优化：例如：去重计算uniqCombined函数，会根据数据量不同选择不同算法，小数据量Array保存，中等数据量使用HashSet；大数据量使用HyperLogLog 多主架构：天然避免单点故障，服务端对客户端都是“孪生兄弟” 分布式：分区、分片 实战型产品：Yandex有大量数据测试回归，不是纸上谈兵 总结：ROLAP，易懂易建模，SQL操作；多种硬件软件层面优化，查询极快；引擎多样化，按需选择；分布式，可水平扩展和高可用；经得起考验的实战型产品。 3. 表引擎 3.1 总览 表引擎类型 引擎名称 MergeTree系列 MergeTree 、ReplacingMergeTree 、SummingMergeTree 、 AggregatingMergeTree、CollapsingMergeTree 、 VersionedCollapsingMergeTree 、GraphiteMergeTree 外部存储类型 HDFS、Mysql、Kafa、JDBC、File 内存类型 Memory、Set、Join、Buffer 日志类型 TinyLog、StripLog、Log 接口类型 Merge、Dictionary、Distributed 其他类型 Live View、Null、URL 下面着重介绍下MergeTree引擎，其他引擎请阅读我在语雀写的ClickHouse表引擎 3.2 MergeTree MergeTree在写入一批数据时，数据总会以数据片段的形式写入磁盘，且数据片段不可修改。为了避免 片段过多，ClickHouse会通过后台线程，定期合并这些数据片段，属于相同分区的数据片段会被合成一个新的片段。这种数据片段往复合并的特点，也正是合并树名称的由来 3.2.1 语法 123456CREATE TABLE [IF NOT EXISTS] [db_name.]table_name ( name1 [type] [DEFAULT|MATERIALIZED|ALIAS expr], name2 [type] [DEFAULT|MATERIALIZED|ALIAS expr], 省略...) ENGINE = MergeTree() [PARTITION BY expr][ORDER BY expr][PRIMARY KEY expr][SAMPLE BY expr][SETTINGS name=value, 省略...] ENGINE：引擎名称。 ORDER BY：必选，col1,col2,… 默认情况它和主键一样（声明了它主键不需要设置） PARTITION BY：可选，分区键，合理使用分区有效减少数据查询扫描范围。 PRIMARY KEY：可选，默认主键是排序字段。若想排序字段与主键不一致，可以单独指定主键字段。可以重复，不要与我们之前熟知的主键观念混淆！ SAMPLE BY：可选，采样字段，如果指定了该字段，那么主键中也必须包含该字段。比如SAMPLE BY intHash32(UserID) ORDER BY (CounterID, EventDate, intHash32(UserID)) TTL：可选，可以是列也可以是表级别。时间到达会删除设置作用域的数据。 SETTINGS：可选。参考 3.2.2 示例 123456789101112131415161718CREATE TABLE emp_mergetree ( emp_id UInt16 COMMENT &#x27;员工id&#x27;, name String COMMENT &#x27;员工姓名&#x27;, work_place String COMMENT &#x27;工作地点&#x27;,age UInt8 COMMENT &#x27;员工年龄&#x27;, depart String COMMENT &#x27;部门&#x27;, salary Decimal32(2) COMMENT &#x27;工资&#x27; )ENGINE = MergeTree() ORDER BY emp_id PARTITION BY work_place; INSERT INTO emp_mergetree VALUES (1, &#x27;tom&#x27;, &#x27;上海&#x27;, 25, &#x27;技术部&#x27;, 20000),(2, &#x27;jack&#x27;, &#x27;上海&#x27;, 26, &#x27;人事部&#x27;, 10000),(3, &#x27;bob&#x27;, &#x27;北京&#x27;, 33, &#x27;财务部&#x27;, 50000),(4, &#x27;tony&#x27;, &#x27;杭州&#x27;, 28, &#x27;销售事部&#x27;, 50000); 3.2.3 数据目录 ├── 1c89a3ba9fe5fd53379716a776c5ac34_3_3_0 // 分区 │ ├── checksums.txt │ ├── columns.txt │ ├── count.txt │ ├── data.bin │ ├── data.mrk3 │ ├── default_compression_codec.txt │ ├── minmax_work_place.idx │ ├── partition.dat │ └── primary.idx ├── 40d45822dbd7fa81583d715338929da9_1_1_0 // 分区 │ ├── checksums.txt │ ├── columns.txt │ ├── count.txt │ ├── data.bin │ ├── data.mrk3 │ ├── default_compression_codec.txt │ ├── minmax_work_place.idx │ ├── partition.dat │ └── primary.idx ├── a6155dcc1997eda1a348cd98b17a93e9_2_2_0 // 分区 │ ├── checksums.txt │ ├── columns.txt │ ├── count.txt │ ├── data.bin │ ├── data.mrk3 │ ├── default_compression_codec.txt │ ├── minmax_work_place.idx │ ├── partition.dat │ └── primary.idx ├── detached └── format_version.txt partition:分区目录，余下各类数据文件(primary.idx、[Column].mrk、[Column].bin等)都是以分 区目录的形式被组织存放的，属于相同分区的数据，最终会被合并到同一个分区目录，而不同分区的数 据，永远不会被合并在一起。 checksums.txt:校验文件，使用二进制格式存储。它保存了余下各类文件(primary.idx、count.txt 等)的size大小及size的哈希值，用于快速校验文件的完整性和正确性。 columns.txt:列信息文件，使用明文格式存储。用于保存此数据分区下的列字段信息。 123456789[root@ck1 emp_mergetree]# cat 1c89a3ba9fe5fd53379716a776c5ac34_3_3_0/columns.txtcolumns format version: 16 columns:`emp_id` UInt16`name` String`work_place` String`age` UInt8`depart` String`salary` Decimal(9, 2) count.txt:计数文件，使用明文格式存储。用于记录当前数据分区目录下数据的总行数。 12[root@ck1 emp_mergetree]# cat 1c89a3ba9fe5fd53379716a776c5ac34_3_3_0/count.txt1 primary.idx:一级索引文件，使用二进制格式存储。用于存放稀疏索引，一张MergeTree表只能声明一次一级索引(通过ORDER BY或者PRIMARY KEY)。借助稀疏索引，在数据查询的时能够排除主键条件范围之外的数据文件，从而有效减少数据扫描范围，加速查询速度。 data.bin: 数据文件，使用压缩格式存储，默认为LZ4压缩格式。 .mrk:列字段标记文件，使用二进制格式存储。标记文件中保存了.bin文件中数据的偏移 量信息。 partition.dat与minmax_[Column].idx：如果使用了分区键，我们这里 PARTITION BY work_place，则会额外生成partition.dat与minmax索引文件，它们均使用二进制格式存储。partition.dat用于保存当前分区下分区表达式最终生成的值；而minmax索引用于记录当前分区下分区字段对应原始数据的最小和最大值。 1234[root@ck1 a6155dcc1997eda1a348cd98b17a93e9_2_2_0]# cat minmax_work_place.idx北京北京[root@ck1 a6155dcc1997eda1a348cd98b17a93e9_2_2_0]# cat partition.dat北京 3.2.3 数据分区目录 MergeTree数据分区的规则由分区ID决定，而具体到每个数据分区所对应的ID，则是由分区键的取值决定的。分区键支持使用任何一个或一组字段表达式声明，其业务语义可以是年、月、日或者组织单位等任 何一种规则。针对取值数据类型的不同，分区ID的生成逻辑目前拥有四种规则: 不指定分区键:如果不使用分区键，即不使用PARTITION BY声明任何分区表达式，则分区ID默认取名为all，所有的数据都会被写入这个all分区。 1234567891011121314151617181920CREATE TABLE no_partition( `id` UInt8)ENGINE = MergeTreeORDER BY idinsert into no_partition values(1),(2)[root@ck1 no_partition]# tree ..├── all_1_1_0│ ├── checksums.txt│ ├── columns.txt│ ├── count.txt│ ├── data.bin│ ├── data.mrk3│ ├── default_compression_codec.txt│ └── primary.idx├── detached└── format_version.txt 使用整型:如果分区键取值属于整型(兼容UInt64，包括有符号整型和无符号整型)，且无法转换为日期类型YYYYMMDD格式，则直接按照该整型的字符形式输出，作为分区ID的取值。 12345678910111213141516171819202122232425262728293031323334CREATE TABLE partition_int( `id` UInt8)ENGINE = MergeTreePARTITION BY idORDER BY idinsert into partition_int values(1),(2);[root@ck1 partition_int]# tree ..├── 1_1_1_0│ ├── checksums.txt│ ├── columns.txt│ ├── count.txt│ ├── data.bin│ ├── data.mrk3│ ├── default_compression_codec.txt│ ├── minmax_id.idx│ ├── partition.dat│ └── primary.idx├── 2_2_2_0│ ├── checksums.txt│ ├── columns.txt│ ├── count.txt│ ├── data.bin│ ├── data.mrk3│ ├── default_compression_codec.txt│ ├── minmax_id.idx│ ├── partition.dat│ └── primary.idx├── detached└── format_version.txt 使用日期类型:如果分区键取值属于日期类型，或者是能够转换为YYYYMMDD格式的整型，则使用按照YYYYMMDD进行格式化后的字符形式输出，并作为分区ID的取值。 12345678910111213141516171819202122232425262728293031323334353637CREATE TABLE partition_date( `id` UInt8, `name` String, `birthday` Date)ENGINE = MergeTree()PARTITION BY birthdayORDER BY idinsert into partition_date values(1,&#x27;zs&#x27;,&#x27;1999-09-09&#x27;),(2,&#x27;ls&#x27;,&#x27;1999-08-08&#x27;);[root@ck1 partition_date]# tree ..├── 19990808_2_2_0│ ├── checksums.txt│ ├── columns.txt│ ├── count.txt│ ├── data.bin│ ├── data.mrk3│ ├── default_compression_codec.txt│ ├── minmax_birthday.idx│ ├── partition.dat│ └── primary.idx├── 19990909_1_1_0│ ├── checksums.txt│ ├── columns.txt│ ├── count.txt│ ├── data.bin│ ├── data.mrk3│ ├── default_compression_codec.txt│ ├── minmax_birthday.idx│ ├── partition.dat│ └── primary.idx├── detached└── format_version.txt 使用其他类型:如果分区键取值既不属于整型，也不属于日期类型，例如String、Float等，则通 过128位Hash算法取其Hash值作为分区ID的取值。 如上创建的emp_mergetree表。 3.2.4 数据分区目录合并规则 数据表所在的磁盘目录后， 会发现MergeTree分区目录的完整物理名称并不是只有ID而已，在ID之后还跟着一串奇怪的数字，例如 201905_1_1_0。那么这些数字又代表着什么呢? 对于MergeTree而言，它最核心的特点是其分区目录的合并动作。但是我们可曾想过，从分 区目录的命名中便能够解读出它的合并逻辑。 上图中，19990808表示分区目录的ID；2_2_0分别表示最小的数据块编号与最大的数据块编号；而最后的_0，则表示目前合并的层级。接下来开始分别解释它们的含义。 PartitionID：根据分区键生成。 MinBlockNum和MaxBlockNum：BlockNum是一个整型的自增长编号，n从1开始，每当新创建一个分区目录时，计数n就会累积加1。对于一个新的分区目录而言， MinBlockNum与MaxBlockNum取值一样。当分区目录发生合并时，对于新产生的合并目录MinBlockNum与MaxBlockNum有着另外的取值规则，稍后介绍。 Level：合并的层级，可以理解为某个分区被合并过的次数，或者这个分区的年龄。数值越高表示 年龄越大。Level计数与BlockNum有所不同，它并不是全局累加的。对于每一个新创建的分区目录而言，其初始值均为0。之后，以分区为单位，如果相同分区发生合并动作，则在相应分区内计数累积加1。 分区特点 在数据写入时候创建 不同批次写入同一个分区，也会生成不同分区目录 写入后（10-15分钟），ck会合并分区，也可以手动执行optimize，会生成一个新的分区目录 合并后的之前分区，不会立即删除，默认8分钟后被删除 合并后分区命名规则 MinBlockNum：取同一分区内所有目录中最小的MinBlockNum值。 MaxBlockNum:取同一分区内所有目录中最大的MaxBlockNum值。 Level:取同一分区内最大Level值并加1 1234567891011121314151617181920212223242526272829303132// 新增数据ck1 :) insert into partition_date values(3,&#x27;ww&#x27;,&#x27;1999-09-09&#x27;),(4,&#x27;zl&#x27;,&#x27;1999-08-08&#x27;);[root@ck1 partition_date]# ls -ltr总用量 4-rw-r----- 1 clickhouse clickhouse 1 3月 8 17:31 format_version.tdrwxr-x--- 2 clickhouse clickhouse 6 3月 8 17:31 detacheddrwxr-x--- 2 clickhouse clickhouse 200 3月 8 17:32 19990909_1_1_0drwxr-x--- 2 clickhouse clickhouse 200 3月 8 17:32 19990808_2_2_0drwxr-x--- 2 clickhouse clickhouse 200 3月 8 18:07 19990909_3_3_0 --insert 后新分区drwxr-x--- 2 clickhouse clickhouse 200 3月 8 18:07 19990808_4_4_0 --insert 后新分区// 手动执行合并ck1 :) optimize table partition_date final[root@ck1 partition_date]# ls -ltr总用量 4-rw-r----- 1 clickhouse clickhouse 1 3月 8 17:31 format_version.txtdrwxr-x--- 2 clickhouse clickhouse 6 3月 8 17:31 detacheddrwxr-x--- 2 clickhouse clickhouse 200 3月 8 17:32 19990909_1_1_0drwxr-x--- 2 clickhouse clickhouse 200 3月 8 17:32 19990808_2_2_0drwxr-x--- 2 clickhouse clickhouse 200 3月 8 18:07 19990909_3_3_0drwxr-x--- 2 clickhouse clickhouse 200 3月 8 18:07 19990808_4_4_0drwxr-x--- 2 clickhouse clickhouse 200 3月 8 18:08 19990909_1_3_1 ---合并后新分区drwxr-x--- 2 clickhouse clickhouse 200 3月 8 18:08 19990808_2_4_1 ---合并后新分区// 等了一会[root@ck1 partition_date]# ls -ltr总用量 4-rw-r----- 1 clickhouse clickhouse 1 3月 8 17:31 format_version.txtdrwxr-x--- 2 clickhouse clickhouse 6 3月 8 17:31 detacheddrwxr-x--- 2 clickhouse clickhouse 200 3月 8 18:08 19990909_1_3_1drwxr-x--- 2 clickhouse clickhouse 200 3月 8 18:08 19990808_2_4_1 3.2.5 索引 一级索引 MergeTree的主键使用PRIMARY KEY定义，待主键定义之后，MergeTree会依据index_granularity间隔 (默认8192行)，为数据表生成一级索引并保存至primary.idx文件内，索引数据按照PRIMARY KEY排序。 相比使用PRIMARY KEY定义，更为常见的简化形式是通过ORDER BY指代主键。在此种情形下， PRIMARY KEY与ORDER BY定义相同，所以索引(primary.idx)和数据(.bin)会按照完全相同的规则排序。 primary.idx文件内的一级索引采用稀疏索引实现。此时有人可能会问，既然提到了稀疏索引，那么是不是也有稠密索引呢?还真有!稀疏索引和稠密索引的区别。 稀疏索引的优势是显而易见的，它仅需使用少量的索引标记就能够记录大量数据的区间位置信息，且数据量越大优势越为明显。以默认的索引粒度(8192，index_granularity参数设置)为例，MergeTree只需要12208行索引标记就能为1亿 行数据记录提供索引。由于稀疏索引占用空间小，所以primary.idx内的索引数据常驻内存，取用速度自然极快。 二级索引 除了一级索引之外，MergeTree同样支持二级索引。二级索引又称跳数索引，由数据的聚合信息构建而成。根据索引类型的不同，其聚合信息的内容也不同。跳数索引的目的与一级索引一样，也是帮助查询时 减少数据扫描的范围。 跳数索引在默认情况下是关闭的，需要设置allow_experimental_data_skipping_indices(该参数在新版本中已被取消)才能使用 1SET allow_experimental_data_skipping_indices = 1 跳数索引需要在CREATE语句内定义，它支持使用元组和表达式的形式声明，其完整的定义语法如下所示: 12345678910111213//语法INDEX index_name expr TYPE index_type(...) GRANULARITY granularity// 示例CREATE TABLE skip_test ( ID String,URL String,Code String,EventTime Date,INDEX a ID TYPE minmax GRANULARITY 5,INDEX b(length(ID) * 8) TYPE set(2) GRANULARITY 5,INDEX c(ID，Code) TYPE ngrambf_v1(3, 256, 2, 0) GRANULARITY 5, INDEX d ID TYPE tokenbf_v1(256, 2, 0) GRANULARITY 5) ENGINE = MergeTree()order by ID; 不同的跳数索引之间，除了它们自身独有的参数之外，还都共同拥有granularity参数。初次接触时，很容易将granularity与index_granularity的概念弄混淆。对于跳数索引而言，index_granularity定义了数据的粒度，而granularity定义了聚合信息汇总的粒度。换言之，granularity定义了一行跳数索引能够跳过多少个 index_granularity区间的数据。 minmax:minmax索引记录了一段数据内的最小和最大极值，其索引的作用类似分区目录的minmax索引，能够快速跳过无用的数据区间，示例如下所示: 1INDEX a ID TYPE minmax GRANULARITY 5 述示例中minmax索引会记录这段数据区间内ID字段的极值。极值的计算涉及每5个index_granularity 区间中的数据 set:set索引直接记录了声明字段或表达式的取值(唯一值，无重复)，其完整形式为 set(max_rows)，其中max_rows是一个阈值，表示在一个index_granularity内，索引最多记录的数据行数。如果max_rows=0，则表示无限制，例如: 1INDEX b(length(ID) * 8) TYPE set(100) GRANULARITY 5 上述示例中set索引会记录数据中ID的长度*8后的取值。其中，每个index_granularity内最多记录100条。 ngrambf_v1:ngrambf_v1索引记录的是数据短语的布隆表过滤器，只支持String和FixedString数据类型。ngrambf_v1只能够提升in、notIn、like、equals和notEquals查询的性能，其完整形式为 12345// n: token长度，依据n的长度将数据切割为token短语。//size_of_bloom_filter_in_bytes:布隆过滤器的大小。// number_of_hash_functions:布隆过滤器中使用Hash函数的个数。// random_seed:Hash函数的随机种子。ngrambf_v1(n,size_of_bloom_filter_in_bytes,number_of_hash_functions,random_seed) tokenbf_v1:tokenbf_v1索引是ngrambf_v1的变种，同样也是一种布隆过滤器索引。tokenbf_v1除 了短语token的处理方法外，其他与ngrambf_v1是完全一样的。tokenbf_v1会自动按照非字符的、数字的字符串分割token，具体用法如下所示: 1INDEX d ID TYPE tokenbf_v1(256, 2, 0) GRANULARITY 5 3.2.6 数据存储 MergeTree也并不是一股脑地将数据直接写入.bin文件，而是经过了一 番精心设计：首先，数据是经过压缩的，目前支持LZ4、ZSTD、Multiple和Delta几种算法，默认使用LZ4算法；其次，数据会事先依照ORDER BY的声明排序；最后，数据是以压缩数据块的形式被组织并写入.bin文 件中的。 压缩 一个压缩数据块由头信息和压缩数据两部分组成。头信息固定使用9个字节表示，具体由1个UInt8(1 字节)整型和2个UInt32(4字节)整型组成，分别代表使用的压缩算法类型、压缩后的数据大小和压缩前 的数据大小。 ClickHouse提供的clickhouse-compressor工具，能够查询某个.bin文件中压缩数据的统计信息，每一行数据代表着一个压缩数据块的头信息 1234567[root@ck1 40d45822dbd7fa81583d715338929da9_1_1_0]# clickhouse-compressor --stat &lt; data.bin4 149 1914 242 1220 318 18 每个压缩数据块的体积，按照其压缩前的数据字节大小，都被严格控制在64KB~1MB，其上下限分别由min_compress_block_size(默认65536)与max_compress_block_size(默认1048576)参数指定。 在.bin文件中引入压缩数据块的目的至少有以下两个: 其一，虽然数据被压缩后能够有效减少数据大小，降低存储空间并加速数据传输效率，但数据的压缩和解压动作，其本身也会带来额外的性能损耗。所以需要控制被压缩数据的大小，以求在性能损耗和压缩率之间寻求一种平衡。 其二，在具体读取某一列数据时(.bin文件)，首先需要将压缩数据加载到内存并解压，这样才能进行后续的数据处理。通过压缩数据块，可以在不读取整个.bin文件的情况下将读取粒度降低到压缩数据块级别，从而进一步缩小数据读取的范围。 数据标记 如果把MergeTree比作一本书，primary.idx一级索引好比这本书的一级章节目录，.bin文件中的数据好比这本书中的文字，那么数据标记(.mrk)会为一级章节目录和具体的文字之间建立关联。对于数据标记而言， 它记录了两点重要信息： 其一，一级章节对应的页码信息; 其二，一段文字在某一页中的起始位置信息。这样一来，通过数据标记就能够很快地从一本书中立即翻到关注内容所在的那一页，并知道从第几行开始阅读。 为了能够与数据衔接，数据标记文件也与.bin文件一一对应。即每一个列字段[Column].bin文件都有一 个与之对应的[Column].mrk数据标记文件，用于记录数据在.bin文件中的偏移量信息。 一行标记数据使用一个元组表示，元组内包含两个整型数值的偏移量信息。它们分别表示在此段数据区间内，在对应的.bin压缩文件中，压缩数据块的起始偏移量；以及将该数据压缩块解压后，其未压缩数据的起始偏移量。 编号 压缩文件中的偏移量 解压缩文件中的偏移量 0 0 0 1 08192 8192 2 12016 0 读取压缩数据块: 在查询某一列数据时，MergeTree无须一次性加载整个.bin文件，而是可以根据需要，只加载特定的压缩数据块。而这项特性需要借助标记文件中所保存的压缩文件中的偏移量。 读取数据: 在读取解压后的数据时，MergeTree并不需要一次性扫描整段解压数据，它可以根据 需要，以index_granularity的粒度加载特定的一小段。为了实现这项特性，需要借助标记文件中保存的解压 数据块中的偏移量。 多对一 多个数据标记对应一个压缩数据块，当一个间隔(index_granularity)内的数据未压缩大小size小于64KB时，会出现这种对应关系 一对一 一个数据标记对应一个压缩数据块，当一个间隔(index_granularity)内的数据未压缩大小size大于等于64KB且小于等于1MB时，会出现这种对应关系 一对多 一个数据标记对应多个压缩数据块，当一个间隔(index_granularity)内的数据未压缩大小size直接大于1MB时，会出现这种对应关系。 3.2.7 分区、索引、标记、压缩 生成分区目录 index_granularity索引粒度生成primary.idx index_granularity索引粒度生成.mrk 生成.bin，每个块64KB-1MB。 案例 用户留存 用户登陆表 1234CREATE TABLE IF NOT EXISTS login_log( user_id INT , log_time TIMESTAMP ) engine=MergeTree() order by user_id; 数据 1234567891011121314151617181920212223242526272829303132INSERT INTO login_log VALUES(1101,&#x27;2021-01-21 22:00:00&#x27;),(1101,&#x27;2021-01-20 22:00:00&#x27;),(1101,&#x27;2021-01-19 22:00:00&#x27;),(1101,&#x27;2021-01-17 22:00:00&#x27;),(1101,&#x27;2021-01-16 22:00:00&#x27;),(1101,&#x27;2021-01-21 22:00:00&#x27;),(1101,&#x27;2021-01-21 23:00:00&#x27;),(1101,&#x27;2021-01-20 23:00:00&#x27;),(1101,&#x27;2021-01-19 23:00:00&#x27;),(1101,&#x27;2021-01-17 23:00:00&#x27;),(1101,&#x27;2021-01-16 23:00:00&#x27;),(1101,&#x27;2021-01-21 23:00:00&#x27;),(4101,&#x27;2021-01-20 22:00:00&#x27;),(4101,&#x27;2021-01-19 22:00:00&#x27;),(4101,&#x27;2021-01-17 22:00:00&#x27;),(4101,&#x27;2021-01-16 22:00:00&#x27;),(2201,&#x27;2021-01-16 14:00:00&#x27;),(2201,&#x27;2021-01-15 23:04:00&#x27;),(2201,&#x27;2021-01-21 18:00:00&#x27;),(2201,&#x27;2021-01-20 21:00:00&#x27;),(2201,&#x27;2021-01-21 23:00:00&#x27;),(2201,&#x27;2021-01-20 23:00:00&#x27;),(3301,&#x27;2021-01-21 22:00:00&#x27;),(3301,&#x27;2021-01-19 22:00:00&#x27;),(3301,&#x27;2021-01-18 23:00:00&#x27;),(3301,&#x27;2021-01-17 23:00:00&#x27;),(3301,&#x27;2021-01-16 23:00:00&#x27;),(3301,&#x27;2021-01-15 23:00:00&#x27;); 如需查询用户在2021-01-17至2021-01-21登录流失情况，则SQL语句如下： 12345678910111213141516171819SELECT user_id, retention( date(log_time) = &#x27;2021-01-21&#x27;, date(log_time) = &#x27;2021-01-20&#x27;, date(log_time) = &#x27;2021-01-19&#x27;, date(log_time) = &#x27;2021-01-18&#x27;, date(log_time) = &#x27;2021-01-17&#x27; ) AS rFROM login_log GROUP BY user_idORDER BY user_id ASC;user_id|r |-------|-----------| 1101|[1,1,1,0,1]| 2201|[1,1,0,0,0]| 3301|[1,0,1,1,1]| 4101|[0,0,0,0,0]| 查询留存 12345678910111213141516171819202122232425262728293031323334353637383940SELECT DATE(TIMESTAMP &#x27;2021-01-15 00:00:00&#x27;) AS first_date, SUM(r[1]) AS &quot;第一天活跃用户&quot;, SUM(r[2])/ SUM(r[1]) AS &quot;次日留存&quot;, SUM(r[3])/ SUM(r[1]) AS &quot;3日留存&quot;, SUM(r[4])/ SUM(r[1]) AS &quot;7日留存&quot; FROM -- 计算2021-01-15活跃用户在第2、3、7日的登录情况，1/0 =&gt; 登录/未登录 ( WITH first_day_table AS ( SELECT TIMESTAMP &#x27;2021-01-15 00:00:00&#x27; AS first_day ) SELECT user_id, retention( DATE(log_time) = (SELECT DATE(first_day) FROM first_day_table), DATE(log_time) = (SELECT DATE(first_day + INTERVAL &#x27;1 day&#x27;) FROM first_day_table), DATE(log_time) = (SELECT DATE(first_day + INTERVAL &#x27;2 day&#x27;) FROM first_day_table), DATE(log_time) = (SELECT DATE(first_day + INTERVAL &#x27;6 day&#x27;) FROM first_day_table) ) AS r -- 过滤2021-01-15活跃用户在后续 1～7 日登录数据 FROM login_log WHERE ( log_time &gt;= TIMESTAMP &#x27;2021-01-15 00:00:00&#x27; ) AND ( log_time &lt;= TIMESTAMP &#x27;2021-01-15 00:00:00&#x27; + INTERVAL &#x27;6 day&#x27; ) GROUP BY user_id ) AS basic_table GROUP BY first_date; first_date|第一天活跃用户|次日留存|3日留存|7日留存|----------|-------|----|----|----|2021-01-15| 2| 1.0| 0.5| 0.0| 函数分析 123456语法：retention(cond1, cond2, ..., cond32);// 示例r1: date=2020-01-01（ cond1 条件）。r2: 2020-01-01=&lt;date&lt;=2020-01-02 (cond1 和 cond2 条件）。r3: 2020-01-01=&lt;date&lt;=2020-01-03 (cond1 和 cond3 条件）。retention(date = &#x27;2020-01-01&#x27;, date = &#x27;2020-01-02&#x27;, date = &#x27;2020-01-03&#x27;) as r 行为漏斗转化 用户行为表 12345678CREATE TABLE IF NOT EXISTS user_action( `uid` Int32, `event_type` String, `time` datetime)ENGINE = MergeTree()ORDER BY uid; 数据 123456789101112131415161718192021222324insert into user_action values(1,&#x27;浏览&#x27;,&#x27;2021-01-02 11:00:00&#x27;),(1,&#x27;点击&#x27;,&#x27;2021-01-02 11:10:00&#x27;),(1,&#x27;下单&#x27;,&#x27;2021-01-02 11:20:00&#x27;),(1,&#x27;支付&#x27;,&#x27;2021-01-02 11:30:00&#x27;),(2,&#x27;浏览&#x27;,&#x27;2021-01-02 10:00:00&#x27;),(2,&#x27;点击&#x27;,&#x27;2021-01-02 10:10:00&#x27;),(2,&#x27;下单&#x27;,&#x27;2021-01-02 11:00:00&#x27;),(2,&#x27;支付&#x27;,&#x27;2021-01-02 11:10:00&#x27;),(3,&#x27;浏览&#x27;,&#x27;2021-01-02 11:20:00&#x27;),(3,&#x27;点击&#x27;,&#x27;2021-01-02 12:00:00&#x27;),(4,&#x27;浏览&#x27;,&#x27;2021-01-02 11:50:00&#x27;),(4,&#x27;点击&#x27;,&#x27;2021-01-02 12:00:00&#x27;),(5,&#x27;浏览&#x27;,&#x27;2021-01-02 11:50:00&#x27;),(5,&#x27;点击&#x27;,&#x27;2021-01-02 12:00:00&#x27;),(5,&#x27;下单&#x27;,&#x27;2021-01-02 12:10:00&#x27;),(6,&#x27;浏览&#x27;,&#x27;2021-01-02 11:50:00&#x27;),(6,&#x27;点击&#x27;,&#x27;2021-01-02 12:00:00&#x27;),(6,&#x27;下单&#x27;,&#x27;2021-01-02 12:10:00&#x27;); 30分钟作为一个时间窗口，查看窗口数据 1234567891011121314SELECT uid, windowFunnel(1800)(time, event_type = &#x27;浏览&#x27;, event_type = &#x27;点击&#x27;, event_type = &#x27;下单&#x27;, event_type = &#x27;支付&#x27;) AS levelFROM user_actionGROUP BY uid order by uiduid|level|---|-----| 1| 4| 2| 2| 3| 1| 4| 2| 5| 3| 6| 3| 分析&quot;2021-01-02&quot;这天 路径为“浏览-&gt;点击-&gt;下单-&gt;支付”的转化情况 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152SELECT level_index,count(1) FROM( SELECT uid, arrayWithConstant(level, 1) levels, arrayJoin(arrayEnumerate(levels)) level_index FROM ( SELECT uid, windowFunnel(1800)( time, event_type = &#x27;浏览&#x27;, event_type = &#x27;点击&#x27; , event_type = &#x27;下单&#x27;, event_type = &#x27;支付&#x27; ) AS level FROM ( SELECT time, event_type , uid FROM user_action WHERE toDate(time) = &#x27;2021-01-02&#x27; ) GROUP BY uid ))group by level_index order by level_index// 内部数据uid|levels |level_index|---|---------|-----------| 4|[1,1] | 1| 4|[1,1] | 2| 3|1 | 1| 2|[1,1] | 1| 2|[1,1] | 2| 5|[1,1,1] | 1| 5|[1,1,1] | 2| 5|[1,1,1] | 3| 1|[1,1,1,1]| 1| 1|[1,1,1,1]| 2| 1|[1,1,1,1]| 3| 1|[1,1,1,1]| 4| 6|[1,1,1] | 1| 6|[1,1,1] | 2| 6|[1,1,1] | 3|// 结果 level_index|count(1)|-----------|--------| 1| 6| 2| 5| 3| 3| 4| 1| 函数分析 123arrayWithConstant(len,const) 构造len长度的，内容是const的数组arrayJoin(arr) ，拆分arr为多行arrayEnumerate(arr)，返回 Array [1, 2, 3, …, index] 路径分析 关键路径分析：已经明确了要分析的路径，需要看下这些访问路径上的用户数据 智能路径分析：不确定有哪些路径，但是清楚目标路径是什么，需要知道用户在指定时间范围内都是通过哪些途径触达目标路径的 下单超过10分钟才支付 1234567891011121314151617181920212223242526272829303132333435--明确了要分析的路径SELECT count(1) AS &quot;userCount&quot;, sum(cn) AS &quot;actionCount&quot;FROM ( SELECT uid, sequenceCount (&#x27;(?1)(?t&gt;=600)(?2)&#x27;) ( toDateTime(time), event_type = &#x27;下单&#x27;, event_type = &#x27;支付&#x27; ) AS cn FROM user_action GROUP BY uid)WHERE cn &gt;= 1// 子查询数据uid|cn|---|--| 4| 0| 3| 0| 2| 1| 5| 0| 1| 1| 6| 0|// 结果userCount|actionCount|---------|-----------| 2| 2| 函数分析 123456// 计算与模式匹配的事件链的数量语法：sequenceCount(pattern)(time, cond1, cond2, …)模式：(?N) — 在位置N匹配条件参数。 条件在编号 [1, 32] 范围。 例如, (?1) 匹配传递给 cond1 参数。.* — 匹配任何事件的数字。 不需要条件参数来匹配这个模式。(?t operator value) — 分开两个事件的时间。 例如： (?1)(?t&gt;1800)(?2) 匹配彼此发生超过1800秒的事件。 这些事件之间可以存在任意数量的任何事件。 您可以使用 &gt;=, &gt;, &lt;, &lt;= 运算符。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"clickHouse","slug":"clickHouse","permalink":"http://yoursite.com/tags/clickHouse/"}]},{"title":"clickhouse运维","slug":"clickhouse运维","date":"2021-02-20T08:48:09.000Z","updated":"2022-11-22T01:27:53.534Z","comments":true,"path":"2021/02/20/clickhouse运维/","link":"","permalink":"http://yoursite.com/2021/02/20/clickhouse%E8%BF%90%E7%BB%B4/","excerpt":"","text":"","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"clickHouse","slug":"clickHouse","permalink":"http://yoursite.com/tags/clickHouse/"}]},{"title":"clickhouse副本和分片","slug":"clickhouse副本和分片","date":"2021-02-20T08:48:02.000Z","updated":"2022-11-22T01:27:53.513Z","comments":true,"path":"2021/02/20/clickhouse副本和分片/","link":"","permalink":"http://yoursite.com/2021/02/20/clickhouse%E5%89%AF%E6%9C%AC%E5%92%8C%E5%88%86%E7%89%87/","excerpt":"","text":"副本和分片是在很多分布式系统中都有，首先简单来看看他们的概念。 副本：数据结构相同，数据相同（数据层面的备份冗余） 分片：数据结构相同，数据不同。（数据量层面水平切分） 集群是副本和分片的基础，它将ClickHouse的服务拓扑由单节点延伸到多个节点，但它并不像Hadoop 生态的某些系统那样，要求所有节点组成一个单一的大集群。ClickHouse的集群配置非常灵活，用户既可以 将所有节点组成一个单一集群，也可以按照业务的诉求，把节点划分为多个小的集群。在每个小的集群区 域之间，它们的节点、分区和副本数量可以各不相同。 副本 在clickhouse中实现副本可以使用下面引擎系列 换言之，若使用了ReplicatedMergeTree复制表系列引擎，就能应用副本的能力（后面会介绍另一种副本的实现方式）。 ReplicatedMergeTree是MergeTree的派生引擎，它是依靠zookeeper监听节点实现副本在节点间的传输。 zookeeper配置 首先，在服务器的/etc/clickhouse-server/config.d目录下创建一个名为metrika.xml的配置文件 1234567891011&lt;?xml version=&quot;1.0&quot;?&gt;&lt;yandex&gt; &lt;!—ZooKeeper配置，名称自定义，一般就用这个名字就好 --&gt; &lt;zookeeper-servers&gt; &lt;node index=&quot;1&quot;&gt; &lt;!—节点配置，可以配置多个地址--&gt; &lt;host&gt;127.0.0.1&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper-servers&gt;&lt;/yandex&gt; 接着，在全局配置config.xml中使用&lt;include_from&gt;标签导入刚才定义的配置: 1&lt;include_from&gt;/etc/clickhouse-server/config.d/metrika.xml&lt;/include_from&gt; 并引用ZooKeeper配置的定义 ，incl与metrika.xml配置文件内的节点名称要彼此对应 1&lt;zookeeper incl=&quot;zookeeper-servers&quot; optional=&quot;false&quot; /&gt; 在clickhouse中一切皆表，重启后会有一张zookeeper代理表，通过它可以查询zk内的数据 1SELECT * FROM system.zookeeper where path = &#x27;/&#x27; 副本的定义 ReplicatedMergeTree 系列引擎 1ENGINE = ReplicatedMergeTree(&#x27;zk_path&#x27;, &#x27;replica_name&#x27;) zk_path：用于指定在ZooKeeper中创建的数据表的路径，路径名称是自定义的，并没有固定规则，但是ck中一般约定俗成为/clickhouse/tables/&#123;shard&#125;/table_name /clickhouse/tables/是约定俗成的路径固定前缀，表示存放数据表的根路径 {shard}表示分片编号，通常用数值替代，例如01、02、03。一张数据表可以有多个分片，而每个分片都拥有自己的副本 table_name表示数据表的名称，为了方便维护，通常与物理表的名字相同（也是不强制） 称是区分不同副本实例的唯一标识，一种约定成俗的命名方式是使用所在服务器的域名称。1234567例如：&#96;&#96;&#96;sql&#x2F;&#x2F;1分片，1副本. zk_path相同，replica_name不同 ENGINE &#x3D; ReplicatedMergeTree(&#39;&#x2F;clickhouse&#x2F;tables&#x2F;01&#x2F;test_1, &#39;linux01&#39;);ENGINE &#x3D; ReplicatedMergeTree(&#39;&#x2F;clickhouse&#x2F;tables&#x2F;01&#x2F;test_1, &#39;linux02&#39;); 实践 12345678910111213// linux01CREATE TABLE replicated_sales_1( id String,price Float64,create_time DateTime) ENGINE = ReplicatedMergeTree(&#x27;/clickhouse/tables/01/replicated_sales_1&#x27;,&#x27;linux01&#x27;) PARTITION BY toYYYYMM(create_time)ORDER BY id// linux02CREATE TABLE replicated_sales_1( id String,price Float64,create_time DateTime) ENGINE = ReplicatedMergeTree(&#x27;/clickhouse/tables/01/replicated_sales_1&#x27;,&#x27;linux01&#x27;) PARTITION BY toYYYYMM(create_time)ORDER BY id 这样后续在任何一个副本上执行INSERT、MERGE、MUTATION和ALTER操作都会影响另外一个副本。 实现原理 ReplicatedMergeTree能够实现副本的核心逻辑是使用了zk的能力。在执行INSERT数据写入、MERGE分区和MUTATION操作的时候，都会涉及与ZooKeeper的通信。在查询数据的时候也不会访问ZooKeeper，所以不必过于担 心ZooKeeper的承载压力。 上述linux01和linux02机器创建为例，大概步骤： linux01 根据zk_path初始化所有的ZooKeeper节点。 在/replicas/节点下注册自己的副本实例linux01。 启动监听任务，监听/log日志节点。 参与副本选举，选举出主副本，选举的方式是向/leader_election/插入子节点，第一个插入成功的副本 就是主副本。 linux02 在/replicas/节点下注册自己的副本实例linux02。 启动监听任务，监听/log日志节点。 参与副本选举，选举出主副本。在这个例子中，linux01副本成为主副本。 后续操作任何一个节点，都会在zk上有对应的log目录，另外一个节点实时监控到log目录的变动，会动态拉取log日志，主动从主副本那里拉取数据更新自己。实际上可能会出现更新操作比较频繁，所以都是将任务放入到队列中，排队拉取更新。 分片 分片可以简单理解为是表结构相同，数据不同一系列表集合。这里就可以延伸出2个问题，写入时候怎么分配，查询时候如何合并结果集。在clickhouse中引入一个Distributed表引擎，充当读写门面，它作为分布式表的一层透明代理，在集群内部自动开展数 据的写入、分发、查询、路由等工作。 集群配置 Distributed表引擎需要读取集群的信息，所以首先必须为ClickHouse添加集群的配置。找到前面在 介绍ZooKeeper配置时增加的metrika.xml配置文件，在ClickHouse中，集群配置用shard代表分片、用replica代表副本。 语义的配置如下所示: 123456789101112131415161718// 1分片、0副本&lt;shard&gt; &lt;!-- 分片 --&gt; &lt;replica&gt; &lt;!—副本 --&gt; &lt;/replica&gt;&lt;/shard&gt;// 1分片、1副本&lt;shard&gt; &lt;!-- 分片 --&gt; &lt;replica&gt; &lt;!—副本 --&gt; &lt;/replica&gt; &lt;replica&gt; &lt;!—副本 --&gt; &lt;/replica&gt;&lt;/shard&gt; shard更像是逻辑层面的分组，而无论是副本还是分片，它们 的载体都是replica。 集群有两种配置形式： 1、不包含副本的分片 如果直接使用node标签定义分片节点，那么该集群将只包含分片，不包含副本。以下面的配置为例 12345678910111213141516171819202122232425// 2个分片、0个副本&lt;yandex&gt; &lt;!--自定义配置名，与config.xml配置的incl属性对应即可 --&gt; &lt;zookeeper-servers&gt; &lt;!--自定义集群名称--&gt; &lt;cluster1&gt; &lt;!--定义ClickHouse节点--&gt; &lt;node&gt; &lt;host&gt;linux01&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;!--选填参数 &lt;weight&gt;1&lt;/weight&gt; &lt;user&gt;&lt;/user&gt; &lt;password&gt;&lt;/password&gt; &lt;secure&gt;&lt;/secure&gt; &lt;compression&gt;&lt;/compression&gt; --&gt; &lt;/node&gt; &lt;node&gt; &lt;host&gt;linux02&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/node&gt; &lt;/cluster1&gt; &lt;/zookeeper-servers&gt;&lt;/yandex&gt; 该配置定义了一个名为cluster1的集群，其包含了2个分片节点，它们分别指向了是linux01和linux02服务器。 现在分别对配置项进行说明: cluster1表示自定义的集群名称，全局唯一，是后续引用集群配置的唯一标识。在一个配置文件内，可以定义任意组集群。 node用于定义分片节点，不包含副本。 host指定部署了ClickHouse节点的服务器地址。 port指定ClickHouse服务的TCP端口。 接下来介绍选填参数: weight分片权重默认为1，在后续小节中会对其详细介绍。 user为ClickHouse用户，默认为default。 password为ClickHouse的用户密码，默认为空字符串。 secure为SSL连接的端口，默认为9440。 compression表示是否开启数据压缩功能，默认为true。 2、自定义分片与副本 集群配置支持自定义分片和副本的数量，这种形式需要使用shard标签代替先前的node，除此之外的配 置完全相同。在这种自定义配置的方式下，分片和副本的数量完全交由配置者掌控。其中，shard表示逻辑 上的数据分片，而物理上的分片则用replica表示。如果在1个shard标签下定义N(N&gt;=1)组replica，则该shard 的语义表示1个分片和N-1个副本。 不包含副本的分片 效果与先前介绍的cluster1集群相同. 1234567891011121314151617181920212223// 2个分片、0个副本 &lt;yandex&gt; &lt;!--自定义配置名，与config.xml配置的incl属性对应即可 --&gt; &lt;zookeeper-servers&gt; &lt;!-- 自定义集群名称 --&gt; &lt;cluster1&gt; &lt;!-- 分片 --&gt; &lt;shard&gt; &lt;replica&gt; &lt;!-- 副本 --&gt; &lt;host&gt;linux01&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;replica&gt; &lt;host&gt;linux02&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/cluster1&gt; &lt;/zookeeper-servers&gt;&lt;/yandex&gt; N个分片和N个副本 123456789101112131415161718192021// 1个分片、1个副本 &lt;yandex&gt; &lt;!--自定义配置名，与config.xml配置的incl属性对应即可 --&gt; &lt;zookeeper-servers&gt; &lt;!-- 自定义集群名称 --&gt; &lt;cluster1&gt; &lt;!-- 分片 --&gt; &lt;shard&gt; &lt;replica&gt; &lt;!-- 副本 --&gt; &lt;host&gt;linux01&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;host&gt;linux02&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/cluster1&gt; &lt;/zookeeper-servers&gt;&lt;/yandex&gt; **查询：**完成上述配置之后，可以查询系统表验证集群配置是否已被加载 1SELECT cluster, host_name FROM system.clusters; 分布式DDL 在副本中，我们利用ReplicatedMergeTree操作副本表，需要登陆到2台机器上，分别操作，麻烦。在引入集群配置后，在ck中可以执行分布式DDL。 语法 12// cluster_name对应了配置文件中的集群名称，这里也就是cluster1CREATE/DROP/RENAME/ALTER TABLE ON CLUSTER cluster_name ReplicatedMergeTree中示例，可以替换成分布式DDL 123// 一个分片，1个副本ENGINE = ReplicatedMergeTree(&#x27;/clickhouse/tables/01/test_1, &#x27;linux01&#x27;);ENGINE = ReplicatedMergeTree(&#x27;/clickhouse/tables/01/test_1, &#x27;linux02&#x27;); 1234567CREATE TABLE test_1 ON CLUSTER cluster1( id String,price Float64,create_time DateTime) ENGINE = ReplicatedMergeTree(&#x27;/clickhouse/tables/&#123;shard&#125;/test_1&#x27;,&#x27;&#123;replica&#125;&#x27;) PARTITION BY toYYYYMM(create_time)ORDER BY id 利用宏变量，用{shard}和{replica}两个动态宏变量代替了硬编码方式。具体是在config.xml配置变量 12345678910// linux01&lt;macros&gt; &lt;shard&gt;01&lt;/shard&gt; &lt;replica&gt;linux01&lt;/replica&gt;&lt;/macros&gt;// linux02&lt;macros&gt; &lt;shard&gt;02&lt;/shard&gt; &lt;replica&gt;linux02&lt;/replica&gt;&lt;/macros&gt; 原理 与ReplicatedMergeTree类似，分布式DDL语句在执行的过程中也需要借助ZooKeeper的协同能力，以实现日志分发。在默认情况下，分布式DDL在ZooKeeper内使用的根路径为: 1/clickhouse/task_queue/ddl 该路径由config.xml内的distributed_ddl配置指定: 1234&lt;distributed_ddl&gt; &lt;!-- Path in ZooKeeper to queue with DDL queries --&gt; &lt;path&gt;/clickhouse/task_queue/ddl&lt;/path&gt;&lt;/distributed_ddl&gt; Distributed引擎 结构 Distributed表引擎是分布式表的代名词，它自身不存储任何数据，而是作为数据分片的透明代理，能够 自动路由数据至集群中的各个节点，所以Distributed表引擎需要和其他数据表引擎一起协同工作。 本地表:通常以_local为后缀进行命名。本地表是承接数据的载体，可以使用非Distributed的任意表引 擎，一张本地表对应了一个数据分片。_ 分布式表:通常以_all为后缀进行命名。分布式表只能使用Distributed表引擎，它与本地表形成一对多 的映射关系，日后将通过分布式表代理操作多张本地表。 定义 1ENGINE = Distributed(cluster, database, table [,sharding_key]) cluster:集群名称，与集群配置中的自定义名称相对应。在对分布式表执行写入和查询的过程中，它会使用集群的配置信息来找到相应的host节点。 database和table:分别对应数据库和表的名称，分布式表使用这组配置映射到本地表。 sharding_key:分片键，选填参数。在数据写入的过程中，分布式表会依据分片键的规则，将数据分布 到各个host节点的本地表。 创建 Distributed表运用的是读时检查的机制，对创建分 布式表和本地表的顺序并没有强制要求。 123456CREATE TABLE test_shard_2_all ON CLUSTER sharding_simple ( id UInt64)ENGINE = Distributed(sharding_simple, default, test_shard_2_local,rand())CREATE TABLE test_shard_2_local ON CLUSTER sharding_simple ( id UInt64)ENGINE = MergeTree() ORDER BY idPARTITION BY id 操作影响 INSERT和SELECT查询：Distributed将会以分布式的方式作用于local本 地表。 CREATE、DROP、RENAME和ALTER：只会修改Distributed表自身，并不会修改local本地表。 分片规则 分片键要求返回一个整型类型的取值，包括Int系列和UInt系列。如果不声明分片键，那么分布式表只能包含一个分片，这意味着只能映射一张本地表，否则，在写入数据时将会异常。 1234--按照随机数划分Distributed(cluster, database, table ,rand())--按照用户id的散列值划分Distributed(cluster, database, table , intHash64(userid)) 分片权重 在集群的配置中，有一项weight(分片权重)的设置: 123456789101112&lt;sharding_simple&gt; &lt;!-- 自定义集群名称 --&gt; &lt;shard&gt; &lt;!-- 分片 --&gt; &lt;weight&gt;10&lt;/weight&gt; &lt;!-- 分片权重 --&gt; ...... &lt;/shard&gt; &lt;shard&gt; &lt;weight&gt;20&lt;/weight&gt; ...... &lt;/shard&gt; ... weight默认为1，虽然可以将它设置成任意整数，但官方建议应该尽可能设置成较小的值。分片权重会 影响数据在分片中的倾斜程度，一个分片权重值越大，那么它被写入的数据就会越多。 slot(槽) slot可以理解成许多小的水槽，如果把数据比作是水的话，那么数据之水会顺着这些水槽流进每个数据 分片。slot的数量等于所有分片的权重之和，假设集群cluter1有两个Shard分片，第一个分片的 weight为10，第二个分片的weight为20，那么slot的数量则等于30。slot按照权重元素的取值区间，与对应的分片形成映射关系。在这个示例中，如果slot值落在[0,10)区间，则对应第一个分片;如果slot值落在[10,20] 区间，则对应第二个分片。 选择函数 选择函数用于判断一行待写入的数据应该被写入哪个分片，整个判断过程大致分成两个步骤： (1)它会找出slot的取值，其计算公式如下: 其中，shard_value是分片键的取值;sum_weight是所有分片的权重之和;slot等于shard_value和 sum_weight的余数。假设某一行数据的shard_value是10，sum_weight是30(两个分片，第一个分片权重为 10，第二个分片权重为20)，那么slot值等于10(10%30=10)。 (2)基于slot值找到对应的数据分片。当slot值等于10的时候，它属于[10,20)区间，所以这行数据会对 应到第二个Shard分片。 副本复制 如果在集群的配置中包含了副本，那么除了分片写入流程，还会触发副本数据的复制流程。数据在多个副本之间，有两种复制实现方式: 借助Distributed表引擎，由它将数据写入副本，Distributed节点需要同时负责分片和副本的写入 在集群的shard配置中增加internal_replication参数并将其设置为true(默认为false)，那么 Distributed表在该shard中只会选择一个合适的replica并对其写入数据。此时，如果使用ReplicatedMergeTree 作为本地表的引擎，则在该shard内，多个replica副本之间的数据复制会交由ReplicatedMergeTree自己处理","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"clickHouse","slug":"clickHouse","permalink":"http://yoursite.com/tags/clickHouse/"}]},{"title":"clickhouse数据查询","slug":"clickhouse数据查询","date":"2021-02-20T08:47:47.000Z","updated":"2022-11-22T01:27:53.530Z","comments":true,"path":"2021/02/20/clickhouse数据查询/","link":"","permalink":"http://yoursite.com/2021/02/20/clickhouse%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2/","excerpt":"","text":"clickhouse作为一款olap分析数据库，大多数情况我们都是使用select来操作它。 查询语法 123456789101112131415SELECT [DISTINCT] expr_list [FROM [db.]table | (subquery) | table_function] [FINAL] [SAMPLE sample_coeff] [ARRAY JOIN ...] [GLOBAL] ANY|ALL INNER|LEFT JOIN (subquery)|table USING columns_list [PREWHERE expr] [WHERE expr] [GROUP BY expr_list] [WITH TOTALS] [HAVING expr] [ORDER BY expr_list] [LIMIT [n, ]m] [UNION ALL ...] [INTO OUTFILE filename] [FORMAT format] [LIMIT n BY columns] 所有的子句都是可选的，除了SELECT之后的表达式列表（expr_list SAMPLE 子句 仅支持MergeTree引擎，且必须在创建时指定抽样表达式 123456789SELECT EventDate, count() * 10 AS PageViewsFROM datasets.hits_v1SAMPLE 0.1WHERE toDate(EventDate) &gt;= toDate(&#x27;2013-01-29&#x27;)GROUP BY EventDateORDER BY PageViews DESC LIMIT 1000 SAMPLE k 当k在0~1之间时，如上例k=0.1，即在10%上的数据执行 SAMPLE k 当k为正整数时，如上例k=1000，运行该查询最多为1000行 Join 语法 在clickhouse中可以用下图表示其支持的join语法 连接精度：ALL、ANY、ASOF 连接类型：外连接、内连接、交叉连接 连接精度 ALL：右表有多行数据与左表匹配，返回全部右表全部连接数据 ANY：右表有多行数据与左表匹配，返回第一行右表连接数据 ASOF：连接匹配键后的新增一个模糊匹配条件 1select a.id,a.name,b.rate,a.time,b.time from join_tb1 as a asof inner join join_tb2 on a.id=b.id and a.time=b.time 这里的a.id=b.id 是正常的连接键，a.time=b.time 是模糊连接条件，等价于 1a.id=b.id and a.time &gt;= b.time ASOF 也支持使用USING(xx,kk)方式，最后一个kk就是模糊连接条件，这里需要注意：asof_cloum 数据类型只能是整数、浮点、日期，且不能和join_key 字段相同。 PreWhere子句 where就不说了，和我们普通使用的where一样。prewhere是个好东西，它只能用在MergeTree系列的引擎中，使用了它就会预先读取prewhere指定列字段数据，用于数据过滤条件，待数据过滤之后再读取select声明列的字段以补全其属性。 在clickhoue中where 会被优化成PREWHERE，这取决于是否开启了优化，0关闭，1开启 1234567891011121302ee7ef9e35d :) select name,value from system.settings where name=&#x27;optimize_move_to_prewhere&#x27;;SELECT name, valueFROM system.settingsWHERE name = &#x27;optimize_move_to_prewhere&#x27;Query id: c88c90a3-8a4d-4d3d-ac4c-7ffbd4f31785┌─name──────────────────────┬─value─┐│ optimize_move_to_prewhere │ 0 │└───────────────────────────┴───────┘ 1select id,name from mer_tb1 prewhere id&gt;1; Group By 子句 Group By 本身不介绍了，主要介绍几个它的修饰符 With RollUp","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"clickHouse","slug":"clickHouse","permalink":"http://yoursite.com/tags/clickHouse/"}]},{"title":"clickHouseb表引擎","slug":"clickHouse表引擎","date":"2021-02-08T02:09:32.000Z","updated":"2022-11-22T01:27:53.462Z","comments":true,"path":"2021/02/08/clickHouse表引擎/","link":"","permalink":"http://yoursite.com/2021/02/08/clickHouse%E8%A1%A8%E5%BC%95%E6%93%8E/","excerpt":"","text":"表引擎犹如人的个性，决定着表的各种不可思议的特性。在Mysql中我们知道InnoDb支持事务，MyISAM支持读多场景。在clickhouse中表引擎大致可以分为6类，约有20多种。 表引擎分类 表引擎类型 引擎名称 MergeTree系列 MergeTree 、ReplacingMergeTree 、SummingMergeTree 、 AggregatingMergeTree CollapsingMergeTree 、 VersionedCollapsingMergeTree 、GraphiteMergeTree 外部存储类型 HDFS、Mysql、Kafa、JDBC、File 内存类型 Memory、Set、Join、Buffer 日志类型 TinyLog、StripLog、Log 接口类型 Merge、Dictionary、Distributed 其他类型 Live View、Null、URL MergeTree系列 MergeTree系列是官方主推的表引擎，使用场景广泛且很强大，名副其实的中流砥柱。 MergeTree 为MergeTree系列中最基础的表引擎，它在写入一批数据后，数据总是以数据片段的方式写入磁盘，且数据片段不可以修改。为了避免片段过多，clickhouse后通过后台线程，定期合并这些数据片段，属于相同分区的数据会被合并成一个新的片段，看看“合并树”这个名字是不是很贴切？ 语法 1234567891011121314CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2], ... INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1, INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2) ENGINE = MergeTree()ORDER BY expr[PARTITION BY expr][PRIMARY KEY expr][SAMPLE BY expr][TTL expr [DELETE|TO DISK &#x27;xxx&#x27;|TO VOLUME &#x27;xxx&#x27;], ...][SETTINGS name=value, ...] ENGINE：引擎名称。 ORDER BY：必选，col1,col2,… 默认情况它和主键一样（声明了它主键不需要设置） PARTITION BY：可选，分区键，合理使用分区有效减少数据查询扫描范围。 类型 样例数据 规则 分区表达式 分区ID 无分区键 直接取分区ID为all 无 all 整型 18,19,20 直接输出该整型的字符形式作为分区ID PARTITION BY Age 分区1:18；分区2:19；分区3:20 整型 ‘A0’,‘A1’,‘A3’ 直接输出该整型的字符形式作为分区ID PARTITION BY length(Code) 分区1:2 日期 2019-02-01,019-06-11 能够直接转换成YYYYMMDD整数，则使用该整数作为分区ID PARTITION BY EventTime 分区1:20190201；分区2:20190611 日期 2019-05-01,2019-06-11 函数输出作为分区ID PARTITION BY toYYYYMM(EventTime) 分区1:201905；分区2:201906 其他 ‘www.oldba.cn’ 128位的hash值作为分区ID PARTITION BY URL 分区1:15r515rs15gr15615wg5e5h5548h3045h PRIMARY KEY：可选，默认主键是排序字段。若想排序字段与主键不一致，可以单独指定主键字段。可以重复，不要与我们之前熟知的主键观念混淆！ SAMPLE BY：可选，采样字段，如果指定了该字段，那么主键中也必须包含该字段。比如SAMPLE BY intHash32(UserID) ORDER BY (CounterID, EventDate, intHash32(UserID)) TTL：可选，可以是列也可以是表级别。时间到达会删除设置作用域的数据。 SETTINGS：可选。参考 ​ 例子 表 12345678910CREATE TABLE emp_mergetree ( emp_id UInt16 COMMENT &#x27;员工id&#x27;, name String COMMENT &#x27;员工姓名&#x27;, work_place String COMMENT &#x27;工作地点&#x27;, age UInt8 COMMENT &#x27;员工年龄&#x27;, depart String COMMENT &#x27;部门&#x27;, salary Decimal32(2) COMMENT &#x27;工资&#x27; )ENGINE=MergeTree() ORDER BY emp_id PARTITION BY work_place; 新增数据 1234567891011INSERT INTO emp_mergetreeVALUES (1, &#x27;tom&#x27;, &#x27;上海&#x27;, 25, &#x27;技术部&#x27; , 20000), (2, &#x27;jack&#x27;, &#x27;上海&#x27;, 26, &#x27;人事部&#x27; , 10000);INSERT INTO emp_mergetreeVALUES (3, &#x27;bob&#x27;, &#x27;北京&#x27;, 33, &#x27;财务部&#x27; , 50000), (4, &#x27;tony&#x27;, &#x27;杭州&#x27;, 28, &#x27;销售事部&#x27; , 50000); 查看 123456789101112131415161718c4b99140ca5a :) select * from emp_mergetree;SELECT *FROM emp_mergetreeQuery id: 03d8ab75-0bdb-4f42-9851-20cf09760fa3┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 20000.00 ││ 2 │ jack │ 上海 │ 26 │ 人事部 │ 10000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 3 │ bob │ 北京 │ 33 │ 财务部 │ 50000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart───┬───salary─┐│ 4 │ tony │ 杭州 │ 28 │ 销售事部 │ 50000.00 │└────────┴──────┴────────────┴─────┴──────────┴──────────┘ 分成了三段，也就是3分分区（城市分区）。我们再来看看磁盘上的数据文件。 12345drwxr-x--- 2 clickhouse clickhouse 64 Feb 8 04:09 detached-rw-r----- 1 root root 1 Feb 8 04:09 format_version.txtdrwxr-x--- 11 root root 352 Feb 8 04:09 40d45822dbd7fa81583d715338929da9_1_1_0drwxr-x--- 11 root root 352 Feb 8 04:10 a6155dcc1997eda1a348cd98b17a93e9_2_2_0drwxr-x--- 11 root root 352 Feb 8 04:10 1c89a3ba9fe5fd53379716a776c5ac34_3_3_0 分区合并 12INSERT INTO emp_mergetreeVALUES (5,&#x27;robin&#x27;,&#x27;北京&#x27;,35,&#x27;财务部&#x27;,50000),(6,&#x27;lilei&#x27;,&#x27;北京&#x27;,38,&#x27;销售事部&#x27;,50000); 123456drwxr-x--- 2 clickhouse clickhouse 64 Feb 8 04:09 detached-rw-r----- 1 root root 1 Feb 8 04:09 format_version.txtdrwxr-x--- 11 root root 352 Feb 8 04:09 40d45822dbd7fa81583d715338929da9_1_1_0drwxr-x--- 11 root root 352 Feb 8 04:10 a6155dcc1997eda1a348cd98b17a93e9_2_2_0drwxr-x--- 11 root root 352 Feb 8 04:10 1c89a3ba9fe5fd53379716a776c5ac34_3_3_0drwxr-x--- 11 root root 352 Feb 8 04:19 a6155dcc1997eda1a348cd98b17a93e9_4_4_0 123456789101112131415161718192021c4b99140ca5a :) select * from emp_mergetree;SELECT *FROM emp_mergetreeQuery id: 4fdc0faf-411f-476f-a001-c9b449e8dc4d┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 3 │ bob │ 北京 │ 33 │ 财务部 │ 50000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 20000.00 ││ 2 │ jack │ 上海 │ 26 │ 人事部 │ 10000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart───┬───salary─┐│ 4 │ tony │ 杭州 │ 28 │ 销售事部 │ 50000.00 │└────────┴──────┴────────────┴─────┴──────────┴──────────┘┌─emp_id─┬─name──┬─work_place─┬─age─┬─depart───┬───salary─┐│ 5 │ robin │ 北京 │ 35 │ 财务部 │ 50000.00 ││ 6 │ lilei │ 北京 │ 38 │ 销售事部 │ 50000.00 │└────────┴───────┴────────────┴─────┴──────────┴──────────┘ 数据文件和sql都可以看到新增的2条“北京”记录没有先前的“北京”分区合并在一起，其实clickhouse在每一次新增（insert语句）都会生成一批新的分区目录，即不同批次写入的数据属于同一分区，也会生成不同的分区目录。一般在10-15分钟左右，clickhouse会通过后台线程来合并相同的分区数据，会生成一个新的分区目录，此刻原来的分区目录还在（一般约8分钟左右会被后台线程删除）。当然，可以手动执行optimize语句执行合并操作。 12345678910111213141516171819202122232425262728293031// optimize 语句OPTIMIZE TABLE emp_mergetree PARTITION &#x27;北京&#x27;;// 查询，合并了c4b99140ca5a :) select * from emp_mergetree;SELECT *FROM emp_mergetreeQuery id: 0437cc61-34d5-4bcd-a984-b31d5cb589da┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 20000.00 ││ 2 │ jack │ 上海 │ 26 │ 人事部 │ 10000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart───┬───salary─┐│ 4 │ tony │ 杭州 │ 28 │ 销售事部 │ 50000.00 │└────────┴──────┴────────────┴─────┴──────────┴──────────┘┌─emp_id─┬─name──┬─work_place─┬─age─┬─depart───┬───salary─┐│ 3 │ bob │ 北京 │ 33 │ 财务部 │ 50000.00 ││ 5 │ robin │ 北京 │ 35 │ 财务部 │ 50000.00 ││ 6 │ lilei │ 北京 │ 38 │ 销售事部 │ 50000.00 │└────────┴───────┴────────────┴─────┴──────────┴──────────┘// 此时文件目录drwxr-x--- 2 clickhouse clickhouse 64 Feb 8 04:09 detached-rw-r----- 1 root root 1 Feb 8 04:09 format_version.txtdrwxr-x--- 11 root root 352 Feb 8 04:09 40d45822dbd7fa81583d715338929da9_1_1_0drwxr-x--- 11 root root 352 Feb 8 04:10 a6155dcc1997eda1a348cd98b17a93e9_2_2_0drwxr-x--- 11 root root 352 Feb 8 04:10 1c89a3ba9fe5fd53379716a776c5ac34_3_3_0drwxr-x--- 11 root root 352 Feb 8 04:19 a6155dcc1997eda1a348cd98b17a93e9_4_4_0drwxr-x--- 11 root root 352 Feb 8 04:26 a6155dcc1997eda1a348cd98b17a93e9_2_4_1 ReplacingMergeTree MergeTree的主要键是可以重复的，先告诉结论：ReplacingMergeTree，能够解决部分数据重复问题，但是不是绝对！ 语法 ​ [ver]：可选参数，列的版本，可以是UInt、Date或者DateTime类型的字段作为版本号。该参数决定了数据去重的方式。 ​ 当没有指定[ver]参数时，保留最新的数据；如果指定了具体的值，保留最大的版本数据。 1234567891011CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ...) ENGINE = ReplacingMergeTree([ver])[PARTITION BY expr][ORDER BY expr][PRIMARY KEY expr][SAMPLE BY expr][SETTINGS name=value, ...] 表 1234567891011CREATE TABLE emp_replacingmergetree ( emp_id UInt16 COMMENT &#x27;员工id&#x27;, name String COMMENT &#x27;员工姓名&#x27;, work_place String COMMENT &#x27;工作地点&#x27;, age UInt8 COMMENT &#x27;员工年龄&#x27;, depart String COMMENT &#x27;部门&#x27;, salary Decimal32(2) COMMENT &#x27;工资&#x27; )ENGINE=ReplacingMergeTree() ORDER BY emp_id PRIMARY KEY emp_id PARTITION BY work_place; 新增 123456INSERT INTO emp_replacingmergetreeVALUES (1,&#x27;tom&#x27;,&#x27;上海&#x27;,25,&#x27;技术部&#x27;,20000),(2,&#x27;jack&#x27;,&#x27;上海&#x27;,26,&#x27;人事部&#x27;,10000);INSERT INTO emp_replacingmergetreeVALUES (3,&#x27;bob&#x27;,&#x27;北京&#x27;,33,&#x27;财务部&#x27;,50000),(4,&#x27;tony&#x27;,&#x27;杭州&#x27;,28,&#x27;销售事部&#x27;,50000);INSERT INTO emp_replacingmergetreeVALUES (1,&#x27;tom&#x27;,&#x27;上海&#x27;,25,&#x27;技术部&#x27;,50000); 查询 1234567891011121314151617181920c4b99140ca5a :) select * from emp_replacingmergetreeSELECT *FROM emp_replacingmergetreeQuery id: df247445-68fc-4bec-99c2-b31596a09d18┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 50000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 3 │ bob │ 北京 │ 33 │ 财务部 │ 50000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart───┬───salary─┐│ 4 │ tony │ 杭州 │ 28 │ 销售事部 │ 50000.00 │└────────┴──────┴────────────┴─────┴──────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 20000.00 ││ 2 │ jack │ 上海 │ 26 │ 人事部 │ 10000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘ 合并 123456789101112131415161718optimize table emp_replacingmergetree final;c4b99140ca5a :) select * from emp_replacingmergetreeSELECT *FROM emp_replacingmergetreeQuery id: ee821fc1-fbfb-407d-9f5b-96faf50c3064┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 50000.00 ││ 2 │ jack │ 上海 │ 26 │ 人事部 │ 10000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart───┬───salary─┐│ 4 │ tony │ 杭州 │ 28 │ 销售事部 │ 50000.00 │└────────┴──────┴────────────┴─────┴──────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 3 │ bob │ 北京 │ 33 │ 财务部 │ 50000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘ 是不是emp_id=1 的2条记录合并了，但是我们的 ORDER BY emp_id 和 PRIMARY KEY emp_id 都是emp_id，到底是以哪个为准呢？我们测试下： 表 排序键：emp_id、name；主键：emp_id。 1234567891011CREATE TABLE emp_replacingmergetree1 ( emp_id UInt16 COMMENT &#x27;员工id&#x27;, name String COMMENT &#x27;员工姓名&#x27;, work_place String COMMENT &#x27;工作地点&#x27;, age UInt8 COMMENT &#x27;员工年龄&#x27;, depart String COMMENT &#x27;部门&#x27;, salary Decimal32(2) COMMENT &#x27;工资&#x27; )ENGINE=ReplacingMergeTree() ORDER BY (emp_id,name) -- 注意排序key是两个字段 PRIMARY KEY emp_id -- 主键是一个字段 PARTITION BY work_place; 新增 123456INSERT INTO emp_replacingmergetree1VALUES (1,&#x27;tom&#x27;,&#x27;上海&#x27;,25,&#x27;技术部&#x27;,20000),(2,&#x27;jack&#x27;,&#x27;上海&#x27;,26,&#x27;人事部&#x27;,10000);INSERT INTO emp_replacingmergetree1VALUES (3,&#x27;bob&#x27;,&#x27;北京&#x27;,33,&#x27;财务部&#x27;,50000),(4,&#x27;tony&#x27;,&#x27;杭州&#x27;,28,&#x27;销售事部&#x27;,50000);INSERT INTO emp_replacingmergetree1VALUES (1,&#x27;tom&#x27;,&#x27;上海&#x27;,25,&#x27;技术部&#x27;,50000),(1,&#x27;sam&#x27;,&#x27;上海&#x27;,25,&#x27;技术部&#x27;,20000); 查询 123456789101112131415161718192021c4b99140ca5a :) select * from emp_replacingmergetree1;SELECT *FROM emp_replacingmergetree1Query id: c7a04fa7-acd3-48b5-9d5f-1badab7401b5┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 20000.00 ││ 2 │ jack │ 上海 │ 26 │ 人事部 │ 10000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart───┬───salary─┐│ 4 │ tony │ 杭州 │ 28 │ 销售事部 │ 50000.00 │└────────┴──────┴────────────┴─────┴──────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 3 │ bob │ 北京 │ 33 │ 财务部 │ 50000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 1 │ sam │ 上海 │ 25 │ 技术部 │ 20000.00 ││ 1 │ tom │ 上海 │ 25 │ 技术部 │ 50000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘ 合并 1234567891011121314151617181920optimize table emp_replacingmergetree1 final;c4b99140ca5a :) select * from emp_replacingmergetree1;SELECT *FROM emp_replacingmergetree1Query id: 6a678a8c-46c5-46ab-a83e-9388b1326b65┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 3 │ bob │ 北京 │ 33 │ 财务部 │ 50000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart───┬───salary─┐│ 4 │ tony │ 杭州 │ 28 │ 销售事部 │ 50000.00 │└────────┴──────┴────────────┴─────┴──────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 1 │ sam │ 上海 │ 25 │ 技术部 │ 20000.00 ││ 1 │ tom │ 上海 │ 25 │ 技术部 │ 50000.00 ││ 2 │ jack │ 上海 │ 26 │ 人事部 │ 10000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘ 从结果来看是根据排序键合并的！但是我们的表是不是还有个分区，目前来看还是在一个分区内去重复。我们新增一条数据不在一个分区： 123456789101112131415161718192021222324// 在北京这个分区INSERT INTO emp_replacingmergetree1 VALUES (1,&#x27;tom&#x27;,&#x27;北京&#x27;,26,&#x27;技术部&#x27;,10000);// 合并optimize table emp_replacingmergetree1 final;// 查询c4b99140ca5a :) select * from emp_replacingmergetree1;SELECT *FROM emp_replacingmergetree1Query id: c37746b5-4792-4002-b77c-b11d64461902┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart───┬───salary─┐│ 4 │ tony │ 杭州 │ 28 │ 销售事部 │ 50000.00 │└────────┴──────┴────────────┴─────┴──────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 1 │ sam │ 上海 │ 25 │ 技术部 │ 20000.00 ││ 1 │ tom │ 上海 │ 25 │ 技术部 │ 50000.00 ││ 2 │ jack │ 上海 │ 26 │ 人事部 │ 10000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 1 │ tom │ 北京 │ 26 │ 技术部 │ 10000.00 ││ 3 │ bob │ 北京 │ 33 │ 财务部 │ 50000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘ 哦嚯，不在一个分区的重复数据没有被去重复！总结下ReplacingMergeTree引擎去重几个点： 重复数据的判断：以ORDER BY排序键为基准的，而不是PRIMARY KEY。 何时删除重复数据：在执行分区合并时，会触发删除重复数据。optimize的合并操作是在后台执行的，无法预测具体执行时间点，除非是手动执行。 何为重复数据：按照ORDER BY排序键，且在一个分区内数据。 去重策略：如果没有设置**[ver]版本号**，则保留同一组重复数据中的最新插入的数据； 如果设置了**[ver]版本号**，则保留同一组重复数据中ver字段取值最大的那一行。 SummingMergeTree 见名思意，求字段的sum，的确如此。这个引擎的作用类似Group By 语句，且它的Group By条件是预先确定的。 语法 12345678910CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ...) ENGINE = SummingMergeTree([columns]) -- 指定合并汇总字段[PARTITION BY expr][ORDER BY expr][SAMPLE BY expr][SETTINGS name=value, ...] 约束：要保证PRIMARY KEY expr指定的主键是ORDER BY expr 指定字段的前缀，比如 1234567-- 允许ORDER BY (A,B,C) PRIMARY KEY A -- 会报错-- DB::Exception: Primary key must be a prefix of the sorting keyORDER BY (A,B,C) PRIMARY KEY B 这种强制约束保障了即便在两者定义不同的情况下，主键仍然是排序键的前缀，不会出现索引与数据顺序混乱的问题。 表 1234567891011CREATE TABLE emp_summingmergetree ( emp_id UInt16 COMMENT &#x27;员工id&#x27;, name String COMMENT &#x27;员工姓名&#x27;, work_place String COMMENT &#x27;工作地点&#x27;, age UInt8 COMMENT &#x27;员工年龄&#x27;, depart String COMMENT &#x27;部门&#x27;, salary Decimal32(2) COMMENT &#x27;工资&#x27; )ENGINE=SummingMergeTree(salary) ORDER BY (emp_id,name) -- 注意排序key是两个字段 PRIMARY KEY emp_id -- 主键是一个字段 PARTITION BY work_place; 新增 1234567INSERT INTO emp_summingmergetreeVALUES (1,&#x27;tom&#x27;,&#x27;上海&#x27;,25,&#x27;技术部&#x27;,20000),(2,&#x27;jack&#x27;,&#x27;上海&#x27;,26,&#x27;人事部&#x27;,10000);INSERT INTO emp_summingmergetreeVALUES (3,&#x27;bob&#x27;,&#x27;北京&#x27;,33,&#x27;财务部&#x27;,50000),(4,&#x27;tony&#x27;,&#x27;杭州&#x27;,28,&#x27;销售事部&#x27;,50000);// 重复数据INSERT INTO emp_summingmergetreeVALUES (1,&#x27;tom&#x27;,&#x27;上海&#x27;,25,&#x27;信息部&#x27;,10000),(1,&#x27;tom&#x27;,&#x27;北京&#x27;,26,&#x27;人事部&#x27;,10000); 查询 1234567891011121314151617181920212223c4b99140ca5a :) select * from emp_summingmergetree;SELECT *FROM emp_summingmergetreeQuery id: b14b7fdf-c3a1-431b-965f-0963ae2b90ac┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 3 │ bob │ 北京 │ 33 │ 财务部 │ 50000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 20000.00 │ ---重复（上海分区）│ 2 │ jack │ 上海 │ 26 │ 人事部 │ 10000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart───┬───salary─┐│ 4 │ tony │ 杭州 │ 28 │ 销售事部 │ 50000.00 │└────────┴──────┴────────────┴─────┴──────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 1 │ tom │ 上海 │ 25 │ 信息部 │ 10000.00 │ ---重复（上海分区）└────────┴──────┴────────────┴─────┴────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 1 │ tom │ 北京 │ 26 │ 人事部 │ 10000.00 │ ---重复（北京分区）└────────┴──────┴────────────┴─────┴────────┴──────────┘ 合并 12345678910111213141516171819202122// 合并语句optimize table emp_summingmergetree final;// 查询结果c4b99140ca5a :) select * from emp_summingmergetree;SELECT *FROM emp_summingmergetreeQuery id: 727ec288-8d71-441e-b726-6e2f420acc0d┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart───┬───salary─┐│ 4 │ tony │ 杭州 │ 28 │ 销售事部 │ 50000.00 │└────────┴──────┴────────────┴─────┴──────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 30000.00 │ ---重复合并了（上海分区），salary=20000+10000│ 2 │ jack │ 上海 │ 26 │ 人事部 │ 10000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 1 │ tom │ 北京 │ 26 │ 人事部 │ 10000.00 │ ---重复没合并（北京分区）│ 3 │ bob │ 北京 │ 33 │ 财务部 │ 50000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘ Aggregatingmergetree 这个引擎有点像数据立方的意思，在合并数据分区的时候，按照数据预先定义的条件聚合数据，并通过二进制的格式存入表内。基本上可以认为它是SummingMergeTree的升级版。在合并时候，是先按照AggregateFunction定义规则计算哪些字段。 语法 12345678910CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ...) ENGINE = AggregatingMergeTree()[PARTITION BY expr][ORDER BY expr][SAMPLE BY expr][SETTINGS name=value, ...] 表 1234567891011CREATE TABLE emp_aggregatingmergeTree ( emp_id UInt16 COMMENT &#x27;员工id&#x27;, name String COMMENT &#x27;员工姓名&#x27;, work_place String COMMENT &#x27;工作地点&#x27;, age UInt8 COMMENT &#x27;员工年龄&#x27;, depart String COMMENT &#x27;部门&#x27;, salary AggregateFunction(sum,Decimal32(2)) COMMENT &#x27;工资&#x27; )ENGINE=AggregatingMergeTree() ORDER BY (emp_id,name) -- 注意排序key是两个字段 PRIMARY KEY emp_id -- 主键是一个字段 PARTITION BY work_place; AggregateFunction 是clickHouse中提供一种特殊的数据类型，它能够以二进制形式存储中间状态结果。在使用的时候需要注意，写入数据时候需要调用*State函数，在查询数据时候，需要调用相应的*Merge函数。譬如在这里*就是上文定义的sum。 在上面定义的表，emp_id,name就是聚合条件，等同于Group By emp_id, name，而salary是聚合字段，等同于 sum(salary) 新增 1234INSERT INTO TABLE emp_aggregatingmergeTreeSELECT 1,&#x27;tom&#x27;,&#x27;上海&#x27;,25,&#x27;信息部&#x27;,sumState(toDecimal32(10000,2));INSERT INTO TABLE emp_aggregatingmergeTreeSELECT 1,&#x27;tom&#x27;,&#x27;上海&#x27;,25,&#x27;信息部&#x27;,sumState(toDecimal32(20000,2)); 查询 1234567891011121314151617181920212223c4b99140ca5a :) SELECT:-] emp_id,:-] name ,:-] sumMerge(salary):-] FROM emp_aggregatingmergeTree:-] GROUP BY emp_id,name;SELECT emp_id, name, sumMerge(salary)FROM emp_aggregatingmergeTreeGROUP BY emp_id, nameQuery id: 671ffb85-2eb0-409e-be44-3caf38199b5d┌─emp_id─┬─name─┬─sumMerge(salary)─┐│ 1 │ tom │ 30000.00 │└────────┴──────┴──────────────────┘1 rows in set. Elapsed: 0.020 sec. 使用起来相当麻烦，新增和查询都和特立独行。其实，日常使用常结合物化视图一起。 反插一句视图，在clickhouse中有普通视图和物化视图，物化视图具有独立的存储，普通视图只是一层简单的查询代理。 1234// 普通视图CREATE VIEW [IF NOT EXISTS] [db.]table_name AS SELECT ...// 物化视图CREATE [MATERIALIZED] VIEW [IF NOT EXISTS] [db.]table_name [TO[db.]name] [ENGINE = engine] [POPULATE] AS SELECT ... POPULATE 这个修饰符决定了物化视图初始化策略： 有：创建视图的同时，会连带将源表中已经存在的数据一并导入。 无：创建视图后是没数据的，在此之后写入到源表的数据才会被同步到视图中。 AggregatingMergeTree通常作为物化视图的表引擎，与普通MergeTree搭配使用。 12345678910111213141516171819202122232425262728293031323334353637383940414243-- 创建一个MereTree引擎的明细表-- 用于存储全量的明细数据-- 对外提供实时查询CREATE TABLE emp_mergetree_base ( emp_id UInt16 COMMENT &#x27;员工id&#x27;, name String COMMENT &#x27;员工姓名&#x27;, work_place String COMMENT &#x27;工作地点&#x27;, age UInt8 COMMENT &#x27;员工年龄&#x27;, depart String COMMENT &#x27;部门&#x27;, salary Decimal32(2) COMMENT &#x27;工资&#x27; )ENGINE=MergeTree() ORDER BY (emp_id,name) PARTITION BY work_place; -- 创建一张物化视图-- 使用AggregatingMergeTree表引擎CREATE MATERIALIZED VIEW view_emp_aggENGINE = AggregatingMergeTree()PARTITION BY emp_idORDER BY (emp_id,name)AS SELECT emp_id, name, sumState(salary) AS salaryFROM emp_mergetree_baseGROUP BY emp_id,name;-- 向基础明细表emp_mergetree_base插入数据INSERT INTO emp_mergetree_baseVALUES (1,&#x27;tom&#x27;,&#x27;上海&#x27;,25,&#x27;技术部&#x27;,20000),(1,&#x27;tom&#x27;,&#x27;上海&#x27;,26,&#x27;人事部&#x27;,10000);-- 查询物化视图SELECT emp_id, name , sumMerge(salary) FROM view_emp_aggGROUP BY emp_id,name;-- 结果┌─emp_id─┬─name─┬─sumMerge(salary)─┐│ 1 │ tom │ 30000.00 │└────────┴──────┴──────────────────┘ CollapsingMergeTree 在大数据领域，删除/更新是一个大事，不可能像关系型数据库那样，一般采用以增代删措施。折叠树就是这样的表引擎。它支持行级数据修改和删除的表引擎。它通过定义一个sign标记位字段，记录数据行的状态。如果sign标记为1，则表示这是一行有效的数据；如果sign标记为-1，则表示这行数据需要被删除。当CollapsingMergeTree分区合并时，同一数据分区内，sign标记为1和-1的一组数据会被抵消删除。 语法 12345678910CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ...) ENGINE = CollapsingMergeTree(sign)[PARTITION BY expr][ORDER BY expr][SAMPLE BY expr][SETTINGS name=value, ...] 表 1234567891011CREATE TABLE emp_collapsingmergetree ( emp_id UInt16 COMMENT &#x27;员工id&#x27;, name String COMMENT &#x27;员工姓名&#x27;, work_place String COMMENT &#x27;工作地点&#x27;, age UInt8 COMMENT &#x27;员工年龄&#x27;, depart String COMMENT &#x27;部门&#x27;, salary Decimal32(2) COMMENT &#x27;工资&#x27;, sign Int8 )ENGINE=CollapsingMergeTree(sign) ORDER BY (emp_id,name) PARTITION BY work_place; 新增 123456789101112-- 插入新增数据,sign=1表示正常数据INSERT INTO emp_collapsingmergetree VALUES (1,&#x27;tom&#x27;,&#x27;上海&#x27;,25,&#x27;技术部&#x27;,20000,1);-- 更新上述的数据-- 首先插入一条与原来相同的数据(ORDER BY字段一致),并将sign置为-1INSERT INTO emp_collapsingmergetree VALUES (1,&#x27;tom&#x27;,&#x27;上海&#x27;,25,&#x27;技术部&#x27;,20000,-1);-- 再插入更新之后的数据INSERT INTO emp_collapsingmergetree VALUES (1,&#x27;tom&#x27;,&#x27;上海&#x27;,25,&#x27;技术部&#x27;,30000,1); 查询 123456789101112SELECT *FROM emp_collapsingmergetree┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┬─sign─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 30000.00 │ 1 │└────────┴──────┴────────────┴─────┴────────┴──────────┴──────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┬─sign─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 20000.00 │ -1 │└────────┴──────┴────────────┴─────┴────────┴──────────┴──────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┬─sign─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 20000.00 │ 1 │└────────┴──────┴────────────┴─────┴────────┴──────────┴──────┘ 合并 1234567891011// 合并optimize table emp_collapsingmergetree;// 查询c4b99140ca5a :) select * from emp_collapsingmergetreeSELECT *FROM emp_collapsingmergetree┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┬─sign─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 30000.00 │ 1 │└────────┴──────┴────────────┴─────┴────────┴──────────┴──────┘ 这里需要注意一点，CollapsingMergeTree 对于写入数据的顺序有严格要求，是它的一命门，先写入sing=1，再写入sing=-1，数据能够正常折叠，反之就会不行！具体示例就不演示了。另外，手动合并命令再生产环境不建议使用，取而代之可以使用having条件语句比较好。 123456789SELECT emp_id, name, sum(salary * sign)FROM emp_collapsingmergetreeGROUP BY emp_id, nameHAVING sum(sign) &gt; 0 VersionedCollapsingMergeTree CollapsingMergeTree对于数据写入有严格顺序要求，不然会出现异常，VersionedCollapsingMergeTree弥补了这一个缺陷。它使用version列来实现乱序情况下的数据折叠。 语法 12345678910CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ...) ENGINE = VersionedCollapsingMergeTree(sign, version)[PARTITION BY expr][ORDER BY expr][SAMPLE BY expr][SETTINGS name=value, ...] 表 123456789101112CREATE TABLE emp_versioned ( emp_id UInt16 COMMENT &#x27;员工id&#x27;, name String COMMENT &#x27;员工姓名&#x27;, work_place String COMMENT &#x27;工作地点&#x27;, age UInt8 COMMENT &#x27;员工年龄&#x27;, depart String COMMENT &#x27;部门&#x27;, salary Decimal32(2) COMMENT &#x27;工资&#x27;, sign Int8, version Int8 )ENGINE=VersionedCollapsingMergeTree(sign, version) ORDER BY (emp_id,name) PARTITION BY work_place; 新增 123456789 -- 先插入需要被删除的数据，即sign=-1的数据INSERT INTO emp_versioned VALUES (1,&#x27;tom&#x27;,&#x27;上海&#x27;,25,&#x27;技术部&#x27;,20000,-1,1);-- 再插入sign=1的数据INSERT INTO emp_versioned VALUES (1,&#x27;tom&#x27;,&#x27;上海&#x27;,25,&#x27;技术部&#x27;,20000,1,1);-- 在插入一个新版本数据INSERT INTO emp_versioned VALUES (1,&#x27;tom&#x27;,&#x27;上海&#x27;,25,&#x27;技术部&#x27;,30000,1,2); 查询 1234567891011121314151617181920212223242526272829303132333435363738394041424344c4b99140ca5a :) select * from emp_versionedSELECT *FROM emp_versionedQuery id: b4ffd96b-8405-47af-b298-e21999da215c┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┬─sign─┬─version─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 30000.00 │ 1 │ 2 │└────────┴──────┴────────────┴─────┴────────┴──────────┴──────┴─────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┬─sign─┬─version─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 20000.00 │ 1 │ 1 │└────────┴──────┴────────────┴─────┴────────┴──────────┴──────┴─────────┘┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┬─sign─┬─version─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 20000.00 │ -1 │ 1 │└────────┴──────┴────────────┴─────┴────────┴──────────┴──────┴─────────┘// HAVING 语句查询结果，正确c4b99140ca5a :) SELECT:-] emp_id,:-] name,:-] sum(salary * sign):-] FROM emp_versioned:-] GROUP BY:-] emp_id,:-] name:-] HAVING sum(sign) &gt; 0;SELECT emp_id, name, sum(salary * sign)FROM emp_versionedGROUP BY emp_id, nameHAVING sum(sign) &gt; 0Query id: 8bc238ff-dbe8-4ce9-9ee1-c7e331a87e0c┌─emp_id─┬─name─┬─sum(multiply(salary, sign))─┐│ 1 │ tom │ 30000.00 │└────────┴──────┴─────────────────────────────┘ 合并 123456789101112131415// 合并optimize table emp_versioned;// 查询c4b99140ca5a :) select * from emp_versionedSELECT *FROM emp_versionedQuery id: 4a0289ab-03f3-4859-868f-7aadcdd6f059┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┬─sign─┬─version─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 30000.00 │ 1 │ 2 │└────────┴──────┴────────────┴─────┴────────┴──────────┴──────┴─────────┘ 可见上面虽然在插入数据乱序的情况下，依然能够实现折叠的效果。之所以能够达到这种效果，是因为在定义version字段之后，VersionedCollapsingMergeTree会自动将version作为排序条件并增加到ORDER BY的末端，就上述的例子而言，最终的排序字段为ORDER BY emp_id,name，version desc。 GraphiteMergeTree 该引擎用来对 Graphite数据进行瘦身及汇总。对于想使用CH来存储Graphite数据的开发者来说可能有用。 如果不需要对Graphite数据做汇总，那么可以使用任意的CH表引擎；但若需要，那就采用 GraphiteMergeTree 引擎。它能减少存储空间，同时能提高Graphite数据的查询效率。我这里不具体研究。 总结 继承 MergeTree系列的引擎，都是以MergeTree引擎位基础，只是在merge合并阶段，调用各自的逻辑。可以借助Java里面的抽象类和实现类理解。 12345678910111213141516171819202122232425262728// MergeTree引擎public abstract class MergeTree &#123; public void write() &#123; &#125; public void optimize() &#123; &#125; public void drop() &#123; &#125; public void truncate() &#123; &#125; public void alter() &#123; &#125; protected void merge() &#123; // TODO 由其他系列引擎实现ReplacingMergeTree 、SummingMergeTree 、 AggregatingMergeTree CollapsingMergeTree 、 VersionedCollapsingMergeTree 、GraphiteMergeTree &#125;&#125; 组合 clickHouse作为一款OLAP数据库，没有理由不支持分布式部署，对应的引擎Replicated*，它组合上述介绍的7种引擎就带来clickHouse里面最重要的14种MergeTree引擎。Replicated*系列的引擎，借助Zookeeper的消息日志广播功能，实现副本实例之间数据同步功能。 外部存储类型 见名思意，外部存储类型的表引擎直接从从其他的存储系统读取数据，目前支持的引擎有：HDFS、Mysql、Kafa、JDBC、File。 HDFS 首先确认hdfs的Kerbros认证关闭了，目前clickhose的hdfs引擎还不支持kerberos认证。HDFS引擎有2种形式： 既负责读文件，又负责写文件 只负责读文件，文件写入工作则由其他外部系统完成 读写 类似hive 表 12345678910//- URI：HDFS文件路径//- format：文件格式，比如CSV、JSON、TSV等CREATE TABLE hdfs_engine_table( emp_id UInt16 COMMENT &#x27;员工id&#x27;, name String COMMENT &#x27;员工姓名&#x27;, work_place String COMMENT &#x27;工作地点&#x27;, age UInt8 COMMENT &#x27;员工年龄&#x27;, depart String COMMENT &#x27;部门&#x27;, salary Decimal32(2) COMMENT &#x27;工资&#x27;) ENGINE=HDFS(&#x27;hdfs://cdh03:8020/user/hive/hdfs_engine_table&#x27;, &#x27;CSV&#x27;); 新增 12INSERT INTO hdfs_engine_table VALUES (1,&#x27;tom&#x27;,&#x27;上海&#x27;,25,&#x27;技术部&#x27;,20000),(2,&#x27;jack&#x27;,&#x27;上海&#x27;,26,&#x27;人事部&#x27;,10000); 查询 1234567891011c4b99140ca5a :) select * from hdfs_engine_table;SELECT *FROM hdfs_engine_tableQuery id: b58a1306-fe43-4092-a911-7812e578a1ad┌─emp_id─┬─name─┬─work_place─┬─age─┬─depart─┬───salary─┐│ 1 │ tom │ 上海 │ 25 │ 技术部 │ 20000.00 ││ 2 │ jack │ 上海 │ 26 │ 人事部 │ 10000.00 │└────────┴──────┴────────────┴─────┴────────┴──────────┘ HDFS 文件 自动生成了目录 123[deploy@cdh ~]$ hdfs dfs -cat /user/hive/hdfs_engine_table1,&quot;tom&quot;,&quot;上海&quot;,25,&quot;技术部&quot;,20000.002,&quot;jack&quot;,&quot;上海&quot;,26,&quot;人事部&quot;,10000.00 只读 这种形式类似于hive的外挂表，由其它系统直接将文件直接写入HDFS，通过参数hdsfs_ui和format与HDFS的文件路径、文件格式建立映射，其中hdfs_uri支持以下几种常见的配置方法： 绝对路径：会指定路径上的单个文件，例如hdfs://mycluster/1.txt 通配符：匹配所有字符，例如hdfs://mycluster/ * ，会读取hdfs://mycluster/路径下的所有文件 ？通配符：匹配单个字符，例如hdfs://mycluster/test_？.txt会匹配所有test_？.txt的文件，？代表任意字符 {M…N}数字区间：匹配指定数字的文件，例如路径hdfs://mycluster/test_{1…3}.txt，则会读取hdfs://mycluster/路径下的文件test_1.txt,test_2.txt,test_3.txt 准备几个csv文件，并上传到hdfs 123scp ~/Desktop/city_* deploy@192.168.90.71:/home/deployhdfs dfs -mkdir -p /clickhouse/cityhdfs dfs -put -f /home/deploy/city_* /clickhouse/city 表 12345CREATE TABLE city( id String COMMENT &#x27;id&#x27;, code String COMMENT &#x27;城市code&#x27;, name String COMMENT &#x27;城市名&#x27;) ENGINE=HDFS(&#x27;hdfs://192.168.90.71:8020/clickhouse/city/*&#x27;, &#x27;CSV&#x27;); 查询 12345678910111213141516171819202122232425c4b99140ca5a :) select * from city;SELECT *FROM cityQuery id: 3fe5cf92-6fcf-4ce3-adc1-2d14e775517b┌─id─┬─code─┬─name──────┐│ 1 │ 001 │ beijing ││ 2 │ 002 │ shanghai ││ 3 │ 003 │ shenzhen ││ 4 │ 004 │ guangzhou │└────┴──────┴───────────┘┌─id─┬─code─┬─name─────┐│ 9 │ 009 │ wuhan ││ 10 │ 010 │ nanchang ││ 11 │ 011 │ xiamen ││ 12 │ 012 │ zhuhai │└────┴──────┴──────────┘┌─id─┬─code─┬─name──────┐│ 5 │ 005 │ nanjing ││ 6 │ 006 │ hangzhou ││ 7 │ 007 │ hefei ││ 8 │ 008 │ zhegnzhou │└────┴──────┴───────────┘ Mysql Mysql表引擎可以与Mysql数据库中的数据表建立映射，并通过SQL向远程发起查询，包含Select、Insert。但是不支持UPDATE和DELETE操作 语法 123456CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2], ...) ENGINE = MySQL(&#x27;host:port&#x27;, &#x27;database&#x27;, &#x27;table&#x27;, &#x27;user&#x27;, &#x27;password&#x27;[, replace_query, &#x27;on_duplicate_clause&#x27;]); 表 123456789CREATE TABLE bas_obj_type( id Int32 COMMENT &#x27;主键&#x27;, ctime DateTime COMMENT &#x27;创建时间&#x27;, mtime DateTime COMMENT &#x27;修改时间&#x27;, obj_type_code UInt8 COMMENT &#x27;对象分类code，即区分在实体关系哪个分类下 &#x27;, name String COMMENT &#x27;名称&#x27;, invalid String COMMENT &#x27; Y(无效) N(有效) &#x27;) ENGINE = MySQL( &#x27;192.168.90.54:3306&#x27;, &#x27;tagmodel&#x27;, &#x27;bas_obj_type&#x27;, &#x27;root&#x27;, &#x27;root@1298&#x27;); 查询 12345678910111213c4b99140ca5a :) select * from bas_obj_type;SELECT *FROM bas_obj_typeQuery id: b6ce5511-e1bb-40a5-bab1-8ffd8d57a9ea┌────────id─┬───────────────ctime─┬───────────────mtime─┬─obj_type_code─┬─name─┬─invalid─┐│ 798294144 │ 2019-08-13 15:02:31 │ 2019-08-13 15:02:31 │ 1 │ 人 │ Y ││ 799211648 │ 2019-08-13 15:02:31 │ 2019-08-13 15:02:31 │ 2 │ 物 │ Y ││ 799801472 │ 2019-08-13 15:02:31 │ 2019-08-13 15:02:31 │ 3 │ 关系 │ N ││ 804671985 │ 2019-10-15 15:02:31 │ 2019-10-15 15:02:31 │ 4 │ 实体 │ N │└───────────┴─────────────────────┴─────────────────────┴───────────────┴──────┴─────────┘ JDBC Mysql 表引擎只能对接mysql数据库，JDBC表引擎不仅可以对接MySQL数据库，还能够与PostgreSQL、SQLite、H2、ES、ZK等多种类型数据库。但是JDBC表引擎需要借助一个叫clickhouse-jdbc-bridge的查询代理服务。下面演示JDBC连接greenplum为例子： 源码编译jar 123456789101112mkdir clickhouse_jdbc &amp;&amp; cd clickhouse_jdbcgit clone https://github.com/ClickHouse/clickhouse-jdbc-bridge.gitcd clickhouse-jdbc-bridgemvn clean package -Dmaven.test.skip=true// 下载postgresql 的jdbc驱动 到drivers 目录mkdir drivers &amp;&amp; cd driverswget https://jdbc.postgresql.org/download/postgresql-42.2.19.jar// 查看clickhouse-jdbc-bridge-2.0.1-SNAPSHOT.jarclickhouse-jdbc-bridge-2.0.1-SNAPSHOT-shaded.jarpostgresql-42.2.19.jar 启动 12345java -jar clickhouse-jdbc-bridge-2.0.1-SNAPSHOT-shaded.jar --driver-path ./drivers --listen-host 192.168.90.147···// 9019端口 Server http://0.0.0.0:9019 started in 818 ms ··· 新增代理服务地址 123456789// 查看ck的配置文件本地数据卷docker inspect ck | grep Mounts 500// 编辑 config.xml&lt;jdbc_bridge&gt; &lt;host&gt;192.168.90.147&lt;/host&gt; &lt;port&gt;9019&lt;/port&gt;&lt;/jdbc_bridge&gt;// 重启服务docker restart ck 语法 12345CREATE TABLE [IF NOT EXISTS] [db.]table_name( columns ...)ENGINE = JDBC(dbms_uri, database/schema(可省略), tablename) 查询 1234567891011121314151617c4b99140ca5a :) select project_name from jdbc(&#x27;jdbc:postgresql://192.168.90.147:5432/tag_hub?user=gpadmin&amp;password=gpadmin&#x27;, &#x27;event&#x27;, &#x27;zt_cust_complaints_event_data&#x27;) limit 0,5;SELECT project_nameFROM jdbc(&#x27;jdbc:postgresql://192.168.90.147:5432/tag_hub?user=gpadmin&amp;password=gpadmin&#x27;, &#x27;event&#x27;, &#x27;zt_cust_complaints_event_data&#x27;)LIMIT 0, 10Query id: b6ff9b9b-80f0-4c98-a526-06a8967b0980┌─project_name─┐│ 澳洲假日 ││ 滟澜山 ││ 欢乐颂 ││ 桂青园 ││ 金桥水岸 │└──────────────┘10 rows in set. Elapsed: 0.654 sec. 新建映射表 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// gp上新建CREATE TABLE event.zt_ehr_position_event_data ( ehr_id character varying(128) DEFAULT NULL::character varying PRIMARY KEY, ehr_name character varying(128), lift_time date, lift_reason character varying(255), pre_elevating_grade character varying(255), post_rise_fall_grade character varying(255), ctime character varying(64) NOT NULL, mtime character varying(64) NOT NULL);COMMENT ON COLUMN event.zt_ehr_position_event_data.ehr_id IS &#x27;id&#x27;;COMMENT ON COLUMN event.zt_ehr_position_event_data.ehr_name IS &#x27;姓名&#x27;;COMMENT ON COLUMN event.zt_ehr_position_event_data.lift_time IS &#x27;升降时间&#x27;;COMMENT ON COLUMN event.zt_ehr_position_event_data.lift_reason IS &#x27;升降原因&#x27;;COMMENT ON COLUMN event.zt_ehr_position_event_data.pre_elevating_grade IS &#x27;升降前原因&#x27;;COMMENT ON COLUMN event.zt_ehr_position_event_data.post_rise_fall_grade IS &#x27;升降后等级&#x27;;COMMENT ON COLUMN event.zt_ehr_position_event_data.ctime IS &#x27;创建时间&#x27;;COMMENT ON COLUMN event.zt_ehr_position_event_data.mtime IS &#x27;修改时间&#x27;;// ck上新建CREATE TABLE zt_ehr_position_event_data ( ehr_id String, ehr_name String, lift_time DateTime, lift_reason String, pre_elevating_grade String, post_rise_fall_grade String, ctime String, mtime String) ENGINE = JDBC(&#x27;jdbc:postgresql://192.168.90.147:5432/tag_hub?user=gpadmin&amp;password=gpadmin&#x27;, &#x27;event&#x27;, &#x27;zt_ehr_position_event_data&#x27;);// 查看c4b99140ca5a :) select ehr_name from zt_ehr_position_event_data limit 0,10;SELECT ehr_nameFROM zt_ehr_position_event_dataLIMIT 0, 10Query id: e7255931-2004-49c5-83ea-ca10720e4b7e┌─ehr_name─┐│ 常津铭 ││ 骆艳飞 ││ 唐平飞 ││ 李朋霖 ││ 魏贤鹏 ││ 蒋文龙 ││ 李小阳 ││ 梁小华 ││ 邵杏元 ││ 林勇强 │└──────────┘10 rows in set. Elapsed: 0.122 sec. Kafka docker-compose.yml 12345678910111213141516171819202122232425262728version: &#x27;3&#x27;services: zookeeper: image: wurstmeister/zookeeper ## 镜像 ports: - &quot;2181:2181&quot; ## 对外暴露的端口号 kafka: image: wurstmeister/kafka ## 镜像 volumes: - /etc/localtime:/etc/localtime ## 挂载位置 ports: - &quot;9092:9092&quot; environment: KAFKA_ADVERTISED_HOST_NAME: 127.0.0.1 ## 修改:宿主机IP KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 ## kafka运行是基于zookeeper的 KAFKA_ADVERTISED_PORT: 9092 KAFKA_LOG_RETENTION_HOURS: 120 KAFKA_MESSAGE_MAX_BYTES: 10000000 KAFKA_REPLICA_FETCH_MAX_BYTES: 10000000 KAFKA_GROUP_MAX_SESSION_TIMEOUT_MS: 60000 KAFKA_NUM_PARTITIONS: 3 KAFKA_DELETE_RETENTION_MS: 1000 kafka-manager: image: sheepkiller/kafka-manager ## 镜像：开源的web管理kafka集群的界面 environment: ZK_HOSTS: 127.0.0.1 ## 修改:宿主机IP ports: - &quot;9001:9000&quot; ## 暴露端口 启动 123456docker-compose up -dCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5d5f44d061ef wurstmeister/kafka &quot;start-kafka.sh&quot; 20 seconds ago Up 18 seconds 0.0.0.0:9092-&gt;9092/tcp kafka_kafka_1abacd2902117 wurstmeister/zookeeper &quot;/bin/sh -c &#x27;/usr/sb…&quot; 20 seconds ago Up 18 seconds 22/tcp, 2888/tcp, 3888/tcp, 0.0.0.0:2181-&gt;2181/tcp kafka_zookeeper_105e0725818fa sheepkiller/kafka-manager &quot;./start-kafka-manag…&quot; 20 seconds ago Up 18 seconds 0.0.0.0:9001-&gt;9000/tcp kafka_kafka-manager_1 测试kafka 123456789101112// 进入docker exec -it kafka_kafka_1 /bin/bash// 创建 test 的topic$KAFKA_HOME/bin/kafka-topics.sh --create --topic test --zookeeper kafka_zookeeper_1:2181 --replication-factor 1 --partitions 1// 查看 topic$KAFKA_HOME/bin/kafka-topics.sh --list --zookeeper kafka_zookeeper_1:2181// 发布几条消息$KAFKA_HOME/bin/kafka-console-producer.sh --topic=test --broker-list kafka_kafka_1:9092&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;hi,kafka&quot;&#125;&#123;&quot;id&quot;:2,&quot;name&quot;:&quot;hi,clickhouse&quot;&#125;// 消费消息$KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server kafka_kafka_1:9092 --from-beginning --topic test 使用 通常是结合物化视图一起使用。 首先创建一张Kafka表引擎的表，用于从Kafka中读取数据 然后再创建一张普通表引擎的表，比如MergeTree，面向终端用户使用 最后创建物化视图，用于将Kafka引擎表实时同步到终端用户所使用的表中 12345678910111213141516171819202122232425 -- 创建Kafka引擎表 CREATE TABLE kafka_table_consumer ( id UInt64, name String ) ENGINE = Kafka() SETTINGS kafka_broker_list = &#x27;192.168.90.147:9092&#x27;, kafka_topic_list = &#x27;test&#x27;, kafka_group_name = &#x27;group1&#x27;, kafka_format = &#x27;JSONEachRow&#x27;; -- 创建一张终端用户使用的表CREATE TABLE kafka_table_mergetree ( id UInt64 , name String )ENGINE=MergeTree() ORDER BY id; -- 创建物化视图，同步数据CREATE MATERIALIZED VIEW consumer TO kafka_table_mergetree AS SELECT id,name FROM kafka_table_consumer ; -- 查询select * from kafka_table_mergetree; 内存类型 Memory Memory内存表引擎，顾名思义数据存在内存中，在重启后数据丢失，用户一般做测试用，其实在ck内部它会作为集群间分发数据的存储载体使用，例如在分布式IN查询的场合中，会利用Memory临时保存IN子句的查询结果，并通过网络将它传输到远端节点。 语法 1234CREATE TABLE memory_table(id UInt8,name String)ENGINE=Memory(); Set Set引擎的表是具有物理存储的，所以断电不掉数据。Set数据结构，是天然去重复，在clickhouse中Set表引擎也是一样，它会对于重复插入的数据忽略。需要注意：Set引擎的表不能直接Select，它一般被用做in语句，类似如下使用： 表 123CREATE TABLE id_set_table(id UInt8)ENGINE=Set() 新增 1insert into id_set_table values(1),(2)(4) 查询 12345678910111202ee7ef9e35d :) select arrayJoin([1,2,3,4]) as id where id in id_set_table;SELECT arrayJoin([1, 2, 3, 4]) AS idWHERE id IN (id_set_table)Query id: 81d4a7da-31dd-422c-98bf-b5831b880ebc┌─id─┐│ 1 ││ 2 ││ 4 │└────┘ Set表引擎存储结构由2部分构成，分别是： [num].bin数据文件：保存的是所有列的字段数据。其中num是自增id，从1开始，随着每一次的写入，会自增新生成一个bin。 tmp临时目录：数据文件手写被写入到这个目录，当一批数据写入完毕之后，数据文件会被移除这个目录。 Join 顾名思义，Join引擎主战场是在Join查询，它与Set引擎一样，存储结构也是有[num].bin和tmp，数据先写入内存，在刷新到磁盘。 它可以直接被select。 语法 1ENGINE=Join(join_strictness,join_type,keys) join_strictness：连接精度，决定在Join查询时候使用何种策略，目前支持：ALL、ANY、ASOF三种。如果不主动声明则默认是ALL。 join_type：连接类型（外连接 内连接和交叉连接）。选项可以是LEFT、INNER、RIGHT、FULL。注意：当join_strictness=ANY时候，join_key重复的会被忽略。 join_key：连接键。 表 1234CREATE TABLE id_val(`id` UInt32, `val` UInt32) ENGINE = TinyLogCREATE TABLE id_value_join(`id` UInt32, `val` UInt8) ENGINE = Join(ANY, LEFT, id) 新增 1234INSERT INTO id_val VALUES (1,11)(2,12)(3,13)// id=1 的记录2条INSERT INTO id_value_join VALUES (1,21)(1,22)(3,23) 查看 join_strictness=ANY，id=1 只保留了一条记录。 1234567891011121302ee7ef9e35d :) select * from id_value_join;SELECT *FROM id_value_joinQuery id: 97b7252f-713a-4476-a1e5-ceb849051b08┌─id─┬─value─┐│ 3 │ 23 ││ 1 │ 21 │└────┴───────┘2 rows in set. Elapsed: 0.006 sec. join 查询 12345678910111213141502ee7ef9e35d :) SELECT * FROM id_val ANY LEFT JOIN id_value_join USING (id);SELECT *FROM id_valANY LEFT JOIN id_value_join USING (id)Query id: c1c4dd6a-dda4-450c-957f-1c228dbf77fe┌─id─┬─val─┬─value─┐│ 1 │ 11 │ 21 ││ 2 │ 12 │ 0 ││ 3 │ 13 │ 23 │└────┴─────┴───────┘3 rows in set. Elapsed: 0.008 sec. Buffer Buffer引擎表完全使用内存装载数据，它的使用场景为：并发写入很高，容易导致写入速度大于合并的速度，会产生很多小的分区。这个时候引入Buffer引擎表很合适。 语法 1ENGINE=Buffer(database,table,num_layers,min_time,max_time,min_rows,max_rows,min_bytes,max_bytes) 参数 说明 database 目标表的数据库 table 目标表 num_layers 线程数字，ck会按照这个数字开启线程数，刷新数据到目标表 刷新极值3组条件 min_time、max_time；min_rows、max_rows；min_bytes、max_bytes 触发刷新时机1 三组条件最小阀值都满足 触发刷新时机2 三组条件最大值至少一个满足 触发刷新时机3 写入一批数据行数&gt;max_rows 或者 数据体量&gt;max_bytes 注意：三组条件在每一个num_layers 都是单独计算。假设num_layers=16 ，则Buffer引擎的表会开启最多16哥线程处理刷新数据。他们会以轮训方式接受请求，在每个线程内会独立判断上述表内条件。比如：max_bytes=10000000(100M)，num_layers=16，那么这个表可以同时处理约16G的数据。 表 1234// 目标表create table memory_1(id UInt32)ENGINE=MergeTree() order by id;// buffer 表create table buffer_to_memory as memory_1 ENGINe=Buffer(default,memory_1,16,10,100,10000,100000,10000000,100000000); 新增100万条到buffer_to_memory 1insert into buffer_to_memory select number from numbers(1000000); 查看memory_1表数据条数 1select count(1) from memory_1; 以上只是演示了触发更新的语句，可以试试一次性没达到100W条，memory_1表数据会不会立刻有？这里就不演示了。 日志类型 一般数据量较小（100万以下），“一次写入，多次查询”模式，使用它作为表引擎是个不错的选择。这个家族的引擎有一些共性：不支持索引，分区，不支持并发读写。 TinyLog 性能较低，存储结构有：数据文件（xx.bin 每列一个）；元数据文件（size.json）记录是各个bin 文件的数据大小。 1create table tinyLog_tb(id UInt8)engine=TinyLog(); StripeLog 性能较好，存储结构有：数据文件(data.bin)；数据标记（index.mrk）保存了数据在data.bin 位置，可以并行读取data.bin 压缩的数据块；元数据文件（size.json）记录data.bin 和index.mrk大小信息 1create table stripeLog_tb(id UInt8)engine=StripeLog(); Log 性能好，它结合了上述2个表引擎的长处，存储结构有：数据文件（xx.bin 每列一个）；数据标记（_marks.mrk）保存了数据在各个.bin 位置，可以并行读取.bin内压缩的数据；元数据文件（size.json）记录各个.bin 和_marks.mrk大小信息 1create table log_tb(id UInt8)engine=Log(); 接口类型 这类引擎本身不存储数据，而是像粘合剂一样整合其他的数据表。使用这类引擎不必担心底层的复杂性。 Merge 不存之任何数据，也不支持插入，它的作用就是查询合并，不过它有几个约束条件，看看它的引擎声明就一目了然 1ENGINE=Merge(database,table_name) 在同一个database中 table_name 支持正则，表之间拥有相同的表结构，但是表引擎可以不同。 有这样一个场景，有2张表，都是存储了各自年份的数据，假如有2019和2020年2个表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 2019 年表create table 2019_tb(id UInt8,code String,time DateTime)Engine=MergeTree() partition by toYYYYMM(time) order by id;insert into 2019_tb values(1,&#x27;2019&#x27;,&#x27;2019-01-01 10:00:00&#x27;),(2,&#x27;2019&#x27;,&#x27;2019-02-01 10:00:00&#x27;),(3,&#x27;2019&#x27;,&#x27;2019-03-01 10:00:00&#x27;),(4,&#x27;2019&#x27;,&#x27;2019-04-01 10:00:00&#x27;),(5,&#x27;2019&#x27;,&#x27;2019-05-01 10:00:00&#x27;);// 2020 年表create table 2020_tb(id UInt8,code String,time DateTime)Engine=MergeTree() partition by toYYYYMM(time) order by id;insert into 2020_tb values(1,&#x27;2020&#x27;,&#x27;2020-01-01 10:00:00&#x27;),(2,&#x27;2020&#x27;,&#x27;2020-02-01 10:00:00&#x27;),(3,&#x27;2020&#x27;,&#x27;2020-03-01 10:00:00&#x27;),(4,&#x27;2020&#x27;,&#x27;2020-04-01 10:00:00&#x27;),(5,&#x27;2020&#x27;,&#x27;2020-05-01 10:00:00&#x27;);// Merge 表create table merge_tb as 2019_tb engine=Merge(default,&#x27;^20&#x27;);//查询02ee7ef9e35d :) select _table,* from merge_tb;SELECT _table, *FROM merge_tbQuery id: 466f10eb-2f79-4daa-84dc-05ba106ea476┌─_table──┬─id─┬─code─┬────────────────time─┐│ 2019_tb │ 3 │ 2019 │ 2019-03-01 10:00:00 │└─────────┴────┴──────┴─────────────────────┘┌─_table──┬─id─┬─code─┬────────────────time─┐│ 2019_tb │ 5 │ 2019 │ 2019-05-01 10:00:00 │└─────────┴────┴──────┴─────────────────────┘┌─_table──┬─id─┬─code─┬────────────────time─┐│ 2019_tb │ 2 │ 2019 │ 2019-02-01 10:00:00 │└─────────┴────┴──────┴─────────────────────┘┌─_table──┬─id─┬─code─┬────────────────time─┐│ 2020_tb │ 5 │ 2020 │ 2020-05-01 10:00:00 │└─────────┴────┴──────┴─────────────────────┘┌─_table──┬─id─┬─code─┬────────────────time─┐│ 2020_tb │ 2 │ 2020 │ 2020-02-01 10:00:00 │└─────────┴────┴──────┴─────────────────────┘┌─_table──┬─id─┬─code─┬────────────────time─┐│ 2019_tb │ 4 │ 2019 │ 2019-04-01 10:00:00 │└─────────┴────┴──────┴─────────────────────┘┌─_table──┬─id─┬─code─┬────────────────time─┐│ 2020_tb │ 3 │ 2020 │ 2020-03-01 10:00:00 │└─────────┴────┴──────┴─────────────────────┘┌─_table──┬─id─┬─code─┬────────────────time─┐│ 2019_tb │ 1 │ 2019 │ 2019-01-01 10:00:00 │└─────────┴────┴──────┴─────────────────────┘┌─_table──┬─id─┬─code─┬────────────────time─┐│ 2020_tb │ 4 │ 2020 │ 2020-04-01 10:00:00 │└─────────┴────┴──────┴─────────────────────┘┌─_table──┬─id─┬─code─┬────────────────time─┐│ 2020_tb │ 1 │ 2020 │ 2020-01-01 10:00:00 │└─────────┴────┴──────┴─────────────────────┘ Dictionary Dictionary 引擎将字典数据展示为一个ClickHouse的表。首先我们配置一个数据字典参考官网 表 1create table products (product_id UInt64, title String) Engine = Dictionary(products); 查询 123456789101112131415SELECT name, type, key, attribute.names, attribute.types, bytes_allocated, element_count, sourceFROM system.dictionariesWHERE name = &#x27;products&#x27;┌─name─────┬─type─┬─key────┬─attribute.names─┬─attribute.types─┬─bytes_allocated─┬─element_count─┬─source──────────┐│ products │ Flat │ UInt64 │ [&#x27;title&#x27;] │ [&#x27;String&#x27;] │ 23065376 │ 175032 │ ODBC: .products │└──────────┴──────┴────────┴─────────────────┴─────────────────┴─────────────────┴───────────────┴─────────────────┘ 其他类型 Live View 这个不是一种表引擎，而是一个视图（19.14后新增），它能够监听一条SQL查询结果，可以及时发出相应。 查看功能是否开启 1234567select name,value from system.settings where name like &#x27;%allow_experimental_live_view&#x27;┌─name─────────────────────────┬─value─┐│ allow_experimental_live_view │ 1 │└──────────────────────────────┴───────┘//没开启set allow_experimental_live_view=1; 监听目标表 12create table listen_tb(id UInt64) ENGINE=Log();create live view lv_listen as select count(1) from listen_tb; 开启监控 123456789// 若有数据insert 到listen_tb version 就会+1WATCH lv_listenQuery id: 10c9b281-6043-444e-b118-1d18bc2f0368┌─count(1)─┬─_version─┐│ 0 │ 1 │└──────────┴──────────┘↘ Progress: 1.00 rows, 16.00 B (9.02 rows/s., 144.40 B/s.) Null 这个表引擎与Unix系统中的/dev/null 很像，是个黑洞，里面什么也看不见。常见的使用方式是，在使用物化视图时候，源头的物理表不希望保存任何数据，就使用Null引擎。 表 12create table null_tb(id UInt8)ENGINE=Null();create materialized view v_null_tb ENGINE=TinyLog as select * from null_tb; 新增 1insert into null_tb values(1),(2); 查询 123456789101112131415161718192021222324// 源头表没数据02ee7ef9e35d :) select * from null_tb;SELECT *FROM null_tbQuery id: 8b4b4bb9-e4e6-41ea-86dc-4c4f289d5984Ok.0 rows in set. Elapsed: 0.005 sec.// 物化视图有数据02ee7ef9e35d :) select * from v_null_tb;SELECT *FROM v_null_tbQuery id: a5186cea-9670-4c2a-937c-cdee3a7268ef┌─id─┐│ 1 ││ 2 │└────┘ URL 这个引擎表类似于Http的客户端，select查询转化get请求，insert转换成post请求。 表 12CREATE TABLE url_engine_table (word String, value UInt64)ENGINE=URL(&#x27;http://127.0.0.1:12345/&#x27;, CSV) 用标准的 Python 3 工具库创建一个基本的 HTTP 服务并 12345678910from http.server import BaseHTTPRequestHandler, HTTPServerclass CSVHTTPServer(BaseHTTPRequestHandler): def do_GET(self): self.send_response(200) self.send_header(&#x27;Content-type&#x27;, &#x27;text/csv&#x27;) self.end_headers() self.wfile.write(bytes(&#x27;Hello,1\\nWorld,2\\n&#x27;, &quot;utf-8&quot;))if __name__ == &quot;__main__&quot;: server_address = (&#x27;127.0.0.1&#x27;, 12345) HTTPServer(server_address, CSVHTTPServer).serve_forever() 启动 1python3 server.py 查询 12345SELECT * FROM url_engine_table┌─word──┬─value─┐│ Hello │ 1 ││ World │ 2 │└───────┴───────┘","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"clickHouse","slug":"clickHouse","permalink":"http://yoursite.com/tags/clickHouse/"}]},{"title":"clickHouse快速入门","slug":"clickHouse快速入门","date":"2021-02-08T02:05:17.000Z","updated":"2022-11-22T01:27:53.461Z","comments":true,"path":"2021/02/08/clickHouse快速入门/","link":"","permalink":"http://yoursite.com/2021/02/08/clickHouse%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/","excerpt":"","text":"clickHouse快速入门 发展历程 简介 2016年俄罗斯Yandex（类似本土的百度）公司，开源的一款olap数据库，它的前身是公司内部的一款在线流量分析产品Yandex.Metrica。 时间轴 历程 olap架构 Yandex.Metrica形态 Mysql时期 ROLAP 固定报告 自主研发Metrage MLOAP 固定报告 OLAPServer HOLAP（Metrage+OLAPServer） 自助报告 ClickHouse ROLAP 自助报告 可以看到ClickHouse在产品上经过了四个发展优化阶段，开始Mysql（使用MyISAM引擎），再Metrage（存储为Key-Value，索引LSM），然后再OLAPServer（优化存储索引等），最后集各家所长开源ClickHouse。它的发展，不是一开始就有所规划的自顶向下的架构设计，而是在不断的需求驱使下改进（自下而上）的发展历程。 特点 完备的DBMS：不仅是个数据库，也是个数据库系统 列存储和数据压缩：典型的olap数据库特性 向量化并行：利用CPU的SIMD（Single INstruction MUltiple Data），单条指令操作多条数据 多线程并行：向量化并行利用硬件采取数据并行（缺陷：不适应较多分支的判断），多线程级并行提高并发 关系模型：有数据库、表、视图和函数。更好清晰的描述实体间的关系 SQL：极高的群众基础 多样化表引擎：合并树、内存、文件、接口等20多种表引擎 多主架构：天然避免单点故障，服务端对客户端都是“孪生兄弟” 分布式：分区、分片 安装部署 只演示在MacOS下使用docker安装。 新建目录 123mkdir -p $HOME/docker/clickhouse/databasemkdir -p $HOME/docker/clickhouse/confmkdir -p $HOME/docker/clickhouse/log 运行临时容器 1docker run --rm --name some-clickhouse-server --ulimit nofile=262144:262144 yandex/clickhouse-server copy 临时容器内文件 12docker cp some-clickhouse-server:/etc/clickhouse-server/users.xml $HOME/docker/clickhouse/conf/users.xmldocker cp some-clickhouse-server:/etc/clickhouse-server/config.xml $HOME/docker/clickhouse/conf/config.xml 生成密码 12345echo &quot;123&quot;; echo -n &quot;123&quot; | sha1sum | tr -d &#x27;-&#x27; | xxd -r -p | sha1sum | tr -d &#x27;-&#x27;//明文、密文12323ae809ddacaf96af0fd78ed04b6a265e05aa257 配置default用户的密码 编辑上面copy的users.xml，注释掉&lt;/ 1&lt;password_double_sha1_hex&gt;23ae809ddacaf96af0fd78ed04b6a265e05aa257&lt;/password_double_sha1_hex&gt; 运行容器 12345678docker run -d --name ck \\--ulimit nofile=262144:262144 \\-p 8123:8123 -p 9000:9000 -p 9009:9009 \\-v $HOME/docker/clickhouse/database:/var/lib/clickhouse:rw \\-v $HOME/docker/clickhouse/conf/config.xml:/etc/clickhouse-server/config.xml \\-v $HOME/docker/clickhouse/conf/users.xml:/etc/clickhouse-server/users.xml \\-v $HOME/docker/clickhouse/log:/var/log/clickhouse-server:rw \\yandex/clickhouse-server 查看数据卷 123456789101112131415161718192021docker inspect ck | grep Mounts -A 500&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/Users/hf/docker/clickhouse/conf/config.xml&quot;, &quot;Destination&quot;: &quot;/etc/clickhouse-server/config.xml&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;, &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/Users/hf/docker/clickhouse/conf/users.xml&quot;, &quot;Destination&quot;: &quot;/etc/clickhouse-server/users.xml&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125; ..... ] 进入容器 1234567891011121314151617181920// 进入容器(base) ➜ ~ ✗ docker exec -it ck /bin/bash// 客户端链接root@c4b99140ca5a:/# clickhouse-client --password &quot;123&quot;ClickHouse client version 21.1.2.15 (official build).Connecting to localhost:9000 as user default.Connected to ClickHouse server version 21.1.2 revision 54443.// 查看库c4b99140ca5a :) show databases;SHOW DATABASESQuery id: 3f016d60-73fd-408e-b28f-7ef57b567260┌─name────┐│ default ││ system │└─────────┘ 演示数据 Yandex公司提供了2个表（hits_v1、visits_v1）的匿名演示数据，我们导入到ck中。 下载导入 1234cd $HOME/docker/clickhouse/databasecurl -O https://datasets.clickhouse.tech/hits/partitions/hits_v1.tarcurl -O https://datasets.clickhouse.tech/visits/partitions/visits_v1.tartar -zxvf hits_v1.tar &amp;&amp; tar -zxvf visits_v1.tar 重启容器 123docker restart ckdocker exec -it ck /bin/bashclickhouse-client --password &quot;123&quot; 查询 123456789101112c4b99140ca5a :) SELECT COUNT(*) FROM datasets.hits_v1SELECT COUNT(*)FROM datasets.hits_v1Query id: 7264c221-6fc5-469b-b180-51b226c37278┌─COUNT()─┐│ 8873898 │└─────────┘1 rows in set. Elapsed: 0.013 sec. 数据类型 可以使用 select * from system.data_type_families 来查看，总共116种。 整数 MySQL Hive Clickhouse 大小(字节) 数据范围 tinyint tinyint Int8 1 [-128 : 127] smallint smallint Int16 2 [-32768 : 32767] int int Int32 3 [-2147483648 : 2147483647] bigint bigint Int64 4 [-9223372036854775808 : 9223372036854775807] MySQL Hive Clickhouse 大小(字节) 数据范围 Tinyint unsigned UInt8 1 [0 : 255] smallint unsigned UInt16 2 [0 : 65535] Int unsigned UInt32 3 [0 : 4294967295] Bigint unsigned UInt64 4 [0 : 18446744073709551615] 浮点 MySQL Clickhouse 大小字节 有效精度（位数） float Float32 4 7 double Flout64 8 16 浮点数需要注意它的精度是有限的，Float32从小数点后第8位，Float64从小数点后的第17位起会产生数据溢出。 名称 等效声明 数据范围 Decimal32(S) Decimal(1-9,S) - ( -1 * 10^(9 - S), 1 * 10^(9 - S) ) Decimal64(S) Decimal(10-18,S) - ( -1 * 10^(18 - S), 1 * 10^(18 - S) ) Decimal128(S) Decimal(19-38,S) - ( -1 * 10^(38 - S), 1 * 10^(38 - S) ) 其中P表示精度precision，决定总位数（整数部分+小数位部分），取值范围为1-38，S代表规模 scale，决定小数位，取值范围是0-P。 布尔 没有单独的类型来存储布尔值。可以使用 UInt8 类型，取值限制为 0 或 1。 字符串 变长字符串 String：字符串可以任意长度的。它可以包含任意的字节集，包含空字节。 定长字符串 FixedString(N) 固定长度 N 的字符串，N 必须是严格的正自然数。当服务端读取长度小于 N 的字符串时候，通过在字符串末尾添加空字节来达到 N 字节长度。 当服务端读取长度大于 N 的字符串时候，将返回错误消息。 与String相比，极少会使用FixedString，因为使用起来不是很方便。 UUID：一般它作为数据主键类型，在ClickHouse中直接把它作为一种数据类型。UUID共有32位，它的格式为8-4-4-4-12 枚举 Enum8(String，Int8)、Enum16(String，Int16) 123456789CREATE TABLE enmu_t ( `season` Enum8 ( Spring = 1, Summer = 2, Fall = 3, Winter = 4 )) ENGINE = Memory;INSERT INTO enmu_t values (&#x27;Summer&#x27;); 数组 Array(T)，全部由 一个T类型元素组成的数组。T 可以是任意类型，包含数组类型。但不推荐使用多维数组，ClickHouse对多维数组的支持有限。例如，不能在MergeTree表中存储多维数组。 12345CREATE TABLE arr_t ( `arr` Array(String)) ENGINE = Memory;INSERT INTO arr_t values ([&#x27;hf&#x27;,&#x27;fan&#x27;]); Nest：嵌套表中的每个字段都是一个数组，并且行与行之间的数组长度无须对齐 1234567891011CREATE TABLE nested_t ( `username` String, `age` UInt8, `sex` UInt8, `address` Nested( `id` UInt8, `addr` String)) ENGINE = Memory;INSERT INTO nested_t VALUES(&#x27;clickhouse&#x27;,4,1,[100,101,102],[&#x27;Russia Moscow&#x27;,&#x27;China Beijing&#x27;,&#x27;China HangZhou&#x27;]); 元组 Tuple(T1, T2, …)，元组，与Array不同的是，Tuple中每个元素都有单独的类型。 123456CREATE TABLE tuple_t ( `info` Tuple(String, Int8)) ENGINE = Memory;INSERT INTO tuple_tVALUES ((&#x27;hf&#x27;, 20)); 日期 时间类型分为DateTime、DateTime64和Date三类。需要注意的是ClickHouse目前没有时间戳类型，也就是说，时间类型最高的精度是秒，所以如果需要处理毫秒、微秒精度的时间，则只能借助UInt类型实现。 Date：yyyy-MM-dd DateTime：yyyy-MM-dd hh:mm:ss DateTime64 yyyy-MM-dd hh:mm:ss.na Nullable：表示某个基础数据类型可以是Null值 12345678CREATE TABLE null_t ( `x` Int8, `y` Nullable(Int8)) ENGINE = Memory;INSERT INTO null_tVALUES (1, NULL), (2, 3); Domain：IPv4、IPv6 12345678CREATE TABLE hits( `url` String, `addr` IPv4) ENGINE = Memory;INSERT INTO hits (url, addr) VALUES(&#x27;https://wikipedia.org&#x27;, &#x27;116.253.40.133&#x27;)(&#x27;https://clickhouse.tech&#x27;, &#x27;183.247.232.58&#x27;);","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"clickHouse","slug":"clickHouse","permalink":"http://yoursite.com/tags/clickHouse/"}]},{"title":"经济学讲义-薛兆丰","slug":"经济学讲义-薛兆丰","date":"2021-01-28T07:35:39.000Z","updated":"2022-11-22T01:27:54.218Z","comments":true,"path":"2021/01/28/经济学讲义-薛兆丰/","link":"","permalink":"http://yoursite.com/2021/01/28/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E8%AE%B2%E4%B9%89-%E8%96%9B%E5%85%86%E4%B8%B0/","excerpt":"","text":"前言 每个人的经济学 反差 人的认知和判断，至今主要还是靠直觉和短距离的人际关系来驱动 人的身体和际遇，却早已置于大规模的陌生人的精妙协作之中 研究的主题 经济学 研究陌生人的互动规律为己任的学问。 四大基本约束 东西不够，生命有限，互相依赖，需要协调 第1章 稀缺 为何商业是最大的慈善 真实世界 经济学的视角 / 003 第001讲 战俘营里的经济组织 / 003 第002讲 马粪争夺案 / 007 第003讲 看得见的和看不见的 / 010 第004讲 区分愿望与结果 / 014 人性观 人是理性和自私的吗 / 018 第005讲 不确定性、进化与经济理论 / 018 第006讲 亚当·斯密的人性观 / 022 第007讲 铅笔的故事 / 026 第008讲 商业是最大的慈善 / 028 区别对待 选择的标准 / 032 第009讲 稀缺 / 032 第010讲 选择和歧视 / 035 第011讲 凡歧视必得付代价 / 038 第012讲 歧视的作用和限制歧视的恶果 / 041 第2章 成本 不要只盯着钱 选择偏好 放弃的最大价值 / 049 第013讲 一句话给成本下定义 / 049 第014讲 你的成本由别人决定 / 053 第015讲 别只盯着钱 / 057 资源的价值 重新理解盈利与亏损 / 061 第016讲 从成本角度理解盈利与亏损 / 061 第017讲 最终产品的供需决定原材料的成本 / 064 第018讲 “租”是对资产的付费 / 067 第019讲 寻租—乞丐没有白拿施舍 / 071 科斯定律 从社会成本看问题 / 075 第020讲 社会成本问题 / 075 第021讲 谁用得好就归谁 / 083 第022讲 是否要不惜一切代价保护环境 / 086 交易费用 平衡的艺术 / 090 第023讲 有人群就有交易费用 / 090 第024讲 征地的权衡 / 094 第025讲 寻求合作解 / 100 第3章 需求 好东西运到远方去 边际革命 不是越多越好 / 107 第026讲 个人主义的主观价值论 / 107 第027讲 边际革命 / 110 需求定律 关于人性的定律 / 115 第028讲 需求第一定律 / 115 第029讲 需求第二定律 / 122 第030讲 需求第三定律 / 131 第4章 价格 如何减少竞争的无谓损失 [2] 资源配置 如何分饼决定饼做多大 / 139 第031讲 经济计算问题 / 139 第032讲 知识在社会中的运用 / 143 第033讲 如何分饼决定饼做多大 / 147 第034讲 换个角度看乘人之危发财的行为 / 151 竞争的逻辑 社会成本决定规则的优劣 / 155 第035讲 竞争的规则 / 155 第036讲 各种竞争规则孰优孰劣 / 158 第037讲 何谓短缺和过剩 / 162 价格管制 人会追求损失最小化 / 166 第038讲 最省事儿地违反价格管制 / 166 第039讲 春运火车票价还不够高 / 169 第040讲 美国的房租管制 / 176 第041讲 解除价格管制—果断与沉稳的拿捏 / 180 第042讲 实物补贴和货币补贴的权衡 / 184 竞争的维度 没有人可以控制市场因素 / 189 第043讲 房价上升好不好 / 189 第044讲 廉价保障房更贵 / 193 第045讲 没有使用需求就没有投资需求 / 196 第5章 权利 自由不等于免费 资源的占有 你的权利从哪里来 / 201 第046讲 权利有别于能力 / 201 第047讲 权利是人赋的而非天赋的 / 203 第048讲 自由不等于免费 / 207 产权界定 保护与限制 / 211 第049讲 产权的兴起 / 211 第050讲 产权：使用权、收益权和转让权 / 215 第051讲 产权保护之一：财产原则 / 218 第052讲 产权保护之二：责任原则 / 222 第053讲 产权保护之三：不可转让原则 / 225 公共服务 公用品与私用品 / 232 第054讲 经济学家论道路拥堵 / 232 第055讲 拥堵费的实践 / 235 第056讲 公用品和私用品 / 240 第057讲 灯塔的故事 / 244 第6章 耐心 想象力决定生产力 不确定性 时间偏好带来的回报 / 251 第058讲 易耗品与耐用品 / 251 第059讲 不耐产生交易 / 255 第060讲 利率管制及其后果 / 258 第061讲 利率变化支配资本现值 / 262 有效市场假说 时间维度上的平衡消费 / 267 第062讲 投资就是时间上的平衡消费 / 267 第063讲 股价究竟能不能预测 / 271 第064讲 股市到底有没有泡沫 / 275 第065讲 肥猪丸与回报率 / 279 预测未来 债券、保险与期货 / 283 第066讲 未雨绸缪：储蓄还是买保险 / 283 第067讲 保险的运作原理 / 286 第068讲 那些不是保险的保险 / 289 第069讲 当商品不被当作商品 / 293 第070讲 用期货合约来重新分配不可避免的风险 / 297 第7章 供应 好钢用在刀刃上 比较优势 天生我才必有用 / 305 第071讲 比较优势原理 / 305 第072讲 顺差逆差，孰优孰劣 / 309 第073讲 贸易对等和贸易报复 / 313 第074讲 贸易保护理由辨析 / 316 价格歧视 定价和竞争的策略 / 321 第075讲 受价者和觅价者 / 321 第076讲 价格歧视能给觅价者脱罪 / 326 第077讲 价格歧视的策略 / 330 第078讲 行业竞争程度不看企业个数 / 334 第8章 信息不对称 谁的话语权更大 [2] 信任的建立 直面信息不对称 / 341 第079讲 柠檬市场的故事 / 341 第080讲 何谓优质 / 345 制度的对策 缔约自由比自由更重要 / 349 第081讲 重复交易与第三方背书 / 349 第082讲 担保、延保与共享合同 / 351 第083讲 沉没成本、人质与抵押 / 356 第084讲 广告代言与形象打扮 / 358 第085讲 特许经营合同中的强者和弱者 / 362 第086讲 如何保障食品、药品和化妆品的质量 / 365 第087讲 婚姻经济学 / 370 责任的分担 让防范的成本最小 / 375 第088讲 汉德公式/ 375 第089讲 监管要看边际效应 / 381 第090讲 监管本身也要引入竞争机制 / 385 第9章 合作 为何同工不同酬 决策权 谁来当老板 / 393 第091讲 企业的团队本质 / 393 第092讲 谁来当老板：资本和劳动力 / 397 第093讲 谁来当老板：专用资源和通用资源 / 399 第094讲 谁来当老板：固定收入和剩余索取 / 403 收入分配 政府是否应该劫富济贫 / 408 第095讲 收入的高低和节奏由什么决定 / 408 第096讲 事与愿违之同工同酬法 / 413 第097讲 事与愿违之最低工资法 / 417 第098讲 基尼系数与收入分配 / 420 脱贫致富 最富裕的穷人在今天 / 425 第099讲 工人违约是否应该成为权利 / 425 第100讲 罢工的性质 / 429 第101讲 工作权利遇到的障碍 / 432 第102讲 最富裕的穷人在今天 / 437 第10章 协调 众人如何彼此影响 货币规律 货币像水又像蜜 / 445 第103讲 货币的起源 / 445 第104讲 商业银行创造货币 / 448 第105讲 通货膨胀的根源 / 453 第106讲 通货膨胀的过程 / 456 第107讲 通货膨胀与失业 / 460 经济周期 聪明人彼此不同意 / 466 第108讲 奥地利学派看经济周期 / 466 第109讲 凯恩斯主义学派看经济周期 / 472 第110讲 货币主义学派和理性预期学派看经济周期 / 476 第111讲 真实经济周期理论 / 481 第112讲 聪明人为什么会彼此不同意 / 484 公共选择 选举未必反映大多数人的意愿 / 492 第113讲 选举由中间派说了算 / 492 第114讲 阿罗不可能定律 / 496 第115讲 民主为何会产生不良经济政策 / 499 第116讲 关于收入再分配的戴雷科特定律 / 508 第117讲 脱贫致富之路知易行难 / 512 第118讲 中国做对了什么 / 516 后记 何为地道的经济学思维 / 521","categories":[{"name":"阅读","slug":"阅读","permalink":"http://yoursite.com/categories/%E9%98%85%E8%AF%BB/"}],"tags":[{"name":"经济学","slug":"经济学","permalink":"http://yoursite.com/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6/"}]},{"title":"Java8日期","slug":"Java8日期","date":"2021-01-26T11:40:07.000Z","updated":"2022-11-22T01:27:53.378Z","comments":true,"path":"2021/01/26/Java8日期/","link":"","permalink":"http://yoursite.com/2021/01/26/Java8%E6%97%A5%E6%9C%9F/","excerpt":"","text":"JDK8的Date-Time API 由主包 java.time 和四个子包组成： java.time 表示日期和时间的 API 的核心。它包括日期，时间，日期和时间相结合的类别， 时区/zones，瞬间/instants，持续时间/duration 和 时钟/clocks。这些类基于 ISO-8601 中定义的日历系统， 并且不可变且线程安全。 java.time.chrono 用于表示除默认 ISO-8601 以外的日历系统的 API。您也可以定义自己的日历系统。本教程不包含任何细节。 java.time.format 用于格式化和分析日期和时间的类。 java.time.temporal 扩展 API 主要用于框架和库编写器，允许日期和时间类之间的互操作，查询和调整。字段（TemporalField 和 ChronoField） 和单位（TemporalUnit 和 ChronoUnit）在此包中定义。 java.time.zone 支持时区的类，时区的偏移和时区规则。如果使用时区，大多数开发人员只需使用 ZonedDateTime 和 ZoneId 或 ZoneOffset 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416package com.michael.utils;import org.apache.commons.lang3.StringUtils;import java.time.*;import java.time.format.DateTimeFormatter;import java.time.temporal.ChronoField;import java.time.temporal.TemporalAdjusters;import java.util.Date;import java.util.TimeZone;public class LocalDateTimeUtils &#123; /** * 日期格式：yyyy-MM-dd */ public static String DATE_PATTERN = &quot;yyyy-MM-dd&quot;; /** * 日期格式：yyyyMMdd */ public static String DATE_PATTERN_A = &quot;yyyyMMdd&quot;; /** * 日期时间格式：yyyy-MM-dd HH:mm:ss */ public static String DATE_TIME_PATTERN = &quot;yyyy-MM-dd HH:mm:ss&quot;; /** * 时间格式(24小时制)：HH:mm:ss */ public static String TIME_PATTERN_24 = &quot;HH:mm:ss&quot;; /** * 时间格式(12小时制)：hh:mm:ss */ public static String TIME_PATTERN_12 = &quot;hh:mm:ss&quot;; /** * 获取年份 */ public static void getYear() &#123; LocalDateTime localTime = LocalDateTime.now(); int year = localTime.get(ChronoField.YEAR); System.out.println(year); &#125; /** * 获取月份 */ public static void getMonth() &#123; LocalDateTime localTime = LocalDateTime.now(); int month = localTime.get(ChronoField.MONTH_OF_YEAR); System.out.println(month); &#125; /** * 获取某月的第几天 */ public static void getMonthOfDay() &#123; LocalDateTime localTime = LocalDateTime.now(); int day = localTime.get(ChronoField.DAY_OF_MONTH); System.out.println(day); &#125; /** * 格式化日期为字符串 * * @param date 需要格式化的日期 * @param pattern 格式，如：yyyy-MM-dd * @return 日期字符串 */ public static String format(Date date, String pattern) &#123; Instant instant = date.toInstant(); LocalDateTime localDateTime = LocalDateTime.ofInstant(instant, ZoneId.systemDefault()); return localDateTime.format(DateTimeFormatter.ofPattern(pattern)); &#125; /** * 解析字符串日期为Date * * @param dateStr 日期字符串 * @param pattern 格式，如：yyyy-MM-dd * @return Date */ public static Date parse(String dateStr, String pattern) &#123; LocalDateTime localDateTime = LocalDateTime.parse(dateStr, DateTimeFormatter.ofPattern(pattern)); Instant instant = localDateTime.atZone(ZoneId.systemDefault()).toInstant(); return Date.from(instant); &#125; /** * 为Date增减分钟(减传负数) * * @param date 日期 * @param minutes 要增减的分钟数 * @return 新的日期 */ public static Date addReduceMinutes(Date date, Long minutes) &#123; LocalDateTime dateTime = LocalDateTime.ofInstant(date.toInstant(), ZoneId.systemDefault()); LocalDateTime newDateTime = dateTime.plusMinutes(minutes); return Date.from(newDateTime.atZone(ZoneId.systemDefault()).toInstant()); &#125; /** * 增加时间 * * @param date date * @param hour 要增加的小时数 * @return 新的日期 */ public static Date addHour(Date date, Long hour) &#123; LocalDateTime dateTime = LocalDateTime.ofInstant(date.toInstant(), ZoneId.systemDefault()); LocalDateTime localDateTime = dateTime.plusHours(hour); return Date.from(localDateTime.atZone(ZoneId.systemDefault()).toInstant()); &#125; /** * 当天的起始时间 * * @return 如：Tue Jun 11 00:00:00 CST 2019 */ public static Date getStartTime() &#123; LocalDateTime now = LocalDateTime.now().withHour(0).withMinute(0).withSecond(0); return localDateTime2Date(now); &#125; /** * 当天的结束时间 * * @return 如：Tue Jun 11 23:59:59 CST 2019 */ public static Date getEndTime() &#123; LocalDateTime now = LocalDateTime.now().withHour(23).withMinute(59).withSecond(59).withNano(999); return localDateTime2Date(now); &#125; /** * 减月份 * * @param monthsToSubtract 月份 * @return Date */ public static Date minusMonths(long monthsToSubtract) &#123; LocalDate localDate = LocalDate.now().minusMonths(monthsToSubtract); return localDate2Date(localDate); &#125; /** * LocalDate类型转为Date * * @param localDate * @return */ public static Date localDate2Date(LocalDate localDate) &#123; ZonedDateTime zonedDateTime = localDate.atStartOfDay(ZoneId.systemDefault()); return Date.from(zonedDateTime.toInstant()); &#125; /** * LocalDateTime类型转为Date * * @param localDateTime * @return Date */ public static Date localDateTime2Date(LocalDateTime localDateTime) &#123; return Date.from(localDateTime.atZone(ZoneId.systemDefault()).toInstant()); &#125; /** * 查询当前年的第一天 * * @param pattern 格式，如：yyyyMMdd * @return 20190101 */ public static String getFirstDayOfCurrentYear(String pattern) &#123; LocalDateTime localDateTime = LocalDateTime.now().withMonth(1).withDayOfMonth(1); return format(localDateTime2Date(localDateTime), StringUtils.isEmpty(pattern) ? DATE_PATTERN_A : pattern); &#125; public static String getFirstDayOfCurrentYear() &#123; LocalDateTime localDateTime = LocalDateTime.now().withMonth(1).withDayOfMonth(1); return format(localDateTime2Date(localDateTime), DATE_PATTERN_A); &#125; /** * 查询前一年最后一个月第一天 * * @param pattern 格式，如：yyyyMMdd * @return 20190101 */ public static String getLastMonthFirstDayOfPreviousYear(String pattern) &#123; LocalDateTime localDateTime = LocalDateTime.now().minusYears(1L).withMonth(12).withDayOfMonth(1); return format(localDateTime2Date(localDateTime), StringUtils.isEmpty(pattern) ? DATE_PATTERN_A : pattern); &#125; public static String getLastMonthFirstDayOfPreviousYear() &#123; LocalDateTime localDateTime = LocalDateTime.now().minusYears(1L).withMonth(12).withDayOfMonth(1); return format(localDateTime2Date(localDateTime), DATE_PATTERN_A); &#125; /** * 查询前一年最后一个月的最后一天 * * @param pattern 格式，如：yyyyMMdd * @return 20190101 */ public static String getLastMonthLastDayOfPreviousYear(String pattern) &#123; LocalDateTime localDateTime = LocalDateTime.now().minusYears(1L).with(TemporalAdjusters.lastDayOfYear()); return format(localDateTime2Date(localDateTime), StringUtils.isEmpty(pattern) ? DATE_PATTERN_A : pattern); &#125; public static String getLastMonthLastDayOfPreviousYear() &#123; LocalDateTime localDateTime = LocalDateTime.now().minusYears(1L).with(TemporalAdjusters.lastDayOfYear()); return format(localDateTime2Date(localDateTime), DATE_PATTERN_A); &#125; /** * 获取当前日期 * * @param pattern 格式，如：yyyy-MM-dd * @return 2019-01-01 */ public static String getCurrentDay(String pattern) &#123; LocalDateTime localDateTime = LocalDateTime.now(); return format(localDateTime2Date(localDateTime), StringUtils.isEmpty(pattern) ? DATE_PATTERN : pattern); &#125; public static String getCurrentDay() &#123; LocalDateTime localDateTime = LocalDateTime.now(); return format(localDateTime2Date(localDateTime), DATE_PATTERN); &#125; /** * 获取时区 */ public static void getZone() &#123; // 默认时区 ZoneId zoneId = ZoneId.systemDefault(); System.out.println(zoneId.toString()); zoneId = ZoneId.of(&quot;Asia/Shanghai&quot;); System.out.println(zoneId.toString()); zoneId = TimeZone.getTimeZone(&quot;CTT&quot;).toZoneId(); System.out.println(zoneId.toString()); &#125; /** * 字符串转时间 */ public static void strToDate() &#123; LocalDate date = LocalDate.parse(&quot;20190522&quot;, DateTimeFormatter.ofPattern(DATE_PATTERN_A)); System.out.println(date); &#125; /** * 时间格式化输出 */ public static void dateToStr() &#123; LocalDate today = LocalDate.now(); System.out.println(&quot;当前日期：&quot; + today.format(DateTimeFormatter.ofPattern(DATE_PATTERN))); System.out.println(); LocalTime time = LocalTime.now(); //24小时制 System.out.println(&quot;当前时间(24小时制)：&quot; + time.format(DateTimeFormatter.ofPattern(TIME_PATTERN_24))); //12小时制 System.out.println(&quot;当前时间(12小时制)：&quot; + time.format(DateTimeFormatter.ofPattern(TIME_PATTERN_12))); System.out.println(); LocalDateTime now = LocalDateTime.of(today, time); //yyyyMMdd System.out.println(now.format(DateTimeFormatter.BASIC_ISO_DATE)); //yyyy-MM-dd System.out.println(now.format(DateTimeFormatter.ISO_DATE)); System.out.println(); //2019-05-28T15:30:21.047 System.out.println(now.format(DateTimeFormatter.ISO_DATE_TIME)); //local date System.out.println(now.format(DateTimeFormatter.ISO_LOCAL_DATE)); //local time 带毫秒 eg:15:33:00.043 System.out.println(now.format(DateTimeFormatter.ISO_LOCAL_TIME)); //local dateTime eg:2019-05-28T15:33:00.043 System.out.println(now.format(DateTimeFormatter.ISO_LOCAL_DATE_TIME)); //2019-W22-2 System.out.println(now.format(DateTimeFormatter.ISO_WEEK_DATE)); &#125; /** * Date 与 Localdatetime 的转换 */ public static void transformWithDate() &#123; // date 转 localdatetime Date date = new Date(); LocalDateTime localDateTime = date.toInstant().atZone(ZoneId.systemDefault()).toLocalDateTime(); System.out.println(localDateTime); // localdatetime 转 date Date date1 = Date.from(LocalDateTime.now().atZone(ZoneId.systemDefault()).toInstant()); System.out.println(date1); &#125; /** * 与时间戳的转换 */ public static void transformWithTimestamp() &#123; //秒级时间戳 long timeStamp = LocalDateTime.now().toEpochSecond(ZoneOffset.ofHours(8)); System.out.println(&quot;秒级时间戳:&quot; + timeStamp); timeStamp = LocalDateTime.now().atZone(ZoneId.systemDefault()).toInstant().getEpochSecond(); System.out.println(&quot;秒级时间戳:&quot; + timeStamp); System.out.println(&quot;秒级时间戳转时间：&quot; + Instant.ofEpochSecond(timeStamp).atZone(ZoneId.systemDefault()).toLocalDateTime()); System.out.println(); //毫秒级时间戳 timeStamp = LocalDateTime.now().atZone(ZoneId.systemDefault()).toInstant().toEpochMilli(); System.out.println(&quot;毫秒级时间戳:&quot; + timeStamp); System.out.println(&quot;毫秒级时间戳转时间：&quot; + Instant.ofEpochMilli(timeStamp).atZone(ZoneId.systemDefault()).toLocalDateTime()); &#125; /** * 时间调整到特定某天 */ public static void adjust() &#123; LocalDateTime now = LocalDateTime.now(); System.out.println(&quot;当前时间：&quot; + now); //同月的第一天 LocalDateTime firstDayInSameMonthOfNow = now.withDayOfMonth(1); System.out.println(&quot;同月的第一天:&quot; + firstDayInSameMonthOfNow); //同年的第一天 LocalDateTime firstDayInSameYearOfNow = now.withDayOfYear(1); System.out.println(&quot;同年的第一天:&quot; + firstDayInSameYearOfNow); //同年的第2月第10天 LocalDateTime time = now.withMonth(2).withDayOfMonth(10); System.out.println(&quot;同年的第2月第10天:&quot; + time); //当天的6点整 time = now.withHour(6).withMinute(0).withSecond(0).withNano(0); System.out.println(&quot;当天的6点整:&quot; + time); &#125; /** * 格式化时间-默认yyyy-MM-dd HH:mm:ss格式 * * @param dateTime LocalDateTime对象 * @param pattern 要格式化的字符串 * @return */ public static String formatDateTime(LocalDateTime dateTime, String pattern) &#123; if (dateTime == null) &#123; return null; &#125; return dateTime.format(DateTimeFormatter.ofPattern(StringUtils.isEmpty(pattern) ? DATE_TIME_PATTERN : pattern)); &#125; public static String formatDateTime(LocalDateTime dateTime) &#123; return formatDateTime(dateTime, DATE_TIME_PATTERN); &#125; /** * 获取某天的00:00:00 * * @param dateTime * @return */ public static String getDayStart(LocalDateTime dateTime) &#123; return formatDateTime(dateTime.with(LocalTime.MIN)); &#125; public static String getDayStart() &#123; return getDayStart(LocalDateTime.now()); &#125; /** * 获取某天的23:59:59 * * @param dateTime * @return */ public static String getDayEnd(LocalDateTime dateTime) &#123; return formatDateTime(dateTime.with(LocalTime.MAX)); &#125; public static String getDayEnd() &#123; return getDayEnd(LocalDateTime.now()); &#125; /** * 获取某月第一天的00:00:00 * * @param dateTime LocalDateTime对象 * @return */ public static String getFirstDayOfMonth(LocalDateTime dateTime) &#123; return formatDateTime(dateTime.with(TemporalAdjusters.firstDayOfMonth()).with(LocalTime.MIN)); &#125; public static String getFirstDayOfMonth() &#123; return getFirstDayOfMonth(LocalDateTime.now()); &#125; /** * 获取某月最后一天的23:59:59 * * @param dateTime LocalDateTime对象 * @return */ public static String getLastDayOfMonth(LocalDateTime dateTime) &#123; return formatDateTime(dateTime.with(TemporalAdjusters.lastDayOfMonth()).with(LocalTime.MAX)); &#125; public static String getLastDayOfMonth() &#123; return getLastDayOfMonth(LocalDateTime.now()); &#125;&#125;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"}]},{"title":"Java8函数编程","slug":"Java8函数编程","date":"2021-01-25T07:57:26.000Z","updated":"2022-11-22T01:27:53.377Z","comments":true,"path":"2021/01/25/Java8函数编程/","link":"","permalink":"http://yoursite.com/2021/01/25/Java8%E5%87%BD%E6%95%B0%E7%BC%96%E7%A8%8B/","excerpt":"","text":"Java 8的核心新特性:Lambda(匿名函数)、流、默认方法。Java 8所做的改变，在许多方面比Java历史上任何一次改变都深远。这些改变会让你编程起来更容易，用不着再写啰嗦的代码。 1、Lambda表达式 (parameters) -&gt; expression 表达式不需要{} (parameters) -&gt; &#123; statements; &#125; 语句需要在{} 方法引用 xx::yyy 1.1 函数式接口 定义：只定义了一个抽象方法的接口，它还可以拥有若干个默认方法，和JDK7及之前有很大区别。 123public interface Adder&#123; int add(int a, int b); &#125; 函数描述符 方法签名，即方法的整体描述：入参，出参。 抽象方法的签名=函数描述符，它是一个统称/术语，不必纠结，记住就ok。抽象方法的签名就是lambda表达式，所以我们可以用上述2种方式书写。 1.2 Lambda表达式使用 为函数式接口的抽象方法提供实现 整个表达式作为函数式接口的实例 12345@FunctionalInterfacepublic interface Runnable &#123; void run();&#125;Runnable r1 = () -&gt; System.out.println(&quot;Hello World 1&quot;); 1.3 方法引用 定义：仅仅调用了特定方法的lambda表达式。即，lambda只是“直接调用这个方法”！目标引用放在分隔符::前，方法的名称放在后面。 例如：Apple::getWeight 分类： 静态方法：Integer::parseInt 参数对象的实例方法：（Apple a）-&gt; a.getWeight() = Apple :: getWeight 局部对象的实例方法：（）-&gt;local.getSth() = local :: getSth 构造方法：Apple :: new 2、流Stream 是什么？ 流可以看作花哨又懒惰的数据集迭代器。它们支持两种类型的操作:中间操作(如filter或map)和 终端操作(如count、findFirst、forEach和reduce)。中间操作可以链接起来，将一个流 转换为另一个流。这些操作不会消耗流，其目的是建立一个流水线。与此相反，终端操作会消 耗流，以产生一个最终结果，例如返回流中的最大元素。它们通常可以通过优化流水线来缩短计算时间。 源（list、arr、map等）—&gt;中间操作返回stream（filter、map、limit、sorted、distinct等）—&gt;终端操作返回非stream（collect，count，forEach、allMatch、anyMatch、noneMatch、findFirst、findAny等 ） 总而言之，流的使用一般包括三件事: 一个数据源(如集合)来执行一个查询; 一个中间操作链，形成一条流的流水线; 一个终端操作，执行流水线，并能生成结果。 构建流 123456789Stream&lt;String&gt; stream = Stream.of(&quot;Java 8 &quot;, &quot;Lambdas &quot;, &quot;In &quot;, &quot;Action&quot;);Stream&lt;String&gt; emptyStream = Stream.empty();int[] numbers = &#123;2, 3, 5, 7, 11, 13&#125;;//IntStream stream(int[] arrayIntStream stream = Arrays.stream(numbers);//&lt;T&gt; Stream&lt;T&gt; iterate(final T seed, final UnaryOperator&lt;T&gt; f) Stream.iterate(0, n -&gt; n + 2).limit(10).forEach(System.out::println);//&lt;T&gt; Stream&lt;T&gt; generate(Supplier&lt;T&gt; s) Stream.generate(Math::random).limit(5).forEach(System.out::println); 使用 筛选：filter、distinct 切片：limit(拿到前n)、skip(跳过前n)：2者互补 映射 map(Function&lt;T,R&gt;)：function作用于每个元素，返回一个lenght个类型的Stream，type的长度为每个元素长度。 mapToInt mapToLong mapToDouble flatMap：再作用一边上述的每个元素，合并为一个。 flatMapToInt flatMapToLong flatMapToDouble 12345678910111213141516 public static void main(String[] args) &#123; List&lt;String&gt; words = Arrays.asList(&quot;hello&quot;, &quot;word&quot;); List&lt;String[]&gt; collect = words.stream().map(w -&gt; w.split(&quot;&quot;)) .distinct().collect(Collectors.toList()); List&lt;String&gt; collect2 = words.stream().map(w -&gt; w.split(&quot;&quot;)).flatMap(Arrays::stream) .distinct().collect(Collectors.toList()); System.out.println(JSON.toJSONString(collect)); System.out.println(JSON.toJSONString(collect2)); &#125;打印如下：[[&quot;h&quot;,&quot;e&quot;,&quot;l&quot;,&quot;l&quot;,&quot;o&quot;],[&quot;w&quot;,&quot;o&quot;,&quot;r&quot;,&quot;d&quot;]][&quot;h&quot;,&quot;e&quot;,&quot;l&quot;,&quot;o&quot;,&quot;w&quot;,&quot;r&quot;,&quot;d&quot;] 规约 reduce 12345int sum = numbers.stream().reduce(0, (a, b) -&gt; a + b);int sum = numbers.stream().reduce(0, Integer::sum);Optional&lt;Integer&gt; sum = numbers.stream().reduce((a, b) -&gt; (a + b));Optional&lt;Integer&gt; max = numbers.stream().reduce(Integer::max);Optional&lt;Integer&gt; min = numbers.stream().reduce(Integer::min); 收集 collect 主要围绕着Collectors工厂方法提供一系列方法。 List&lt;Transaction&gt; transactions =transactionStream.collect(Collectors.toList()); 3、常用函数式接口 接口 描述符 Predicate T-&gt;boolean Consumer T-&gt;void Function&lt;T,R&gt; T-&gt;R Supplier ()-&gt;T UnaryOperator T-&gt;T BinaryOperator (T,T)-&gt;T BiPredicate&lt;L,R&gt; (L,R)-&gt;boolean BiConsumer&lt;T,U&gt; (T,U)-&gt;void BiFunction&lt;T,U,R&gt; (T,U)-&gt;R","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"基础","slug":"基础","permalink":"http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"}]},{"title":"springMVC","slug":"springMVC","date":"2021-01-11T08:57:21.000Z","updated":"2022-11-22T01:27:53.830Z","comments":true,"path":"2021/01/11/springMVC/","link":"","permalink":"http://yoursite.com/2021/01/11/springMVC/","excerpt":"","text":"","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/tags/spring/"}]},{"title":"springAOP","slug":"springAOP","date":"2021-01-11T08:57:16.000Z","updated":"2022-11-22T01:27:53.805Z","comments":true,"path":"2021/01/11/springAOP/","link":"","permalink":"http://yoursite.com/2021/01/11/springAOP/","excerpt":"","text":"","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/tags/spring/"}]},{"title":"spring","slug":"spring","date":"2021-01-11T08:56:33.000Z","updated":"2022-11-22T01:27:53.804Z","comments":true,"path":"2021/01/11/spring/","link":"","permalink":"http://yoursite.com/2021/01/11/spring/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // Prepare this context for refreshing. // 1. 初始化前的预处理 prepareRefresh(); // Tell the subclass to refresh the internal bean factory. // 2. 获取BeanFactory，加载所有xml配置文件中bean的定义信息（未实例化） ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. // 3. BeanFactory的预处理配置 prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. // 4. 准备BeanFactory完成后进行的后置处理 postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. // 5. 执行BeanFactory创建后的后置处理器 invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. // 6. 注册Bean的后置处理器 registerBeanPostProcessors(beanFactory); // Initialize message source for this context. // 7. 初始化MessageSource initMessageSource(); // Initialize event multicaster for this context. // 8. 初始化事件派发器 initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. // 9. 子类的多态onRefresh onRefresh(); // Check for listener beans and register them. // 10. 注册监听器 registerListeners(); //到此为止，BeanFactory已创建完成 // Instantiate all remaining (non-lazy-init) singletons. // 11. 初始化所有剩下的单例Bean finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. // 12. 完成容器的创建工作 finishRefresh(); &#125; // catch ...... finally &#123; // Reset common introspection caches in Spring&#x27;s core, since we // might not ever need metadata for singleton beans anymore... // 13. 清除缓存 resetCommonCaches(); &#125; &#125;&#125;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/tags/spring/"}]},{"title":"springIOC","slug":"springIOC","date":"2021-01-11T08:40:11.000Z","updated":"2022-11-22T01:27:53.827Z","comments":true,"path":"2021/01/11/springIOC/","link":"","permalink":"http://yoursite.com/2021/01/11/springIOC/","excerpt":"","text":"打开spring官网，首先映入眼帘的就是下面这个图： 一个大的Bean容器（IOC容器），Java 的pojo和配置的Java元数据（可以理解为非pojo的bean）在容器内，开发者就可以轻松便捷使用其中的bean，无需程序处理bean的实例化和销毁。 元数据配置 xml方式 1234567891011121314151617&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans https://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;...&quot; class=&quot;...&quot;&gt; &lt;!-- collaborators and configuration for this bean go here --&gt; &lt;/bean&gt; &lt;bean id=&quot;...&quot; class=&quot;...&quot;&gt; &lt;!-- collaborators and configuration for this bean go here --&gt; &lt;/bean&gt; &lt;!-- more bean definitions go here --&gt;&lt;/beans&gt; Java Config 方式 12345678@Configurationpublic class AppConfig &#123; @Bean public MyService myService() &#123; return new MyServiceImpl(); &#125;&#125; 注解形式 ​ @Autowired、@Service、@Component 等。","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/tags/spring/"}]},{"title":"spring概述","slug":"spring概述","date":"2021-01-11T07:02:14.000Z","updated":"2022-11-22T01:27:53.833Z","comments":true,"path":"2021/01/11/spring概述/","link":"","permalink":"http://yoursite.com/2021/01/11/spring%E6%A6%82%E8%BF%B0/","excerpt":"","text":"spring 在Java开发圈里是个举足轻重的框架，可以说搞Java的无人不知无人不晓！2002 年，有一个人叫 Rod Johnson ，他写了一本书：《Expert One-on-One J2EE design and development》 ，里面对当时现有的 J2EE 应用的架构和框架（EJB）存在的臃肿、低效等问题提出了质疑，并且积极寻找和探索解决方案，2004 年 SpringFramework 1.0.0 横空出世，Rod Johnson 又写了一本书：《Expert one-on-one J2EE Development without EJB》 ，从此Java程序员的“春天”来了。 里程碑 SpringFramework版本 JDK版本 特性 1.x 1.3 基于 xml 的配置 2.x 1.4 改良 xml 文件、初步支持注解式配置 3.x 1.5 注解式配置、JavaConfig 编程式配置、Environment 抽象 4.x 1.6 SpringBoot 1.x、核心容器增强、条件装配、WebMvc 基于 Servlet3.0 5.x 1.8 SpringBoot 2.x、响应式编程、SpringWebFlux、支持 Kotlin 模块 beans、core、context、expression 【核心包】 aop 【切面编程】 jdbc 【整合 jdbc 】 orm 【整合 ORM 框架】 tx 【事务控制】 web 【 Web 层技术】 test 【整合测试】 后续将从IOC、AOP、MVC三个大的层面展开。","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/tags/spring/"}]},{"title":"泛型","slug":"泛型","date":"2020-12-02T09:43:07.000Z","updated":"2022-11-22T01:27:54.173Z","comments":true,"path":"2020/12/02/泛型/","link":"","permalink":"http://yoursite.com/2020/12/02/%E6%B3%9B%E5%9E%8B/","excerpt":"","text":"JDK5之后引入泛型，从此我们的看源码时候，充斥着各种?、T、R、V奇怪符号，单独这些符号还好，可怕的是他们会连接在一起，出现在接口，类，方法上。看起来杂乱无章，很是头痛。 为什么 123456789101112public static void main(String[] args) &#123; List list = new ArrayList(); list.add(1); list.add(&quot;1&quot;); list.add(1d); for (int i = 0; i &lt; list.size(); i++) &#123; Object o = list.get(i); System.out.println(&quot;list element is :&quot; + o); &#125; &#125; 上面代码在Java没有泛型时候可以这么写，但是有没有觉得很不优雅，这里我是直接取出来打印，不会出错。但是我们正常的业务这样的流程没什么意义，一般都是转换成某个具体类型，操作它。这里有2个问题： 放到集合中的元素，乱七八糟，没有做任何限制，这是不符合集合使用原则的 取出来元素类型不定，没办法做强制类型转换，否则报错 Java的泛型由此而生，放入类型事先定好，取出来默认帮我们转换，是不是美哉！ 泛型 是什么 参数有具体的类型，通常不会用Object 作为一个参数类型，基本都是一个具体类型，这个类型就是限制了后续我们在操作它的时候，什么是可以操作什么不可以（根据参数类型API决定）。Java中类class是对象模版，类有自己的类型Class，那么参数是否也可以有类型呢？对，它就是泛型。 使用方式 泛型接口：public interface Future&lt;V&gt; 泛型类：public class Country&lt;T&gt; 泛型方法：public &lt;T&gt; T doSth (T t) 静态泛型方法：public static &lt;T&gt; T doSth (T t) 泛型接口和泛型类使用方式基本一样，规则是：接口/类名称&lt;泛型标识&gt;。泛型方法，它的规则是：类/接口&lt;泛型标识&gt;。 泛型方法稍微容易迷糊一点。什么是泛型方法？并不是方法中使用了各种T R V E就是泛型方法，对于它的理解我是这样的，必须要在方法中定了某个/几个泛型（&lt;泛型标识&gt;）才算是泛型方法。 静态泛型方法也需要特别注意，它是不能使用类上定义的泛型的，因为泛型是在类实例化时候才能确定的具体是什么的，static 修饰的方法在类实例化前面就加载了（此时类上的泛型还不知道是什么类型），所以它在使用泛型时候就需要自己定义。 上下界 结论：PECS (Producer Extends 、Consumer Super) 12List&lt;? extends Number&gt; list = new ArrayList&lt;&gt;();List&lt;? super Integer&gt; list1 = new ArrayList&lt;&gt;(); 这个怎么理解呢？ 读多就使用extend T，在容器来看就是生产者Producer，提供元素给读。 写多就使用 super T，在容器来看就是消费者Consumer，接受写入元素。 泛型类型获取 只有在泛型是固定的时候才能获取，什么是固定的呢？**定义子类/子接口时候，明确了父类/接口的泛型的类型。**类似的方式在fastjson中 12345678910111213141516171819202122232425&#96;&#96;&#96;java&#x2F;&#x2F; 泛型类@Datapublic class Company&lt;T&gt; &#123; private List&lt;T&gt; employees; private String name;&#125;&#x2F;&#x2F; 子类public class SubCompany extends Company&lt;User&gt; &#123; &#125; Company&lt;User&gt; company&#x3D;new SubCompany();&#x2F;&#x2F; 匿名类 Company&lt;User&gt; company&#x3D;new Company&lt;User&gt;()&#123;&#125;; &#x2F;&#x2F; 获取泛型类的泛型类 Type genericSuperclass &#x3D; company.getClass().getGenericSuperclass(); if(genericSuperclass instanceof ParameterizedType)&#123; ParameterizedType parameterizedType &#x3D; (ParameterizedType) genericSuperclass; Type actualTypeArgument &#x3D; parameterizedType.getActualTypeArguments()[0]; System.out.println(actualTypeArgument); &#125;输出--&gt; company泛型类的泛型类是 : class com.my.entity.User 接口类似，可以有子接口和匿名类形式获取，不演示了。","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"泛型","slug":"泛型","permalink":"http://yoursite.com/tags/%E6%B3%9B%E5%9E%8B/"}]},{"title":"深度拷贝","slug":"深度拷贝","date":"2020-12-02T03:24:35.000Z","updated":"2022-11-22T01:27:54.174Z","comments":true,"path":"2020/12/02/深度拷贝/","link":"","permalink":"http://yoursite.com/2020/12/02/%E6%B7%B1%E5%BA%A6%E6%8B%B7%E8%B4%9D/","excerpt":"","text":"在说对象拷贝问题之前，我们先看看JVM内存模型，看看对象是怎么在内存里存在的。 对象实例存在堆内存里，它的引用（指针）在被使用时候，被栈里的user所持有。是不是明白了，我们平时在开发时候，所指对象d饿属性被修改了，对应引用的对象值都会变化。 概述 深拷贝对应的反面是浅拷贝，先做个统一的大白话定义： 浅拷贝：增加了一个指针，指向已存在的内存地址（原对象地址） 深拷贝：增加了一个指针，指向新的内存地址（新对象地址） 拷贝类型：基本类型（boolean、byte、char、int、short、float、long、double），引用类型（String、对象引用、数组等）。 拷贝过程：对于基本类型它是直接拷贝值的，引用类型可以分为（直接引用赋值、开辟内存再引用赋值） 区别：对于基本类型，深浅拷贝都是一样的，引用类型是不一样。 实现 为什么要有对象拷贝？深拷贝和浅拷贝，无非就是在权衡对象使用时候，对象间是否有互相影响问题（倘若属性值被改）。 浅拷贝方式： BeanUtils.copyProperties(source,target); spring 的和apache 都一样 PropertyUtils.copyProperties(target, source) apache的 obj.clone() 被拷贝对象需要实现Cloneable接口 深拷贝方式： 手动new，再赋值，属性过多时候搞死人 json序列化 JSON.parseObject(JSON.toJSONString(obj), Object.class) fastjson 库 字节流序列化 SerializationUtils.clone 被拷贝对象需要实现Serializable接口 总结：对象深浅拷贝，属于比较容易忽略的点，理解它后对于Java是值传递or引用传递是有帮助的。上述列举的工具类和方式不是全部，可以按照自己的喜好选择其他。","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[]},{"title":"spring装配","slug":"spring装配","date":"2020-11-21T02:13:52.000Z","updated":"2022-11-22T01:27:53.834Z","comments":true,"path":"2020/11/21/spring装配/","link":"","permalink":"http://yoursite.com/2020/11/21/spring%E8%A3%85%E9%85%8D/","excerpt":"","text":"spring从一定层面上讲是Bean容器，那么它是怎么实现Bean的装配，这块对于理解springBoot自动装配有很大帮助。这里说的装配，我指的是基于注解/代码的方式，毕竟现在开发都是注解先行，至于xml方式就不讨论了。 方式 模式注解 @Component 等（Spring2.5+） 配置类 @Configuration 与 @Bean （Spring3.0+） 模块装配 @EnableXXX 与 @Import （Spring3.1+） 模式注解 使用它在我们日常开发中很常见，@Component、@Service、@Repository 等。但是它又个天然的缺陷，只能在我们自己写的代码中，对于jar包中组件，无能为力。 配置类 @Configuration 与 @Bean 方式，这种方式我们在开发中也经常使用，例如配置数据源，一些中间件。但是它也有个天然的缺点，配置一多，类就显得很臃肿，编码和修改成本都比价高。 模块装配 这是个杀手锏，特别对于需要装配较多的bean到容器，一般它的套路是：@Enablexxx、@Import 注解来完成。 自定义元注解@EnableBean 12345@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)@Import(User.class)public @interface EnableBean &#123;&#125; 实体 12345678910@Datapublic class User &#123; private Integer id; private String name; public User() &#123; System.out.println(&quot;user init ...&quot;); &#125;&#125; 配置类 12345@Configuration@EnableBeanpublic class Config &#123;&#125; 启动容器 12345public static void main(String[] args) &#123; AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(Config.class); String[] beanDefinitionNames = ctx.getBeanDefinitionNames(); Stream.of(beanDefinitionNames).forEach(System.out::println);&#125; @Import 特别说明 这个注解隐含了模块装配的秘密，我们看看。 123456789101112@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Import &#123; /** * &#123;@link Configuration @Configuration&#125;, &#123;@link ImportSelector&#125;, * &#123;@link ImportBeanDefinitionRegistrar&#125;, or regular component classes to import. */ Class&lt;?&gt;[] value();&#125; 翻译下它的备注：@Import里面的值是个数组。可以是： 被@Configuration修饰的配置类 普通的Java bean 实现ImportSelector的类 实现ImportBeanDefinitionRegistrar的类 配置类、普通Java类就不说了，简单易懂。重点看看实现了：ImportSelector和ImportBeanDefinitionRegistrar的类。 ImportSelector 123456public class UserImport implements ImportSelector &#123; @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123; return new String[]&#123;User.class.getName()&#125;; &#125;&#125; ImportBeanDefinitionRegistrar 1234567public class FruitImport implements ImportBeanDefinitionRegistrar &#123; @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; registry.registerBeanDefinition(&quot;apple&quot;, new RootBeanDefinition(Apple.class)); registry.registerBeanDefinition(&quot;orange&quot;, new RootBeanDefinition(Orange.class)); &#125;&#125; 使用 12345@Configuration@Import(&#123;UserImport.class, FruitImport.class&#125;)public class Config &#123;&#125;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/tags/spring/"}]},{"title":"Java的日志体系","slug":"Java的日志体系","date":"2020-11-18T07:34:12.000Z","updated":"2022-11-22T01:27:53.381Z","comments":true,"path":"2020/11/18/Java的日志体系/","link":"","permalink":"http://yoursite.com/2020/11/18/Java%E7%9A%84%E6%97%A5%E5%BF%97%E4%BD%93%E7%B3%BB/","excerpt":"","text":"日志 Java体系日志很多，一直没有整理，乱乱的。好，我们先来看看日志框架组件以及大致处理流程： Loggers：Logger负责捕捉事件并将其发送给合适的Appender Appenders：也称为Handlers，负责从Logger中取出日志消息，并使用Layout来格式化消息，然后将消息发送出去，比如发送到控制台、文件或其他日志收集系统。 Layouts：也称为Formatters，负责对日志事件进中的数据进行转换和格式化。 Filters：过滤器，根据需要定制哪些信息会被记录，哪些信息会被放过。 框架 log4j(最早的Java领域日志，作者Ceki Gülcü，现归属Apache) jul（jdk1.4后新增，看到log4j后加的） logback（log4j作者Ceki Gülcü另外一个框架） log4j2（log4j的改良版本，Apache） 门面 jcl（Apache 推出 Jakarta Commons Logging） slf4j（log4j作者Ceki Gülcü写的，和logback一起出现的） 时间轴 使用 log4j log4j 1.x 版本，2015年5月，Apache宣布log4j1.x 停止更新。最新版为1.2.17。 依赖 12345&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt;&lt;/dependency&gt; classpath下配置log4j.properties 12345678910111213141516171819202122232425262728293031323334353637383940414243### 日志记录器Logger的全局设置 ####第一个参数指定输出的最低日志级别，即只输出该级别及以上级别的日志,D和E为自定义名字，对应下面appenderlog4j.rootLogger=DEBUG,stdout,D,E#*******设置控制台输出********log4j.appender.stdout=org.apache.log4j.ConsoleAppender#指定标准输出设备为系统输出设备log4j.appender.stdout.Target=System.out#指定使用自定义的格式化器log4j.appender.stdout.layout=org.apache.log4j.PatternLayout#指定日志格式log4j.appender.stdout.layout.ConversionPattern=[%-5p] %d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; method:%l%n%m%n#********文件输出每天一个文件********log4j.appender.D=org.apache.log4j.DailyRollingFileAppender#指定将日志输出到哪个文件中log4j.appender.D.File=./debug.log#指定文件写入方式为追加log4j.appender.D.Append=true#指定最低输出级别，针对这个appenderlog4j.appender.D.Threshold=DEBUGlog4j.appender.D.DatePattern=&#x27;.&#x27;yyyy-MM-dd-HH-mm#指定日志格式log4j.appender.D.layout=org.apache.log4j.SimpleLayout#********文件输出按照大小来生产文件********log4j.appender.E=org.apache.log4j.RollingFileAppenderlog4j.appender.E.File=./error.loglog4j.appender.E.Append=true#指定最低输出级别，针对这个appenderlog4j.appender.E.Threshold=ERROR#指定日志文件的最大尺寸log4j.appender.E.MaxFileSize=20KB#指定最大备份数为5log4j.appender.E.MaxBackupIndex=5#指定日志输出格式使用自定义格式log4j.appender.E.layout=org.apache.log4j.PatternLayout#指定日志格式log4j.appender.E.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss&#125; [ %t:%r ] - [ %p ] %m%n#********指定具体包打印日志级别********log4j.logger.org.mybatis=DEBUG,D appender 全路径 备注 org.apache.log4j.ConsoleAppender 控制台 org.apache.log4j.FileAppender 文件 org.apache.log4j.DailyRollingFileAppender 每天产生一个日志文件 org.apache.log4j.RollingFileAppender 文件大小到达指定尺寸的时候产生一个新的文件 org.apache.log4j.WriterAppender 将日志信息以流格式发送到任意指定的地方 属性配置 不同的appender有的属性不同，动手点到类里瞧瞧。例如RollingFileAppender的 File、Append、Threshold等。 伪Java 代码 1234import org.apache.log4j.Logger;// 获取logger Logger logger = Logger.getLogger(App.class.getName()); logger.error(&quot;11111&quot;); log4j2 2.x版本配置文件后缀，只能为&quot;.xml&quot;,&quot;.json&quot;或者&quot;.jsn&quot;。一般建议log4j2使用xml形式配置。默认没有配置文件情况，也会输出error以上级别的日志到控制台。 依赖 12345678910 &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-api&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt;&lt;/dependency&gt; classpath下配置log4j2.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration status=&quot;WARN&quot; monitorInterval=&quot;30&quot;&gt; &lt;!--先定义所有的appender--&gt; &lt;appenders&gt; &lt;!--这个输出控制台的配置--&gt; &lt;console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt; &lt;PatternLayout pattern=&quot;[%d&#123;HH:mm:ss:SSS&#125;] [%p] - %l - %m%n&quot;/&gt; &lt;/console&gt; &lt;!--文件会打印出所有信息，这个log每次运行程序会自动清空，由append属性决定，这个也挺有用的，适合临时测试用--&gt; &lt;File name=&quot;log&quot; fileName=&quot;log/test.log&quot; append=&quot;false&quot;&gt; &lt;PatternLayout pattern=&quot;%d&#123;HH:mm:ss.SSS&#125; %-5level %class&#123;36&#125; %L %M - %msg%xEx%n&quot;/&gt; &lt;/File&gt; &lt;!-- 这个会打印出所有的info及以下级别的信息，每次大小超过size，则这size大小的日志会自动存入按年份-月份建立的文件夹下面并进行压缩，作为存档--&gt; &lt;RollingFile name=&quot;RollingFileInfo&quot; fileName=&quot;$&#123;sys:user.home&#125;/logs/info.log&quot; filePattern=&quot;$&#123;sys:user.home&#125;/logs/$$&#123;date:yyyy-MM&#125;/info-%d&#123;yyyy-MM-dd&#125;-%i.log&quot;&gt; &lt;!--控制台只输出level及以上级别的信息（onMatch），其他的直接拒绝（onMismatch）--&gt; &lt;ThresholdFilter level=&quot;info&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot;/&gt; &lt;PatternLayout pattern=&quot;[%d&#123;HH:mm:ss:SSS&#125;] [%p] - %l - %m%n&quot;/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size=&quot;100 MB&quot;/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;RollingFile name=&quot;RollingFileWarn&quot; fileName=&quot;$&#123;sys:user.home&#125;/logs/warn.log&quot; filePattern=&quot;$&#123;sys:user.home&#125;/logs/$$&#123;date:yyyy-MM&#125;/warn-%d&#123;yyyy-MM-dd&#125;-%i.log&quot;&gt; &lt;ThresholdFilter level=&quot;warn&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot;/&gt; &lt;PatternLayout pattern=&quot;[%d&#123;HH:mm:ss:SSS&#125;] [%p] - %l - %m%n&quot;/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size=&quot;100 MB&quot;/&gt; &lt;/Policies&gt; &lt;!-- DefaultRolloverStrategy属性如不设置，则默认为最多同一文件夹下7个文件，这里设置了20 --&gt; &lt;DefaultRolloverStrategy max=&quot;20&quot;/&gt; &lt;/RollingFile&gt; &lt;RollingFile name=&quot;RollingFileError&quot; fileName=&quot;$&#123;sys:user.home&#125;/logs/error.log&quot; filePattern=&quot;$&#123;sys:user.home&#125;/logs/$$&#123;date:yyyy-MM&#125;/error-%d&#123;yyyy-MM-dd&#125;-%i.log&quot;&gt; &lt;ThresholdFilter level=&quot;error&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot;/&gt; &lt;PatternLayout pattern=&quot;[%d&#123;HH:mm:ss:SSS&#125;] [%p] - %l - %m%n&quot;/&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;SizeBasedTriggeringPolicy size=&quot;100 MB&quot;/&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;/appenders&gt; &lt;!--然后定义logger，只有定义了logger并引入的appender，appender才会生效，也就是说打印的日志内容会到相应文件中--&gt; &lt;loggers&gt; &lt;!--过滤掉spring和mybatis的一些无用的DEBUG信息，可以作为定义具体包日志级别--&gt; &lt;logger name=&quot;org.springframework&quot; level=&quot;INFO&quot;&gt;&lt;/logger&gt; &lt;logger name=&quot;org.mybatis&quot; level=&quot;INFO&quot;&gt;&lt;/logger&gt; &lt;root level=&quot;all&quot;&gt; &lt;appender-ref ref=&quot;Console&quot;/&gt; &lt;appender-ref ref=&quot;RollingFileInfo&quot;/&gt; &lt;appender-ref ref=&quot;RollingFileWarn&quot;/&gt; &lt;appender-ref ref=&quot;RollingFileError&quot;/&gt; &lt;/root&gt; &lt;/loggers&gt;&lt;/configuration&gt; 配置详解 节点写法不区分大小写，试验过。 1.根节点Configuration有两个属性：status和monitorinterval，有两个子节点：Appenders和Loggers(表明可以定义多个Appender和Logger)： status用来指定log4j本身的打印日志的级别。 monitorinterval用于指定log4j自动重新配置的监测间隔时间，单位是s，最小是5s 2.Appenders节点，常见的有三种子节点：Console、RollingFile、File： Console节点用来定义输出到控制台的Appender （1）name：指定 Appender 的名字 （2）target：SYSTEM_OUT 或SYSTEM_ERR，一般只设置默认：SYSTEM_OUT （3）PatternLayout：输出格式，不设置默认为%m%n File节点用来定义输出到指定位置的文件的Appender （1）name：指定Appender的名字 （2）fileName：指定输出日志的目的文件带全路径的文件名 （3）PatternLayout：输出格式，不设置默认为%m%n RollingFile节点用来定义超过指定大小自动删除旧的创建新的的Appender （1）name：指定Appender的名字 （2）fileName：指定输出日志的目的文件带全路径的文件名 （3）PatternLayout：输出格式，不设置默认为%m%n （4）filePattern：指定新建日志文件的名称格式 （5）Policies：指定滚动日志的策略，就是什么时候进行新建日志文件输出日志 （6）TimeBasedTriggeringPolicy：Policies子节点，基于时间的滚动策略，interval属性用来指定多久滚动一次，默认是1 hour。modulate=true用来调整时间：比如现在是早上3am，interval是4，那么第一次滚动是在4am，接着是8am，12am…而不是7am （7）SizeBasedTriggeringPolicy：Policies子节点，基于指定文件大小的滚动策略，size 属性用来定义每个日志文件的大小. （8）DefaultRolloverStrategy：用来指定同一个文件夹下最多有几个日志文件时开始删除最旧的，创建新的(通过max属性)。 3.Loggers节点，常见的有两种Root和Logger： Root 节点用来指定项目的根日志，如果没有单独指定 Logger，那么就会默认使用该 Root日志输出 level：日志输出级别，共有8个级别，按照从低到高为：All &lt; Trace &lt; Debug &lt; Info &lt; Warn &lt; Error &lt; Fatal &lt; OFF AppenderRef：Root的子节点，用来指定该日志输出到哪个Appender Logger 节点用来单独指定日志的形式，比如要为指定包下的class指定不同的日志级别等。 level：日志输出级别，共有8个级别，按照从低到高为：All &lt; Trace &lt; Debug &lt; Info &lt; Warn &lt; Error &lt; Fatal &lt; OFF name：用来指定该 Logger所适用的类或者类所在的包全路径,继承自 Root节点. AppenderRef：Logger的子节点，用来指定该日志输出到哪个Appender，如果没有指定，就会默认继承自 Root。如果指定了，那么会在指定的这个Appender和 Root的Appender中都会输出，此时我们可以设置 Logger的 additivity=&quot;false&quot;只在自定义的 Appender中进行输出。 关于日志 level 共有8个级别，按照从低到高为：All &lt; Trace &lt; Debug &lt; Info &lt; Warn &lt; Error &lt; Fatal &lt; OFF： All：最低等级的，用于打开所有日志记录. Trace：是追踪，就是程序推进以下，你就可以写个trace输出，所以trace应该会特别多，不过没关系，我们可以设置最低日志级别不让他输出. Debug：指出细粒度信息事件对调试应用程序是非常有帮助的. Info：消息在粗粒度级别上突出强调应用程序的运行过程. Warn：输出警告及warn以下级别的日志. Error：输出错误信息日志. Fatal：输出每个严重的错误事件将会导致应用程序的退出的日志. OFF：最高等级的，用于关闭所有日志记录. 程序会打印高于或等于所设置级别的日志，设置的日志等级越高，打印出来的日志就越少。 log4j和log4j2 日志级别有8个：按照从低到高为：All（打开所有日志级别）&lt; Trace &lt; Debug &lt; Info &lt; Warn &lt; Error &lt; Fatal &lt; OFF（关闭所有日志级别） JUL JDK 1.4 之后自带的log，位于java.util.logging包下。它使用起来也是很简单，不需要依赖额外的jar。 日志级别 ​ FINEST（最低值）–&gt;FINER–&gt;FINE–&gt;CONFIG–&gt;INFO（默认级别）–&gt;WARNING–&gt;SEVERE（最高值） 默认配置 $JAVA_HOME/jre/lib/logging.properties 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960############################################################# Default Logging Configuration File## You can use a different file by specifying a filename# with the java.util.logging.config.file system property.# For example java -Djava.util.logging.config.file=myfile######################################################################################################################### Global properties############################################################# &quot;handlers&quot; specifies a comma separated list of log Handler# classes. These handlers will be installed during VM startup.# Note that these classes must be on the system classpath.# By default we only configure a ConsoleHandler, which will only# show messages at the INFO and above levels.handlers= java.util.logging.ConsoleHandler# To also add the FileHandler, use the following line instead.#handlers= java.util.logging.FileHandler, java.util.logging.ConsoleHandler# Default global logging level.# This specifies which kinds of events are logged across# all loggers. For any given facility this global level# can be overriden by a facility specific level# Note that the ConsoleHandler also has a separate level# setting to limit messages printed to the console..level= INFO############################################################# Handler specific properties.# Describes specific configuration info for Handlers.############################################################# default file output is in user&#x27;s home directory.java.util.logging.FileHandler.pattern = %h/java%u.logjava.util.logging.FileHandler.limit = 50000java.util.logging.FileHandler.count = 1java.util.logging.FileHandler.formatter = java.util.logging.XMLFormatter# Limit the message that are printed on the console to INFO and above.java.util.logging.ConsoleHandler.level = INFOjava.util.logging.ConsoleHandler.formatter = java.util.logging.SimpleFormatter# Example to customize the SimpleFormatter output format# to print one-line log message like this:# &lt;level&gt;: &lt;log message&gt; [&lt;date/time&gt;]## java.util.logging.SimpleFormatter.format=%4$s: %5$s [%1$tc]%n############################################################# Facility specific properties.# Provides extra control for each logger.############################################################# For example, set the com.xyz.foo logger to only log SEVERE# messages:com.xyz.foo.level = SEVERE 自定义配置 1234567891011121314151617181920212223242526272829#配置RootLogger的Handler，有java.util.logging.ConsoleHandler,java.util.logging.FileHandlerhandlers= java.util.logging.ConsoleHandler,java.util.logging.FileHandler#针对所有handler与包的根日志级别.level= ALL#文件位置，可以是任何path，这样配置默认在项目根路径下java.util.logging.FileHandler.pattern = java%u.log#默认一个文件最多50000条日志记录java.util.logging.FileHandler.limit = 50000#设置FileHandle的日志级别为WARNING（低于这个级别就不打印到文件里）java.util.logging.FileHandler.level= WARNING#配置生成一个文件java.util.logging.FileHandler.count = 1#配置使用SimpleFormatter格式器java.util.logging.FileHandler.formatter = java.util.logging.SimpleFormatter#配置追加模式java.util.logging.FileHandler.append=true#ConsoleHandler的日志级别默认是INFO(低于这个级别就不打印到控制台)java.util.logging.ConsoleHandler.level = INFO#ConsoleHandler的默认格式化器时SimpleFormatterjava.util.logging.ConsoleHandler.formatter = java.util.logging.SimpleFormatter#设置日志格式java.util.logging.SimpleFormatter.format= %1$tc %2$s%n%4$s: %5$s%6$s%n 加载配置文件 12345678910public class JULLogConfig &#123; public static void main(String[] args) throws IOException &#123; InputStream inputStream = new FileInputStream(&quot;$&#123;file.path&#125;/logging.properties&quot;); LogManager manager = LogManager.getLogManager(); manager.readConfiguration(inputStream); Logger logger = Logger.getLogger(JULLogConfig.class.getName()); // 高于设置的WARNING级别，因此会打印到log文件里 logger.severe(&quot;999999999&quot;); &#125;&#125; logback logback和log4j是一个人写的 springboot默认使用的日志框架是logback。 三个模块组成 logback-core logback-classic logback-access 其他的关于性能，关于内存占用，关于测试，关于文档详见源码及官网说明 logback-core 是其它模块的基础设施，其它模块基于它构建，显然，logback-core 提供了一些关键的通用机制。logback-classic 的地位和作用等同于 Log4J，它也被认为是 Log4J 的一个改进版，并且它实现了简单日志门面 SLF4J；而 logback-access 主要作为一个与 Servlet 容器交互的模块，比如说tomcat或者 jetty，提供一些与 HTTP 访问相关的功能。 依赖 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;/dependency&gt; 配置文件logback.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;configuration&gt; &lt;!-- 属性文件:在properties文件中找到对应的配置项 --&gt; &lt;springProperty scope=&quot;context&quot; name=&quot;logging.path&quot; source=&quot;logging.path&quot;/&gt; &lt;springProperty scope=&quot;context&quot; name=&quot;logging.level&quot; source=&quot;logging.level.com.astilt.spring.boot&quot;/&gt; &lt;!-- 默认的控制台日志输出，一般生产环境都是后台启动，这个没太大作用 --&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;Pattern&gt;%d&#123;HH:mm:ss.SSS&#125; %-5level %logger&#123;80&#125; - %msg%n&lt;/Pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name=&quot;logFile&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;append&gt;true&lt;/append&gt; &lt;!--临界值过滤器，过滤掉低于指定临界值的日志--&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;$&#123;logging.level&#125;&lt;/level&gt; &lt;/filter&gt; &lt;!--指定被写入的文件名，可以是相对目录，也可以是绝对目录，如果上级目录不存在会自动创建，没有默认值--&gt; &lt;file&gt; $&#123;logging.path&#125;/glmapper-spring-boot/glmapper-loggerone.log &lt;/file&gt; &lt;!-- 每天生成一个日志文件，保存30天的日志文件 --&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;FileNamePattern&gt;$&#123;logging.path&#125;/glmapper-spring-boot/glmapper-loggerone.log.%d&#123;yyyy-MM-dd&#125;&lt;/FileNamePattern&gt; &lt;!--日志文件保留天数--&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- com.astilt.controller这个包下的$&#123;logging.level&#125;级别的日志将会使用logFile来打印--&gt; &lt;logger name=&quot;com.astilt.controller&quot; level=&quot;$&#123;logging.level&#125;&quot; additivity=&quot;false&quot;&gt; &lt;appender-ref ref=&quot;logFile&quot; /&gt;&lt;/logger&gt; &lt;root level=&quot;$&#123;logging.level&#125;&quot;&gt; &lt;appender-ref ref=&quot;STDOUT&quot;/&gt;&lt;/root&gt;&lt;/configuration&gt; xml的节点： 12345678910111213141516&lt;configuration scan=&quot;true&quot; scanPeriod=&quot;60 seconds&quot; debug=&quot;false&quot;&gt; &lt;property name=&quot;glmapper-name&quot; value=&quot;glmapper-demo&quot; /&gt; &lt;contextName&gt;$&#123;glmapper-name&#125;&lt;/contextName&gt; &lt;appender&gt; //xxxx &lt;/appender&gt; &lt;!-- 用来设置某一个包或者具体的某一个类的日志打印级别以及指定appender。--&gt; &lt;logger&gt; //xxxx &lt;/logger&gt; &lt;root&gt; //xxxx &lt;/root&gt; &lt;/configuration&gt; scan:当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true。 scanPeriod:设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟。 debug:当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。 节点 root：根logger，也是logger logger：普通的logger appender：需要被 logger或者root的appender-ref指向，它种类有 ConsoleAppender：把日志添加到控制台 FileAppender：把日志添加到文件 RollingFileAppender：滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件。它是FileAppender的子类 JCL Jakarta Commons-logging（JCL）是Apache 最早提供的日志的框架接口。它本身并没有记录log的功能，只是统一了JDK Logging与Log4j的AP。 动态查找原理：Log 是一个接口声明。LogFactory 的内部会去装载具体的日志系统，并获得实现该Log 接口的实现类。LogFactory 内部装载日志系统的流程如下： 首先，寻找org.apache.commons.logging.LogFactory 属性配置。 否则，利用JDK1.3 开始提供的service 发现机制，会扫描classpah 下的META-INF/services/org.apache.commons.logging.LogFactory文件，若找到则装载里面的配置，使用里面的配置。 否则，从Classpath 里寻找commons-logging.properties ，找到则根据里面的配置加载。 否则，使用默认的配置：如果能找到Log4j 则默认使用log4j 实现，如果没有则使用JDK14Logger 实现，再没有则使用commons-logging 内部提供的SimpleLog 实现。 依赖 1234567891011 &lt;dependency&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 我是用是log4j--&gt;&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt;&lt;/dependency&gt; 伪Java 代码 1234import org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;Log logger = LogFactory.getLog(App.class);logger.info(&quot;111111&quot;); log4j.properties 如上，不列了。 Slf4J The Simple Logging Facade for Java（SLF4J）可作为各种日志记录框架（例如java.util.logging，logback和log4j）的简单外观或抽象。 说白了就是提供统一接口，实现由各个具体的日志框架实现。目前支持的日志框架有： slf4j-log4j12-1.7.28.jar （log4j 1.2.x 版本）+log4j slf4j-jdk14-1.7.28.jar（JDK 1.4日志记录） slf4j-nop-1.7.28.jar slf4j-simple-1.7.28.jar slf4j-jcl-1.7.28.jar logback-classic-1.2.3.jar 、logback-core-1.2.3.jar 按需添加自己使用的日志jar。并加上对应的日志配置文件。 使用的伪Java代码如下： 1234import org.slf4j.Logger;import org.slf4j.LoggerFactory;Logger logger = LoggerFactory.getLogger(App.class);logger.error(&quot;11111&quot;); 在SprngBoot中自带了slf4j+logback，不需要导入包，开箱即用，也是我平时使用最多的方式。 在使用层面，我觉得log4j、logback最方便，配置和概念都比较清晰，也比较相像。至于JUL、log4j2 使用的比较少，JCL也不多，目前基本上slf4j+logback可以畅行江湖，推荐！","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"日志","slug":"日志","permalink":"http://yoursite.com/tags/%E6%97%A5%E5%BF%97/"}]},{"title":"mybatis原理","slug":"mybatis原理","date":"2020-11-18T03:33:23.000Z","updated":"2022-11-22T01:27:53.701Z","comments":true,"path":"2020/11/18/mybatis原理/","link":"","permalink":"http://yoursite.com/2020/11/18/mybatis%E5%8E%9F%E7%90%86/","excerpt":"","text":"JDBC 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public static void main(String[] args) &#123; Connection conn = null; Statement stat = null; ResultSet result = null; try &#123; // 1、注册驱动 Class.forName(&quot;com.mysql.jdbc.Driver&quot;); // 2、获得连接 conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/test?characterEncoding=utf-8&amp;serverTimezone=UTC&amp;useSSL=false&quot;, &quot;root&quot;, &quot;123456&quot;); // 3、获取数据库操作对象 stat = conn.createStatement(); // 4、执行sql result = stat.executeQuery(&quot;select id,name,age from emp&quot;); // 5、处理查询结果集 while (result.next()) &#123; int id = result.getInt(&quot;id&quot;); String name = result.getString(&quot;name&quot;); int age = result.getInt(&quot;age&quot;); &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; // 6、释放资源 if (result != null) &#123; try &#123; stat.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (stat != null) &#123; try &#123; stat.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (conn != null) &#123; try &#123; conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"myBatis","slug":"myBatis","permalink":"http://yoursite.com/tags/myBatis/"}]},{"title":"mybatis使用","slug":"mybatis使用","date":"2020-11-18T03:32:07.000Z","updated":"2022-11-22T01:27:53.700Z","comments":true,"path":"2020/11/18/mybatis使用/","link":"","permalink":"http://yoursite.com/2020/11/18/mybatis%E4%BD%BF%E7%94%A8/","excerpt":"","text":"Mybatis 官网地址 快速开始 Xml 方式 1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;environments default=&quot;development&quot;&gt; &lt;environment id=&quot;development&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot;/&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;$&#123;driver&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot;/&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;username&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;password&#125;&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;mapper resource=&quot;org/mybatis/example/BasRole.xml&quot;/&gt; &lt;/mappers&gt;&lt;/configuration&gt; BasRole.xml 123456789&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;org.example.mapper.BasRoleMapper&quot;&gt; &lt;select id=&quot;getOne&quot; resultType=&quot;org.example.entity.BasRole&quot; parameterType=&quot;int&quot; useCache=&quot;true&quot;&gt; SELECT id,role_code,role_name,descr FROM bas_role where id =#&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; 构建 SqlSessionFactory 123String resource = &quot;org/mybatis/example/mybatis-config.xml&quot;;InputStream inputStream = Resources.getResourceAsStream(resource);SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); Java代码方式 1234567// BasRoleDataSourceFactory可以替换成自己的，或者第三方的的链接池DataSource dataSource = BasRoleDataSourceFactory.getDataSource();TransactionFactory transactionFactory = new JdbcTransactionFactory();Environment environment = new Environment(&quot;development&quot;, transactionFactory, dataSource);Configuration configuration = new Configuration(environment);configuration.addMapper(BasRoleMapper.class);SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(configuration); 获取SqlSession并执行 SqlSessionFactory–&gt;SqlSession–&gt;sql SqlSessionFactory–&gt;SqlSession–&gt;Mapper–&gt;sql 1234567try (SqlSession session = sqlSessionFactory.openSession()) &#123; // SqlSession执行sql BasRole basRole = (BasRole)session.selectOne(&quot;org.example.mapper.BasRoleMapper.getOne&quot;, 8); // Mapper执行sql BasRoleMapper mapper = session.getMapper(BasRoleMapper.class); BasRole bassRole2 = mapper.getOne(8);&#125; 核心组件 SqlSessionFactoryBuilder：看名知意，产生SqlSessionFactory，一般是基于xml/Java配置方式构建SessionFactory。它的**生命周期在构建完SqlSessionFactory就结束了 SqlSessionFactory：看名知意，产生SqlSession，每次和db交互都需要从这里取sqlSession。它的生命周期是整个mybatis应用，一般一个数据库一个实例。 SqlSession：它相当于JDBC里面的Connection，每次访问数据库的“回话”，它的生命周期是每次执行“回话”过程。使用完就关闭！ Sql Mapper：一般我们可以认为它由Java接口+xml文件组成。负责执行sql并返回结果，生命周期为sqlSession的“回话”内，方法级别。 缓存 12345678910111213try (SqlSession session = sqlSessionFactory.openSession()) &#123; BasRole basRole = (BasRole)session.selectOne(&quot;org.example.mapper.BasRoleMapper.getOne&quot;, 8); System.out.println(&quot;一级缓存，第一个session，第一次执行&quot;); BasRole basRole2 = (BasRole)session.selectOne(&quot;org.example.mapper.BasRoleMapper.getOne&quot;, 8); System.out.println(&quot;一级缓存，第一个session，第二次执行&quot;); &#125;try (SqlSession session2 = sqlSessionFactory.openSession()) &#123; BasRole basRole = (BasRole)session2.selectOne(&quot;org.example.mapper.BasRoleMapper.getOne&quot;, 8); System.out.println(&quot;一级缓存，第二个session，第一次执行&quot;); BasRole basRole4 = (BasRole)session2.selectOne(&quot;org.example.mapper.BasRoleMapper.getOne&quot;, 8); System.out.println(&quot;一级缓存，第二个session，第二次执行&quot;); &#125; 一级缓存 默认开启，在一个sqlSession的“会话里生效”。日志如下： 123456789101112131415161718192021222324252627282930313233org.apache.ibatis.logging.jdbc.BaseJdbcLogger 2020-11-18 09:53:06 ==&gt; Preparing: SELECT id,role_code,role_name,descr FROM bas_role where id =? org.apache.ibatis.logging.jdbc.BaseJdbcLogger 2020-11-18 09:53:06 ==&gt; Parameters: 8(Integer) org.apache.ibatis.logging.jdbc.BaseJdbcLogger 2020-11-18 09:53:06 &lt;== Total: 1 一级缓存，第一个session，第一次执行一级缓存，第一个session，第二次执行org.apache.ibatis.transaction.jdbc.JdbcTransaction 2020-11-18 09:53:06 Resetting autocommit to true on JDBC Connection [com.mysql.cj.jdbc.ConnectionImpl@47eaca72] org.apache.ibatis.transaction.jdbc.JdbcTransaction 2020-11-18 09:53:06 Closing JDBC Connection [com.mysql.cj.jdbc.ConnectionImpl@47eaca72] org.apache.ibatis.datasource.pooled.PooledDataSource 2020-11-18 09:53:06 Returned connection 1206569586 to pool. org.apache.ibatis.transaction.jdbc.JdbcTransaction 2020-11-18 09:53:06 Opening JDBC Connection org.apache.ibatis.datasource.pooled.PooledDataSource 2020-11-18 09:53:06 Checked out connection 1206569586 from pool. org.apache.ibatis.transaction.jdbc.JdbcTransaction 2020-11-18 09:53:06 Setting autocommit to false on JDBC Connection [com.mysql.cj.jdbc.ConnectionImpl@47eaca72] org.apache.ibatis.logging.jdbc.BaseJdbcLogger 2020-11-18 09:53:06 ==&gt; Preparing: SELECT id,role_code,role_name,descr FROM bas_role where id =? org.apache.ibatis.logging.jdbc.BaseJdbcLogger 2020-11-18 09:53:06 ==&gt; Parameters: 8(Integer) org.apache.ibatis.logging.jdbc.BaseJdbcLogger 2020-11-18 09:53:06 &lt;== Total: 1 一级缓存，第二个session，第一次执行一级缓存，第二个session，第二次执行org.apache.ibatis.logging.jdbc.BaseJdbcLogger 2020-11-18 09:53:06 ==&gt; Preparing: SELECT id,role_code,role_name,descr FROM bas_role where id =? org.apache.ibatis.logging.jdbc.BaseJdbcLogger 2020-11-18 09:53:06 ==&gt; Parameters: 8(Integer) org.apache.ibatis.logging.jdbc.BaseJdbcLogger 2020-11-18 09:53:06 &lt;== Total: 1 一级缓存，第一个session，第一次执行一级缓存，第一个session，第二次执行org.apache.ibatis.transaction.jdbc.JdbcTransaction 2020-11-18 09:53:06 Resetting autocommit to true on JDBC Connection [com.mysql.cj.jdbc.ConnectionImpl@47eaca72] org.apache.ibatis.transaction.jdbc.JdbcTransaction 2020-11-18 09:53:06 Closing JDBC Connection [com.mysql.cj.jdbc.ConnectionImpl@47eaca72] org.apache.ibatis.datasource.pooled.PooledDataSource 2020-11-18 09:53:06 Returned connection 1206569586 to pool. org.apache.ibatis.transaction.jdbc.JdbcTransaction 2020-11-18 09:53:06 Opening JDBC Connection org.apache.ibatis.datasource.pooled.PooledDataSource 2020-11-18 09:53:06 Checked out connection 1206569586 from pool. org.apache.ibatis.transaction.jdbc.JdbcTransaction 2020-11-18 09:53:06 Setting autocommit to false on JDBC Connection [com.mysql.cj.jdbc.ConnectionImpl@47eaca72] org.apache.ibatis.logging.jdbc.BaseJdbcLogger 2020-11-18 09:53:06 ==&gt; Preparing: SELECT id,role_code,role_name,descr FROM bas_role where id =? org.apache.ibatis.logging.jdbc.BaseJdbcLogger 2020-11-18 09:53:06 ==&gt; Parameters: 8(Integer) org.apache.ibatis.logging.jdbc.BaseJdbcLogger 2020-11-18 09:53:06 &lt;== Total: 1 一级缓存，第二个session，第一次执行一级缓存，第二个session，第二次执行 二级缓存 默认不开启，需要在SQL 映射文件中配置&lt;cache/&gt;,最好POJO对象是可序列化的，即实现 Serializable。 1&lt;cache eviction=&quot;FIFO&quot; flushInterval=&quot;60000&quot; size=&quot;512&quot; readOnly=&quot;true&quot;/&gt; 这个更高级的配置创建了一个 FIFO 缓存，每隔 60 秒刷新，最多可以存储结果对象或列表的 512 个引用，而且返回的对象被认为是只读的，因此对它们进行修改可能会在不同线程中的调用者产生冲突。 可用的清除策略有： LRU – 最近最少使用：移除最长时间不被使用的对象。 FIFO – 先进先出：按对象进入缓存的顺序来移除它们。 SOFT – 软引用：基于垃圾回收器状态和软引用规则移除对象。 WEAK – 弱引用：更积极地基于垃圾收集器状态和弱引用规则移除对象。 默认的清除策略是 LRU。 flushInterval（刷新间隔）属性可以被设置为任意的正整数，设置的值应该是一个以毫秒为单位的合理时间量。 默认情况是不设置，也就是没有刷新间隔，缓存仅仅会在调用语句时刷新。 size（引用数目）属性可以被设置为任意正整数，要注意欲缓存对象的大小和运行环境中可用的内存资源。默认值是 1024。 readOnly（只读）属性可以被设置为 true 或 false。只读的缓存会给所有调用者返回缓存对象的相同实例。 因此这些对象不能被修改。这就提供了可观的性能提升。而可读写的缓存会（通过序列化）返回缓存对象的拷贝。 速度上会慢一些，但是更安全，因此默认值是 false。 看看日志： 12345678910111213org.apache.ibatis.logging.jdbc.BaseJdbcLogger 2020-11-18 09:54:36 &#x3D;&#x3D;&gt; Preparing: SELECT id,role_code,role_name,descr FROM bas_role where id &#x3D;? org.apache.ibatis.logging.jdbc.BaseJdbcLogger 2020-11-18 09:54:36 &#x3D;&#x3D;&gt; Parameters: 8(Integer) org.apache.ibatis.logging.jdbc.BaseJdbcLogger 2020-11-18 09:54:36 &lt;&#x3D;&#x3D; Total: 1 一级缓存，第一个session，第一次执行org.apache.ibatis.cache.decorators.LoggingCache 2020-11-18 09:54:36 Cache Hit Ratio [org.example.mapper.BasRoleMapper]: 0.0 一级缓存，第一个session，第二次执行org.apache.ibatis.transaction.jdbc.JdbcTransaction 2020-11-18 09:54:36 Resetting autocommit to true on JDBC Connection [com.mysql.cj.jdbc.ConnectionImpl@1bb5a082] org.apache.ibatis.transaction.jdbc.JdbcTransaction 2020-11-18 09:54:36 Closing JDBC Connection [com.mysql.cj.jdbc.ConnectionImpl@1bb5a082] org.apache.ibatis.datasource.pooled.PooledDataSource 2020-11-18 09:54:36 Returned connection 464887938 to pool. org.apache.ibatis.cache.decorators.LoggingCache 2020-11-18 09:54:36 Cache Hit Ratio [org.example.mapper.BasRoleMapper]: 0.3333333333333333 一级缓存，第二个session，第一次执行org.apache.ibatis.cache.decorators.LoggingCache 2020-11-18 09:54:36 Cache Hit Ratio [org.example.mapper.BasRoleMapper]: 0.5 一级缓存，第二个session，第二次执行 总结：一级缓存默认开启，作用在一个sqlSession，二级缓存默认不开启，开启后作用域在一个所有sqlSession的一个mapper上。 自定义缓存 可是实现 org.apache.ibatis.cache.Cache 的接口，覆盖mybaits自带的二级缓存。例如使用redis作为缓存介质。 123456789101112131415161718192021222324252627282930313233343536373839404142@Componentpublic class MyCache implements Cache &#123; @Autowired private RedisUtil redisUtil; @Override public String getId() &#123; return null; &#125; @Override public void putObject(Object key, Object o1) &#123; // 自行发挥。。。 &#125; @Override public Object getObject(Object key) &#123; // 自行发挥。。。 return null; &#125; @Override public Object removeObject(Object o) &#123; return null; &#125; @Override public void clear() &#123; &#125; @Override public int getSize() &#123; return 0; &#125; @Override public ReadWriteLock getReadWriteLock() &#123; return null; &#125;&#125; 动态SQL 元素 作用 备注 if 判断语句 单条件分支 choose(when、otherwise) 类似Java里面 if(){}i … f(){}else{} 多条件分支 trim(where、set) 辅助 处理sql拼接 foreach 循环 批量处理 Bind 定义ognl表达式 like模糊查询、不同db函数不兼容问题（or || &amp;&amp; 等） if 123&lt;if test=&quot;author != null and author.name != null&quot;&gt; AND author_name like #&#123;author.name&#125; &lt;/if&gt; choose 123456789101112131415&lt;select id=&quot;findActiveBlogLike&quot; resultType=&quot;Blog&quot;&gt; SELECT * FROM BLOG WHERE state = ‘ACTIVE’ &lt;choose&gt; &lt;when test=&quot;title != null&quot;&gt; AND title like #&#123;title&#125; &lt;/when&gt; &lt;when test=&quot;author != null and author.name != null&quot;&gt; AND author_name like #&#123;author.name&#125; &lt;/when&gt; &lt;otherwise&gt; AND featured = 1 &lt;/otherwise&gt; &lt;/choose&gt;&lt;/select&gt; where 1234567891011&lt;where&gt; &lt;if test=&quot;state != null&quot;&gt; state = #&#123;state&#125; &lt;/if&gt; &lt;if test=&quot;title != null&quot;&gt; AND title like #&#123;title&#125; &lt;/if&gt; &lt;if test=&quot;author != null and author.name != null&quot;&gt; AND author_name like #&#123;author.name&#125; &lt;/if&gt; &lt;/where&gt; trim prefix:在trim标签内sql语句加上前缀。 suffix: 在trim标签内sql语句加上后缀。 suffixOverrides:指定去除多余的后缀内容，如：suffixOverrides=&quot;,&quot;，去除trim标签内sql语句多余的后缀&quot;,&quot;。 prefixOverrides: 指定去除多余的前缀内容 1234567891011121314151617181920212223&lt;trim prefix=&quot;values (&quot; suffix=&quot;)&quot; suffixOverrides=&quot;,&quot;&gt; &lt;if test=&quot;id != null&quot;&gt; #&#123;id,jdbcType=BIGINT&#125;, &lt;/if&gt; &lt;if test=&quot;userId != null&quot;&gt; #&#123;userId,jdbcType=BIGINT&#125;, &lt;/if&gt; &lt;if test=&quot;dealId != null&quot;&gt; #&#123;dealId,jdbcType=BIGINT&#125;, &lt;/if&gt; &lt;if test=&quot;dealSkuId != null&quot;&gt; #&#123;dealSkuId,jdbcType=BIGINT&#125;, &lt;/if&gt; &lt;if test=&quot;count != null&quot;&gt; #&#123;count,jdbcType=INTEGER&#125;, &lt;/if&gt; &lt;if test=&quot;createTime != null&quot;&gt; #&#123;createTime,jdbcType=TIMESTAMP&#125;, &lt;/if&gt; &lt;if test=&quot;updateTime != null&quot;&gt; #&#123;updateTime,jdbcType=TIMESTAMP&#125;, &lt;/if&gt; &lt;/trim&gt; set 12345678&lt;update id=&quot;updateUserBySet&quot; parameterType=&quot;com.po.MyUser&quot;&gt; update user &lt;set&gt; &lt;if test=&quot;uname!=null&quot;&gt;uname=#&#123;uname&#125;&lt;/if&gt; &lt;if test=&quot;usex!=null&quot;&gt;usex=#&#123;usex&#125;&lt;/if&gt; &lt;/set&gt; where uid=#&#123;uid&#125;&lt;/update&gt; foreach item表示集合中每一个元素进行迭代时的别名， index指 定一个名字，用于表示在迭代过程中，每次迭代到的位置， open表示该语句以什么开始， separator表示在每次进行迭代之间以什么符号作为分隔 符， close表示以什么结束。 123456&lt;select id=&quot;dynamicForeachTest&quot; resultType=&quot;Blog&quot;&gt; select * from t_blog where id in &lt;foreach collection=&quot;list&quot; index=&quot;index&quot; item=&quot;item&quot; open=&quot;(&quot; separator=&quot;,&quot; close=&quot;)&quot;&gt; #&#123;item&#125; &lt;/foreach&gt; &lt;/select&gt; bind 123456789101112131415&lt;select id=&quot;selectUseIf&quot; parameterType=&quot;com.soft.test.model.DynamicTestModel&quot; resultMap=&quot;userMap&quot;&gt; &lt;if test=&quot;username != null and username neq &#x27;&#x27;&quot;&gt; &lt;bind name=&quot;pattern&quot; value=&quot;&#x27;%&#x27; + _parameter.getUsername() + &#x27;%&#x27;&quot; /&gt; &lt;/if&gt; select * from t_user &lt;where&gt; &lt;if test=&quot;username != null and username neq &#x27;&#x27;&quot;&gt; username like #&#123;pattern&#125; &lt;/if&gt; &lt;!-- 不然就使用如下的concat函数--&gt; &lt;!-- &lt;if test=&quot;name!=null and name!=&#x27;&#x27;&quot;&gt;--&gt; &lt;!-- username like CONCAT(&#x27;%&#x27;, &#x27;$&#123;username&#125;&#x27;, &#x27;%&#x27;)--&gt; &lt;!-- &lt;/if&gt;--&gt; &lt;/where&gt; &lt;/select&gt;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"myBatis","slug":"myBatis","permalink":"http://yoursite.com/tags/myBatis/"}]},{"title":"Java8经典用法","slug":"Java8经典用法","date":"2020-11-13T06:04:45.000Z","updated":"2022-12-16T05:48:06.260Z","comments":true,"path":"2020/11/13/Java8经典用法/","link":"","permalink":"http://yoursite.com/2020/11/13/Java8%E7%BB%8F%E5%85%B8%E7%94%A8%E6%B3%95/","excerpt":"","text":"List --&gt; Map Map &lt;属性，属性&gt; 123List&lt;User&gt; = getUsers();Map&lt;String,Integer&gt; userMap = Maps.newHashMap();userMap = list.stream().collect(Collectors.toMap(User::getName, User::getAge)); Map &lt;属性，集合&gt; 12345678910111213userList.stream().collect(Collectors.toMap(User::getId, user -&gt; user));// import com.google.common.collect.Lists;Map&lt;String, List&lt;User&gt;&gt; mapList = userList.stream() .collect(Collectors.toMap( User::getUserNumber, Lists::newArrayList, (List&lt;User&gt; newList, List&lt;User&gt; oldList) -&gt; &#123; oldList.addAll(newList); return oldList; &#125;)); Map&lt;Long, List&lt;BasBehaviorRecord&gt;&gt; userBehaviorMap = behaviorRecordList.stream().collect(Collectors.groupingBy(BasBehaviorRecord::getUserId)); List 根据属性去重 123456import static java.util.stream.Collectors.collectingAndThen;import static java.util.stream.Collectors.toCollection;List&lt;AdmCustBuyHis&gt; all = buyHisService.list();all = all.stream().collect(collectingAndThen(toCollection(() -&gt; new TreeSet&lt;&gt;(Comparator.comparing(AdmCustBuyHis::getCustId))), ArrayList::new)); List 123456789101112131415161718192021222324252627// 交集 List&lt;String&gt; intersection = list1.stream().filter(item -&gt; list2.contains(item)).collect(toList()); System.out.println(&quot;---交集 intersection---&quot;); intersection.parallelStream().forEach(System.out::println); // 差集 (list1 - list2) List&lt;String&gt; reduce1 = list1.stream().filter(item -&gt; !list2.contains(item)).collect(toList()); System.out.println(&quot;---差集 reduce1 (list1 - list2)---&quot;); reduce1.parallelStream().forEach(System.out::println); // 差集 (list2 - list1) List&lt;String&gt; reduce2 = list2.stream().filter(item -&gt; !list1.contains(item)).collect(toList()); System.out.println(&quot;---差集 reduce2 (list2 - list1)---&quot;); reduce2.parallelStream().forEach(System.out::println); // 并集 List&lt;String&gt; listAll = list1.parallelStream().collect(toList()); List&lt;String&gt; listAll2 = list2.parallelStream().collect(toList()); listAll.addAll(listAll2); System.out.println(&quot;---并集 listAll---&quot;); listAll.parallelStream().forEachOrdered(System.out::println); // 去重并集 List&lt;String&gt; listAllDistinct = listAll.stream().distinct().collect(toList()); System.out.println(&quot;---得到去重并集 listAllDistinct---&quot;); listAllDistinct.parallelStream().forEachOrdered(System.out::println);","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"lambda","slug":"lambda","permalink":"http://yoursite.com/tags/lambda/"}]},{"title":"idea快捷键备忘","slug":"idea快捷键备忘","date":"2020-11-05T12:33:56.000Z","updated":"2022-11-22T01:27:53.659Z","comments":true,"path":"2020/11/05/idea快捷键备忘/","link":"","permalink":"http://yoursite.com/2020/11/05/idea%E5%BF%AB%E6%8D%B7%E9%94%AE%E5%A4%87%E5%BF%98/","excerpt":"","text":"记录为MacOS 下的idea的快捷键，备忘 optimize imports：⌃+⌥+o 全屏/退出：⌃+⌘+F","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"idea","slug":"idea","permalink":"http://yoursite.com/tags/idea/"}]},{"title":"postgresql简单使用","slug":"postgresql简单使用","date":"2020-11-04T03:54:46.000Z","updated":"2022-11-22T02:17:28.708Z","comments":true,"path":"2020/11/04/postgresql简单使用/","link":"","permalink":"http://yoursite.com/2020/11/04/postgresql%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/","excerpt":"","text":"在pg里面有schema的概念。这里我们可以简单理解下：database（仓库）–&gt; schema（房间）–&gt;对象（表、视图、触发器） 安装 拉取镜像并运行：docker run -it --name greenplum -p 5432:5432 -d kevinmtrowbridge/greenplumdb_singlenode 进入到容器内：docker exec -it greenplum /bin/bash su - gpadmin 查看gp和pg的版本号：gpstate -i gp：4.3.7.1 pg：8.2.15 登陆：psql -h localhost -p 5432 -U gpadmin template1 切换到 postgres库：\\c postgres 创建用户：CREATE USER astilt PASSWORD 'astilt'; 修改密码：alter user postgres with password 'new password'; 授权 12select &#39;grant all on table &#39; || schemaname || &#39;.&#39; || tablename || &#39; to username;&#39; from pg_tableswhere schemaname &#x3D; &#39;schemaName&#39; 客户端navicat登陆吧，username：astilt，pwd：astilt 配置允许远程客户端连接 使用find [pg的安装目录] -name pg_hba.conf 最后一行加：host all all 0.0.0.0/0 md5 切换到gpadmin，执行gpstop -u 重新加载配置文件 mysql对比 操作 操作 MySQL GP 退出 \\q 切换数据库 USE {dbname} \\c {dbname} 查看当前库/用户 无 \\c 查看所有数据库 SHOW DATABASE \\l 切换Schema 无 SET search_path={schemaname} 查看Schema 无 \\dn 列出某个schema的表 \\dt public.* 列出所有schema的表 \\dt . 查看所用用户 select * from mysql.user \\du 查看所有表 SHOW TABLES \\d 查看表结构 DESC {tablename} \\d {tablename} 按列展示结果 \\G（跟在SQL语句后） \\x（单独运行，再次运行切回按行） 查看运行时间 默认展示 \\timing 查看客户端连接 SHOW PROCESSLIST SELECT * FROM pg_stat_activity 客户端工具 mysql -h{hostname} -P{port} -u{user} -p{pwd} {dbname} PGPASSWORD={pwd} psql -h{hostname} -p{port} -U{user} -d {dbname} 1 基本语法 自增长 Mysql 12345CREATE TABLE `mysql_test` ( `id` bigint NOT NULL AUTO_INCREMENT, `name` varchar(32) NOT NULL, PRIMARY KEY(`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 pg 使用的 Sequence 12345678910111213// 1CREATE TABLE gp_test ( id SERIAL NOT NULL, name varchar(32) NOT NULL , PRIMARY KEY(id) ) // 2 CREATE SEQUENCE users_id_seq INCREMENT BY 2 MINVALUE 10000 MAXVALUE 99999; CREATE TABLE gp_test ( id bigint NOT NULL DEFAULT NEXTVAL (&#x27;users_id_seq&#x27;), name varchar(32) NOT NULL , PRIMARY KEY(id) ) 字段类型 MySQL GP TINYINT SMALLINT MEDIUMINT INTEGER TINYINT UNSIGNED SMALLINT check({columnname} &gt;= 0) FLOAT REAL DATETIME TIMESTAMP LONGTEXT TEXT MEDIUMTEXT TEXT BLOB BYTEA PG 常用创建 用户/角色 12345678CREATE ROLE custom PASSWORD &#x27;custom&#x27; [with login]; (默认创建后不能登陆)CREATE USER custom PASSWORD &#x27;custom&#x27;;（创建后能登陆）ALTER ROLE custom LOGIN;（新增login权限，可以登陆了）GRANT ALL PRIVILEGES ON DATABASE hf TO custom;（将hf数据库的所有权限都赋予custom用户）// 登陆psql -U custom -d hf -h 192.168.90.112 -p 5432// 修改密码\\password [USERNAME] 用户资源队列 1234567891011// 查看用户和用户的资源队列SELECT rolname, rsqname FROM pg_roles, gp_toolkit.gp_resqueue_status WHERE pg_roles.rolresqueue=gp_toolkit.gp_resqueue_status.queueid;// 创建资源队列 连接数=3，最大内存使用：1024MB,优先级：低DROP RESOURCE QUEUE test_queuecreate resource queue test_queue with (active_statements=3,MEMORY_LIMIT=&#x27;1024MB&#x27;,PRIORITY=LOW)；// 创建用户 并指定默认资源队列CREATE ROLE tuser WITH LOGIN PASSWORD &#x27;tuser&#x27; resource queue test_queue;//select &#x27;grant all on SCHEMA &#x27; || tt.autnspname || &#x27; to tuser;&#x27; as grant_script from gp_toolkit.__gp_user_tables tt -- group by tt.autnspname -- 给数据库授权unionselect &#x27;grant all on table &#x27; || tt.autnspname || &#x27;.&#x27; ||tt.autrelname || &#x27; to tuser;&#x27; grant_script from gp_toolkit.__gp_user_tables tt; -给表授权 schema 12345678// 创建一个新模式ord，对应于登录用户custom：CREATE SCHEMA ord; ALTER SCHEMA ord OWNER TO custom;// 再次创建一张test表，这次这张表要指明模式ordCREATE TABLE ord.test (id integer not null);drop schema gpadmin; --空schemadrop schema gpadmin cascade;--一起删除 database 123// 数据库所有者是当前创建数据库的角色，默认的表空间是系统的默认表空间pg_default。在`PostgreSQL`中，数据的创建是通过**克隆**数据库模板来实现的，这与SQL SERVER是同样的机制。由于`CREATE DATABASE dbname`并没有指明数据库模板，所以系统将默认克隆`template1`数据库，得到新的数据库`dbname`，template1`数据库的默认表空间是`pg_defaultCREATE DATABASE hf;drop databasem hf; 表空间 1234CREATE TABLESPACE ts OWNER custom LOCATION &#x27;/tmp/data/tsmars&#x27;;// 指定模版和表空间创建数据库CREATE DATABASE hf TEMPLATE template1 TABLESPACE ts;ALTER DATABASE hf OWNER TO custom; 表 123456789101112131415161718192021222324// 新建create table ([字段名1] [类型1] &lt;references 关联表名(关联的字段名)&gt;;,[字段名2] [类型2],......&lt;,primary key (字段名m,字段名n,...)&gt;;); // 一摸一样表create table xx like yyyy;// 删除drop table [表名]; // 重命名一个表alter table [表名A] rename to [表名B]; // 在已有的表里添加字段alter table [表名] add column [字段名] [类型]; //删除表中的字段alter table [表名] drop column [字段名]; //重命名一个字段alter table [表名] rename column [字段名A] to [字段名B]; //给一个字段设置缺省值alter table [表名] alter column [字段名] set default [新的默认值];//去除缺省值alter table [表名] alter column [字段名] drop default; //表中插入数据insert into 表名 ([字段名m],[字段名n],......) values ([列m的值],[列n的值],......); //修改表中的某行某列的数据update [表名] set [目标字段名]=[目标值] where [该行特征]; //删除表中某行数据delete from [表名] where [该行特征]; 执行sql文件 1psql -h localhost -p 5432 -U gpadmin -d tag_hub -f /sql在容器内的路径 查看视图是否存在 1select count(1) as count from pg_views where schemaname = &#x27;schemaName&#x27; and viewname = &#x27;viewName&#x27; 日期和时间间字段 date:日期字段，格式:2016-04-15 timestamp:时间字段，格式:2016-04-15 20:00:00 获取当时时间函数 select now(); select current_timestamp; select CURRENT_TIME; select LOCALTIME; select LOCALTIMESTAMP; 获取当天日期 select now()+interval ‘2 day’; 时间截取 select extract(year from now()); 时间转换: select timestamp ‘2012-05-12 18:54:54’; select date ‘2012-05-12 18:54:54’;","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"postgresql","slug":"postgresql","permalink":"http://yoursite.com/tags/postgresql/"}]},{"title":"aop","slug":"aop","date":"2020-11-03T11:02:10.000Z","updated":"2022-11-22T01:27:53.451Z","comments":true,"path":"2020/11/03/aop/","link":"","permalink":"http://yoursite.com/2020/11/03/aop/","excerpt":"","text":"之前经常使用spring的aop来做些自定义的动作，但是没有真正系统的了解过它，最近整理下。 概念 Aspect-oriented Programming (AOP)，相对于Object-oriented Programming (OOP)来说是另外一种编程风格。在spring中，可以使用xml和aspectJ两种风格来定义切面。 Aspect 添加了**@Aspect**的类 基于xml配置的类 123456789&lt;aop:config&gt; &lt;aop:aspect id=&quot;myAspect&quot; ref=&quot;aBean&quot;&gt; ... &lt;/aop:aspect&gt;&lt;/aop:config&gt;&lt;bean id=&quot;aBean&quot; class=&quot;...&quot;&gt; ...&lt;/bean&gt; Join Point 连接点，在spring代表一个方法的执行。spring中可以简单认为所有方法都是连接点。 PointCut 切点，是用来描述连接点，通过它可以确定哪些连接点可以被增强。切点表达式，是aop的核心，稍后介绍。 Advice 由 aspect 添加到特定的Join Point(即满足 PointCut规则的 join point) 的一段代码。 Advice类型 @Before：在一个方法执行前被调用 @AfterReturning：仅当方法成功完成后执行的通知，异常后不执行 @AfterThrowing：方法抛出异常退出时执行的通知 @After：方法执行之后调用的通知，无论方法执行是否成功 @Around：方法执行之前和之后调用的通知（大而全的类型，可以替代很多类型） 切点定义 方法匹配模式：execution() 任意公共方法的执行：execution(public * *(..)) 任何一个以“set”开始的方法的执行：execution(* set*(..)) AccountService 接口的任意方法的执行：execution(* com.xyz.service.AccountService.*(..)) 定义在service包里的任意方法的执行：execution(* com.xyz.service.*.*(..)) 定义在service包或者子包里的任意方法的执行：execution(* com.xyz.service..*.*(..)) 在service包里的任意连接点（在Spring AOP中只是方法执行）：within(com.xyz.service.*) 在service包或者子包里的任意连接点（在Spring AOP中只是方法执行）：within(com.xyz.service..*)","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"aop","slug":"aop","permalink":"http://yoursite.com/tags/aop/"}]},{"title":"常用命令记录","slug":"常用命令记录","date":"2020-10-29T12:46:58.000Z","updated":"2022-11-22T01:27:53.946Z","comments":true,"path":"2020/10/29/常用命令记录/","link":"","permalink":"http://yoursite.com/2020/10/29/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95/","excerpt":"","text":"Linux下centos版本号 1cat /etc/redhat-release Linux修改主机名 1234567891011121314151617//临时生效hostname yourname// 永久生效vim /etc/sysconfig/networkNETWORKING=yesHOSTNAME=yourname #在这修改hostnamevim /etc/hosts127.0.0.1 localhost.localdomain localhost127.0.0.1 yourname #在这修改hostname,若没有这一行，则添加centos7vim /etc/hostnameyourname// 重启 Centos7 关闭防火墙 12systemctl stop firewalld.servicesystemctl disable firewalld.service#禁止防火墙开机后自启动 查看分区和磁盘 123456789lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 200G 0 disk├─sda1 8:1 0 1M 0 part├─sda2 8:2 0 512M 0 part /boot└─sda3 8:3 0 99.5G 0 part └─vg_root-lv_root 253:0 0 99.5G 0 lvm /sr0 11:0 1 1024M 0 rom grep 查找${app}进程，但是去掉grep的进程 1ps -ef | grep $&#123;app&#125; | grep -v grep","categories":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/categories/linux/"}],"tags":[{"name":"Mac 、linux","slug":"Mac-、linux","permalink":"http://yoursite.com/tags/Mac-%E3%80%81linux/"}]},{"title":"shiro","slug":"shiro","date":"2020-10-22T09:07:52.000Z","updated":"2022-11-22T01:27:53.744Z","comments":true,"path":"2020/10/22/shiro/","link":"","permalink":"http://yoursite.com/2020/10/22/shiro/","excerpt":"","text":"能做什么？ Apache Shiro™ is a powerful and easy-to-use Java security framework that performs authentication, authorization, cryptography, and session management. With Shiro’s easy-to-understand API, you can quickly and easily secure any application – from the smallest mobile applications to the largest web and enterprise applications. Apache Shiro是一个功能强大且易于使用的Java安全框架，用于执行身份验证，授权，加密和会话管理。使用Shiro易于理解的API，您可以快速轻松地保护任何应用程序-从最小的移动应用程序到最大的Web和企业应用程序。 Shiro以Shiro开发团队称四大基石：身份验证，授权，会话管理、加密，和特性如下图 Shiro以Shiro开发团队所谓的“应用程序安全性的四个基石”：身份验证，授权，会话管理和加密 **身份验证：**有时称为“登录”，这是证明用户是他们所说的身份的行为。 **授权：**访问控制的过程，即确定“谁”有权访问“什么”。 **会话管理：**即使在非Web或EJB应用程序中，也管理用户特定的会话。 **加密：**使用密码算法保持数据安全，同时仍然易于使用。 在不同的应用程序环境中，还具有其他功能来支持和加强这些问题，尤其是： Web支持：Shiro的Web支持API可帮助轻松保护Web应用程序。 缓存：缓存是Apache Shiro API的第一层公民，可确保安全操作保持快速有效。 并发性：Apache Shiro的并发功能支持多线程应用程序。 测试：测试支持可以帮助您编写单元测试和集成测试，并确保您的代码将按预期进行保护。 “运行方式”：一种功能，允许用户采用其他用户的身份（如果允许），有时在管理方案中很有用。 “记住我”：在整个会话中记住用户的身份，因此他们仅在必要时登录。 架构和术语 架构图 术语 Subject： 当前与软件交互的实体（用户，第三方服务，计划任务等） SecurityManager： Shiro体系结构的核心。它主要是一个“伞”对象，用于协调其托管组件以确保它们能够顺利协同工作。它还管理Shiro对每个应用程序用户的“视图”，因此它知道如何对每个用户执行安全性操作。 Authenticator： 认证器，说白了就是从Realms中认证subject的真实身份，另外如果配置了认证策略（AuthenticationStrategy），会根据策略（所有领域都必须成功吗？只有第一个领域？） **Authorizer：**授权，负责确定用户在该应用程序的访问控制。它是最终表明是否允许用户做某事的机制 **SessionManager：**管理session的，可以整合现有的session机制，例如servlet容器，主要通过SessionDAO来CURD。 **CacheManager：**管理一个叫Cache的实例，可以缓存信息，方便使用。 **Cryptography：**一套加密API机制，比如加密密钥等 **Realms：**和数据库交互，例如认证、授权等需要从DB里数据来判断，就是通过它操作的。 小试牛刀 1234567891011121314151617181920public class ShiroTest &#123; private SimpleAccountRealm simpleAccountRealm = new SimpleAccountRealm(); @Before public void addUser() &#123; simpleAccountRealm.addAccount(&quot;astilt&quot;, &quot;123&quot;); &#125; @Test public void testLogin() &#123; // 1.设置SecurityManager环境 DefaultSecurityManager defaultSecurityManager = new DefaultSecurityManager(); defaultSecurityManager.setRealm(simpleAccountRealm); SecurityUtils.setSecurityManager(defaultSecurityManager); // 3.登录（使用自带token，也可以基于JWT自定义实现token） UsernamePasswordToken token = new UsernamePasswordToken(&quot;astilt&quot;, &quot;123&quot;); Subject subject = SecurityUtils.getSubject(); subject.login(token); &#125;&#125; 初始化一个用户，放到SimpleAccountRealm里 SecurityUtils里设置SecurityManager环境，设置Realm为SimpleAccountRealm 构建Token，这里使用shiro自带的UsernamePasswordToken SecurityUtils里获取Subject，使用subject的ogin方法登陆，参数为上一步的token","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"权限管理","slug":"权限管理","permalink":"http://yoursite.com/tags/%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/"}]},{"title":"spring容器刷新","slug":"spring容器刷新","date":"2020-10-21T02:12:11.000Z","updated":"2023-03-20T09:04:54.488Z","comments":true,"path":"2020/10/21/spring容器刷新/","link":"","permalink":"http://yoursite.com/2020/10/21/spring%E5%AE%B9%E5%99%A8%E5%88%B7%E6%96%B0/","excerpt":"","text":"1、 刷新方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Overridepublic void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // 准备工作，记录下容器的启动时间、标记“已启动”状态、检验配置文件格式 prepareRefresh(); // 获取 Spring 容器 ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // 设置 BeanFactory 的类加载器，添加几个 BeanPostProcessor，手动注册几个特殊的 bean 等 prepareBeanFactory(beanFactory); try &#123; // BeanFactory 准备工作完成后进行的后置处理工作，子类可以自定义实现，Spring Boot 中是个空方法 postProcessBeanFactory(beanFactory); //=======以上是 BeanFactory 的预准备工作======= // 调用 BeanFactoryPostProcessor 各个实现类的 postProcessBeanFactory(factory) 方法 // SpringBoot 会在这里扫描 @Component 注解和进行自动配置 invokeBeanFactoryPostProcessors(beanFactory); // 注册和创建 BeanPostProcessor 的实现类（注意和之前的 BeanFactoryPostProcessor 的区别） registerBeanPostProcessors(beanFactory); // 初始化 MessageSource 组件（做国际化功能；消息绑定，消息解析） initMessageSource(); // 初始化当前 ApplicationContext 的事件广播器 initApplicationEventMulticaster(); // 具体的子类可以在这里初始化一些特殊的 Bean（在初始化 singleton beans 之前），Spring Boot 中新建webServer onRefresh(); // 注册事件监听器，监听器需要实现 ApplicationListener 接口 registerListeners(); // 初始化所有的 singleton beans（lazy-init 的除外） finishBeanFactoryInitialization(beanFactory); // 容器刷新完成操作，会完成webServer启动 finishRefresh(); &#125; catch (BeansException ex) &#123; if (logger.isWarnEnabled()) &#123; logger.warn(&quot;Exception encountered during context initialization - &quot; + &quot;cancelling refresh attempt: &quot; + ex); &#125; destroyBeans(); cancelRefresh(ex); throw ex; &#125; finally &#123; resetCommonCaches(); &#125; &#125;&#125; 2、刷新前的准备工作 这步比较简单，直接看代码中的注释即可。 // AbstractApplicationContext 580 12345678910111213141516protected void prepareRefresh() &#123; // 记录启动时间， // 将 active 属性设置为 true，closed 属性设置为 false，它们都是 AtomicBoolean 类型 this.startupDate = System.currentTimeMillis(); this.closed.set(false); this.active.set(true); // Spring Boot 中是个空方法 initPropertySources(); // 校验配置属性的合法性 getEnvironment().validateRequiredProperties(); // 记录早期的事件 this.earlyApplicationEvents = new LinkedHashSet&lt;&gt;();&#125; 3、获取 Bean 容器 前面说过 ApplicationContext 内部持有了一个 BeanFactory，这步就是获取 ApplicationContext 中的 BeanFactory。在 ClassPathXmlApplicationContext 中会做很多工作，因为一开始 ClassPathXmlApplicationContext 中的 BeanFactory 并没有创建，但在 AnnotationConfigApplicationContext 比较简单，直接返回即可。 // AbstractApplicationContext 621 1234567891011protected ConfigurableListableBeanFactory obtainFreshBeanFactory() &#123; // 通过 cas 设置刷新状态 if (!this.refreshed.compareAndSet(false, true)) &#123; throw new IllegalStateException( &quot;GenericApplicationContext does not support multiple refresh attempts: just call &#x27;refresh&#x27; once&quot;); &#125; // 设置序列号 this.beanFactory.setSerializationId(getId()); // 返回已创建的 BeanFactory return this.beanFactory;&#125; 4、准备 Bean 容器 BeanFactory 获取之后并不能马上使用，还要在 BeanFactory 中做一些准备工作，包括类加载器、表达式解析器的设置，几个特殊的 BeanPostProcessor 的添加等。 // AbstractApplicationContext 631 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758protected void prepareBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123; // 设置 BeanFactory 的类加载器，这里设置为当前 ApplicationContext 的类加载器 beanFactory.setBeanClassLoader(getClassLoader()); // 设置表达式解析器 beanFactory.setBeanExpressionResolver(new StandardBeanExpressionResolver(beanFactory.getBeanClassLoader())); beanFactory.addPropertyEditorRegistrar(new ResourceEditorRegistrar(this, getEnvironment())); // 添加Aware后置处理器，实现了 Aware 接口的 beans 在初始化的时候，这个 processor 负责回调 beanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this)); /** * 下面几行的意思是，如果某个 bean 依赖于以下几个接口的实现类，在自动装配的时候忽略它们，Spring 会通过其他方式来处理这些依赖 */ beanFactory.ignoreDependencyInterface(EnvironmentAware.class); beanFactory.ignoreDependencyInterface(EmbeddedValueResolverAware.class); beanFactory.ignoreDependencyInterface(ResourceLoaderAware.class); beanFactory.ignoreDependencyInterface(ApplicationEventPublisherAware.class); beanFactory.ignoreDependencyInterface(MessageSourceAware.class); beanFactory.ignoreDependencyInterface(ApplicationContextAware.class); /** * 下面几行是为了解决特殊的依赖，如果有 bean 依赖了以下几个（可以发现都是跟容器相关的接口），会注入这边相应的值， * 这是因为 Spring 容器里面不保存容器本身，所以容器相关的依赖要到 resolvableDependencies 里面找。上文有提到过， * ApplicationContext 继承了 ResourceLoader、ApplicationEventPublisher、MessageSource，所以对于这几个依赖， * 可以赋值为 this，注意 this 是一个 ApplicationContext。 * 那这里怎么没看到为 MessageSource 赋值呢？那是因为 MessageSource 被注册成为了一个普通的 bean */ beanFactory.registerResolvableDependency(BeanFactory.class, beanFactory); beanFactory.registerResolvableDependency(ResourceLoader.class, this); beanFactory.registerResolvableDependency(ApplicationEventPublisher.class, this); beanFactory.registerResolvableDependency(ApplicationContext.class, this); /** * 这也是个 BeanPostProcessor ，在 bean 实例化后，如果是 ApplicationListener 的子类，那么将其添加到 listener 列表中， * 可以理解成：注册监听器 */ beanFactory.addBeanPostProcessor(new ApplicationListenerDetector(this)); if (beanFactory.containsBean(LOAD_TIME_WEAVER_BEAN_NAME)) &#123; beanFactory.addBeanPostProcessor(new LoadTimeWeaverAwareProcessor(beanFactory)); beanFactory.setTempClassLoader(new ContextTypeMatchClassLoader(beanFactory.getBeanClassLoader())); &#125; /** * 从下面几行代码我们可以知道，Spring 往往很 &quot;智能&quot; 就是因为它会帮我们默认注册一些有用的 bean，我们也可以选择覆盖 */ if (!beanFactory.containsLocalBean(ENVIRONMENT_BEAN_NAME)) &#123; beanFactory.registerSingleton(ENVIRONMENT_BEAN_NAME, getEnvironment()); &#125; if (!beanFactory.containsLocalBean(SYSTEM_PROPERTIES_BEAN_NAME)) &#123; beanFactory.registerSingleton(SYSTEM_PROPERTIES_BEAN_NAME, getEnvironment().getSystemProperties()); &#125; if (!beanFactory.containsLocalBean(SYSTEM_ENVIRONMENT_BEAN_NAME)) &#123; beanFactory.registerSingleton(SYSTEM_ENVIRONMENT_BEAN_NAME, getEnvironment().getSystemEnvironment()); &#125;&#125; 5、调用 BeanFactory 后置处理器 调用 BeanDefinitionRegistryPostProcessor 的 postProcessBeanDefinitionRegistry(registry) 方法和 BeanFactoryPostProcessor 的 postProcessBeanFactory(beanFactory) 方法，它允许在 beanFactory 准备完成之后对 beanFactory 进行一些修改，比如在 bean 初始化之前对 beanFactory 中的 beanDefinition 进行修改。@ComponentScan 和 @EnableAutoConfiguration 这两个注解都是在这里实现的。 这里有个非常重要的后置处理器：ConfigurationClassPostProcessor，它的作用是在这里解析 @Configuration 标签。@PropertySource、@ComponentScan、@Import、@ImportResource、@Bean 这些注解都和 @Configuration 相关，都会在这里被解析。我们常用的 @Component 注解和 SpringBoot 的自动配置，都是在这里被实现。ConfigurationClassPostProcessor 会以我们在 Spring 启动时传入的启动类作为解析 @Configuration 的根节点（SpringApplication.run(SpringTest.class,args);），递归地扫描其它 @Configuration 节点，最终把所有用户自定义的 bean 以 Map&lt;beanName,beanDefinition&gt; 的形式保存在容器中。 相关博客：Spring源码解析 – @Configuration配置类及注解Bean的解析 6、注册各类 Bean 后置处理器 也是一个名字就体现功能的方法，把各种 BeanPostProcessor 注册到 BeanFactory 中（需要注意的是这里的注册会直接调用 getBean 创建对象），BeanPostProcessor 允许在 bean 初始化前后插手对 bean 的初始化过程，不展开了。 7、初始化事件分派器 Event 会有单独的篇幅详解，这里就不展开了。 8、初始化所有非懒加载 singleton beans 这是容器刷新中最重要的方法。Spring 需要在这个阶段完成所有的 singleton beans 的实例化。这一步骤非常重要而且过程非常长，下一篇中我们来专门分析这个方法。 9、容器刷新完成 // AbstractApplicationContext 871 1234567891011121314protected void finishRefresh() &#123; clearResourceCaches(); // 创建Lifecycle处理器 initLifecycleProcessor(); // 调用LifecycleProcessor的onRefresh()方法，默认是调用所有Lifecycle的start()方法 getLifecycleProcessor().onRefresh(); // 发布容器刷新完成事件 publishEvent(new ContextRefreshedEvent(this)); LiveBeansView.registerApplicationContext(this);&#125;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"spring","slug":"spring","permalink":"http://yoursite.com/tags/spring/"}]},{"title":"git分环境配置提交用户信息","slug":"git分环境配置用户和邮件","date":"2020-10-14T12:35:33.000Z","updated":"2022-11-22T01:27:53.614Z","comments":true,"path":"2020/10/14/git分环境配置用户和邮件/","link":"","permalink":"http://yoursite.com/2020/10/14/git%E5%88%86%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E7%94%A8%E6%88%B7%E5%92%8C%E9%82%AE%E4%BB%B6/","excerpt":"","text":"现在大部分公司使用git作为版本控制工具，有没有过在github上提交一个commit时候，发现提交记录信息是公司的，太不专业了！ 其实git可以针对不同的文件夹配置提交用户信息，下面是我自己的配置： 全局配置文件（～/.gitconfig） 123456789101112131415[alias] co = checkout ci = commit br = branch st = status# 默认的用户和邮箱[user] name = astilt email = astilt@163.com# 公司项目的目录[includeIf &quot;gitdir:~/IdeaProjects/&quot;] path = ~/.gitconfig-work# 个人项目目录 [includeIf &quot;gitdir:~/MyProjects/&quot;] path = ~/.gitconfig-play 个人配置文件（~/.gitconfig-play） 123[user] name = astilt email = astilt@163.com 同理~/.gitconfig-work 配置公司用户名和邮箱就可以了","categories":[{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"}]},{"title":"数仓建模","slug":"数仓建模","date":"2020-10-13T09:06:27.000Z","updated":"2022-11-22T01:27:54.014Z","comments":true,"path":"2020/10/13/数仓建模/","link":"","permalink":"http://yoursite.com/2020/10/13/%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1/","excerpt":"","text":"为什么写这个主题呢？因为最近一年多来，都是在做数据资产应用，说的更具体一点是：标签中心。我们可以给标签一个大方向的基调定义，标签是一种数据吐出的方式，本质它也是数据库里面一/几个字段。回归数据仓库(Data Warehouse) 建模问题，和做普通业务一样，数据结构的好坏直接关系到后续业务使用的便利和迭代。 数仓分层 ODS：数据引入层，一般是经过ETL后的，结构和数据基本与业务系统数据一致 CMD：数据公共层（E-R建模：实体，关系；维度建模：DIM，DWD，DWS） ADS：业务 E-R模型 和传统DBMS一样，E-R模型在数仓建设中也占据了很大部分，选择它的好处就是各个实体之间的关系明了，需要仔细梳理各种关系，它强调是3NF，在数据量大业务复杂的情况下，可谓是一个大“魔鬼”，很考验梳理人员耐心和业务熟悉情况。 补充知识：3NF：列原子性，非主键列完全依赖主键，非主键列直接依赖主键。 构建步骤 抽象主体 主体属性 梳理主体之间关系 画出E-R图 维度模型 与E-R模型的区别，E-R模型需要从整体考虑，梳理出每个业务实体和之间关系，工作量很大。而维度是从一个基本事件出发，可以在不了解其他业务情况下，快速构建当前业务的数据模型，因为是围绕业务事件，所以它需要很多原子信息来满足无法预估的需求。 构建步骤 基于一个事件构建事实表，例如用户订单、用户近一个月购买次数、某商品2018-2020年销售额 维度表：商品、支付方式、商品产地，订单金额 分类 星型 雪花型 星座型 总结 选择什么模型进行建模，需要结合业务，没有谁好或坏。不过现在互联网公司，业务错综复杂，还需要快速应用构建，所以维度建模比较常见，而常见的类型又以星座型居多","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"方法论","slug":"方法论","permalink":"http://yoursite.com/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"}]},{"title":"Java单元测试","slug":"Java单元测试","date":"2020-09-29T03:11:37.000Z","updated":"2022-11-22T01:27:53.380Z","comments":true,"path":"2020/09/29/Java单元测试/","link":"","permalink":"http://yoursite.com/2020/09/29/Java%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/","excerpt":"","text":"单元测试 阿里巴巴Java开发手册，对于单元测试有如下描述，我大致抽象了下： 错误意识 那是测试同学干的事情，单元测试代码是多余的！汽车的整体功能与各单元部件的测试正常与否是强相关？（认知需转变） 单元测试代码不需要维护！一年半载后，那么单元测试几乎处于废弃状态。（跟随业务维护） 单元测试与线上故障没有辩证关系！好的单元测试能够最大限度地规避线上故障（做比没做要好） AIR 原则 单元测试在线上运行时候，感觉像空气（air）一样并不存在，但是在质量保证上却又十分关键。 A：Automatic（测试框架通常是定期执行的，执行过程必须完全自动化才有意义） I：Independent（单元测试用例之间决不能互相调用，也不能依赖执行的先后次序） R：Repeatable（单元测试是可以重复执行的，不能受到外界环境的影响） BCDE 原则 B: Border，边界值测试，包括循环边界、特殊取值、特殊时间点，数据顺序。 C: Correct，正确的输入，并得到预期的结果。 D: Design，与设计文档相结合，来编写单元测试。 E: Error，单元测试的目的是证明程序有错，而不是证明程序无错。为了发现代码中潜在的错误，我们需要在编写测试用例时有一些强制的错误输入（如非法数据、异常流程、非业务允许输入等）来得到预期的错误结果 规范 强制 粒度尽量小，最大至方法级别 核心业务、核心应用、核心模块确保有单元测试 代码必须写在如下工程目录：src/test/java 推荐 DAO层，Manager层，可重用度高的Service，都应该进行单元测试 不手动操作数据库，请使用用程序准备符合业务的数据 数据库相关的单元测试，可以设置自动回滚机制，不造脏数据 不可测的代码需要做重构，不要为了要求测试而写不规范的测试代码 在设计评审阶段，开发人员需要和测试人员一起确定单元测试范围，单元测试最好覆盖所有测试用例（UC） 业务代码 为了更方便地进行单元测试，业务代码应避免以下情况 构造方法中做的事情过多 存在过多的全局变量和静态方法 存在过多的外部依赖 存在过多的条件语句 Junit+Mockito 基于springBoot 1.5.19 一般我们开发都是使用springBoot，引入了类似如下的依赖： 12345 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; JUnit —用于 Java 应用程序的单元测试的事实上的标准。（4.x版本） Spring Test 和 Spring Boot 测试—对 Spring Boot 应用程序的 Util 和集成测试支持。 Hamcrest —匹配器对象库(也称为约束或谓词)。 Mockito — Java 模拟框架。（1.x版本） AssertJ —流畅的 assert 库。 JSONassert — JSON 的 assert 库。 JsonPath — JSON 的 XPath。 Junit 参考：https://github.com/junit-team/junit4/wiki/Getting-started JUnit 是一个 Java 编程语言的单元测试框架。JUnit 在测试驱动的开发方面有很重要的发展，是起源于 JUnit 的一个统称为 xUnit 的单元测试框架之一。 相关注解 注解 说明 @Test(excepted==xx.class,timeout=毫秒数) 修饰一个方法为测试方法，excepted参数可以忽略某些异常类 @Before 在每一个测试方法被运行前执行一次 @BeforeClass 在所有测试方法执行前执行 @After 在每一个测试方法运行后执行一次 @AfterClass 在所有测试方法执行后执行 @Ignore 修饰的类或方法会被测试运行器忽略 @RunWith 更改测试运行器 常规用法 套件 123456789101112131415161718192021222324252627282930public class TaskOneTest &#123; @Test public void test() &#123; System.out.println(&quot;Task one do.&quot;); &#125;&#125;public class TaskTwoTest &#123; @Test public void test() &#123; System.out.println(&quot;Task two do.&quot;); &#125;&#125;public class TaskThreeTest &#123; @Test public void test() &#123; System.out.println(&quot;Task Three.&quot;); &#125;&#125;// 1. 更改测试运行方式为 Suite@RunWith(Suite.class) // 2. 将测试类传入进来@Suite.SuiteClasses(&#123;TaskOneTest.class, TaskTwoTest.class, TaskThreeTest.class&#125;)public class SuitTest &#123; /** * 测试套件的入口类只是组织测试类一起进行测试，无任何测试方法， */&#125; 参数化测试 参数化测试允许开发人员使用不同的值反复运行同一个测试，遵循 5 个步骤来创建参数化测试。 用 @RunWith(Parameterized.class)来注释 test 类 创建一个由 @Parameters 注释的公共的静态方法，始化所有需要测试的参数对 为测试类声明几个变量，分别用于存放期望值和测试所用数据 创建一个公共的构造函数，为上述声明的几个变量赋值 类有一个测试，它需要注解@Test到方法。 1234567891011121314151617181920212223242526272829303132333435363738394041//1@RunWith(Parameterized.class)public class FibonacciTest &#123; //2 @Parameterized.Parameters public static Collection&lt;Object[]&gt; data() &#123; return Arrays.asList(new Object[][]&#123; &#123;0, 0&#125;, &#123;1, 1&#125;, &#123;2, 1&#125;, &#123;3, 2&#125;, &#123;4, 3&#125;, &#123;5, 5&#125;, &#123;6, 8&#125; &#125;); &#125; //3 private int fInput, fExpected; //4 public FibonacciTest(int input, int expected) &#123; this.fInput = input; this.fExpected = expected; &#125; //5 @Test public void test() &#123; assertEquals(fExpected, Fibonacci.compute(fInput)); &#125;&#125;// 被测试的类的方法 public class Fibonacci &#123; public static int compute(int n) &#123; int result = 0; if (n &lt;= 1) &#123; result = n; &#125; else &#123; result = compute(n - 1) + compute(n - 2); &#125; return result; &#125;&#125; 若参数为单个的还可以这样做： 123456789@Parameterspublic static Iterable&lt;? extends Object&gt; data() &#123; return Arrays.asList(&quot;first test&quot;, &quot;second test&quot;);&#125;@Parameterspublic static Object[] data() &#123; return new Object[] &#123; &quot;first test&quot;, &quot;second test&quot; &#125;;&#125; 断言 junit 断言，表格里面和assertThat。 断言 说明 assertArrayEquals(expecteds, actuals) 查看两个数组是否相等。 assertEquals(expected, actual) 查看两个对象是否相等。类似于字符串比较使用的equals()方法。 assertNotEquals(first, second) 查看两个对象是否不相等。 assertNull(object) 查看对象是否为空。 assertNotNull(object) 查看对象是否不为空。 assertSame(expected, actual) 查看两个对象的引用是否相等。类似于使用“==”比较两个对象。 assertNotSame(unexpected, actual) 查看两个对象的引用是否不相等。类似于使用“!=”比较两个对象。 assertTrue(condition) 查看运行结果是否为true。 assertFalse(condition) 查看运行结果是否为false。 assertThat与Matcher 包括两个包org.hamcrest.CoreMatchers和org.hamcrest.Matchers org.hamcrest.CoreMatchers assertThat( number, allOf( greaterThan(10), lessThan(15) ) ):相当于“与”（&amp;&amp;）,即allOf中条件都必须成立 assertThat( number, anyOf( greaterThan(30), lessThan(1) ) )：相当于“或”（||），即只要anyOf中有一个条件成立就可以了 assertThat( number, anything() )：无论什么条件，永远为true assertThat( string, is( “engine” ) )和assertThat( string, equalTo( expectedValue ) )：两个作用是一样的，只是写法不同，都是Object的equals方法 assertThat( string, not( “engine” ) ):not匹配符和is匹配符正好相反 org.hamcrest.Matchers （1）字符串相关匹配符 assertThat( string, containsString( “engine” ) ):包含子字符串”engine”则测试通过 assertThat( string, endsWith( “engine” ) )：以子字符串”engine”结尾则测试通过 assertThat( string, startsWith( “engine” ) )：以子字符串”engine”开始则测试通过 assertThat( testedString, equalToIgnoringCase( “engine” ) ); 忽略大小写的情况 assertThat( testedString, equalToIgnoringWhiteSpace( “engine” ) )：忽略头尾的任意个空格 assertThat( string, is( “engine” ) )：判断两个字符串相等 （2）数值相关匹配符 assertThat( number, closeTo( 10.0, 0.1 ) ):在10.0±0.1范围之内则测试通过 assertThat( number, greaterThan(11) ):大于11则测试通过 assertThat( number, lessThan (11) ):小于11则测试通过 assertThat( number, greaterThanOrEqualTo (11) )：大于等于11则测试通过 assertThat( number, lessThanOrEqualTo (11) )：小于等于11则测试通过 （3）collection相关匹配符 assertThat( map, hasEntry( “key”, “value” ) ):mapObject含有一个键值为”key”对应元素值为”value”测试通过 assertThat( list, hasItem ( “element” ) )：iterableObject含有元素“element”项则测试通过 assertThat( map, hasKey ( “key” ) )：mapObject含有键值“key”则测试通过 assertThat( map, hasValue ( “key” ) )：mapObject含有元素值“value”则测试通过 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class AssertTests &#123; @Test public void testAssertArrayEquals() &#123; byte[] expected = &quot;trial&quot;.getBytes(); byte[] actual = &quot;trial&quot;.getBytes(); org.junit.Assert.assertArrayEquals(&quot;failure - byte arrays not same&quot;, expected, actual); &#125; @Test public void testAssertEquals() &#123; org.junit.Assert.assertEquals(&quot;failure - strings are not equal&quot;, &quot;text&quot;, &quot;text&quot;); &#125; @Test public void testAssertFalse() &#123; org.junit.Assert.assertFalse(&quot;failure - should be false&quot;, false); &#125; @Test public void testAssertNotNull() &#123; org.junit.Assert.assertNotNull(&quot;should not be null&quot;, new Object()); &#125; @Test public void testAssertNotSame() &#123; org.junit.Assert.assertNotSame(&quot;should not be same Object&quot;, new Object(), new Object()); &#125; @Test public void testAssertNull() &#123; org.junit.Assert.assertNull(&quot;should be null&quot;, null); &#125; @Test public void testAssertSame() &#123; Integer aNumber = Integer.valueOf(768); org.junit.Assert.assertSame(&quot;should be same&quot;, aNumber, aNumber); &#125; // JUnit Matchers assertThat @Test public void testAssertThatBothContainsString() &#123; org.junit.Assert.assertThat(&quot;albumen&quot;, both(containsString(&quot;a&quot;)).and(containsString(&quot;b&quot;))); &#125; @Test public void testAssertThathasItemsContainsString() &#123; org.junit.Assert.assertThat(Arrays.asList(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;), hasItems(&quot;one&quot;, &quot;three&quot;)); &#125; @Test public void testAssertThatEveryItemContainsString() &#123; org.junit.Assert.assertThat(Arrays.asList(new String[] &#123; &quot;fun&quot;, &quot;ban&quot;, &quot;net&quot; &#125;), everyItem(containsString(&quot;n&quot;))); &#125; // Core Hamcrest Matchers with assertThat @Test public void testAssertThatHamcrestCoreMatchers() &#123; assertThat(&quot;good&quot;, allOf(equalTo(&quot;good&quot;), startsWith(&quot;good&quot;))); assertThat(&quot;good&quot;, not(allOf(equalTo(&quot;bad&quot;), equalTo(&quot;good&quot;)))); assertThat(&quot;good&quot;, anyOf(equalTo(&quot;bad&quot;), equalTo(&quot;good&quot;))); assertThat(7, not(CombinableMatcher.&lt;Integer&gt; either(equalTo(3)).or(equalTo(4)))); assertThat(new Object(), not(sameInstance(new Object()))); &#125; @Test public void testAssertTrue() &#123; org.junit.Assert.assertTrue(&quot;failure - should be true&quot;, true); &#125;&#125;","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"单元测试","slug":"单元测试","permalink":"http://yoursite.com/tags/%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/"}]},{"title":"hive安装","slug":"hive安装","date":"2020-09-25T02:33:12.000Z","updated":"2022-11-22T01:27:53.658Z","comments":true,"path":"2020/09/25/hive安装/","link":"","permalink":"http://yoursite.com/2020/09/25/hive%E5%AE%89%E8%A3%85/","excerpt":"","text":"Hive 上一篇安装了hadoop，这次我们来使用hive操作haddop。 Mac macOS Catalina 10.15.6 JDK 1.8.0_261 homebrew 安装 brew install hive 配置 环境变量 12echo export HIVE_HOME=/usr/local/Cellar/hive/3.1.2_1 &gt;&gt; ~/.zshrcexport PATH=$HIVE_HOME/bin:$PATH hive-site.xml cd /usr/local/Cellar/hive/3.1.2_1/libexec/conf，没有就：mv hive-default.xml.template hive-site.xml，然后添加如下configuration 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/metastore&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;!--mysql用户名--&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;!--mysql密码--&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;!-- hive用来存储不同阶段的map/reduce的执行计划的目录，同时也存储中间输出结果 ，默认是/tmp/&lt;user.name&gt;/hive,我们实际一般会按组区分，然后组内自建一个tmp目录存储 --&gt; &lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/data/hive/warehouse&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置元数据库存 装好mysql，创建数据库metastore。 下载mysql驱动放到hive的lib目录下：https://dev.mysql.com/downloads/file/?id=496588 到bin目录下，schematool -initSchema -dbType mysql 运行 启动metastore：nohup hive --service metastore &amp; nohup hiveserver2 &amp; (若需要jdbc连接) 运行：hive Centos7 下载解压 1234567891011# 位置cd /opt/hive#包下载wget https://dlcdn.apache.org/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz# 解压tar -zxvf export PATH=$PATH:$HADOOP_HOME/sbin# 环境变量vim /etc/profileexport HIVE_HOME=/opt/hive/apache-hive-3.1.2-binexport PATH=$PATH:$HIVE_HOME/binsource /etc/profile 配置 hadoop的core-site.xml 12345678910&lt;configuration&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; mysql 在本机安装，并create 库hive，不写了，用户名root，密码root hive-env.sh cp $HIVE_HOME/conf/hive-default.xml.template hive-site.xml，修改如下property ，c1为主机名 12345678910111213141516171819202122232425262728293031323334353637383940&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;amp;serverTimezone=GMT%2B8&amp;amp;useUnicode=true&amp;amp;characterEncoding=utf-8&lt;&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;!-- 指定存储元数据metastore要连接的地址 --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://c1:9083&lt;/value&gt; &lt;/property&gt; &lt;description&gt;Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore. &lt;/description&gt;&lt;!-- 配置hiveserver2--&gt;&lt;property&gt; &lt;name&gt;hive.server2.thrift.port&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt; &lt;description&gt;Port number of HiveServer2 Thrift interface when hive.server2.transport.mode is &#x27;binary&#x27;.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt; &lt;value&gt;c1&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 替换system:java.io.tmpdir、{system:java.io.tmpdir}、system:java.io.tmpdir、{system:user.name} 的system: ，不然会报错！ 对比guava版本 123456789ls $HADOOP_HOME/share/hadoop/common/lib | grep &#x27;^guava-&#x27;guava-27.0-jre.jarls $HIVE_HOME/lib | grep &#x27;^guava-&#x27;guava-19.0.jar# 留下高版本，删除低版本cp $HADOOP_HOME/share/hadoop/common/lib/guava-27.0-jre.jar $HIVE_HOME/librm -rf $HIVE_HOME/lib/guava-19.0.jar 初始化mysql 123# copy /mysql-connector-java-8.0.20.jar 到 $HIVE_HOME/lib# 初始化schematool -dbType mysql -initSchema 启动metastore和hiveserver2 123cd $HIVE_HOME/bin/nohup ./hive --service metastore &amp;nohup ./hive --service hiveserver2 &amp; 启动 123456789101112131415161718# 没有配置用户名和密码，直接回车# 方式1beelinebeeline&gt; !connect jdbc:hive2://c1:10000Connecting to jdbc:hive2://c1:10000Enter username for jdbc:hive2://c1:10000:Enter password for jdbc:hive2://c1:10000:Connected to: Apache Hive (version 3.1.2)Driver: Hive JDBC (version 3.1.2)Transaction isolation: TRANSACTION_REPEATABLE_READ0: jdbc:hive2://c1:10000&gt; !quit# 方式2beeline -u jdbc:hive2://c1:100000: jdbc:hive2://c1:10000&gt; !quit# 方式3hivehive&gt; exit;","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"haddop安装","slug":"haddop安装","date":"2020-09-24T12:27:26.000Z","updated":"2022-11-22T01:27:53.643Z","comments":true,"path":"2020/09/24/haddop安装/","link":"","permalink":"http://yoursite.com/2020/09/24/haddop%E5%AE%89%E8%A3%85/","excerpt":"","text":"环境 1、Mac macOS Catalina 10.15.6 JDK 1.8.0_261 homebrew 安装 ssh locahost（免密码登陆） 123ssh-keygen -t rsa -P &quot;you-email&quot;cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keysssh localhost 安装 1brew install hadoop 配置 所有的配置均在： /usr/local/Cellar/hadoop/3.3.0/libexec/etc/hadoop下 配置文件 配置文件的名称 作用 core-site.xml 核心配置文件，主要定义了我们文件访问的格式 hdfs:// hadoop-env.sh 主要配置的java路径 hdfs-site.xml 主要定义配置我们的hdfs的相关配置 mapred-site.xml 主要定义我们的mapreduce相关的一些配置 slaves 控制我们的从节点在哪里 datanode nodemanager在哪些机器上 yarm-site.xml 配置我们的resourcemanager资源调度 指定Java环境：hadoop-env.sh 1export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_261.jdk/Contents/Home core-site.xml 123456789101112131415&lt;configuration&gt; &lt;!-- 临时数据存放的位置 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/data/hadoop/hdfs/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;!-- 单一namenode，配置fs.default.name， 高可用namenode的HA (namenode 的 highavaliable) 则配置fs.defaultFS HDFS和MapReduce组件都需要用到它 --&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;localhost:9010&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 1234567&lt;!-- 我们只有一台主机和一个伪分布式模式的DataNode，将此值修改为1--&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 运行 格式化hdfs文件系统 12cd /usr/local/Cellar/hadoop/3.3.0/binhadoop namenode -format 运行 123456789cd /usr/local/Cellar/hadoop/3.3.0/sbin./start-all.shjps57969 SecondaryNameNode58161 ResourceManager57734 NameNode58263 NodeManager57835 DataNode61182 Jps 查看 hdfs web ui: http://localhost:9870/ yarn web ui: http://localhost:8088/ Centos7 Linux release 7.9.2009 (Core) JDK 1.8.0_301 3台机器 12310.211.55.51 c1 #主节点地址10.211.55.52 c210.211.55.53 c3 修改主机名 3台机器都执行。 123456789101112131415161718#临时生效hostname yourname#永久生效vim /etc/sysconfig/networkNETWORKING=yesHOSTNAME=yourname #在这修改hostnamevim /etc/hosts127.0.0.1 localhost.localdomain localhost127.0.0.1 yourname #在这修改hostname,若没有这一行，则添加#centos7vim /etc/hostnameyourname#重启reboot 关闭防火墙 123systemctl stop firewalld.service# 禁止防火墙开机后自启动systemctl disable firewalld.service ssh 互通 1234567// 每台机器执行ssh-keygen -t rsachmod 700 ~/.sshchmod 600 ~/.ssh/authorized_keysssh-copy-id c1 或者 ssh-copy-id user@hostssh-copy-id c2 或者 ssh-copy-id user@hostssh-copy-id c3 或者 ssh-copy-id user@host hadoop包 12345678910111213# 在c1机器mkdir -p /opt/hadoopwget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz tar -zxvf hadoop-3.3.1.tar.gz -C /opt/hadoop# c1 c2 c3 都执行vim /etc/profileexport HADOOP_HOME=/opt/hadoop/hadoop-3.3.1 export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbinexport HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoopsource /etc/profile#c1上验证Hadoop配置是否生效hadoop version 配置hadoop 配置JAVA_HOME 1234567#进入Hadoop安装目录下的etc/hadoop目录cd $HADOOP_HOME/etc/hadoop#查看jdk安装路径（rpm安装方式）update-alternatives --config java/usr/java/jdk1.8.0_301-amd64/bin/java#分别在hadoop-env.sh、mapred-env.sh、yarn-env.sh文件中添加jdk安装路径export JAVA_HOME=/usr/java/jdk1.8.0_301-amd64/bin/java core-sit.xml 123456789101112 &lt;configuration&gt; &lt;property&gt; &lt;!-- 配置hdfs地址 --&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://c1:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 保存临时文件目录，需先在/opt/hadoop/hadoop-3.3.1下创建tmp目录 --&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop/hadoop-3.3.1/tmp&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml 12345678910111213141516171819202122232425262728293031&lt;configuration&gt; &lt;property&gt; &lt;!-- 主节点地址 --&gt; &lt;name&gt;dfs.namenode.http-address&lt;/name&gt; &lt;value&gt;c1:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/opt/hadoop/hadoop-3.3.1/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/opt/hadoop/hadoop-3.3.1/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 备份数为默认值3 --&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt; 配置为false后，可以允许不要检查权限就生成dfs上的文件，方便倒是方便了，但是你需要防止误删除 &lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; mapred-site.xml 1234567891011121314151617181920212223242526272829 &lt;configuration&gt; &lt;property&gt; &lt;!--设置MapReduce的运行平台为yarn--&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;c1:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;c1:19888&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.application.classpath&lt;/name&gt; &lt;value&gt; /opt/hadoop/hadoop-3.3.1/etc/hadoop, /opt/hadoop/hadoop-3.3.1/share/hadoop/common/*, /opt/hadoop/hadoop-3.3.1/share/hadoop/common/lib/*, /opt/hadoop/hadoop-3.3.1/share/hadoop/hdfs/*, /opt/hadoop/hadoop-3.3.1/share/hadoop/hdfs/lib/*, /opt/hadoop/hadoop-3.3.1/share/hadoop/mapreduce/*, /opt/hadoop/hadoop-3.3.1/share/hadoop/mapreduce/lib/*, /opt/hadoop/hadoop-3.3.1/share/hadoop/yarn/*, /opt/hadoop/hadoop-3.3.1/share/hadoop/yarn/lib/* &lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-sit.xml 1234567891011121314151617181920212223242526272829303132&lt;configuration&gt;&lt;property&gt; &lt;description&gt;指定YARN的老大（ResourceManager）的地址&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;c1&lt;/value&gt; &lt;/property&gt;&lt;!-- NodeManager上运行的附属服务。需要配置成mapreduce_shfffle,才可运行MapReduce程序默认值 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;discription&gt;每个节点可用内存,单位MB,默认8182MB&lt;/discription&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt; &lt;value&gt; JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR, CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME &lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; works 文件 12c2c3 拷贝到从节点 12scp -r /opt/hadoop/hadoop-3.3.1 c2:/opt/scp -r /opt/hadoop/hadoop-3.3.1 c3:/opt/ 添加HDFS和Yarn权限 12345#start-dfs.sh、stop-dfs.sh HDFS_DATANODE_USER=rootHDFS_DATANODE_SECURE_USER=hdfsHDFS_NAMENODE_USER=rootHDFS_SECONDARYNAMENODE_USER=root 添加Yarn权限 1234# start-yarn.sh stop-yarn.sh YARN_RESOURCEMANAGER_USER=rootHDFS_DATANODE_SECURE_USER=yarnYARN_NODEMANAGER_USER=root 启动 123456789101112131415# 格式化hdfsbin/hdfs namenode -format#启动sbin/start-all.sh# 主节点验证jps24800 Jps8982 NameNode9256 SecondaryNameNode9497 ResourceManager# 从节点验证jps14897 Jps30898 DataNode31069 NodeManager 查看 1234# resourceManager页面http://c1:8088# namenode页面http://c1:50070","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"Python多版本管理","slug":"Python多版本管理","date":"2020-09-22T08:53:36.000Z","updated":"2024-02-28T06:05:18.266Z","comments":true,"path":"2020/09/22/Python多版本管理/","link":"","permalink":"http://yoursite.com/2020/09/22/Python%E5%A4%9A%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86/","excerpt":"","text":"前言 在日常的开发中，我们经常需要在多版本的python之间切换，手动修改bash文件比较麻烦，pyenv应运而生。 安装 安装homebrew 1/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)&quot; 安装pyenv 1brew install pyenv 设置环境变量 我是用的oh-my-zsh，所以编辑 .zshrc文件。 12345678910111、vim ~/.zshrc# 将下面3句话放在文件最后# 使用bash，同理就放在～/.bash_profile里# 别忘了 source export PYENV_ROOT=/usr/local/var/pyenvexport PATH=&quot;$PYENV_ROOT/bin:$PATH&quot;eval &quot;$(/usr/local/bin/pyenv init -)&quot;if which pyenv &gt; /dev/null; then eval &quot;$(pyenv init -)&quot;; fi2、source ~/.zshrc 安装指定版本python 在线安装 1pyenv install 3.8.5 离线安装 使用pyenv install 安装指定python版本时候很慢，最好的办法是离线下载下来 cd $PYENV_ROOT &amp;&amp; mkdir cache 下载 python版本，并拷贝到$PYENV_ROOT/cache 中 pyenv install Python-xxx.tar.xz pyenv global xxx pyenv rehash （一定要，不然不生效） 常用命令 受托管的python版本们：pyenv versions 当前使用的python版本：pyenv version 安装指定python版本。例如：pyenv install 3.8.5 卸载指定python版本：pyenv uninstall [python版本号] 全局生效：pyenv global xxx 当前shell生效：pyenv shell xxx 本文件夹下生效：pyenv local xxx","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"python 多环境","slug":"python-多环境","permalink":"http://yoursite.com/tags/python-%E5%A4%9A%E7%8E%AF%E5%A2%83/"}]},{"title":"Linux命令","slug":"Linux命令","date":"2020-09-21T00:45:49.000Z","updated":"2022-11-22T01:27:53.386Z","comments":true,"path":"2020/09/21/Linux命令/","link":"","permalink":"http://yoursite.com/2020/09/21/Linux%E5%91%BD%E4%BB%A4/","excerpt":"","text":"JDK 准备 jdk-8u221-linux-x64.rpm https://www.oracle.com/java/technologies/downloads/#java8 安装：rpm -ivh jdk-8u221-linux-x64.rpm 找到安装路径：update-alternatives --config java 12345共有 1 个提供“java”的程序。 选项 命令-----------------------------------------------*+ 1 /usr/java/jdk1.8.0_291-amd64/bin/java 环境变量 123456// /ect/profile 添加export JAVA_HOME=/usr/java/jdk1.8.0_301-amd64export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jarsource /etc/profile su和su -区别 su [user]切换到其他用户，但是不切换环境变量 su - [user]则是完整的切换到新的用户环境。 Linux版本号 日常使用了很多linux命令，有时候会忘了，这里列举常用的命令，使用系统如下： 123456789101112131415# contos$ lsb_release -aLSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.3.1611 (Core)Release: 7.3.1611Codename: Core# ubuntu$ lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 18.04.5 LTSRelease: 18.04Codename: bionic Linux内核版本 123$ cat /proc/version$ cat /proc/versionLinux version 3.10.0-1160.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-44) (GCC) ) #1 SMP Mon Oct 19 16:18:59 UTC 2020 关机、重启 关机 shutdown -h now：关机不掉电 halt：同上意义 init 0：同上意义 poweroff：关闭电源 重启 shutdown -r now：立即重启 reboot：同上 init：同上 用户和组 查看用户 123id hfwhowhoami 看所有用户 1cat /etc/passwd 新建工作组 1groupadd ghf 删除工作组 1groupdel ghf 新建用户 12useradd hfpasswd 123 删除用户 1userdel hf 用户与用户组 123456// 新建用户并加入到用户组useradd -g ghf hf// 存在hf用户加入到ghf组，并移除其他所属组usermod -G ghf hf// 存在hf用户加入到ghd组，其他所属组不变usermod -a ghd hf 文件和目录操作命令 pwd print working directory，显示当前目录 12[deploy@localhost ~]$ pwd/home/deploy cd chang directory，切换目录。 1cd /opt tree 以树类目形式展示目录结构 有些系统没有安装此命令，没有请安装。例如：只显示第一层的/opt 。 1234567891011121314151617[deploy@localhost opt]$ tree -L 1 /opt/opt├── client├── code├── conf├── data├── keytab├── plugins├── third└── workspace# 参数 -a 显示所有文件,包含隐藏文件。 -d 只显示目录。 -f 显示完整路径。 -L level 控制显示的目录深度。 -o filename 将结果输出到文件 mkdir Make directory 创建目录。 -p 创建多级目录，-v 显示创建过程。 12$ mkdir -v -p hf/fan$ mkdir -p /data/alertmanager/&#123;bin,conf,logs,data,templates&#125; touch 创建文件，注意不是目录。 创建2个文件，touch 后面可以接多个需要被新建的文件名。 1[deploy@localhost opt]$ touch a.txt b.txt ls 列出文件，最长用的命令了。 列举几个本人常常使用的命令组合 ls：列出当前位置文件 ls -l 、ll ：2命令等同。都表示列出当前位置的所有文件，并列出目录的信息 ls -al、ll -a : 2命令等同。意义同上，额外显示隐藏的.文件 ls -ltr ：列出当前位置的所有文件，并列出目录的信息，且按照修改时间自然排序，即可以使文件最新创建/修改的在最后一个 123456789101112131415161718192021222324[deploy@localhost opt]$ lsa.txt b.txt workspace[deploy@localhost opt]$ ll总用量 8-rw-rw-r-- 1 deploy deploy 0 9月 20 14:17 a.txt-rw-rw-r-- 1 deploy deploy 0 9月 20 14:17 b.txtdrwxrwxr-x 3 deploy deploy 17 9月 20 14:12 hfdrwxrwxr-x 43 deploy deploy 4096 9月 18 22:42 workspace[deploy@localhost opt]$ ll -a总用量 8drwxr-xr-x. 11 deploy deploy 151 9月 20 14:17 .dr-xr-xr-x. 18 root root 256 5月 19 09:21 ..-rw-rw-r-- 1 deploy deploy 0 9月 20 14:17 a.txt-rw-rw-r-- 1 deploy deploy 0 9月 20 14:17 b.txtdrwxrwxr-x 3 deploy deploy 17 9月 20 14:12 hfdrwxrwxr-x 43 deploy deploy 4096 9月 18 22:42 workspace[deploy@localhost opt]$ ls -ltr总用量 8drwxrwxr-x 43 deploy deploy 4096 9月 18 22:42 workspacedrwxrwxr-x 3 deploy deploy 17 9月 20 14:12 hf-rw-rw-r-- 1 deploy deploy 0 9月 20 14:17 b.txt-rw-rw-r-- 1 deploy deploy 0 9月 20 14:17 a.txt cp copy，复制 单个文件复制 文件夹复制 12[deploy@localhost opt]$ cp a.txt hf/[deploy@localhost workspace]$ cp -r hf/ workspace/ mv move 剪切、重命名。 123[deploy@localhost opt]$ mv b.txt hf/[deploy@localhost opt]$ cd hf/[deploy@localhost hf]$ mv b.txt bb.txt rm 删除 经常使用的命令，-rf 递归强制删除 1[deploy@localhost hf]$ rm -rf bb.txt 除了sql-5.1.2 都删除 1ls | grep -v sql-5.1.2 | xargs rm -rf ln link 链接，ln [参数][源文件或目录][目标文件或目录] 索引节点（inode）：在linux中所有的文件都有一个inode，且inode和文件的关系是：1:n。 软链接 类似windows上的快捷方式，是符号链接。目标文件不能存在，目标文件删除不影响源文件，源文件删除了，目标文件仍存在，但是不能访问了，也就是没入口了。 123[deploy@localhost hf]$ ln -s /home/deploy/fan /home/deploy/soft_fan[deploy@localhost hf]$ lsa.txt fan soft_fan 硬链接 索引节点（inode）：linux文件系统中（etx2，ext3，ext4）每个文件都有一个inode节点，且inode节点和文件的关系是1:n。硬链接就是通过inode共享方式存在的。源文件删除了，不影响硬链接，反之一样。只有硬链接和源文件都没了，文件实体彻底没了。 不能使用目录来作为硬链接，一般文件即可哦 1[deploy@localhost fan]$ sudo ln /etc/hosts hard_fan find 查找目录下文件 find [目录] [参数] [限定条件] [执行动作] 目录：符合linux规定的目录路径 参数（基本不使用，这里只做介绍下）： -depth：从指定目录喜爱最深的子目录开始查找 -maxdepth levels：查找最大的目录级数，level为自然数 -regxtype type：改变正则的模式，默认为emacs，可以有posix-awk、posix-basic、posix-egrep、posix-extened 限定条件（选项很多，介绍几个常用的） -name：按照文件名字查找，支持 * ? [] 通配符（常用） -type：查找文件类型（f：普通文件；d 目录；l 符号链接；）（常用） ！：反，例如 ! -type d，查找不是目录 -regex：接受正则（常用） -atime -/+/ n：文件访问时间，单位天，-n天内，+n天之前，n距离现在n天（明确的） -ctime -/+/ n：文件改变时间，单位天，-n天内，+n天之前，n距离现在n天（明确的） -size -n[bkMG]：查找指定大小的文件。bkMG为单位 -perm xxx：按照权限，例如-perm 755 执行动作（使用的也很少） -delete：查出后删除 -exec：对查询出的文件执行后续的shell指令 12345 // 查找log文件，且大小大于6M [deploy@localhost opt]$ find . -name *.log -size +6M // 查找当前路径下，只查找1级目录下，类型是文件的显示出 [deploy@localhost opt]$ find . -maxdepth 1 -type f | xargs ls -l-rw-rw-r-- 1 deploy deploy 0 9月 20 14:17 ./a.txt rename 通过字符串替换的方式修改文件名字 将当前路径下所有的txt文件修改为md文件 1[deploy@localhost opt]$ rename .txt .md *.txt md5sum 计算文件md5值，一般用于检查传输的文件是否破坏 chown 改变文件/目录的用户和用户组 chown [-R] [owner] [:[group]] [file] 更改文件所属用户 1chown -R deploy client 更改用户所属用户组 123// :和.都可以chown -R :deploy clientchown -R .deploy client 更改用户所属用户和用户组 1chown -R deploy:deploy client chmod 改变文件权限，只有文件所主和root用户才能执行 chmod [-R] [mode] [file] 用户类型和操作符 权限位 含义 用户类型 属主：u；属组：g；其他：o 操作字符 加：+；减-；设置：= 权限 权限字母 对应数字 含义 r 4 读 w 2 写 x 1 执行 - 0 没有任何权限 权限解释 例如：755/rwxr-xr-x 7 | rwx 属主权限 5 | r-x 属组权限 5 | r-x 其他用户权限 12345[deploy@localhost opt]$ chmod 755 hf[deploy@localhost opt]$ chmod o+w hf[deploy@localhost opt]$ chmod g+rwx hf[deploy@localhost opt]$ chmod o= hf[deploy@localhost opt]$ chmod u=rwx,g=rx,o= hf 文件过滤和内容编辑命令 cat 一般用作查看文件内容，或者重定向到指定文件。 输出内容并编号 123[deploy@localhost opt]$ cat -n b 1 bb 2 bbb 生成文件 &lt;&lt;EOF 代表输入EOF后标准输入就结束了，&gt;&gt; f 代表输入到文件f中，切实追加，覆盖使用&gt; 123456[deploy@localhost opt]$ cat &gt;&gt;f&lt;&lt;EOF&gt; 0908322&gt; EOF[deploy@localhost opt]$ cat f0908322 编辑文件 12345678910111213// 覆盖，记得enter后ctr+c[deploy@localhost opt]$ cat &gt;f66666666^C[deploy@localhost opt]$ cat f66666666// 追加，记得enter后ctr+c[deploy@localhost opt]$ cat &gt;&gt;f77777777^C[deploy@localhost opt]$ cat f6666666677777777 more 一页页将文件内容显示 1[deploy@localhost logs]$ more data.log Enter: 向下一行 空格 | f：向下滚一屏 b：向上滚一屏 -s：连续多个空行显示为一行 / 查找文本 v：调用vi q：退出 less more 的高级版本，包含more的参数 -i：搜索时忽略大小写 -m：类似more，可以显示百分比 -N ：显示行号 G：最后一行 g：开头一行 head 显示文件头几行，默认10 -n ：头几行 1[deploy@localhost logs]$ head -100 data.log tail 显示文件尾几行，默认10 -n ：头几行 -f：实时显示文件变化后追加的数据 1[deploy@localhost logs]$ tail -100f data.log tailf 和tail -f 几乎等效，默认也是输出文件最后10行。但是如果文件不增长，不会去访问磁盘文件，也不更改文件访问时间。 1[deploy@localhost logs]$ tailf -100 data.log tar tar.gz =tgz 12345678910111213# 解压缩tar -zxvf test.tar.gz# 压缩，隐藏文件不会被打包（例如：.env）tar -zcvf test.tar.gz 待压缩目录的路径# 压缩，隐藏文件会被打包（例如：.env），且可以排除某些文件tar -zcvf test.tar.gz * .[!.]* --exclude .git-c, --create 创建一个新归档-x, --extract, --get 从归档中解出文件-f, --file=ARCHIVE 使用归档文件-z, --gzip, --gunzip, --ungzip 通过 gzip 过滤归档-C, --directory=DIR 改变至目录 DIR-v, --verbose 详细地列出处理的文件 curl 12345GET: curl URL?a=1&amp;b=nihao# -d, --data &lt;data&gt;POST: curl -X POST -d &#x27;p1=1&amp;p2=2&#x27; URL curl -H &quot;Content-Type:application/json&quot; -X POST -d &#x27;&#123;&quot;p1&quot;:1,&quot;p2&quot;:&quot;2&quot;&#125;&#x27; URL wget 下载资源使用。 12345678910111213# 直接下载wget https://download.redis.io/releases/redis-6.0.8.tar.gz# 其他名称保存下载的文件wget -O redis.tar.gz https://download.redis.io/releases/redis-6.0.8.tar.gz# 将文件下载到指定目录wget -P /usr/software https://download.redis.io/releases/redis-6.0.8.tar.gz# 中途网络断开导致没有下载完成，我们就可以使用命令的-c选项恢复下载，让下载从断点续传，无需从头下载wget -c https://download.redis.io/releases/redis-6.0.8.tar.gz# 后台下载wget -b https://download.redis.io/releases/redis-6.0.8.tar.gz# 下载多个文件，所有的url添加到该文件中，每个url都必须是单独的一行vim download_list.txtwget -i download_list.txt ping 123ping 域名/ip# 指定次数ping 域名/ip -c 3 telnet 测试tcp端口 1telnet 域名&#x2F;ip 8099 nc nc命令 全称netcat，用于设置路由器。它能通过 TCP 和 UDP 在网络中读写数据。 12345678nc -zvn -w 1 172.18.100.220 5007// 常用nc -z 172.18.100.220 5007-z 参数告诉netcat使用0输入输出模式，即连接成功后立即关闭连接，不进行数据交换- u udp端口-v 参数指使用冗余选项（译者注：即详细输出）-n 参数告诉netcat 不要使用DNS反向查询IP地址的域名-w 参数是超时参数，不加的话，可能一直不返回 apt Debian和其衍生系统命令工具。apt（Advanced Packaging Tool）是一个在 Debian 和 Ubuntu 中的 Shell 前端软件包管理器。 apt 命令时，用户不必再由 apt-get 转到 apt-cache 或 apt-config，首先使用 apt。偏向在线安装！可以修改源文件：/etc/apt/sources.list 添加其他源。 1234567891011121314安装指定的软件命令：sudo apt install &lt;package_name&gt;更新指定的软件命令：sudo apt update &lt;package_name&gt;删除软件包命令：sudo apt remove &lt;package_name&gt;查找软件包命令： sudo apt search &lt;keyword&gt;列出所有已安装的包：apt list --installed列出所有已安装的包的版本信息：apt list --all-versions安装多个软件包：sudo apt install &lt;package_1&gt; &lt;package_2&gt; &lt;package_3&gt;列出所有可更新的软件清单命令：sudo apt update升级软件包：sudo apt upgrade列出可更新的软件包及版本信息：apt list --upgradeable升级软件包，升级前先删除需要更新软件包：sudo apt full-upgrade显示软件包具体信息,例如：版本号，安装大小，依赖关系等等：sudo apt show &lt;package_name&gt;清理不再使用的依赖和库文件: sudo apt autoremove移除软件包及配置文件: sudo apt purge &lt;package_name&gt; dpkg 是“Debian Packager ”的简写。较apt，它主要使用场景为本地里deb包,可以对其安装、卸载、deb打包、deb解压等操作。即，偏向离线安装！ 12345678-i：安装软件包； -r：删除软件包； -P：删除软件包的同时删除其配置文件； -L：显示于软件包关联的文件； -l：显示已安装软件包列表； --unpack：解开软件包； -c：显示软件包内文件列表； --confiugre：配置软件包。 rpm RPM是RedHat Package Manager（RedHat软件包管理工具) 12345678910111213141516171819202122rpm -q samba &#x2F;&#x2F;查询程序是否安装rpm -ivh &#x2F;media&#x2F;cdrom&#x2F;RedHat&#x2F;RPMS&#x2F;samba-3.0.10-1.4E.i386.rpm &#x2F;&#x2F;按路径安装并显示进度rpm -ivh --relocate &#x2F;&#x3D;&#x2F;opt&#x2F;gaim gaim-1.3.0-1.fc4.i386.rpm &#x2F;&#x2F;指定安装目录rpm -ivh --test gaim-1.3.0-1.fc4.i386.rpm &#x2F;&#x2F;用来检查依赖关系；并不是真正的安装；rpm -Uvh --oldpackage gaim-1.3.0-1.fc4.i386.rpm &#x2F;&#x2F;新版本降级为旧版本rpm -qa | grep httpd ＃[搜索指定rpm包是否安装]--all搜索*httpd*rpm -ql httpd ＃[搜索rpm包]--list所有文件安装目录rpm -qpi Linux-1.4-6.i368.rpm ＃[查看rpm包]--query--package--install package信息rpm -qpf Linux-1.4-6.i368.rpm ＃[查看rpm包]--filerpm -qpR file.rpm ＃[查看包]依赖关系rpm2cpio file.rpm |cpio -div ＃[抽出文件]rpm -ivh file.rpm ＃[安装新的rpm]--install--verbose--hashrpm -Uvh file.rpm ＃[升级一个rpm]--upgraderpm -e file.rpm ＃[删除一个rpm包]--erase－ivh：安装显示安装进度--install--verbose--hash－Uvh：升级软件包--Update；－qpl：列出RPM软件包内的文件信息[Query Package list]；－qpi：列出RPM软件包的描述信息[Query Package install package(s)]；－qf：查找指定文件属于哪个RPM软件包[Query File]；－Va：校验所有的RPM软件包，查找丢失的文件[View Lost]；－e：删除包 snap 123456789101112#snap安装软件sudo snap install firefox#列出安装的软件snap list#搜索软件snap find#更新软件sudo snap refresh firefox# 更新全部sudo snap refresh all#卸载软件snap remove firefox /dev/null ​ 可以将/dev/null看作&quot;黑洞&quot;. 它非常等价于一个只写文件. 所有写入它的内容都会永远丢失. 而尝试从它那儿读取内容则什么也读不到. 然而, /dev/null对命令行和脚本都非常的有用。shell中可能经常能看到：&gt;/dev/null 2&gt;&amp;1 等同于1&gt; /dev/null 2&gt; &amp;1 1：&gt; 代表重定向到哪里，例如：echo “123” &gt; /home/123.txt 2：/dev/null 代表空设备文件 3：2&gt; 表示stderr标准错误 4：&amp; 表示等同于的意思，2&gt;&amp;1，表示2的输出重定向等同于1 5：1 表示stdout标准输出，系统默认值是1，所以&quot;&gt;/dev/null&quot;等同于 “1&gt;/dev/null” 执行过程：&gt;/dev/null ：首先表示标准输出重定向到空设备文件，也就是不输出任何信息到终端，说白了就是不显示任何信息。 2&gt;&amp;1 ：接着，标准错误输出重定向 到 标准输出，因为之前标准输出已经重定向到了空设备文件，所以标准错误输出也重定向到空设备文件。 ​ command &gt; file 2&gt;file 与command &gt; file 2&gt;&amp;1 它们有什么不同的地方吗？ 首先command &gt; file 2&gt;file 的意思是将命令所产生的标准输出信息,和错误的输出信息送到file 中.command &gt; file 2&gt;file 这样的写法,stdout和stderr都直接送到file中, file会被打开两次,这样stdout和stderr会互相覆盖,这样写相当使用了FD1和FD2两个同时去抢占file 的管道。 而command &gt;file 2&gt;&amp;1 这条命令就将stdout直接送向file, stderr 继承了FD1管道后,再被送往file,此时,file 只被打开了一次,也只使用了一个管道FD1,它包括了stdout和stderr的内容。 从IO效率上,前一条命令的效率要比后面一条的命令效率要低,所以在编写shell脚本的时候,较多的时候我们会command &gt; file 2&gt;&amp;1 这样的写法。 磁盘 系统磁盘使用情况 1df -h [路径] 目录使用情况 12du -sh [--max-depth=1] [路径] 分区 123456789101112131415// 查看[deploy@dubhe-node ~]$ sudo fdisk -l /dev/sdb1磁盘 /dev/sdb1：53.7 GB, 53686042624 字节，104855552 个扇区Units = 扇区 of 1 * 512 = 512 bytes扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：512 字节 / 512 字节// 管理命令使用格式：fdisk /dev/sd# ：创建，删除，保存磁盘分区配置。fdisk提供了一个交互式接口来管理分区，它有许多子命令，分别用于不同的管理功能；所有的操作均在内存中完成，没有直接同步到磁盘；直到使用w命令保存至磁盘上；以下命令选项为m中的常用选项： n：创建新分区 d：删除已有分区 t：修改分区类型 l：查看所有已经ID w：保存并退出 q：不保存并退出 m：查看帮助信息 p：显示现有分区信息 CPU lscpu 查看CPU型号 1cat /proc/cpuinfo | grep name | sort | uniq 查看物理CPU数目 1234// 多少个cpucat /proc/cpuinfo | grep &quot;physical id&quot;// 用管道排序去重后直接输出物理cpu的个数cat /proc/cpuinfo | grep &quot;physical id&quot; | sort | uniq | wc -l 查看每个物理CPU中的core的个数 1cat /proc/cpuinfo| grep &quot;cpu cores&quot;| uniq 查看逻辑CPU数目（几核） 1cat /proc/cpuinfo| grep &quot;processor&quot;| wc -l cpu是x86还是arm 12uname -a | awk -F &quot; &quot; &#x27;&#123;print $(NF-1)&#125;&#x27;lscpu 查看占用CPU/内存前N ps -aux 查看的信息多些，一般第三列为CPU，第四列为内存。k3第三列开始排序，nr 按照数字倒序，head -5 显示前面5 1234USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 191160 4120 ? Ss 4月21 4:21 /usr/lib/systemd/systemd --switched-root --system --deserialize 22root 2 0.0 0.0 0 0 ? S 4月21 0:00 [kthreadd]root 4 0.0 0.0 0 0 ? S&lt; 4月21 0:00 [kworker/0:0H] cpu前五 1ps -aux | sort -k3nr | head -5 cpu降序 1ps aux --sort=-%cpu 内存前五 1ps -aux | sort -k4nr | head -5 替换 12345678910111213 :s/vivian/sky/ 替换当前行第一个 vivian 为 sky :s/vivian/sky/g 替换当前行所有 vivian 为 sky :n,$s/vivian/sky/ 替换第 n 行开始到最后一行中每一行的第一个 vivian 为 sky :n,$s/vivian/sky/g 替换第 n 行开始到最后一行中每一行所有 vivian 为 sky n 为数字，若 n 为 .，表示从当前行开始到最后一行 :%s/vivian/sky/(等同于 :g/vivian/s//sky/) 替换每一行的第一个 vivian 为 sky :%s/vivian/sky/g(等同于 :g/vivian/s//sky/g) 替换每一行中所有 vivian 为 sky 网卡 网卡速率 12345678910111213141516171819202122232425262728293031323334// 方法1[deploy@localhost ~]$ sudo mii-tool eth0// 1000baseT 千兆eth0: negotiated 1000baseT-FD flow-control, link ok//方法2[deploy@localhost ~]$ ethtool eth0Settings for eth0: Supported ports: [ TP ] // 打满千兆 Supported link modes: 10baseT/Half 10baseT/Full 100baseT/Half 100baseT/Full 1000baseT/Full Supported pause frame use: No // 可协商的速率 Supports auto-negotiation: Yes Supported FEC modes: Not reported Advertised link modes: 10baseT/Half 10baseT/Full 100baseT/Half 100baseT/Full 1000baseT/Full Advertised pause frame use: No Advertised auto-negotiation: Yes Advertised FEC modes: Not reported Speed: 1000Mb/s Duplex: Full Port: Twisted Pair PHYAD: 0 Transceiver: internal Auto-negotiation: on MDI-X: off (auto)Cannot get wake-on-lan settings: Operation not permitted Current message level: 0x00000007 (7) drv probe link Link detected: yes 网卡驱动 1234567891011[deploy@localhost ~]$ ethtool -i eth0driver: e1000version: 7.3.21-k8-NAPIfirmware-version:expansion-rom-version:bus-info: 0000:02:00.0supports-statistics: yessupports-test: yessupports-eeprom-access: yessupports-register-dump: yessupports-priv-flags: no 网卡型号 12[deploy@localhost ~]$ lspci |grep Ethernet02:00.0 Ethernet controller: Intel Corporation 82545EM Gigabit Ethernet Controller (Copper) (rev 01) 网卡流量 123yum -y install sysstat# 查看网卡流量，每秒输出一次，输出100次。sar -n DEV 1 100 查看链接信息 123456789// 方法1[deploy@localhost ~]$ netstat -n | awk &#x27;/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;&#x27;ESTABLISHED 58TIME_WAIT 50// 方法2[deploy@localhost ~]$ ss -tan|awk &#x27;NR&gt;1&#123;++S[$1]&#125;END&#123;for (a in S) print a,S[a]&#125;&#x27;LISTEN 24ESTAB 58TIME-WAIT 41 GPU信息 nvidia-smi nvidia-sim简称NVSMI，提供监控GPU使用情况和更改GPU状态的功能，是一个跨平台工具，支持所有标准的NVIDIA驱动程序支持的Linux和WindowsServer 2008 R2 开始的64位系统。这个工具是N卡驱动附带的，只要装好驱动，就会有这个命令！ lshw -C display 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162WARNING: you should run this program as super-user. *-display description: VGA compatible controller product: GP102 [GeForce GTX 1080 Ti] vendor: NVIDIA Corporation physical id: 0 bus info: pci@0000:04:00.0 version: a1 width: 64 bits clock: 33MHz capabilities: vga_controller bus_master cap_list rom configuration: driver&#x3D;nvidia latency&#x3D;0 resources: iomemory:3bf0-3bef iomemory:3bf0-3bef irq:117 memory:93000000-93ffffff memory:3bfc0000000-3bfcfffffff memory:3bfd0000000-3bfd1ffffff ioport:3000(size&#x3D;128) memory:94080000-940fffff *-display description: VGA compatible controller product: GP102 [GeForce GTX 1080 Ti] vendor: NVIDIA Corporation physical id: 0 bus info: pci@0000:02:00.0 version: a1 width: 64 bits clock: 33MHz capabilities: vga_controller bus_master cap_list rom configuration: driver&#x3D;nvidia latency&#x3D;0 resources: iomemory:3bf0-3bef iomemory:3bf0-3bef irq:116 memory:91000000-91ffffff memory:3bfe0000000-3bfefffffff memory:3bff0000000-3bff1ffffff ioport:2000(size&#x3D;128) memory:92080000-920fffff *-display description: VGA compatible controller product: G200eR2 vendor: Matrox Electronics Systems Ltd. physical id: 0 bus info: pci@0000:09:00.0 version: 01 width: 32 bits clock: 33MHz capabilities: vga_controller bus_master cap_list rom configuration: driver&#x3D;mgag200 latency&#x3D;64 maxlatency&#x3D;32 mingnt&#x3D;16 resources: irq:17 memory:90000000-90ffffff memory:95000000-95003fff memory:94800000-94ffffff memory:c0000-dffff *-display description: VGA compatible controller product: GP102 [GeForce GTX 1080 Ti] vendor: NVIDIA Corporation physical id: 0 bus info: pci@0000:83:00.0 version: a1 width: 64 bits clock: 33MHz capabilities: vga_controller bus_master cap_list rom configuration: driver&#x3D;nvidia latency&#x3D;0 resources: iomemory:3ff0-3fef iomemory:3ff0-3fef irq:118 memory:ca000000-caffffff memory:3ffc0000000-3ffcfffffff memory:3ffd0000000-3ffd1ffffff ioport:9000(size&#x3D;128) memory:cb080000-cb0fffff *-display description: VGA compatible controller product: GP102 [GeForce GTX 1080 Ti] vendor: NVIDIA Corporation physical id: 0 bus info: pci@0000:84:00.0 version: a1 width: 64 bits clock: 33MHz capabilities: vga_controller bus_master cap_list rom configuration: driver&#x3D;nvidia latency&#x3D;0 resources: iomemory:3ff0-3fef iomemory:3ff0-3fef irq:119 memory:c8000000-c8ffffff memory:3ffe0000000-3ffefffffff memory:3fff0000000-3fff1ffffff ioport:8000(size&#x3D;128) memory:c9080000-c90fffffWARNING: output may be incomplete or inaccurate, you should run this program as super-user. lspci | grep -i nvidia PCI是Peripheral Component Interconnect(外设部件互连标准)的缩写，它是目前个人电脑中使用最为广泛的接口，几乎所有的主板产品上都带有这种插槽。 lspci，是一个用来显示系统中所有PCI总线设备或连接到该总线上的所有设备的工具。pci是一种总线，而通过pci总线连接的设备就是pci设备了。如今，我们常用的设备很多都是采用pci总线了，如：网卡、存储等。查找忽略大小写的nvidia设备（lspci | grep -i nvidia）。 1234567802:00.0 VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)02:00.1 Audio device: NVIDIA Corporation GP102 HDMI Audio Controller (rev a1)04:00.0 VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)04:00.1 Audio device: NVIDIA Corporation GP102 HDMI Audio Controller (rev a1)83:00.0 VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)83:00.1 Audio device: NVIDIA Corporation GP102 HDMI Audio Controller (rev a1)84:00.0 VGA compatible controller: NVIDIA Corporation GP102 [GeForce GTX 1080 Ti] (rev a1)84:00.1 Audio device: NVIDIA Corporation GP102 HDMI Audio Controller (rev a1) nvidia-smi -L 1234GPU 0: GeForce GTX 1080 Ti (UUID: GPU-bd742450-3edb-bc21-2208-aced9e42c68a)GPU 1: GeForce GTX 1080 Ti (UUID: GPU-cf6289bb-92d0-8a57-5641-5c2cd5e117ba)GPU 2: GeForce GTX 1080 Ti (UUID: GPU-ca9aab03-aed0-5410-9b0b-a84548cf1e25)GPU 3: GeForce GTX 1080 Ti (UUID: GPU-2a0fc328-b64b-53f3-292c-5e43db75553c) nvidia-smi 表格参数详解： GPU：本机中的GPU编号（有多块显卡的时候，从0开始编号）图上GPU的编号是：0 Fan：风扇转速（0%-100%），N/A表示没有风扇 Name：GPU类型，图上GPU的类型是：Tesla T4 Temp：GPU的温度（GPU温度过高会导致GPU的频率下降） Perf：GPU的性能状态，从P0（最大性能）到P12（最小性能），图上是：P0 Persistence-M：持续模式的状态，持续模式虽然耗能大，但是在新的GPU应用启动时花费的时间更少，图上显示的是：off Pwr：Usager/Cap：能耗表示，Usage：用了多少，Cap总共多少 Bus-Id：GPU总线相关显示，domain：bus：device.function Disp.A：Display Active ，表示GPU的显示是否初始化 Memory-Usage：显存使用率 Volatile GPU-Util：GPU使用率 Uncorr. ECC：关于ECC的东西，是否开启错误检查和纠正技术，0/disabled,1/enabled Compute M：计算模式，0/DEFAULT,1/EXCLUSIVE_PROCESS,2/PROHIBITED Processes：显示每个进程占用的显存使用率、进程号、占用的哪个GPU 每一秒刷新一次查看GPU信息 1watch -n 1 nvidia-smi 查看指定id的nvidia:nvidia-smi - i 0 12345678910111213141516+-----------------------------------------------------------------------------+| NVIDIA-SMI 440.33.01 Driver Version: 440.33.01 CUDA Version: 10.2 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage&#x2F;Cap| Memory-Usage | GPU-Util Compute M. ||&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;+&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;|| 0 GeForce GTX 108... On | 00000000:02:00.0 Off | N&#x2F;A || 54% 71C P2 240W &#x2F; 250W | 2523MiB &#x2F; 11178MiB | 75% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;|| 0 44505 C .&#x2F;sj2_onnx 2511MiB |+-----------------------------------------------------------------------------+ DCGM DCGM(Data Center GPU Manager)即数据中心GPU管理器，是一套用于在集群环境中管理和监视Tesla™GPU的工具。 1docker run -d --gpus all --rm -p 9400:9400 nvcr.io/nvidia/k8s/dcgm-exporter:2.0.13-2.1.2-ubuntu18.04 静态IP 1234567vim /etc/sysconfig/network-scripts/ifcfg-eth0BOOTPROTO=&quot;static&quot; #dhcp改为static ONBOOT=&quot;yes&quot; #开机启用本配置IPADDR=192.168.7.106 #静态IPGATEWAY=192.168.7.1 #默认网关NETMASK=255.255.255.0 #子网掩码DNS1=192.168.7.1 #DNS 配置 crontab 12345678910111213141516171819crontab -e# 内容1 0 * * * /bin/sh /home/sa/clean.sh# 时间表达式含义f1 f2 f3 f4 f5 program* * * * *- - - - -| | | | || | | | +----- 星期中星期几 (0 - 6) (星期天 为0)| | | +---------- 月份 (1 - 12) | | +--------------- 一个月中的第几天 (1 - 31)| +-------------------- 小时 (0 - 23)+------------------------- 分钟 (0 - 59)# clean.sh 内容如下#!/bin/bashsudo find . -mtime +1 -type f -name &quot;**.jpg&quot; -print0 |sudo xargs -0 rm -fmysql -u root -proot@1298 &lt;clean.sql find 这里是根据 -mtime查找当前时间前一天文件 制定type f只查找文件，忽略目录。find -mtime 后跟一个数字参数 +N/-N/N，说明如下： 1、-mtime n : n为数字，意思为在n天之前的“一天之内”被更改过内容的文件 2、-mtime +n : 列出在n天之前（不含n天本身）被更改过内容的文件名 3、-mtime -n : 列出在n天之内（含n天本身）被更改过内容的文件名 xargs 因为文件格式为 yyyy-MM-dd HH:mm:ss.jpg 格式。但是xargs默认是以空格作为分割传入参数，因此直接使用 xargs 会出现yyyy-MM-dd HH:mm:ss.jpg 变成2段了：yyyy-MM-dd 和 HH:mm:ss.jpg。所以使用 -print0 |xargs -0 -print0表示在find的每一个结果之后加一个NULL字符，而不是默认加一个换行符。 xargs -0表示xargs用NULL来作为分隔符（默认是空格）。","categories":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/categories/linux/"}],"tags":[{"name":"命令","slug":"命令","permalink":"http://yoursite.com/tags/%E5%91%BD%E4%BB%A4/"}]},{"title":"datax","slug":"datax","date":"2020-09-18T09:50:02.000Z","updated":"2022-11-22T01:27:53.537Z","comments":true,"path":"2020/09/18/datax/","link":"","permalink":"http://yoursite.com/2020/09/18/datax/","excerpt":"","text":"是什么 GitHub 地址：https://github.com/alibaba/DataX DataX 是阿里巴巴集团内被广泛使用的离线数据同步工具/平台，实现包括 MySQL、Oracle、SqlServer、Postgre、HDFS、Hive、ADS、HBase、TableStore(OTS)、MaxCompute(ODPS)、DRDS 等各种异构数据源之间高效的数据同步功能。 安装 环境要求 Linux｜Macos JDK(1.8以上，推荐1.8) Python(推荐Python2.6.X) Apache Maven 3.x (Compile DataX) 下载安装 tar.gz包下载：http://datax-opensource.oss-cn-hangzhou.aliyuncs.com/datax.tar.gz 源码编译 1234567git clone git@github.com:alibaba&#x2F;DataX.gitcd &#123;DataX_source_code_home&#125;mvn -U clean package assembly:assembly -Dmaven.test.skip&#x3D;true&#x2F;&#x2F;打包成功后的DataX包位于 &#123;DataX_source_code_home&#125;&#x2F;target&#x2F;datax&#x2F;datax&#x2F;cd &#123;DataX_source_code_home&#125;ls .&#x2F;target&#x2F;datax&#x2F;datax&#x2F;bin conf job lib log log_perf plugin script tmp 小试牛刀 怎么执行 datax 通过bin目录下的datax.py 文件 运行json文件来进行数据同步。解压缩包里有个测试datax的json文件。 1234567891011cd &#123;DataX_source_code_home&#125;./bin/datax.py ./job/job.json// 运行后结果2020-09-18 18:22:47.282 [job-0] INFO JobContainer - 任务启动时刻 : 2020-09-18 18:22:37 │任务结束时刻 : 2020-09-18 18:22:47 │任务总计耗时 : 10s │任务平均流量 : 253.91KB/s │记录写入速度 : 10000rec/s │读出记录总数 : 100000 │读写失败总数 : 0 msyql 同步到es mysql 脚本 1234567891011121314151617181920ROP TABLE IF EXISTS `test`;CREATE TABLE `test` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;key&#x27;, `col_ip` varchar(100) DEFAULT NULL COMMENT &#x27;projectcol_ipname&#x27;, `col_double` double(16,2) DEFAULT NULL COMMENT &#x27;col_double&#x27;, `col_long` bigint(20) DEFAULT NULL COMMENT &#x27;col_long&#x27;, `col_integer` int(11) DEFAULT &#x27;1&#x27; COMMENT &#x27;col_integer&#x27;, `col_keyword` varchar(100) DEFAULT NULL COMMENT &#x27;col_keyword&#x27;, `col_text` varchar(100) DEFAULT NULL COMMENT &#x27;col_text&#x27;, `col_geo_point` varchar(100) DEFAULT NULL COMMENT &#x27;col_geo_point&#x27;, `col_date` datetime DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;col_date time&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 ROW_FORMAT=DYNAMIC;LOCK TABLES `test` WRITE;INSERT INTO `test` (`id`, `col_ip`, `col_double`, `col_long`, `col_integer`, `col_keyword`, `col_text`, `col_geo_point`, `col_date`)VALUES (1,&#x27;1.1.1.1&#x27;,19890604.00,19890604,19890604,&#x27;hello world&#x27;,&#x27;hello world&#x27;,&#x27;41.12,-71.34&#x27;,&#x27;2020-09-18 19:04:29&#x27;);UNLOCK TABLES; myql2es.json 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495&#123; &quot;job&quot;:&#123; &quot;setting&quot;:&#123; &quot;speed&quot;:&#123; &quot;channel&quot;:1 &#125; &#125;, &quot;content&quot;:[ &#123; &quot;reader&quot;:&#123; &quot;name&quot;:&quot;mysqlreader&quot;, &quot;parameter&quot;:&#123; &quot;username&quot;:&quot;root&quot;, &quot;password&quot;:&quot;root&quot;, &quot;column&quot;:[ &quot;col_ip&quot;, &quot;col_double&quot;, &quot;col_long&quot;, &quot;col_integer&quot;, &quot;col_keyword&quot;, &quot;col_text&quot;, &quot;col_geo_point&quot;, &quot;col_date&quot; ], &quot;splitPk&quot;:&quot;&quot;, &quot;connection&quot;:[ &#123; &quot;table&quot;:[ &quot;test&quot; ], &quot;jdbcUrl&quot;:[ &quot;jdbc:mysql://127.0.0.1:3306/test&quot; ] &#125; ] &#125; &#125;, &quot;writer&quot;:&#123; &quot;name&quot;:&quot;elasticsearchwriter&quot;, &quot;parameter&quot;:&#123; &quot;endpoint&quot;:&quot;http://192.168.90.124:9200&quot;, &quot;index&quot;:&quot;mytest&quot;, &quot;type&quot;:&quot;default&quot;, &quot;accessId&quot;: &quot;1&quot;, // 不写报错，，没有认证就随写个值 &quot;accessKey&quot;: &quot;1&quot;, // 不写报错，没有认证就随写个值 &quot;cleanup&quot;:true, &quot;settings&quot;:&#123; &quot;index&quot;:&#123; &quot;number_of_shards&quot;:5, &quot;number_of_replicas&quot;:0 &#125; &#125;, &quot;discovery&quot;:false, &quot;batchSize&quot;:1000, &quot;splitter&quot;:&quot;,&quot;, &quot;column&quot;:[ &#123; &quot;name&quot;:&quot;col_ip&quot;, &quot;type&quot;:&quot;ip&quot; &#125;, &#123; &quot;name&quot;:&quot;col_double&quot;, &quot;type&quot;:&quot;double&quot; &#125;, &#123; &quot;name&quot;:&quot;col_long&quot;, &quot;type&quot;:&quot;long&quot; &#125;, &#123; &quot;name&quot;:&quot;col_integer&quot;, &quot;type&quot;:&quot;integer&quot; &#125;, &#123; &quot;name&quot;:&quot;col_keyword&quot;, &quot;type&quot;:&quot;keyword&quot; &#125;, &#123; &quot;name&quot;:&quot;col_text&quot;, &quot;type&quot;:&quot;text&quot; &#125;, &#123; &quot;name&quot;:&quot;col_geo_point&quot;, &quot;type&quot;:&quot;geo_point&quot; &#125;, &#123; &quot;name&quot;:&quot;col_date&quot;, &quot;type&quot;:&quot;date&quot; &#125; ] &#125; &#125; &#125; ] &#125;&#125; 使用kibana查看，同步成功 总结 datax 解放了我们在数据开发时候异构数据源间etl处理，使用起来还是很简单的，更多高级功能或者坑遇到再写。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"datax","slug":"datax","permalink":"http://yoursite.com/tags/datax/"}]},{"title":"Elasticsearch查询","slug":"Elasticsearch查询","date":"2020-09-14T09:14:02.000Z","updated":"2022-11-22T01:27:53.357Z","comments":true,"path":"2020/09/14/Elasticsearch查询/","link":"","permalink":"http://yoursite.com/2020/09/14/Elasticsearch%E6%9F%A5%E8%AF%A2/","excerpt":"","text":"ES查询文档：https://www.elastic.co/guide/en/elasticsearch/reference/6.8/search-aggregations-metrics-weight-avg-aggregation.html 数据准备 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@Test public void createIndex() throws Exception &#123; // 1.准备关于索引的setting Settings.Builder settings = Settings.builder() .put(&quot;number_of_shards&quot;, 3) .put(&quot;number_of_replicas&quot;, 1); // 2.准备关于索引的mapping XContentBuilder mappings = JsonXContent.contentBuilder() .startObject() .startObject(&quot;properties&quot;) .startObject(&quot;corpName&quot;) .field(&quot;type&quot;, &quot;keyword&quot;) .endObject() .startObject(&quot;createDate&quot;) .field(&quot;type&quot;, &quot;date&quot;) .field(&quot;format&quot;, &quot;yyyy-MM-dd&quot;) .endObject() .startObject(&quot;fee&quot;) .field(&quot;type&quot;, &quot;long&quot;) .endObject() .startObject(&quot;ipAddr&quot;) .field(&quot;type&quot;, &quot;ip&quot;) .endObject() .startObject(&quot;longCode&quot;) .field(&quot;type&quot;, &quot;keyword&quot;) .endObject() .startObject(&quot;mobile&quot;) .field(&quot;type&quot;, &quot;keyword&quot;) .endObject() .startObject(&quot;operatorId&quot;) .field(&quot;type&quot;, &quot;integer&quot;) .endObject() .startObject(&quot;province&quot;) .field(&quot;type&quot;, &quot;keyword&quot;) .endObject() .startObject(&quot;replyTotal&quot;) .field(&quot;type&quot;, &quot;integer&quot;) .endObject() .startObject(&quot;sendDate&quot;) .field(&quot;type&quot;, &quot;date&quot;) .field(&quot;format&quot;, &quot;yyyy-MM-dd&quot;) .endObject() .startObject(&quot;smsContent&quot;) .field(&quot;type&quot;, &quot;text&quot;) .field(&quot;analyzer&quot;, &quot;ik_max_word&quot;) .endObject() .startObject(&quot;state&quot;) .field(&quot;type&quot;, &quot;integer&quot;) .endObject() .endObject() .endObject(); // 3.将settings和mappings 封装到到一个Request对象中 CreateIndexRequest request = new CreateIndexRequest(index) .settings(settings) .mapping(type, mappings); // 4.使用client 去连接ES CreateIndexResponse response = client.indices().create(request, RequestOptions.DEFAULT); &#125; term和terms 查询 term 和terms 查询会直接对查询关键字不分词，直接去索引表里匹配，能匹配的到id，直接拿id去数据存储地方查询 term term : where province =北京 kibana 123456789101112POST /sms-logs-index/sms-logs-type/_search&#123; &quot;from&quot;: 0, &quot;size&quot;: 20, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;province&quot;: &#123; &quot;value&quot;: &quot;北京&quot; &#125; &#125; &#125;&#125; Java代码 123456789101112131415161718192021222324252627282930public class TermSearch &#123; ObjectMapper mapper = new ObjectMapper(); RestHighLevelClient client = EsClient.getClient(); String index = &quot;sms-logs-index&quot;; String type=&quot;sms-logs-type&quot;; @Test public void termSearchTest() throws IOException &#123; // 1.创建request对象 SearchRequest request = new SearchRequest(index); request.types(type); // 2.创建查询条件 SearchSourceBuilder builder = new SearchSourceBuilder(); builder.from(0); builder.size(5); builder.query(QueryBuilders.termQuery(&quot;province&quot;,&quot;北京&quot;)); request.source(builder); // 3.执行查询 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.输出查询结果 for (SearchHit hit : response.getHits().getHits()) &#123; Map&lt;String, Object&gt; sourceAsMap = hit.getSourceAsMap(); System.out.println(sourceAsMap); &#125; &#125;&#125; terms tterms: where province = 北京 or province =? (类似于mysql 中的 in) kibana 1234567891011POST /sms-logs-index/sms-logs-type/_search&#123; &quot;query&quot;: &#123; &quot;terms&quot;: &#123; &quot;province&quot;: [ &quot;北京&quot;, &quot;晋城&quot; ] &#125; &#125;&#125; Java代码 12345678910111213141516171819202122232425public class TermsSearch &#123; ObjectMapper mapper = new ObjectMapper(); RestHighLevelClient client = EsClient.getClient(); String index = &quot;sms-logs-index&quot;; String type=&quot;sms-logs-type&quot;; @Test public void termsSearchTest() throws IOException &#123; // 1.创建request对象 SearchRequest request = new SearchRequest(index); request.types(type); // 2.创建查询条件 SearchSourceBuilder builder = new SearchSourceBuilder(); builder.query(QueryBuilders.termsQuery(&quot;province&quot;,&quot;北京&quot;,&quot;晋城&quot;)); request.source(builder); // 3.执行查询 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 输出查询结果 for (SearchHit hit : response.getHits().getHits()) &#123; System.out.println(hit.getSourceAsMap()); &#125; &#125;&#125; match 查询 12345match 查询属于高级查询，会根据你查询字段的类型不一样，采用不同的查询方式查询的是日期或者数值，他会将你基于字符串的查询内容转换为日期或数值对待 如果查询的内容是一个不能被分词的内容（keyword）,match 不会将你指定的关键字进行分词 如果查询的内容是一个可以被分词的内容（text）,match 查询会将你指定的内容根据一定的方式进行分词，去分词库中匹配指定的内容match 查询，实际底层就是多个term 查询，将多个term查询的结果给你封装到一起 match_all kibana 1234567#match_all 查询，查询全部内容，不指定查询条件POST /sms-logs-index/sms-logs-type/_search&#123; &quot;query&quot;:&#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; Java代码 12345678910111213141516171819202122232425262728public class MatchAllSearch &#123; ObjectMapper mapper = new ObjectMapper(); RestHighLevelClient client = EsClient.getClient(); String index = &quot;sms-logs-index&quot;; String type=&quot;sms-logs-type&quot;; @Test public void matchAllSearch() throws IOException &#123; // 1.创建request对象 SearchRequest request = new SearchRequest(index); request.types(type); // 2.创建查询条件 SearchSourceBuilder builder = new SearchSourceBuilder(); builder.query(QueryBuilders.matchAllQuery()); // ES 默认只查询10条数据 builder.size(20); request.source(builder); // 3.执行查询 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.输出查询结果 for (SearchHit hit : response.getHits().getHits()) &#123; System.out.println(hit.getSourceAsMap()); &#125; System.out.println(response.getHits().getHits().length); &#125;&#125; match kibana 123456789#match 查询POST /sms-logs-index/sms-logs-type/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;smsContent&quot;: &quot;伟大战士&quot; &#125; &#125;&#125; Java代码 123456789101112131415161718192021222324252627public class MatchSearch &#123; ObjectMapper mapper = new ObjectMapper(); RestHighLevelClient client = EsClient.getClient(); String index = &quot;sms-logs-index&quot;; String type=&quot;sms-logs-type&quot;; @Test public void matchSearch() throws IOException &#123; // 1.创建request对象 SearchRequest request = new SearchRequest(index); request.types(type); // 2.创建查询条件 SearchSourceBuilder builder = new SearchSourceBuilder(); //-------------------------------------------------------------- builder.query(QueryBuilders.matchQuery(&quot;smsContent&quot;,&quot;伟大战士&quot;)); //-------------------------------------------------------------- builder.size(20); request.source(builder); // 3.执行查询 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.输出查询结果 for (SearchHit hit : response.getHits().getHits()) &#123; System.out.println(hit.getSourceAsMap()); &#125; System.out.println(response.getHits().getHits().length); &#125;&#125; 布尔match kibana 12345678910111213141516171819202122232425#布尔match查询 POST /sms-logs-index/sms-logs-type/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;smsContent&quot;: &#123; &quot;query&quot;: &quot;六七岁 有&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;&#125;#布尔match查询 POST /sms-logs-index/sms-logs-type/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;smsContent&quot;: &#123; &quot;query&quot;: &quot;战士 团队&quot;, &quot;operator&quot;: &quot;or&quot; &#125; &#125; &#125;&#125; Java代码 1234567891011121314151617181920@Test public void booleanMatchSearch() throws IOException &#123; // 1.创建request对象 SearchRequest request = new SearchRequest(index); request.types(type); // 2.创建查询条件 SearchSourceBuilder builder = new SearchSourceBuilder(); //-------------------------------------------------------------- builder.query(QueryBuilders.matchQuery(&quot;smsContent&quot;,&quot;战士 团队&quot;).operator(Operator.AND)); //-------------------------------------------------------------- builder.size(20); request.source(builder); // 3.执行查询 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.输出查询结果 for (SearchHit hit : response.getHits().getHits()) &#123; System.out.println(hit.getSourceAsMap()); &#125; System.out.println(response.getHits().getHits().length); &#125; multi_match match 针对一个field 做检索，multi_math 针对多个field 进行检索，多个field对应一个文本。即，fileds—&gt;text kibana 12345678910#multi_math 查询POST /sms-logs-index/sms-logs-type/_search&#123; &quot;query&quot;:&#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;北京&quot;, &quot;fields&quot;: [&quot;province&quot;,&quot;smsContent&quot;] &#125; &#125;&#125; Java 代码 123456789101112131415161718192021public void multiMatchSearch() throws IOException &#123; // 1.创建request对象 SearchRequest request = new SearchRequest(index); request.types(type); // 2.创建查询条件 SearchSourceBuilder builder = new SearchSourceBuilder(); //-------------------------------------------------------------- builder.query(QueryBuilders.multiMatchQuery(&quot;北京&quot;,&quot;province&quot;,&quot;smsContent&quot;)); //-------------------------------------------------------------- builder.size(20); request.source(builder); // 3.执行查询 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.输出查询结果 for (SearchHit hit : response.getHits().getHits()) &#123; System.out.println(hit.getSourceAsMap()); &#125; System.out.println(response.getHits().getHits().length); &#125; Id 和Ids查询 Id 查询 kibana 12#id 查询GET /sms-logs-index/sms-logs-type/1 Java 代码 12345678910111213141516171819public class IdGetSearch &#123; ObjectMapper mapper = new ObjectMapper(); RestHighLevelClient client = EsClient.getClient(); String index = &quot;sms-logs-index&quot;; String type=&quot;sms-logs-type&quot;; @Test public void findById() throws IOException &#123; // 创建GetRequest对象 GetRequest request = new GetRequest(index,type,&quot;1&quot;); // 执行查询 GetResponse response = client.get(request, RequestOptions.DEFAULT); // 输出结果 System.out.println(response.getSourceAsMap()); &#125;&#125; Ids 查询 根据多个id 查询,类似 mysql 中的 where in (id1,id2…) kibana 123456789#ids 查询POST /sms-logs-index/sms-logs-type/_search&#123; &quot;query&quot;: &#123; &quot;ids&quot;: &#123; &quot;values&quot;: [&quot;1&quot;,&quot;2&quot;,&quot;3&quot;] &#125; &#125;&#125; Java 代码 1234567891011121314151617public void findByIds() throws IOException &#123; // 创建request对象 SearchRequest request = new SearchRequest(index); request.types(type); // 指定查询条件 SearchSourceBuilder builder = new SearchSourceBuilder(); //-------------------------------------------------- builder.query(QueryBuilders.idsQuery().addIds(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;)); //------------------------------------------------------ request.source(builder); // 执行 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 输出结果 for (SearchHit hit : response.getHits().getHits()) &#123; System.out.println(hit.getSourceAsMap()); &#125; &#125; prefix 查询 前缀查询，可以通过一个关键字去指定一个field 的前缀，从而查询到指定文档 kibana 12345678910POST /sms-logs-index/sms-logs-type/_search&#123; &quot;query&quot;: &#123; &quot;prefix&quot;: &#123; &quot;corpName&quot;: &#123; &quot;value&quot;: &quot;海&quot; &#125; &#125; &#125;&#125; Java代码 1234567891011121314151617public void findByPrefix() throws IOException &#123; // 创建request对象 SearchRequest request = new SearchRequest(index); request.types(type); // 指定查询条件 SearchSourceBuilder builder = new SearchSourceBuilder(); //-------------------------------------------------- builder.query(QueryBuilders.prefixQuery(&quot;corpName&quot;,&quot;阿&quot;)); //------------------------------------------------------ request.source(builder); // 执行 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 输出结果 for (SearchHit hit : response.getHits().getHits()) &#123; System.out.println(hit.getSourceAsMap()); &#125; &#125; fuzzy 查询 模糊查询，和mysql的模糊还有点不太一样，这里是可以输入个大概，部分字错误也可以 kibana 123456789101112POST /sms-logs-index/sms-logs-type/_search&#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;: &#123; &quot;corpName&quot;: &#123; &quot;value&quot;: &quot;腾讯客堂&quot;, #指定前边几个字符是不允许出现错误的 &quot;prefix_length&quot;: 2 &#125; &#125; &#125;&#125; Java代码 123456789101112131415161718public void findByFuzzy() throws IOException &#123; // 创建request对象 SearchRequest request = new SearchRequest(index); request.types(type); // 指定查询条件 SearchSourceBuilder builder = new SearchSourceBuilder(); //-------------------------------------------------- builder.query(QueryBuilders.fuzzyQuery(&quot;corpName&quot;,&quot;腾讯客堂&quot;).prefixLength(2)); //------------------------------------------------------ request.source(builder); // 执行 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 输出结果 for (SearchHit hit : response.getHits().getHits()) &#123; System.out.println(hit.getSourceAsMap()); &#125;&#125; 通配符 wildcard查询 通配查询，同mysql中的like 是一样的，可以在查询时，在字符串中指定通配符*和占位符？ kibana 12345678910111213141516171819202122POST /sms-logs-index/sms-logs-type/_search&#123; &quot;query&quot;: &#123; &quot;wildcard&quot;: &#123; &quot;corpName&quot;: &#123; &quot;value&quot;: &quot;海尔*&quot; &#125; &#125; &#125;&#125;#wildcard 查询POST /sms-logs-index/sms-logs-type/_search&#123; &quot;query&quot;: &#123; &quot;wildcard&quot;: &#123; &quot;corpName&quot;: &#123; &quot;value&quot;: &quot;海尔??&quot; &#125; &#125; &#125;&#125; Java 代码 1234567891011121314151617public void findByWildCard() throws IOException &#123; // 创建request对象 SearchRequest request = new SearchRequest(index); request.types(type); // 指定查询条件 SearchSourceBuilder builder = new SearchSourceBuilder(); //-------------------------------------------------- builder.query(QueryBuilders.wildcardQuery(&quot;corpName&quot;,&quot;海尔*&quot;)); //------------------------------------------------------ request.source(builder); // 执行 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 输出结果 for (SearchHit hit : response.getHits().getHits()) &#123; System.out.println(hit.getSourceAsMap()); &#125; &#125; rang 查询 范围查询，只针对数值类型，对一个field进行大于等于或小于等于范围指定 kibana 1234567891011POST /sms-logs-index/sms-logs-type/_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;fee&quot;: &#123; &quot;gte&quot;: 10, &quot;lte&quot;: 12 &#125; &#125; &#125;&#125; Java 代码 1234567891011121314151617public void findByRang() throws IOException &#123; // 创建request对象 SearchRequest request = new SearchRequest(index); request.types(type); // 指定查询条件 SearchSourceBuilder builder = new SearchSourceBuilder(); //-------------------------------------------------- builder.query(QueryBuilders.rangeQuery(&quot;fee&quot;).gt(10).lte(30)); //------------------------------------------------------ request.source(builder); // 执行 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 输出结果 for (SearchHit hit : response.getHits().getHits()) &#123; System.out.println(hit.getSourceAsMap()); &#125; &#125; regexp 查询 kibana 123456789#regexp 查询POST /sms-logs-index/sms-logs-type/_search&#123; &quot;query&quot;: &#123; &quot;regexp&quot;: &#123; &quot;mobile&quot;: &quot;138[0-9]&#123;8&#125;&quot; &#125; &#125;&#125; Java代码 1234567891011121314151617public void findByRegexp() throws IOException &#123; // 创建request对象 SearchRequest request = new SearchRequest(index); request.types(type); // 指定查询条件 SearchSourceBuilder builder = new SearchSourceBuilder(); //-------------------------------------------------- builder.query(QueryBuilders.regexpQuery(&quot;mobile&quot;,&quot;138[0-9]&#123;8&#125;&quot;)); //------------------------------------------------------ request.source(builder); // 执行 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 输出结果 for (SearchHit hit : response.getHits().getHits()) &#123; System.out.println(hit.getSourceAsMap()); &#125;&#125; 以上介绍的prefix wildcard fuzzy 和regexp 查询效率比较低 ,在要求效率比较高时，避免使用！ scroll分页 存在的意义：es中对from+size 形式的分页有一个限制，就是在from+size&gt;10000时候，效率极具下降。这个时候基于游标形式的分页就产生了。 分页原理 from +size 原理 将查询关键字分词 关键字到分词库查询，得到多个文档id 去各个分片里fetch数据（耗时较长） 根据score排序（耗时较长） 根据from和size截取数据 返回结果 scroll 原理 将查询关键字分词 关键字到分词库查询，得到多个文档id 文档id放入到一个上下文中， 设置缓存时间 根据【scrollId】+ 设置的size（没有设置默认为10），从缓存里拿到ids，到es中检索数据，取完文档后，删除其缓存的文档ids 返回数据，并返回scrollId 重复上述2步骤 scroll 实践 kibana 12345678910111213141516171819202122232425#scroll 查询,返回第一页数据，并将文档id信息存放在ES上下文中，并指定生存时间POST /sms-logs-index/sms-logs-type/_search?scroll=1m&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;size&quot;: 2, &quot;sort&quot;: [ &#123; &quot;fee&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125;#根据scroll 查询下一页数据POST _search/scroll&#123; &quot;scroll_id&quot;:&quot;DnF1ZXJ5VGhlbkZldGNoAwAAAAAAABbqFk04VlZ1cjlUU2t1eHpsQWNRY1YwWWcAAAAAAAAW7BZNOFZWdXI5VFNrdXh6bEFjUWNWMFlnAAAAAAAAFusWTThWVnVyOVRTa3V4emxBY1FjVjBZZw==&quot;, &quot;scroll&quot;:&quot;1m&quot;&#125;#删除scroll上下文中的数据DELETE _search/scroll/DnF1ZXJ5VGhlbkZldGNoAwAAAAAAABchFk04VlZ1cjlUU2t1eHpsQWNRY1YwWWcAAAAAAAAXIBZNOFZWdXI5VFNrdXh6bEFjUWNWMFlnAAAAAAAAFx8WTThWVnVyOVRTa3V4emxBY1FjVjBZZw== Java代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class ScrollSearch &#123; ObjectMapper mapper = new ObjectMapper(); RestHighLevelClient client = EsClient.getClient(); String index = &quot;sms-logs-index&quot;; String type=&quot;sms-logs-type&quot;; @Test public void scrollSearch() throws IOException &#123; // 1.创建request SearchRequest searchRequest = new SearchRequest(index); searchRequest.types(type); // 2.指定scroll信息,过期时间 searchRequest.scroll(TimeValue.timeValueMinutes(1L)); // 3.指定查询条件 SearchSourceBuilder builder = new SearchSourceBuilder(); builder.size(4); builder.sort(&quot;fee&quot;, SortOrder.DESC); searchRequest.source(builder); // 4.获取返回结果scrollId,获取source SearchResponse response = client.search(searchRequest, RequestOptions.DEFAULT); String scrollId = response.getScrollId(); System.out.println(&quot;-------------首页数据---------------------&quot;); for (SearchHit hit : response.getHits().getHits()) &#123; System.out.println(hit.getSourceAsMap()); &#125; while (true)&#123; // 5.创建scroll request SearchScrollRequest scrollRequest = new SearchScrollRequest(scrollId); // 6.指定scroll 有效时间 scrollRequest.scroll(TimeValue.timeValueMinutes(1L)); // 7.执行查询，返回查询结果 SearchResponse scroll = client.scroll(scrollRequest, RequestOptions.DEFAULT); // 8.判断是否查询到数据，查询到输出 SearchHit[] searchHits = scroll.getHits().getHits(); if(searchHits!=null &amp;&amp; searchHits.length &gt;0)&#123; System.out.println(&quot;-------------下一页数据---------------------&quot;); for (SearchHit hit : searchHits) &#123; System.out.println(hit.getSourceAsMap()); &#125; &#125;else&#123; // 9.没有数据，结束 System.out.println(&quot;-------------结束---------------------&quot;); break; &#125; &#125; // 10.创建 clearScrollRequest ClearScrollRequest clearScrollRequest = new ClearScrollRequest(); // 11.指定scrollId clearScrollRequest.addScrollId(scrollId); //12.删除scroll ClearScrollResponse clearScrollResponse = client.clearScroll(clearScrollRequest, RequestOptions.DEFAULT); // 13.输出结果 System.out.println(&quot;删除scroll:&quot;+clearScrollResponse.isSucceeded()); &#125;&#125; delete-by-query kibana 12345678910POST /sms-logs-index/sms-logs-type/_delete_by_query&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;fee&quot;: &#123; &quot;lt&quot;: 20 &#125; &#125; &#125;&#125; Java 代码 1234567891011public void deleteByQuery() throws IOException &#123; // 1.创建DeleteByQueryRequest DeleteByQueryRequest request = new DeleteByQueryRequest(index); request.types(type); // 2.指定条件 request.setQuery(QueryBuilders.rangeQuery(&quot;fee&quot;).lt(20)); // 3.执行 BulkByScrollResponse response = client.deleteByQuery(request, RequestOptions.DEFAULT); // 4.输出返回结果 System.out.println(response.toString()); &#125; 复合查询 must：所有的条件全部匹配，类似mysql的and must_not：所有的条件都不匹配，类似mysql的!= should：所有的条件或的关系，类似mysql的or kibana 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#省是 晋城 或者北京#运营商不能是联通#smsContent 包含 三个中年人 北边POST /sms-logs-index/sms-logs-type/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;should&quot;: [ &#123; &quot;term&quot;: &#123; &quot;province&quot;: &#123; &quot;value&quot;: &quot;晋城&quot; &#125; &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;province&quot;: &#123; &quot;value&quot;: &quot;北京&quot; &#125; &#125; &#125; ], &quot;must_not&quot;: [ &#123; &quot;term&quot;: &#123; &quot;operatorId&quot;: &#123; &quot;value&quot;: &quot;2&quot; &#125; &#125; &#125; ], &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;smsContent&quot;: &quot;三个中年人&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;smsContent&quot;: &quot;北边&quot; &#125; &#125; ] &#125; &#125;&#125; Java代码 12345678910111213141516171819202122232425262728public void boolSearch() throws IOException &#123; // 1.创建 searchRequest SearchRequest request = new SearchRequest(index); request.types(type); // 2.指定查询条件 SearchSourceBuilder builder = new SearchSourceBuilder(); BoolQueryBuilder boolQueryBuilder = new BoolQueryBuilder(); // #省是 晋城 或者北京 boolQueryBuilder.should(QueryBuilders.termQuery(&quot;province&quot;,&quot;北京&quot;)); boolQueryBuilder.should(QueryBuilders.termQuery(&quot;province&quot;,&quot;晋城&quot;)); //# 运营商不能是联通 boolQueryBuilder.mustNot(QueryBuilders.termQuery(&quot;operatorId&quot;,2)); //#smsContent 包含 战士 和的 boolQueryBuilder.must(QueryBuilders.matchQuery(&quot;smsContent&quot;,&quot;战士&quot;)); boolQueryBuilder.must(QueryBuilders.matchQuery(&quot;smsContent&quot;,&quot;的&quot;)); builder.query(boolQueryBuilder); request.source(builder); // 3.执行查询 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.输出结果 for (SearchHit hit : response.getHits().getHits()) &#123; System.out.println(hit.getSourceAsMap()); &#125; &#125; boosting 查询 12345678boosting 查询可以帮助我们去影响查询后的score positive:只有匹配上positive 查询的内容，才会被放到返回的结果集中 negative: 如果匹配上了positive 也匹配上了negative, 就可以 降低这样的文档score. negative_boost:指定系数,必须小于1 0.5 关于查询时，分数时如何计算的： 搜索的关键字再文档中出现的频次越高，分数越高 指定的文档内容越短，分数越高。 我们再搜索时，指定的关键字也会被分词，这个被分词的内容，被分词库匹配的个数越多，分数就越高。 kibana 123456789101112131415161718POST /sms-logs-index/sms-logs-type/_search&#123; &quot;query&quot;: &#123; &quot;boosting&quot;: &#123; &quot;positive&quot;: &#123; &quot;match&quot;: &#123; &quot;smsContent&quot;: &quot;战士&quot; &#125; &#125;, &quot;negative&quot;: &#123; &quot;match&quot;: &#123; &quot;smsContent&quot;: &quot;团队&quot; &#125; &#125;, &quot;negative_boost&quot;: 0.2 &#125; &#125;&#125; Java 代码 12345678910111213141516171819public void boostSearch() throws IOException &#123; // 1.创建 searchRequest SearchRequest request = new SearchRequest(index); request.types(type); // 2.指定查询条件 SearchSourceBuilder builder = new SearchSourceBuilder(); BoostingQueryBuilder boost = QueryBuilders.boostingQuery( QueryBuilders.matchQuery(&quot;smsContent&quot;, &quot;战士&quot;), QueryBuilders.matchQuery(&quot;smsContent&quot;, &quot;团队&quot;) ).negativeBoost(0.2f); builder.query(boost); request.source(builder); // 3.执行查询 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.输出结果 for (SearchHit hit : response.getHits().getHits()) &#123; System.out.println(hit.getSourceAsMap()); &#125; &#125; filter 查询 query 查询：根据你的查询条件，去计算文档的匹配度得到一个分数，并根据分数排序，不会做缓存的。 filter 查询：根据查询条件去查询文档，不去计算分数，而且filter会对经常被过滤的数据进行缓存。 kibana 123456789101112131415161718192021POST /sms-logs-index/sms-logs-type/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: [ &#123; &quot;term&quot;: &#123; &quot;corpName&quot;: &quot;海尔智家公司&quot; &#125; &#125;, &#123; &quot;range&quot;:&#123; &quot;fee&quot;:&#123; &quot;lte&quot;:50 &#125; &#125; &#125; ] &#125; &#125;&#125; Java代码 12345678910111213141516171819public void filter() throws IOException &#123; // 1.searchRequest SearchRequest searchRequest = new SearchRequest(index); searchRequest.types(type); // 2.指定查询条件 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); BoolQueryBuilder boolBuilder = QueryBuilders.boolQuery(); boolBuilder.filter(QueryBuilders.termQuery(&quot;corpName&quot;,&quot;海尔智家公司&quot;)); boolBuilder.filter(QueryBuilders.rangeQuery(&quot;fee&quot;).gt(20)); sourceBuilder.query(boolBuilder); searchRequest.source(sourceBuilder); // 3.执行 SearchResponse response = client.search(searchRequest, RequestOptions.DEFAULT); // 4. 输出结果 for (SearchHit hit : response.getHits().getHits()) &#123; System.out.println(hit.getSourceAsMap()); System.out.println(hit.getId()+&quot;的分数是：&quot;+hit.getScore()); &#125;&#125; 高亮查询 123456高亮查询就是用户输入的关键字，以一定特殊样式展示给用户，让用户知道为什么这个结果被检索出来高亮展示的数据，本身就是文档中的一个field,单独将field以highlight的形式返回给用户ES提供了一个highlight 属性，他和query 同级别。 frament_size: 指定高亮数据展示多少个字符回来 pre_tags:指定前缀标签&lt;front color&#x3D;&quot;red&quot;&gt; post_tags:指定后缀标签 &lt;&#x2F;font&gt; kibana 12345678910111213141516POST /sms-logs-index/sms-logs-type/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;smsContent&quot;: &quot;团队&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;smsContent&quot;:&#123;&#125; &#125;, &quot;pre_tags&quot;:&quot;&lt;font color=&#x27;red&#x27;&gt;&quot;, &quot;post_tags&quot;:&quot;&lt;/font&gt;&quot;, &quot;fragment_size&quot;:10 &#125;&#125; Java 代码 1234567891011121314151617181920public void highLightQuery() throws IOException &#123; // 1.创建request SearchRequest request = new SearchRequest(index); request.types(type); // 2.指定查询条件，指定高亮 SearchSourceBuilder builder = new SearchSourceBuilder(); builder.query(QueryBuilders.matchQuery(&quot;smsContent&quot;,&quot;团队&quot;)); HighlightBuilder highlightBuilder = new HighlightBuilder(); highlightBuilder.field(&quot;smsContent&quot;,10) .preTags(&quot;&lt;font colr=&#x27;red&#x27;&gt;&quot;) .postTags(&quot;&lt;/font&gt;&quot;); builder.highlighter(highlightBuilder); request.source(builder); // 3.执行 SearchResponse response = client.search(request, RequestOptions.DEFAULT); //4. 输出结果 for (SearchHit hit : response.getHits().getHits()) &#123; System.out.println(hit.getHighlightFields().get(&quot;smsContent&quot;)); &#125; &#125; 聚合查询 1234567891011# 语法POST /index/type/_search&#123; &quot;aggs&quot;:&#123; &quot;(自定义名字)agg&quot;:&#123; &quot;（函数）agg_type&quot;:&#123; &quot;属性&quot;：&quot;值&quot; &#125; &#125; &#125;&#125; 去重统计 kibana 12345678910POST /sms-logs-index/sms-logs-type/_search&#123; &quot;aggs&quot;: &#123; &quot;provinceAgg&quot;: &#123; &quot;cardinality&quot;: &#123; &quot;field&quot;: &quot;province&quot; &#125; &#125; &#125;&#125; Java代码 123456789101112131415public void aggCardinalityC() throws IOException &#123; // 1.创建request SearchRequest request = new SearchRequest(index); request.types(type); // 2. 指定使用聚合查询方式 SearchSourceBuilder builder = new SearchSourceBuilder(); builder.aggregation(AggregationBuilders.cardinality(&quot;provinceAgg&quot;).field(&quot;province&quot;)); request.source(builder); // 3.执行查询 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.输出返回结果 Cardinality agg = response.getAggregations().get(&quot;provinceAgg&quot;); System.out.println(agg.getValue());&#125; range 区间聚合 数值范围 kibana 1234567891011121314151617181920212223// fee &lt;30,30-60,&gt;60 的文档个数POST /sms-logs-index/sms-logs-type/_search&#123; &quot;aggs&quot;: &#123; &quot;agg&quot;: &#123; &quot;range&quot;: &#123; &quot;field&quot;: &quot;fee&quot;, &quot;ranges&quot;: [ &#123; &quot;to&quot;: 30 &#125;, &#123; &quot;from&quot;: 30, &quot;to&quot;: 60 &#125;, &#123; &quot;from&quot;: 60 &#125; ] &#125; &#125; &#125;&#125; Java 代码 1234567891011121314151617181920212223242526public void aggRang() throws IOException &#123; // 1.创建request SearchRequest request = new SearchRequest(index); request.types(type); // 2. 指定使用聚合查询方式 SearchSourceBuilder builder = new SearchSourceBuilder(); builder.aggregation(AggregationBuilders.range(&quot;agg&quot;).field(&quot;fee&quot;) .addUnboundedTo(30) .addRange(30,60) .addUnboundedFrom(60)); request.source(builder); // 3.执行查询 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.输出返回结果 Range agg = response.getAggregations().get(&quot;agg&quot;); for (Range.Bucket bucket : agg.getBuckets()) &#123; String key = bucket.getKeyAsString(); Object from = bucket.getFrom(); Object to = bucket.getTo(); long docCount = bucket.getDocCount(); System.out.println(String.format(&quot;key: %s ,from: %s ,to: %s ,docCount: %s&quot;,key,from,to,docCount)); &#125; &#125; 时间范围 kibana 123456789101112131415161718POST /sms-logs-index/sms-logs-type/_search&#123; &quot;aggs&quot;: &#123; &quot;agg&quot;: &#123; &quot;date_range&quot;: &#123; &quot;field&quot;: &quot;sendDate&quot;, &quot;format&quot;: &quot;yyyy-MM-dd&quot;, &quot;ranges&quot;: [ &#123; &quot;to&quot;: &quot;2021-12-12&quot; &#125;,&#123; &quot;from&quot;: &quot;2021-01-01&quot; &#125; ] &#125; &#125; &#125;&#125; ip范围 kibana 123456789101112131415161718POST /sms-logs-index/sms-logs-type/_search&#123; &quot;aggs&quot;: &#123; &quot;agg&quot;: &#123; &quot;ip_range&quot;: &#123; &quot;field&quot;: &quot;ipAddr&quot;, &quot;ranges&quot;: [ &#123; &quot;to&quot;: &quot;127.0.0.8&quot; &#125;, &#123; &quot;from&quot;: &quot;127.0.0.8&quot; &#125; ] &#125; &#125; &#125;&#125; 统计集合 返回值包含：最大值、最小值、平均值、综合、方差，标准差，标准差等 kibana 12345678910POST /sms-logs-index/sms-logs-type/_search&#123; &quot;aggs&quot;: &#123; &quot;agg&quot;: &#123; &quot;extended_stats&quot;: &#123; &quot;field&quot;: &quot;fee&quot; &#125; &#125; &#125;&#125; Java 代码 1234567891011121314public void aggExtendedStats() throws IOException &#123; // 1.创建request SearchRequest request = new SearchRequest(index); request.types(type); // 2. 指定使用聚合查询方式 SearchSourceBuilder builder = new SearchSourceBuilder(); builder.aggregation(AggregationBuilders.extendedStats(&quot;agg&quot;).field(&quot;fee&quot;)); request.source(builder); // 3.执行查询 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.输出返回结果 ExtendedStats extendedStats = response.getAggregations().get(&quot;agg&quot;); System.out.println(&quot;最大值：&quot;+extendedStats.getMaxAsString()+&quot;,最小值：&quot;+extendedStats.getMinAsString()); &#125; 经纬度查询 数据准备 新建索引 12345678910111213141516171819PUT /map&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 5, &quot;number_of_replicas&quot;: 1 &#125;, &quot;mappings&quot;: &#123; &quot;map&quot;:&#123; &quot;properties&quot;:&#123; &quot;name&quot;:&#123; &quot;type&quot;:&quot;text&quot; &#125;, &quot;location&quot;:&#123; &quot;type&quot;:&quot;geo_point&quot; &#125; &#125; &#125; &#125;&#125; 添加数据 1234567891011121314151617181920212223242526PUT /map/map/1&#123; &quot;name&quot;:&quot;天安门&quot;, &quot;location&quot;:&#123; &quot;lon&quot;: 116.403694, &quot;lat&quot;:39.914492 &#125;&#125;PUT /map/map/2&#123; &quot;name&quot;:&quot;百望山&quot;, &quot;location&quot;:&#123; &quot;lon&quot;: 116.26284, &quot;lat&quot;:40.036576 &#125;&#125;PUT /map/map/3&#123; &quot;name&quot;:&quot;北京动物园&quot;, &quot;location&quot;:&#123; &quot;lon&quot;: 116.347352, &quot;lat&quot;:39.947468 &#125;&#125; 查询 geo_distance :直线距离检索方式 12345678910111213141516POST /map/map/_search&#123; &quot;query&quot;: &#123; &quot;geo_distance&quot;:&#123; #确定一个点 &quot;location&quot;:&#123; &quot;lon&quot;:116.434739, &quot;lat&quot;:39.909843 &#125;, #确定半径 &quot;distance&quot;:20000, #指定形状为圆形 &quot;distance_type&quot;:&quot;arc&quot; &#125; &#125;&#125; geo_bounding_box: 以2个点确定一个矩形，获取再矩形内的数据 1234567891011121314151617POST /map/map/_search&#123; &quot;query&quot;:&#123; &quot;geo_bounding_box&quot;:&#123; &quot;location&quot;:&#123; &quot;top_left&quot;:&#123; &quot;lon&quot;:116.327805, &quot;lat&quot;:39.95499 &#125;, &quot;bottom_right&quot;:&#123; &quot;lon&quot;: 116.363162, &quot;lat&quot;:39.938395 &#125; &#125; &#125; &#125;&#125; geo_polygon:以多个点，确定一个多边形，获取多边形的全部数据 123456789101112131415161718192021222324POST /map/map/_search&#123; &quot;query&quot;:&#123; &quot;geo_polygon&quot;:&#123; &quot;location&quot;:&#123; # 指定多个点确定 位置 &quot;points&quot;:[ &#123; &quot;lon&quot;:116.220296, &quot;lat&quot;:40.075013 &#125;, &#123; &quot;lon&quot;:116.346777, &quot;lat&quot;:40.044751 &#125;, &#123; &quot;lon&quot;:116.236106, &quot;lat&quot;:39.981533 &#125; ] &#125; &#125; &#125;&#125; Java 代码实现geo_polygon 1234567891011121314151617181920212223242526 public class GeoDemo &#123; RestHighLevelClient client = EsClient.getClient(); String index = &quot;map&quot;; String type=&quot;map&quot;; @Test public void GeoPolygon() throws IOException &#123; // 1.创建searchRequest SearchRequest request = new SearchRequest(index); request.types(type); // 2.指定 检索方式 SearchSourceBuilder builder = new SearchSourceBuilder(); List&lt;GeoPoint&gt; points = new ArrayList&lt;&gt;(); points.add(new GeoPoint(40.075013,116.220296)); points.add(new GeoPoint(40.044751,116.346777)); points.add(new GeoPoint(39.981533,116.236106)); builder.query(QueryBuilders.geoPolygonQuery(&quot;location&quot;,points)); request.source(builder); // 3.执行 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.输出结果 for (SearchHit hit : response.getHits().getHits()) &#123; System.out.println(hit.getSourceAsMap()); &#125; &#125;&#125; 父子关系 ES里面有2种查询可以实现类似关系数据库里面的关联查询。nested嵌套、父子查询。 作家文章和评论 nested 嵌套查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156// 新建类型和映射PUT /prose&#123; &quot;mappings&quot;: &#123; &quot;article&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;author&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot; &#125;, &quot;created&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;comments&quot;: &#123; &quot;type&quot;: &quot;nested&quot;, &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;created&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125;// 添加文档POST /prose/article&#123; &quot;title&quot;: &quot;少年闰土&quot;, &quot;author&quot;: &quot;鲁迅&quot;, &quot;content&quot;: &quot;深蓝的天空中挂着一轮金黄的圆月,下面是海边的沙地,都种着一望无际的碧绿的西瓜。。。&quot;, &quot;created&quot;: -1546329600, &quot;comments&quot;: [ &#123; &quot;name&quot;: &quot;张三&quot;, &quot;content&quot;: &quot;􏳏真好&quot;, &quot;created&quot;: 1577808000 &#125;, &#123; &quot;name&quot;: &quot;里斯&quot;, &quot;content&quot;: &quot;太棒&quot;, &quot;created&quot;: 1577808000 &#125; ]&#125;&#123; &quot;title&quot;: &quot;父亲的背影&quot;, &quot;author&quot;: &quot;朱自清&quot;, &quot;content&quot;: &quot;深蓝的天空中挂着一轮金黄的圆月,下面是海边的沙地,都种着一望无际的碧绿的西瓜。。。&quot;, &quot;created&quot;: -1546329600, &quot;comments&quot;: [ &#123; &quot;name&quot;: &quot;张三&quot;, &quot;content&quot;: &quot;􏳏真好&quot;, &quot;created&quot;: 1577808000 &#125;, &#123; &quot;name&quot;: &quot;里斯&quot;, &quot;content&quot;: &quot;太棒&quot;, &quot;created&quot;: 1577808000 &#125; ]&#125;&#123; &quot;title&quot;: &quot;社戏&quot;, &quot;author&quot;: &quot;鲁迅&quot;, &quot;content&quot;: &quot;我们鲁镇的习惯，本来是凡有...&quot;, &quot;created&quot;: -1420099200, &quot;comments&quot;: [ &#123; &quot;name&quot;: &quot;王武&quot;, &quot;content&quot;: &quot;真好啊&quot;, &quot;created&quot;: 1577808000 &#125;, &#123; &quot;name&quot;: &quot;王六&quot;, &quot;content&quot;: &quot;太棒啊&quot;, &quot;created&quot;: 1577808000 &#125; ]&#125;// 鲁迅文章评论POST /prose/article/_search&#123; &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;must&quot;:[ &#123; &quot;match&quot;:&#123; &quot;author.keyword&quot;:&quot;鲁迅&quot; &#125; &#125;, &#123; &quot;nested&quot;:&#123; &quot;path&quot;:&quot;comments&quot;, &quot;query&quot;:&#123; &quot;bool&quot;:&#123; &quot;must&quot;:[ &#123; &quot;match&quot;:&#123; &quot;comments.content&quot;:&quot;真好&quot; &#125; &#125; ] &#125; &#125; &#125; &#125; ] &#125; &#125;&#125; 父子文档 􏱍ES6.x 里面一个索引index只能有一个类型type，es定义了一种数据类型：join 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131// 新建类型和映射POST /prose2&#123; &quot;mappings&quot;: &#123; &quot;article2&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;author&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot; &#125;, &quot;created&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;comments&quot;: &#123; &quot;type&quot;: &quot;join&quot;, &quot;relations&quot;: &#123; &quot;article&quot;: &quot;comment&quot; &#125; &#125; &#125; &#125; &#125;&#125;// 插入父文档POST /prose2/article2/1&#123; &quot;title&quot;: &quot;从百草园到三味书屋&quot;, &quot;author&quot;: &quot;鲁迅&quot;, &quot;content&quot;: &quot;我家的后面有一个很大的园，相传叫作百草园。现在是早已并1屋子一起卖给朱文公的子孙2了，连那最末次的相见也已经隔了七八年，其中似乎确凿3只有一些野草；但那时却是我的乐园。。。&quot;, &quot;created&quot;: -1546329600, &quot;comments&quot;: &quot;article&quot;&#125;POST /prose2/article2/2&#123; &quot;title&quot;: &quot;祝福&quot;, &quot;author&quot;: &quot;鲁迅&quot;, &quot;content&quot;: &quot;旧历的年底毕竟最像年底，村镇上不必说，就在天空中也显出将到新年的气象来。灰白色的沉重的晚云中间时时发出闪光，接着一声钝响，是送灶的爆竹;近处燃放的可就更强烈了，震耳的大音还没有息，空气里已经散满了幽微的火药香。。。&quot;, &quot;created&quot;: -1546329600, &quot;comments&quot;: &quot;article&quot;&#125;POST /prose2/article2/3&#123; &quot;title&quot;: &quot;春&quot;, &quot;author&quot;: &quot;朱自清&quot;, &quot;content&quot;: &quot;盼望着，盼望着，东风来了，bai春天脚步近了。一切都像刚睡醒的样子，欣欣然张开了眼。山朗润起来了，水涨起来了，太阳的脸红起来了。。。。&quot;, &quot;created&quot;: -1546329600, &quot;comments&quot;: &quot;article&quot;&#125;// 插入子文档POST /prose2/article2/4?routing=1&#123; &quot;name&quot;: &quot;张三&quot;, &quot;content&quot;: &quot;想起了童年&quot;, &quot;created&quot;: 1562141689000, &quot;comments&quot;: &#123; &quot;name&quot;: &quot;comment&quot;, &quot;parent&quot;: 1 &#125;&#125;POST /prose2/article/5?routing=2&#123; &quot;name&quot;: &quot;李四&quot;, &quot;content&quot;: &quot;感人&quot;, &quot;created&quot;: 1562141689000, &quot;comments&quot;: &#123; &quot;name&quot;: &quot;comment&quot;, &quot;parent&quot;: 2 &#125;&#125;POST /prose2/article2/6?routing=3&#123; &quot;name&quot;: &quot;王武&quot;, &quot;content&quot;: &quot;宁静的感觉&quot;, &quot;created&quot;: 1562141689000, &quot;comments&quot;: &#123; &quot;name&quot;: &quot;comment&quot;, &quot;parent&quot;: 3 &#125;&#125;// 查询评论朱自清的文章的文档（父查子）POST /prose2/article2/_search&#123; &quot;query&quot;: &#123; &quot;has_parent&quot;: &#123; &quot;parent_type&quot;: &quot;article&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;author.keyword&quot;: &quot;朱自清&quot; &#125; &#125; &#125; &#125;&#125;// 评论包含“童年”的评论（子查父）POST /prose2/article2/_search&#123; &quot;query&quot;: &#123; &quot;has_child&quot;: &#123; &quot;type&quot;: &quot;comment&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;童年&quot; &#125; &#125; &#125; &#125;&#125; 总结：nested不管怎么查询都是返回一整个文档，也说明了它的对象是一个文档。父子查询可以返回父/子文档，从插入和查询返回都可以看出，它的对象为2个文档，另外父子文档，在子文档插入的时候需要指定routing=父id，保证父子文档在一个分片上！nested对象任何修改都会是文档重新索引，而父子文档是分别独立的不影响。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}]},{"title":"Elasticsearch客户端","slug":"Elasticsearch客户端","date":"2020-09-12T06:56:53.000Z","updated":"2022-11-22T01:27:53.356Z","comments":true,"path":"2020/09/12/Elasticsearch客户端/","link":"","permalink":"http://yoursite.com/2020/09/12/Elasticsearch%E5%AE%A2%E6%88%B7%E7%AB%AF/","excerpt":"","text":"maven 依赖 123456789101112131415161718192021222324 &lt;!--elasticsearch--&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;6.8.10&lt;/version&gt; &lt;/dependency&gt;&lt;!--elasticsearch 高级API--&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;6.8.10&lt;/version&gt; &lt;/dependency&gt; &lt;!--junit--&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;!--lombok--&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.22&lt;/version&gt; &lt;/dependency&gt; 客户端 12345678910111213141516171819202122package com.utils;import org.apache.http.HttpHost;import org.elasticsearch.client.RestClient;import org.elasticsearch.client.RestClientBuilder;import org.elasticsearch.client.RestHighLevelClient;public class EsClient &#123; public static RestHighLevelClient getClient()&#123; // 创建 HttpHost HttpHost httpHost = new HttpHost(&quot;127.0.0.1&quot;,9200); // 创建 RestClientBuilder RestClientBuilder builder = RestClient.builder(httpHost); // 创建 RestHighLevelClient RestHighLevelClient client = new RestHighLevelClient(builder); return client; &#125;&#125; 索引操作 新建索引 1234567891011121314151617181920212223242526272829303132333435363738public class CreateInx&#123; RestHighLevelClient client = EsClient.getClient(); String index = &quot;person&quot;; String type=&quot;man&quot;; @Test public void createIndx() throws Exception&#123; // 1.准备关于索引的setting Settings.Builder settings = Settings.builder() .put(&quot;number_of_shards&quot;, 2) .put(&quot;number_of_replicas&quot;, 1); // 2.准备关于索引的mapping XContentBuilder mappings = JsonXContent.contentBuilder() .startObject() .startObject(&quot;properties&quot;) .startObject(&quot;name&quot;) .field(&quot;type&quot;, &quot;text&quot;) .endObject() .startObject(&quot;age&quot;) .field(&quot;type&quot;, &quot;integer&quot;) .endObject() .startObject(&quot;birthday&quot;) .field(&quot;type&quot;, &quot;date&quot;) .field(&quot;format&quot;, &quot;yyyy-MM-dd&quot;) .endObject() .endObject() .endObject(); // 3.将settings和mappings 封装到到一个Request对象中 CreateIndexRequest request = new CreateIndexRequest(index) .settings(settings) .mapping(type,mappings); // 4.使用client 去连接ES CreateIndexResponse response = client.indices().create(request, RequestOptions.DEFAULT); System.out.println(&quot;response:&quot;+response.toString()); &#125;&#125; 检查索引是否存在 12345678910111213141516public class CheckIdxExit &#123; RestHighLevelClient client = EsClient.getClient(); String index = &quot;person&quot;; String type=&quot;man&quot;; @Test public void existTest() throws IOException &#123; // 1.准备request 对象 GetIndexRequest request = new GetIndexRequest(index); // 2.通过client 去 操作 boolean exists = client.indices().exists(request, RequestOptions.DEFAULT); // 3输出结果 System.out.println(exists); &#125;&#125; 删除索引 123456789101112131415public class DeleteIdx &#123; RestHighLevelClient client = EsClient.getClient(); String index = &quot;person&quot;; String type=&quot;man&quot;; @Test public void testDelete() throws IOException &#123; // 1.获取request DeleteIndexRequest request = new DeleteIndexRequest(index); // 2.使用client 操作request AcknowledgedResponse delete = client.indices().delete(request, RequestOptions.DEFAULT); // 3.输出结果 System.out.println(delete.isAcknowledged()); &#125;&#125; 文档操作 新建文档 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class CreateDoc &#123; ObjectMapper mapper = new ObjectMapper(); RestHighLevelClient client = EsClient.getClient(); String index = &quot;person&quot;; String type=&quot;man&quot;; // 单个新建 @Test public void createDoc() throws IOException &#123; // 1.准备一个json数据 Person person = new Person(1,&quot;张三&quot;,33,new Date()); String json = mapper.writeValueAsString(person); // 2.创建一个request对象(手动指定的方式创建) IndexRequest request = new IndexRequest(index,type,person.getId().toString()); request.source(json, XContentType.JSON); // 3.使用client 操作request对象生成doc IndexResponse response = client.index(request, RequestOptions.DEFAULT); // 4.输出返回结果 System.out.println(response.getResult().toString()); &#125; // 批量新建 @Test public void bulkCreateDoc() throws Exception&#123; // 1.准备多个json 对象 Person p1 = new Person(1,&quot;张三&quot;,23,new Date()); Person p2 = new Person(2,&quot;里斯&quot;,24,new Date()); Person p3 = new Person(3,&quot;王武&quot;,24,new Date()); String json1 = mapper.writeValueAsString(p1); String json2 = mapper.writeValueAsString(p2); String json3 = mapper.writeValueAsString(p3); // 2.创建request BulkRequest bulkRequest = new BulkRequest(); bulkRequest.add(new IndexRequest(index,type,p1.getId().toString()).source(json1,XContentType.JSON)) .add(new IndexRequest(index,type,p2.getId().toString()).source(json2,XContentType.JSON)) .add(new IndexRequest(index,type,p3.getId().toString()).source(json3,XContentType.JSON)); // 3.client 执行 BulkResponse responses = client.bulk(bulkRequest, RequestOptions.DEFAULT); // 4.输出结果 System.out.println(responses.getItems().toString()); &#125; &#125; 修改文档 1234567891011121314151617181920public class UpdateDoc &#123; ObjectMapper mapper = new ObjectMapper(); RestHighLevelClient client = EsClient.getClient(); String index = &quot;person&quot;; String type=&quot;man&quot;; @Test public void updateDocTest() throws Exception&#123; // 1.创建要跟新的Map Map&lt;String,Object&gt; doc = new HashMap&lt;&gt;(); doc.put(&quot;name&quot;,&quot;张三三&quot;); // 2.创建request, 将doc 封装进去 UpdateRequest request = new UpdateRequest(index,type,&quot;1&quot;); request.doc(doc); // 3. client 去操作 request UpdateResponse response = client.update(request, RequestOptions.DEFAULT); // 4.输出 更新结果 System.out.println(response.getResult()); &#125;&#125; 删除文档 12345678910111213141516171819202122232425262728293031public class DeleteDoc &#123; ObjectMapper mapper = new ObjectMapper(); RestHighLevelClient client = EsClient.getClient(); String index = &quot;person&quot;; String type=&quot;man&quot;; // 单个删除 @Test public void deleteDocTest() throws Exception&#123; // 1.封装删除对象 DeleteRequest request = new DeleteRequest(index,type,&quot;1&quot;); // 2 client 操作 request对象 DeleteResponse response = client.delete(request, RequestOptions.DEFAULT); // 3.输出结果 System.out.println(response.getResult().toString()); &#125; // 批量删除 @Test public void bulkDelete() throws Exception &#123; // 1.创建Request 对象 BulkRequest bulkRequest = new BulkRequest(); bulkRequest.add(new DeleteRequest(index, type, &quot;1&quot;)); bulkRequest.add(new DeleteRequest(index, type, &quot;2&quot;)); bulkRequest.add(new DeleteRequest(index, type, &quot;3&quot;)); // 2.执行 BulkResponse re = client.bulk(bulkRequest, RequestOptions.DEFAULT); // 3.输出结果 System.out.println(re.toString()); &#125;&#125;","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}]},{"title":"Elasticsearch简介","slug":"Elasticsearch简介","date":"2020-09-11T07:01:34.000Z","updated":"2024-02-02T07:54:39.707Z","comments":true,"path":"2020/09/11/Elasticsearch简介/","link":"","permalink":"http://yoursite.com/2020/09/11/Elasticsearch%E7%AE%80%E4%BB%8B/","excerpt":"","text":"介绍 特点 ES 是基于Lucence编写的开源分布式搜索引擎，它有以下几个特点 lucence：底层搜索引擎 分布式：横向可扩展能力 全文检索：将一段词语进行分词，并将分出的词语统一的放在一个分词库中，再搜索时，根据关键字取分词库中检索，找到匹配的内容（倒排索引）。 概念 ES使用倒排索引来检索数数据。类比：书本（index）–&gt;小说｜散文｜杂志（type）–&gt;书里面的一行一行内容（document）–&gt; 字｜词组｜句子（filed） Index索引：es里面使用的倒排索引来检索数据，所以我们这里有个索引的概念，它有2个含义，一个可以理解为代表倒排索引，一个为“索引数据”即存储数据到es里面（会经过倒排索引这个步骤）。 type类型 5.x 版本一个index下可以有多个type 6.x 版本一个index下只可以有一个type 7.x 版本，index里面没有type document文档，实质存储的内容，类比mysql表里面的row filed 字段：文档里面字段，类比mysql 每个表里面的字段field。 查询步骤 将存放的数据以一定的方式进行分词，并将分词的内容存放到一个单独的分词库中。 当用户取查询数据时，会将用户的查询关键字进行分词，然后去分词库中匹配内容，最终得到数据的id标识 根据id标识去存放数据的位置拉去指定数据 对比solr solr 查询死数据，速度比es快。但是数据如果是改变的，solr查询速度会降低很多，ES的查询速度没有明显的改变 solr搭建集群 依赖ZK，ES本身就支持集群搭建 最开始solr 的社区很火爆，针对国内文档少，ES出现后，国内社区火爆程度上升，ES的文档非常健全 ES对云计算和大数据支持很好 ES 安装 docker-compose 安装 编写docker-compose.yml 123456789101112131415161718version: &quot;3.1&quot;services: elasticsearch: image: daocloud.io/library/elasticsearch:6.5.4 restart: always container_name: elasticsearch ports: - 9200:9200 kibana: image: daocloud.io/library/kibana:6.5.4 restart: always container_name: kibana ports: - 5601:5601 environment: - elasticsearch_url=http://127.0.0.1:9200 depends_on: - elasticsearch ik分词器安装 1234567891011121314151617cd /path/to/es/install/bin./elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.5.4/elasticsearch-analysis-ik-6.5.4.zip或者使用docker-compose exec -it elasticsearch /bin/bashcd pluginsmkdir ikexit docker cp path/to/plugin containerId:path/to/container/plugins/ikdocker-compose exec -it elasticsearch /bin/bashcd plugins/ikunzip elasticsearch-analysis-ik-6.5.4.zip exitdocker-compose restart注意：plugins需要有权限，不然启动会报错，最好安装后设置整个plugins为777chmod -R 777 plugins 官网下载 elasticsearch :https://www.elastic.co/downloads/past-releases/elasticsearch-6-5-4 kibana: https://www.elastic.co/downloads/past-releases/kibana-6-5-4 Elasticsearch 基本操作 restful语法 123456789101112GET请求： http://ip:port/index :查询索引信息 http://ip:port/index/type/doc_id :查询指定的文档信息POST请求： http://ip:port/index/type/_search: 查询文档，可以在请求体中添加json字符串来代表查询条件 http://ip:port/index/type/doc_id/_update: 修改文档，在请求体中添加json字符串来代表修改的信息PUT请求： http://ip:port/index : 创建一个索引，需要在请求体中指定索引的信息 http://ip:port/index/type/_mappings:代表创建索引时，指定索引文档存储属性的信息DELETE 请求： http://ip:port/index： 删除跑路 http://ip:port/index/type/doc_id: 删除指定的文档 Kibana 中操作 我们主要关注Kibana中的Dev Tools（操作es） 和Mangement（查看索引信息） 增删改查 新建索引不指定mapping 123456789#number_of_shards 分片#number_of_replicas 备份PUT /person&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 5, &quot;number_of_replicas&quot;: 1 &#125;&#125; 新建索引并指定mapping 制定了mapping名字novel，但是在7.x. 中不支持制定了，需要去掉novel 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697#创建索引，指定数据类型PUT /book&#123; &quot;settings&quot;: &#123; #分片数 &quot;number_of_shards&quot;: 5, #备份数 &quot;number_of_replicas&quot;: 1 &#125;, #指定数据类型 &quot;mappings&quot;: &#123; #类型 Type &quot;novel&quot;:&#123; #文档存储的field &quot;properties&quot;:&#123; #field属性名 &quot;name&quot;:&#123; #类型 &quot;type&quot;:&quot;text&quot;, #指定分词器 &quot;analyzer&quot;:&quot;ik_max_word&quot;, #指定当前的field可以被作为查询的条件 &quot;index&quot;:true, #是否需要额外存储 &quot;store&quot;:false &#125;, &quot;author&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125;, &quot;count&quot;:&#123; &quot;type&quot;:&quot;long&quot; &#125;, &quot;on-sale&quot;:&#123; &quot;type&quot;:&quot;date&quot;, #指定时间类型的格式化方式 &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&quot; &#125;, &quot;descr&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;:&quot;ik_max_word&quot; &#125; &#125; &#125; &#125;&#125;# 嵌套类型PUT /camera_message &#123; &quot;mappings&quot;:&#123; &quot;camera_message&quot;:&#123; &quot;properties&quot;:&#123; &quot;content&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;fields&quot;:&#123; &quot;keyword&quot;:&#123; &quot;type&quot;:&quot;keyword&quot;, &quot;ignore_above&quot;:256 &#125; &#125; &#125;, &quot;ctime&quot;:&#123; &quot;type&quot;:&quot;long&quot; &#125;, &quot;msgId&quot;:&#123; &quot;type&quot;:&quot;long&quot; &#125;, &quot;sourceId&quot;:&#123; &quot;type&quot;:&quot;long&quot; &#125;, &quot;type&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;fields&quot;:&#123; &quot;keyword&quot;:&#123; &quot;type&quot;:&quot;keyword&quot;, &quot;ignore_above&quot;:256 &#125; &#125; &#125;, &quot;userStatuses&quot;:&#123; &quot;type&quot;:&quot;nested&quot;, &quot;properties&quot;:&#123; &quot;deleteStatus&quot;:&#123; &quot;type&quot;:&quot;long&quot; &#125;, &quot;status&quot;:&#123; &quot;type&quot;:&quot;long&quot; &#125;, &quot;userId&quot;:&#123; &quot;type&quot;:&quot;long&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 半动态指定mapping 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667PUT /book_hf&#123; &quot;mappings&quot;: &#123; &quot;novel&quot;:&#123; &quot;properties&quot;:&#123; &quot;tenantId&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125;, &quot;wkspId&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125;, &quot;graphId&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125;, &quot;vid&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125;, &quot;tag&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125;, &quot;displayName&quot;:&#123; &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;:&quot;ik_max_word&quot;, &quot;index&quot;:true, &quot;store&quot;:false &#125; &#125; &#125; &#125;&#125;POST /book_hf/novel&#123; &quot;tenantId&quot;:&quot;11&quot;, &quot;wkspId&quot;:&quot;21&quot;, &quot;graphId&quot;:&quot;170214376448808656898&quot;, &quot;vid&quot;:&quot;12fb53f206f53f94fa6cdfaf477fb439&quot;, &quot;tag&quot;:&quot;user_17_1704331445570904065_entity&quot;, &quot;displayName&quot;:&quot;红2121楼梦&quot;, &quot;author&quot;:&quot;曹雪芹&quot;, &quot;descr&quot;:&quot;中国古代章回体长篇小说，中国古典四大名著之一，一般认为是清代作家曹雪芹所著。小说以贾、史、王、薛四大家族的兴衰为背景，以富贵公子贾宝玉为视角，以贾宝玉与林黛玉、薛宝钗的爱情婚姻悲剧为主线，描绘了一批举止见识出于须眉之上的闺阁佳人的人生百态，展现了真正的人性美和悲剧美&quot;&#125;POST /book_hf/novel&#123; &quot;tenantId&quot;:&quot;11&quot;, &quot;wkspId&quot;:&quot;21&quot;, &quot;graphId&quot;:&quot;170214376448808656898&quot;, &quot;vid&quot;:&quot;12fb53f206f53f94fa6cdfaf477fb439&quot;, &quot;tag&quot;:&quot;user_17_1704331445570904065_entity&quot;, &quot;displayName&quot;:&quot;红楼梦&quot;, &quot;author&quot;:&quot;曹雪芹&quot;, &quot;descr&quot;:&quot;中国古代章回体长篇小说，中国古典四大名著之一，一般认为是清代作家曹雪芹所著。小说以贾、史、王、薛四大家族的兴衰为背景，以富贵公子贾宝玉为视角，以贾宝玉与林黛玉、薛宝钗的爱情婚姻悲剧为主线，描绘了一批举止见识出于须眉之上的闺阁佳人的人生百态，展现了真正的人性美和悲剧美&quot;&#125;# 使用了ik分词器，则都可以出来GET /book_hf/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;displayName&quot;: &#123; &quot;query&quot;: &quot;红楼梦&quot;, &quot;fuzziness&quot;: &quot;auto&quot; &#125; &#125; &#125;&#125; 查询索引 1GET /person 更新索引 123456789#number_of_shards 分片#number_of_replicas 备份PUT /person&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 5, &quot;number_of_replicas&quot;: 1 &#125;&#125; 删除索引 1DELETE /person Filed 类型 官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/6.8/mapping-types.html 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475字符串类型: text: 一般用于全文检索，将当前field 进行分词 keyword:当前field 不会进行分词数值类型： long: Intger: short: byte: double: float: half_float: 精度比float 小一半 scaled_float:根据一个long 和scaled 来表达一个浮点型 long-345, -scaled 100 -&gt;3.45时间类型： date类型,根据时间类型指定具体的格式 PUT my_index &#123; &quot;mappings&quot;: &#123; &quot;_doc&quot;: &#123; &quot;properties&quot;: &#123; &quot;date&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&quot; &#125; &#125; &#125; &#125; &#125;布尔类型： boolean 类型，表达true 和false二进制类型： binary类型暂时支持Base64编码的字符串范围类型： integer_range： float_range： long_range：赋值时，无需指定具体的内容，只需存储一个范围即可，gte,lte,gt,lt, double_range： date_range： ip_range： PUT range_index &#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 2 &#125;, &quot;mappings&quot;: &#123; &quot;_doc&quot;: &#123; &quot;properties&quot;: &#123; &quot;expected_attendees&quot;: &#123; &quot;type&quot;: &quot;integer_range&quot; &#125;, &quot;time_frame&quot;: &#123; &quot;type&quot;: &quot;date_range&quot;, &quot;format&quot;: &quot;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis&quot; &#125; &#125; &#125; &#125; &#125; PUT range_index/_doc/1?refresh &#123; &quot;expected_attendees&quot; : &#123; &quot;gte&quot; : 10, &quot;lte&quot; : 20 &#125;, &quot;time_frame&quot; : &#123; &quot;gte&quot; : &quot;2015-10-31 12:00:00&quot;, &quot;lte&quot; : &quot;2015-11-01&quot; &#125; &#125;经纬度类型： geo_point:用来存储经纬度IP类型： ip:可以存储IPV4 和IPV6其他的数据类型，参考官网 文档操作 新建 1234567891011121314151617181920自动生成id#添加文档，自动生成idPOST /book/novel&#123; &quot;name&quot;:&quot;盘龙&quot;, &quot;author&quot;:&quot;我吃西红柿&quot;, &quot;count&quot;:100000, &quot;on-sale&quot;:&quot;2001-01-01&quot;, &quot;descr&quot;:&quot;大小的血睛鬃毛狮，力大无穷的紫睛金毛猿，毁天灭地的九头蛇皇，携带着毁灭雷电的恐怖雷龙……这里无奇不有，这是一个广博的魔幻世界。强者可以站在黑色巨龙的头顶遨游天际，恐怖的魔法可以焚烧江河，可以毁灭城池，可以夷平山岳……&quot;&#125;#添加文档,手动指定idPUT /book/novel/1&#123; &quot;name&quot;:&quot;红楼梦&quot;, &quot;author&quot;:&quot;曹雪芹&quot;, &quot;count&quot;:10000000, &quot;on-sale&quot;:&quot;2501-01-01&quot;, &quot;descr&quot;:&quot;中国古代章回体长篇小说，中国古典四大名著之一，一般认为是清代作家曹雪芹所著。小说以贾、史、王、薛四大家族的兴衰为背景，以富贵公子贾宝玉为视角，以贾宝玉与林黛玉、薛宝钗的爱情婚姻悲剧为主线，描绘了一批举止见识出于须眉之上的闺阁佳人的人生百态，展现了真正的人性美和悲剧美&quot;&#125; 修改 12345678#修改文档，使用doc 方式POST /book/novel/1/_update&#123; &quot;doc&quot;:&#123; #指定需要修改的field和对应的值 &quot;count&quot;:566666 &#125; 删除 12#根据id删除文档DELETE /book/novel/3mEnk3MBaSKoGN4T2olw","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"}]}],"categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"},{"name":"向量数据库","slug":"向量数据库","permalink":"http://yoursite.com/categories/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"图数据库","slug":"图数据库","permalink":"http://yoursite.com/categories/%E5%9B%BE%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"图计算","slug":"图计算","permalink":"http://yoursite.com/categories/%E5%9B%BE%E8%AE%A1%E7%AE%97/"},{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/categories/k8s/"},{"name":"spark","slug":"spark","permalink":"http://yoursite.com/categories/spark/"},{"name":"kettle","slug":"kettle","permalink":"http://yoursite.com/categories/kettle/"},{"name":"ETL","slug":"ETL","permalink":"http://yoursite.com/categories/ETL/"},{"name":"知识图谱","slug":"知识图谱","permalink":"http://yoursite.com/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"},{"name":"图算法","slug":"图算法","permalink":"http://yoursite.com/categories/%E5%9B%BE%E7%AE%97%E6%B3%95/"},{"name":"pdf","slug":"pdf","permalink":"http://yoursite.com/categories/pdf/"},{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/categories/hadoop/"},{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/categories/JVM/"},{"name":"分布式事务","slug":"分布式事务","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"name":"运维","slug":"运维","permalink":"http://yoursite.com/categories/%E8%BF%90%E7%BB%B4/"},{"name":"分布式存储","slug":"分布式存储","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"},{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"},{"name":"c++s","slug":"c-s","permalink":"http://yoursite.com/categories/c-s/"},{"name":"c++","slug":"c","permalink":"http://yoursite.com/categories/c/"},{"name":"javaCV","slug":"javaCV","permalink":"http://yoursite.com/categories/javaCV/"},{"name":"ffmpeg","slug":"ffmpeg","permalink":"http://yoursite.com/categories/ffmpeg/"},{"name":"linux","slug":"linux","permalink":"http://yoursite.com/categories/linux/"},{"name":"命令","slug":"命令","permalink":"http://yoursite.com/categories/%E5%91%BD%E4%BB%A4/"},{"name":"springBoot、maven","slug":"springBoot、maven","permalink":"http://yoursite.com/categories/springBoot%E3%80%81maven/"},{"name":"中间件","slug":"中间件","permalink":"http://yoursite.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"对象存储","slug":"对象存储","permalink":"http://yoursite.com/categories/%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8/"},{"name":"视频流","slug":"视频流","permalink":"http://yoursite.com/categories/%E8%A7%86%E9%A2%91%E6%B5%81/"},{"name":"操作系统","slug":"操作系统","permalink":"http://yoursite.com/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"canal","slug":"canal","permalink":"http://yoursite.com/categories/canal/"},{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"},{"name":"网络编程","slug":"网络编程","permalink":"http://yoursite.com/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"代码片段","slug":"代码片段","permalink":"http://yoursite.com/categories/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5/"},{"name":"springCloud","slug":"springCloud","permalink":"http://yoursite.com/categories/springCloud/"},{"name":"Reactor","slug":"Reactor","permalink":"http://yoursite.com/categories/Reactor/"},{"name":"工具","slug":"工具","permalink":"http://yoursite.com/categories/%E5%B7%A5%E5%85%B7/"},{"name":"反射","slug":"反射","permalink":"http://yoursite.com/categories/%E5%8F%8D%E5%B0%84/"},{"name":"json","slug":"json","permalink":"http://yoursite.com/categories/json/"},{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"架构图","slug":"架构图","permalink":"http://yoursite.com/categories/%E6%9E%B6%E6%9E%84%E5%9B%BE/"},{"name":"springBoot","slug":"springBoot","permalink":"http://yoursite.com/categories/springBoot/"},{"name":"高并发","slug":"高并发","permalink":"http://yoursite.com/categories/%E9%AB%98%E5%B9%B6%E5%8F%91/"},{"name":"单点登录","slug":"单点登录","permalink":"http://yoursite.com/categories/%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95/"},{"name":"用户中心","slug":"用户中心","permalink":"http://yoursite.com/categories/%E7%94%A8%E6%88%B7%E4%B8%AD%E5%BF%83/"},{"name":"索引","slug":"索引","permalink":"http://yoursite.com/categories/%E7%B4%A2%E5%BC%95/"},{"name":"命令行","slug":"命令行","permalink":"http://yoursite.com/categories/%E5%91%BD%E4%BB%A4%E8%A1%8C/"},{"name":"多线程/并发","slug":"多线程-并发","permalink":"http://yoursite.com/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%B9%B6%E5%8F%91/"},{"name":"阅读","slug":"阅读","permalink":"http://yoursite.com/categories/%E9%98%85%E8%AF%BB/"}],"tags":[{"name":"python虚拟环境","slug":"python虚拟环境","permalink":"http://yoursite.com/tags/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"},{"name":"weaviate","slug":"weaviate","permalink":"http://yoursite.com/tags/weaviate/"},{"name":"nebule","slug":"nebule","permalink":"http://yoursite.com/tags/nebule/"},{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"},{"name":"minikube","slug":"minikube","permalink":"http://yoursite.com/tags/minikube/"},{"name":"spark性能优化","slug":"spark性能优化","permalink":"http://yoursite.com/tags/spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"},{"name":"插件","slug":"插件","permalink":"http://yoursite.com/tags/%E6%8F%92%E4%BB%B6/"},{"name":"ETL","slug":"ETL","permalink":"http://yoursite.com/tags/ETL/"},{"name":"推理","slug":"推理","permalink":"http://yoursite.com/tags/%E6%8E%A8%E7%90%86/"},{"name":"Jaccard","slug":"Jaccard","permalink":"http://yoursite.com/tags/Jaccard/"},{"name":"添加类目","slug":"添加类目","permalink":"http://yoursite.com/tags/%E6%B7%BB%E5%8A%A0%E7%B1%BB%E7%9B%AE/"},{"name":"neo4j","slug":"neo4j","permalink":"http://yoursite.com/tags/neo4j/"},{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"访问者","slug":"访问者","permalink":"http://yoursite.com/tags/%E8%AE%BF%E9%97%AE%E8%80%85/"},{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"},{"name":"分布式事务","slug":"分布式事务","permalink":"http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"name":"亲和性","slug":"亲和性","permalink":"http://yoursite.com/tags/%E4%BA%B2%E5%92%8C%E6%80%A7/"},{"name":"kubeapps","slug":"kubeapps","permalink":"http://yoursite.com/tags/kubeapps/"},{"name":"Prometheus","slug":"Prometheus","permalink":"http://yoursite.com/tags/Prometheus/"},{"name":"持久化卷","slug":"持久化卷","permalink":"http://yoursite.com/tags/%E6%8C%81%E4%B9%85%E5%8C%96%E5%8D%B7/"},{"name":"分布式存储","slug":"分布式存储","permalink":"http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8/"},{"name":"harbor","slug":"harbor","permalink":"http://yoursite.com/tags/harbor/"},{"name":"性能","slug":"性能","permalink":"http://yoursite.com/tags/%E6%80%A7%E8%83%BD/"},{"name":"helm","slug":"helm","permalink":"http://yoursite.com/tags/helm/"},{"name":"k8s资源对象","slug":"k8s资源对象","permalink":"http://yoursite.com/tags/k8s%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/"},{"name":"c++","slug":"c","permalink":"http://yoursite.com/tags/c/"},{"name":"编译工具","slug":"编译工具","permalink":"http://yoursite.com/tags/%E7%BC%96%E8%AF%91%E5%B7%A5%E5%85%B7/"},{"name":"javaCV","slug":"javaCV","permalink":"http://yoursite.com/tags/javaCV/"},{"name":"ffmpeg","slug":"ffmpeg","permalink":"http://yoursite.com/tags/ffmpeg/"},{"name":"版本","slug":"版本","permalink":"http://yoursite.com/tags/%E7%89%88%E6%9C%AC/"},{"name":"Mac","slug":"Mac","permalink":"http://yoursite.com/tags/Mac/"},{"name":"命令","slug":"命令","permalink":"http://yoursite.com/tags/%E5%91%BD%E4%BB%A4/"},{"name":"pod","slug":"pod","permalink":"http://yoursite.com/tags/pod/"},{"name":"多环境","slug":"多环境","permalink":"http://yoursite.com/tags/%E5%A4%9A%E7%8E%AF%E5%A2%83/"},{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"},{"name":"minio","slug":"minio","permalink":"http://yoursite.com/tags/minio/"},{"name":"grafana","slug":"grafana","permalink":"http://yoursite.com/tags/grafana/"},{"name":"信令","slug":"信令","permalink":"http://yoursite.com/tags/%E4%BF%A1%E4%BB%A4/"},{"name":"视频流","slug":"视频流","permalink":"http://yoursite.com/tags/%E8%A7%86%E9%A2%91%E6%B5%81/"},{"name":"视频网关","slug":"视频网关","permalink":"http://yoursite.com/tags/%E8%A7%86%E9%A2%91%E7%BD%91%E5%85%B3/"},{"name":"osi","slug":"osi","permalink":"http://yoursite.com/tags/osi/"},{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"},{"name":"操作系统","slug":"操作系统","permalink":"http://yoursite.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"http://yoursite.com/tags/Elasticsearch/"},{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"},{"name":"canal","slug":"canal","permalink":"http://yoursite.com/tags/canal/"},{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/tags/k8s/"},{"name":"磁盘管理","slug":"磁盘管理","permalink":"http://yoursite.com/tags/%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86/"},{"name":"nio","slug":"nio","permalink":"http://yoursite.com/tags/nio/"},{"name":"netty","slug":"netty","permalink":"http://yoursite.com/tags/netty/"},{"name":"socket","slug":"socket","permalink":"http://yoursite.com/tags/socket/"},{"name":"IO","slug":"IO","permalink":"http://yoursite.com/tags/IO/"},{"name":"代码片段","slug":"代码片段","permalink":"http://yoursite.com/tags/%E4%BB%A3%E7%A0%81%E7%89%87%E6%AE%B5/"},{"name":"路由","slug":"路由","permalink":"http://yoursite.com/tags/%E8%B7%AF%E7%94%B1/"},{"name":"Reactor","slug":"Reactor","permalink":"http://yoursite.com/tags/Reactor/"},{"name":"greenplum","slug":"greenplum","permalink":"http://yoursite.com/tags/greenplum/"},{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"},{"name":"Type","slug":"Type","permalink":"http://yoursite.com/tags/Type/"},{"name":"json","slug":"json","permalink":"http://yoursite.com/tags/json/"},{"name":"挂载","slug":"挂载","permalink":"http://yoursite.com/tags/%E6%8C%82%E8%BD%BD/"},{"name":"volatile","slug":"volatile","permalink":"http://yoursite.com/tags/volatile/"},{"name":"对象头","slug":"对象头","permalink":"http://yoursite.com/tags/%E5%AF%B9%E8%B1%A1%E5%A4%B4/"},{"name":"锁膨胀","slug":"锁膨胀","permalink":"http://yoursite.com/tags/%E9%94%81%E8%86%A8%E8%83%80/"},{"name":"锁","slug":"锁","permalink":"http://yoursite.com/tags/%E9%94%81/"},{"name":"逃逸分析","slug":"逃逸分析","permalink":"http://yoursite.com/tags/%E9%80%83%E9%80%B8%E5%88%86%E6%9E%90/"},{"name":"即时编译","slug":"即时编译","permalink":"http://yoursite.com/tags/%E5%8D%B3%E6%97%B6%E7%BC%96%E8%AF%91/"},{"name":"架构图","slug":"架构图","permalink":"http://yoursite.com/tags/%E6%9E%B6%E6%9E%84%E5%9B%BE/"},{"name":"限流","slug":"限流","permalink":"http://yoursite.com/tags/%E9%99%90%E6%B5%81/"},{"name":"datax","slug":"datax","permalink":"http://yoursite.com/tags/datax/"},{"name":"springBoot","slug":"springBoot","permalink":"http://yoursite.com/tags/springBoot/"},{"name":"自动装配","slug":"自动装配","permalink":"http://yoursite.com/tags/%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D/"},{"name":"sso","slug":"sso","permalink":"http://yoursite.com/tags/sso/"},{"name":"rbac","slug":"rbac","permalink":"http://yoursite.com/tags/rbac/"},{"name":"CDH","slug":"CDH","permalink":"http://yoursite.com/tags/CDH/"},{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"},{"name":"B-Tree","slug":"B-Tree","permalink":"http://yoursite.com/tags/B-Tree/"},{"name":"spring netty","slug":"spring-netty","permalink":"http://yoursite.com/tags/spring-netty/"},{"name":"flink","slug":"flink","permalink":"http://yoursite.com/tags/flink/"},{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"maven","slug":"maven","permalink":"http://yoursite.com/tags/maven/"},{"name":"多线程/并发","slug":"多线程-并发","permalink":"http://yoursite.com/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E5%B9%B6%E5%8F%91/"},{"name":"Presto","slug":"Presto","permalink":"http://yoursite.com/tags/Presto/"},{"name":"SPI","slug":"SPI","permalink":"http://yoursite.com/tags/SPI/"},{"name":"myBatis","slug":"myBatis","permalink":"http://yoursite.com/tags/myBatis/"},{"name":"brew","slug":"brew","permalink":"http://yoursite.com/tags/brew/"},{"name":"fastjson","slug":"fastjson","permalink":"http://yoursite.com/tags/fastjson/"},{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"},{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"},{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"},{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"},{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"},{"name":"工具","slug":"工具","permalink":"http://yoursite.com/tags/%E5%B7%A5%E5%85%B7/"},{"name":"clickHouse","slug":"clickHouse","permalink":"http://yoursite.com/tags/clickHouse/"},{"name":"vim","slug":"vim","permalink":"http://yoursite.com/tags/vim/"},{"name":"tmux","slug":"tmux","permalink":"http://yoursite.com/tags/tmux/"},{"name":"经济学","slug":"经济学","permalink":"http://yoursite.com/tags/%E7%BB%8F%E6%B5%8E%E5%AD%A6/"},{"name":"基础","slug":"基础","permalink":"http://yoursite.com/tags/%E5%9F%BA%E7%A1%80/"},{"name":"spring","slug":"spring","permalink":"http://yoursite.com/tags/spring/"},{"name":"泛型","slug":"泛型","permalink":"http://yoursite.com/tags/%E6%B3%9B%E5%9E%8B/"},{"name":"日志","slug":"日志","permalink":"http://yoursite.com/tags/%E6%97%A5%E5%BF%97/"},{"name":"lambda","slug":"lambda","permalink":"http://yoursite.com/tags/lambda/"},{"name":"idea","slug":"idea","permalink":"http://yoursite.com/tags/idea/"},{"name":"postgresql","slug":"postgresql","permalink":"http://yoursite.com/tags/postgresql/"},{"name":"aop","slug":"aop","permalink":"http://yoursite.com/tags/aop/"},{"name":"Mac 、linux","slug":"Mac-、linux","permalink":"http://yoursite.com/tags/Mac-%E3%80%81linux/"},{"name":"权限管理","slug":"权限管理","permalink":"http://yoursite.com/tags/%E6%9D%83%E9%99%90%E7%AE%A1%E7%90%86/"},{"name":"方法论","slug":"方法论","permalink":"http://yoursite.com/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"},{"name":"单元测试","slug":"单元测试","permalink":"http://yoursite.com/tags/%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/"},{"name":"python 多环境","slug":"python-多环境","permalink":"http://yoursite.com/tags/python-%E5%A4%9A%E7%8E%AF%E5%A2%83/"}]}